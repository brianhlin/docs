{
    "docs": [
        {
            "location": "/", 
            "text": "OSG Site Administrator Documentation\n\n\nWelcome to the home page of the Open Science Grid (OSG) Site Administrator documentation! If you are not a site adminstrator...\n\n\n\n\nIf you are a researcher interested in using OSG resources, you can find user documentation \nhere\n. \n\n\nIf you'd like to learn more about the OSG and our mission, visit our website \nhere\n.\n\n\n\n\nThis document outlines the overall installation process for an OSG site and provides many links into detailed installation, configuration, troubleshooting, and similar pages. If you do not see software-related technical documentation listed here, try the search bar to the left or contacting us at \ngoc@opensciencegrid.org\n.\n\n\nPlan the Site\n\n\nIf you have not done so already, \nplan the overall architecture of your OSG site\n. It is recommended that your plan be sufficiently detailed to include the OSG hosts that are needed and the main software components for each host. Be sure to consider \nthe operating systems that OSG supports\n. For example, a basic site might include:\n\n\n\n\n\n\n\n\nPurpose\n\n\nHost\n\n\nMajor Software\n\n\n\n\n\n\n\n\n\n\nCompute Element (CE)\n\n\nosg-ce.example.edu\n\n\nOSG CE, HTCondor Central Manager, etc. (\nosg-ce-condor\n)\n\n\n\n\n\n\nWorker Nodes\n\n\nwNNN.cluster.example.edu\n\n\nOSG worker node client (\nosg-wn-client\n)\n\n\n\n\n\n\n\n\nPrepare the Batch System\n\n\nThe assumption is that you have an existing batch system at your site. Currently, we support \nHTCondor\n, \nLSF\n, \nPBS\n and \nTORQUE\n, \nSGE\n, and \nSlurm\n batch systems.\n\n\nFor smaller sites (less than 50 worker nodes), the most common way to add a site to OSG is to install the OSG Compute Element (CE) on the central host of your batch system.  At such a site - especially if you have minimal time to maintain a CE - you may want to contact \n to ask about using an OSG-hosted CE instead of running your own.  Before proceeding with an install, be sure that you can submit and successfully run a job from your OSG CE host into your batch system.\n\n\nAdd OSG Software\n\n\nIf necessary, provision all OSG hosts that are in your site plan and that do not exist yet.\n\n\n\n\nNote\n\n\nFor sites with more than a handful of worker nodes, it is recommended to use some sort of configuration management tool to install, configure, and maintain your site. While beyond the scope of OSG\u2019s documentation to explain how to select and use such a system, some popular configuration management tools are \nPuppet\n, \nChef\n, \nAnsible\n, and \nCFEngine\n.\n\n\n\n\nGeneral Installation Instructions\n\n\n\n\nSecurity information for OSG signed RPMs\n\n\nUsing Yum and RPM\n\n\nInstall the OSG repositories\n\n\nOSG Software release series\n - look here to upgrade from OSG 3.1 to OSG 3.2 or from OSG 3.2 to OSG 3.3\n\n\nInstallation best practices\n\n\n\n\nInstalling and Managing Certificates for Site Security\n\n\n\n\nInstalling the grid certificate authorities (CAs)\n\n\nHow do I get PKI host and service X.509 certificates?\n\n\nAutomatically updating the grid certificate authorities (CAs)\n\n\nSHA-2 certificates and minimum required OSG software versions\n\n\nOSG PKI command line client reference\n\n\n\n\nAdding OSG Software to Worker Nodes\n\n\n\n\nWorker Node (WN) Client Overview\n\n\nInstall the WN client software on every worker node \u2013 pick a method:\n\n\nUsing RPMs\n \u2013 useful when managing your worker nodes with a tool (e.g., Puppet, Chef)\n\n\nUsing a tarball\n \u2013 useful for installation onto a shared filesystem (does not require root access)\n\n\nUsing OASIS\n \u2013 useful when \nCVMFS\n is already mounted on your worker nodes\n\n\n\n\n\n\n(optional) \nInstall the CernVM-FS client\n to make it easy for user jobs to use needed software from OSG's OASIS repositories\n\n\n(optional) \nInstall singularity on the OSG worker node\n, to allow pilot jobs to isolate user jobs.\n\n\n\n\nInstalling and Configuring the Compute Element\n\n\n\n\nPreparing to install the compute element\n\n\nInstall the compute element (HTCondor-CE and other software):\n\n\nOverview and architecture\n\n\nInstall HTCondor-CE\n\n\nConfigure the HTCondor-CE job router\n, including common recipes\n\n\nTroubleshooting HTCondor-CE installations\n\n\nSubmitting jobs to HTCondor-CE\n\n\n\n\n\n\nTroubleshooting osg-configure\n\n\n\n\nInstalling and Configuring Other Nodes\n\n\nAll of these node types and their services are optional, although OSG requires the Frontier Squid caching service if you have installed \nCVMFS\n on your worker nodes.\n\n\n\n\nInstall Frontier Squid, the HTTP caching proxy service\n\n\nRSV monitoring to monitor and report to OSG on the health of your site\n\n\nInstall RSV\n\n\nTroubleshooting RSV\n\n\n\n\n\n\nInstall the GlideinWMS VO Frontend\n if your want your users\u2019 jobs to run on the OSG\n\n\nInstall the RSV GlideinWMS Tester\n if you want to test your front-end's ability to submit jobs to sites in the OSG\n\n\n\n\n\n\nStorage element (pick one):\n\n\nGridFTP\n\n\nInstall standalone OSG GridFTP\n: GridFTP server\n\n\n(optional) \nInstall load-balanced OSG GridFTP\n: when a single GridFTP server isn't enough\n\n\n\n\n\n\nBeStMan\n\n\nInstall Bestman SE\n: BeStMan2 SRM server + GridFTP server\n\n\nInstall Bestman Gateway Hadoop\n: BeStMan2 SRM server + GridFTP server + Hadoop\n\n\n\n\n\n\nHadoop Distributed File System (HDFS)\n\n\nHadoop Overview\n: HDFS information, planning, and guides\n\n\n\n\n\n\nXRootD\n\n\nXRootd Overview\n: XRootD information, planning, and guides\n\n\nInstall Xrootd Server\n: XRootD redirector installation\n\n\nInstall BeStMan-Gateway XRootD\n: BeStMan2 SRM server + GridFTP server + XRootD fuse\n\n\n\n\n\n\n\n\n\n\n\n\nTest OSG Software\n\n\nAt very least, it is vital to test \nmanual\n submission of jobs from inside and outside of your site through your CE to your batch system. If this process does not work manually, it will probably not work for the glideinWMS pilot factory either.\n\n\n\n\nTest job submission into an HTCondor-CE\n\n\n\n\nStart GlideinWMS Pilot Submissions\n\n\nTo begin running \nGlideinWMS\n pilot jobs at your site, e-mail \n and tell them that you want to start accepting Glideins. Please provide them with the following information:\n\n\n\n\nThe type of CE (HTCondor-CE or the now-unsupported GRAM-CE)\n\n\nThe fully qualified hostname of the CE\n\n\nResource/WLCG name\n\n\nOS major version of your worker nodes\u00a0\u2014 EL\u00a06, EL\u00a07, or a mix of both?\n\n\nDo you accept multicore jobs?\n\n\nMaximum job walltime\n\n\nMaximum job memory usage\n\n\n\n\nOnce the factory team has enough information, they will start submitting pilots from the test factory to your CE. Initially, this will be one pilot at a time but once the factory verifies that pilot jobs are running successfully, that number will be ramped up to 10, then 100.\n\n\nVerify Reporting and Monitoring\n\n\nTo verify that your site is correctly reporting to the OSG, check \nOSG's Accounting Portal\n for records of your site reports (select your site from the drop-down box). If you have enabled the OSG VO, you can also check \nhttp://osg-flock.grid.iu.edu/monitoring/condor/sites/all_1day.html\n.\n\n\nScale Up Site to Full Production\n\n\nAfter successfully running all the pilot jobs that are submitted by the test factory and verifying your site reports, your site will be deemed production ready. No action is required on your end, factory operations will start submitting pilot jobs from the production factory.", 
            "title": "Home"
        }, 
        {
            "location": "/#osg-site-administrator-documentation", 
            "text": "Welcome to the home page of the Open Science Grid (OSG) Site Administrator documentation! If you are not a site adminstrator...   If you are a researcher interested in using OSG resources, you can find user documentation  here .   If you'd like to learn more about the OSG and our mission, visit our website  here .   This document outlines the overall installation process for an OSG site and provides many links into detailed installation, configuration, troubleshooting, and similar pages. If you do not see software-related technical documentation listed here, try the search bar to the left or contacting us at  goc@opensciencegrid.org .", 
            "title": "OSG Site Administrator Documentation"
        }, 
        {
            "location": "/#plan-the-site", 
            "text": "If you have not done so already,  plan the overall architecture of your OSG site . It is recommended that your plan be sufficiently detailed to include the OSG hosts that are needed and the main software components for each host. Be sure to consider  the operating systems that OSG supports . For example, a basic site might include:     Purpose  Host  Major Software      Compute Element (CE)  osg-ce.example.edu  OSG CE, HTCondor Central Manager, etc. ( osg-ce-condor )    Worker Nodes  wNNN.cluster.example.edu  OSG worker node client ( osg-wn-client )", 
            "title": "Plan the Site"
        }, 
        {
            "location": "/#prepare-the-batch-system", 
            "text": "The assumption is that you have an existing batch system at your site. Currently, we support  HTCondor ,  LSF ,  PBS  and  TORQUE ,  SGE , and  Slurm  batch systems.  For smaller sites (less than 50 worker nodes), the most common way to add a site to OSG is to install the OSG Compute Element (CE) on the central host of your batch system.  At such a site - especially if you have minimal time to maintain a CE - you may want to contact   to ask about using an OSG-hosted CE instead of running your own.  Before proceeding with an install, be sure that you can submit and successfully run a job from your OSG CE host into your batch system.", 
            "title": "Prepare the Batch System"
        }, 
        {
            "location": "/#add-osg-software", 
            "text": "If necessary, provision all OSG hosts that are in your site plan and that do not exist yet.   Note  For sites with more than a handful of worker nodes, it is recommended to use some sort of configuration management tool to install, configure, and maintain your site. While beyond the scope of OSG\u2019s documentation to explain how to select and use such a system, some popular configuration management tools are  Puppet ,  Chef ,  Ansible , and  CFEngine .", 
            "title": "Add OSG Software"
        }, 
        {
            "location": "/#general-installation-instructions", 
            "text": "Security information for OSG signed RPMs  Using Yum and RPM  Install the OSG repositories  OSG Software release series  - look here to upgrade from OSG 3.1 to OSG 3.2 or from OSG 3.2 to OSG 3.3  Installation best practices", 
            "title": "General Installation Instructions"
        }, 
        {
            "location": "/#installing-and-managing-certificates-for-site-security", 
            "text": "Installing the grid certificate authorities (CAs)  How do I get PKI host and service X.509 certificates?  Automatically updating the grid certificate authorities (CAs)  SHA-2 certificates and minimum required OSG software versions  OSG PKI command line client reference", 
            "title": "Installing and Managing Certificates for Site Security"
        }, 
        {
            "location": "/#adding-osg-software-to-worker-nodes", 
            "text": "Worker Node (WN) Client Overview  Install the WN client software on every worker node \u2013 pick a method:  Using RPMs  \u2013 useful when managing your worker nodes with a tool (e.g., Puppet, Chef)  Using a tarball  \u2013 useful for installation onto a shared filesystem (does not require root access)  Using OASIS  \u2013 useful when  CVMFS  is already mounted on your worker nodes    (optional)  Install the CernVM-FS client  to make it easy for user jobs to use needed software from OSG's OASIS repositories  (optional)  Install singularity on the OSG worker node , to allow pilot jobs to isolate user jobs.", 
            "title": "Adding OSG Software to Worker Nodes"
        }, 
        {
            "location": "/#installing-and-configuring-the-compute-element", 
            "text": "Preparing to install the compute element  Install the compute element (HTCondor-CE and other software):  Overview and architecture  Install HTCondor-CE  Configure the HTCondor-CE job router , including common recipes  Troubleshooting HTCondor-CE installations  Submitting jobs to HTCondor-CE    Troubleshooting osg-configure", 
            "title": "Installing and Configuring the Compute Element"
        }, 
        {
            "location": "/#installing-and-configuring-other-nodes", 
            "text": "All of these node types and their services are optional, although OSG requires the Frontier Squid caching service if you have installed  CVMFS  on your worker nodes.   Install Frontier Squid, the HTTP caching proxy service  RSV monitoring to monitor and report to OSG on the health of your site  Install RSV  Troubleshooting RSV    Install the GlideinWMS VO Frontend  if your want your users\u2019 jobs to run on the OSG  Install the RSV GlideinWMS Tester  if you want to test your front-end's ability to submit jobs to sites in the OSG    Storage element (pick one):  GridFTP  Install standalone OSG GridFTP : GridFTP server  (optional)  Install load-balanced OSG GridFTP : when a single GridFTP server isn't enough    BeStMan  Install Bestman SE : BeStMan2 SRM server + GridFTP server  Install Bestman Gateway Hadoop : BeStMan2 SRM server + GridFTP server + Hadoop    Hadoop Distributed File System (HDFS)  Hadoop Overview : HDFS information, planning, and guides    XRootD  XRootd Overview : XRootD information, planning, and guides  Install Xrootd Server : XRootD redirector installation  Install BeStMan-Gateway XRootD : BeStMan2 SRM server + GridFTP server + XRootD fuse", 
            "title": "Installing and Configuring Other Nodes"
        }, 
        {
            "location": "/#test-osg-software", 
            "text": "At very least, it is vital to test  manual  submission of jobs from inside and outside of your site through your CE to your batch system. If this process does not work manually, it will probably not work for the glideinWMS pilot factory either.   Test job submission into an HTCondor-CE", 
            "title": "Test OSG Software"
        }, 
        {
            "location": "/#start-glideinwms-pilot-submissions", 
            "text": "To begin running  GlideinWMS  pilot jobs at your site, e-mail   and tell them that you want to start accepting Glideins. Please provide them with the following information:   The type of CE (HTCondor-CE or the now-unsupported GRAM-CE)  The fully qualified hostname of the CE  Resource/WLCG name  OS major version of your worker nodes\u00a0\u2014 EL\u00a06, EL\u00a07, or a mix of both?  Do you accept multicore jobs?  Maximum job walltime  Maximum job memory usage   Once the factory team has enough information, they will start submitting pilots from the test factory to your CE. Initially, this will be one pilot at a time but once the factory verifies that pilot jobs are running successfully, that number will be ramped up to 10, then 100.", 
            "title": "Start GlideinWMS Pilot Submissions"
        }, 
        {
            "location": "/#verify-reporting-and-monitoring", 
            "text": "To verify that your site is correctly reporting to the OSG, check  OSG's Accounting Portal  for records of your site reports (select your site from the drop-down box). If you have enabled the OSG VO, you can also check  http://osg-flock.grid.iu.edu/monitoring/condor/sites/all_1day.html .", 
            "title": "Verify Reporting and Monitoring"
        }, 
        {
            "location": "/#scale-up-site-to-full-production", 
            "text": "After successfully running all the pilot jobs that are submitted by the test factory and verifying your site reports, your site will be deemed production ready. No action is required on your end, factory operations will start submitting pilot jobs from the production factory.", 
            "title": "Scale Up Site to Full Production"
        }, 
        {
            "location": "/site-planning/", 
            "text": "Site Planning\n\n\n\n\nWarning\n\n\nThis documentation is outdated and some of the listed technologies are no longer in use:\n\n\n\n\nGRAM and the Globus Gatekeeper have been removed in favor of HTCondor-CE\n\n\nManaged Fork has been removed along with GRAM\n\n\nGUMS and EDG-Mkgridmap are deprecated in favor of \nLCMAPS-VOMS authentication\n\n\nBeStMan is deprecated in favor of HDFS and load-balanced GridFTP\n\n\nNFSLite has been removed\n\n\n\n\nPlease be patient while we update the document to reflect current procedures and technologies.\n\n\n\n\nAbout this Document\n\n\nThis document is for \nSystem Administrators\n. The purpose of the document is to provide an overview about the different ways to setup an OSG site and to encourage you to plan your site before you continue to install the OSG software on your site.\n\n\nAfter reading this document you should be able to identify the site elements needed to setup your OSG site and choose among different technology choices presented.\n\n\nBackground\n\n\nThe goal for the Open Science Grid software stack is to provide a uniform computing and storage interface across many independently managed computing and storage clusters. Scientists, researchers, and students, organized as virtual organizations (VOs), are the consumers of the CPU cycles and storage.\n\n\nYour site is encouraged to support as many OSG-registered VOs as possible, but you are not required to support all of them.\n\n\nAs the administrator responsible for deployment of the OSG software stack, your task is to make your existing computing and storage cluster available to and reliable for the VOs that you support. The OSG expects you to set up a gatekeeper node called a Compute Element (CE) on which the bulk of the OSG software gets installed. The end-user sends jobs into your cluster's batch system, your CE receives them and passes them out to Worker Nodes (WN) for execution. Some VOs and end-users require non-negligible amounts of data as input, or generate non-negligible amounts of data as output. They will need to store that data in a Storage Element (SE). A site is not required to provide both a CE and an SE.\n\n\nSite Policies\n\n\nOSG expects you to clearly specify your site's policies regarding resource access. Please write them on a web page, make this page part of your site registration, and make it available via the GOC publishing tool \nMyOSG\n and the OSG information management system, OIM. We encourage you to allow all virtual organizations registered with the OSG at least \"opportunistic use\" of your resources. You may need to preempt those jobs when higher priority jobs come around. The end-users using the OSG generally prefer having access to your site subject to preemption over having no access at all.\n\n\nOSG Site Elements\n\n\nThe OSG provides software and documentation to install and operate following services:\n\n\n\n\n\n\n\n\nElement\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nAuthorization Service\n\n\nenables grid users to authorize with your site using their grid or voms proxies\n\n\n\n\n\n\nCompute Element\n\n\nenables grid users to run jobs on your site\n\n\n\n\n\n\nWorker Node Client\n\n\nenables grid jobs running on worker nodes to access grid tools\n\n\n\n\n\n\nStorage Element\n\n\nenables grid users to store large amounts of data at your site\n\n\n\n\n\n\nVO Management Service\n\n\nprovides functionality for VO Managers to manage the membership information of their users\n\n\n\n\n\n\n\n\nAuthorization Service\n\n\nGrid users will authorization with your site using their \ngrid or voms proxy\n. The OSG provides two different services that let you control the authorization process:\n\n\n\n\n\n\n\n\nService\n\n\nDescription\n\n\nAdvantages\n\n\nDisadvantages\n\n\n\n\n\n\n\n\n\n\nedg-mkgridmap\n\n\na simple program that contacts VOMS servers and creates a gridmap file\n\n\neasy to install and maintain\n\n\ndoes not support voms proxies\n\n\n\n\n\n\nGUMS\n\n\na web service providing sophisticated controls of how users authorization\n\n\nsupports voms proxies\n\n\nrequires Tomcat to be run as a web service\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nA \nVOMS Server\n is not an element of your site. Each \nVirtual Organization\n operates a central VOMS Server to manage membership information of its grid users. Please contact the \nVO Manager\n for your virtual organization to obtain more details.\n\n\n\n\nCompute Element\n\n\nA \nCompute Element\n allows grid users to run jobs on your site. It is software that provides following services when run on your \ngatekeeper\n: The standard installation is based on HTCondor-CE with RSV for monitoring.\n\n\nYou must determine your security policy with regard to Unix ID management on the cluster. You may choose group accounts and/or dynamic accounts for all users.\n\n\nYou must choose the OS (Red Hat Enterprise Linux derivative), the batch system (Condor, PBS, LSF, SGE, and Slurm are presently supported), and the network architecture of your cluster. The default network assumption is public/private with NAT so you will need to advertise your architecture by changing some settings by hand if yours isn't like this. In addition, there are some configuration choices, including one that avoids all NFS exports from the CE to the compute cluster (NFS-lite).\n\n\nThe CE hosts information provider(s) and monitoring services, most of which are configured correctly by default. We require all OSG sites to deploy Gratia, the OSG accounting system. Your site thus sends accounting records to OSG about jobs run on your site and data transfers involving your site. Aggregated summaries of this information can be viewed via the \nGRACC\n.\n\n\n\n\n\n\n\n\nService\n\n\nDescription\n\n\nComments\n\n\n\n\n\n\n\n\n\n\nHTCondor-CE\n\n\nHTCondor-CE, based on the HTCondor batch system, provide a public entry point to your local batch system.\n\nIt also allows grid users to \nfork\n jobs on your gatekeeper by default.\n\n\nRequired\n\n\n\n\n\n\nRSV\n\n\nR\nesource \nS\nervice \nV\nalidation system which schedules execution of local \nprobes\n of your CE (and SE), and reports the results up to the GOC. This is important for service availablity monitoring of OSG sites.\n\n\nRequired\n\n\n\n\n\n\nSquid\n\n\nSquid is a caching proxy for the Web that enables restricted access of worker nodes to the web.\n\n\nOptional\n\n\n\n\n\n\nSyslog-ng\n\n\nA logging service that can be used to forward CE logfiles to the GOC for troubleshooting purposes.\n\n\nOptional\n\n\n\n\n\n\n\n\n\nAdditionally two shared file systems are for grid users to install applications \nOSG_APP\n (required) and to save data \nOSG_DATA\n (optional). If available, both must be mounted on the gatekeeper and all worker nodes:\n\n\n\n\n\n\n\n\nShared Filesystem\n\n\nDescription\n\n\nRecommended Size\n\n\nTypical Size[TB]\n\n\nComments\n\n\n\n\n\n\n\n\n\n\nHOME\n\n\nspace for grid user home directories\n\n\n10GB for each VO\n\n\n0.1 to 1\n\n\nrequired, auto-cleanup\n\n\n\n\n\n\nOSG_APP\n\n\nspace for grid users to install applications\n\n\n10GB for each VO\n\n\n0.1 to 1\n\n\nrequired, no auto-cleanup\n\n\n\n\n\n\nOSG_DATA\n\n\nspace for grid users to stage data\n\n\n10GB for each VO and worker node\n\n\n0.1 to 10\n\n\noptional, no quota, auto-cleanup\n\n\n\n\n\n\nOSG_WN_TMP\n\n\ntmp space for users on worker nodes\n\n\n2GB for each cpu core\n\n\n0.1\n\n\nrequired, auto-cleanup\n\n\n\n\n\n\nOSG_GRID\n\n\nlocation of the worker node client\n\n\n10GB\n\n\n0.1\n\n\noptional/required (see \nNOTE\n)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nIf you install wn-client on each node via RPM all the client software is available in the default path. There is no need for OSG_GRID. The RPM installation creates \n/etc/osg/wn-client/\n with dummy setup files for compatibility with old jobs looking for a OSG_GRID. New jobs should source the setup file in OSG_GRID if this is defined; if not, they should expect all the client binaries in the default PATH.\n\n\n\n\nWorker Node Client\n\n\nThe \nWorker Node Client\n is software installed on each worker node to give programs running on the worker nodes access to grid utilities. While it is technically optional, it is \nstrongly recommended\n that you install it on the gatekeeper and all worker nodes. This can be done as a local installation, in which the software is installed individually on every worker node, or as a shared installation in which the software is installed on one machine that shares it via a global file system to all the worker nodes. All the site configurations below are showing a local installation of the worker node client.\n\n\nStorage Element\n\n\nA \nStorage Element\n provides grid users the possibility to read and write large amounts of data on your site using the \nS\ntorage \nR\nesource \nM\nanager (\nSRM\n). All Storage Element implementations in the OSG support the gsiftp protocol and full or partial SRM specification. The selection of storage suitable for your site varies on anticipated usage patterns, available hardware, the choice of underlying distributed storage, support for a tape-archival backend etc.\n\n\nThere are two types of storage element services provided by OSG which implement SRM v2. See pointers to instructions for these services:\n\n\n\n\nBeStMan\n - Sits in front of any POSIX filesystem. There is also a version which supports xrootd filesystems.\n\n\nHadoop\n - Map-reduce based solution to aggregate off-the-shelf disks into a scalable reliable system.\n\n\n\n\nAn SE must run correctly configured Grid Information Providers, Gratia accounting and RSV probes.\n\n\nThese services are supported by the OSG Storage group. Please email \n for installation and support questions for these services.\n\n\n\n\n\n\n\n\nStorage Requirements\n\n\nMin Hardware Requirements\n\n\nOSG SE Solution\n\n\n\n\n\n\n\n\n\n\nSRM interface, Dynamic Space Management Support\n\n\nServer with local disk\n\n\nBeStMan-fullmode\n\n\n\n\n\n\nSRM interface, No or Static Space Management Support\n\n\nServer with local disk or NFS\n\n\nBeStMan-gateway\n\n\n\n\n\n\nSRM interface, No or Static Space Management Support, jobs need root protocol to access data\n\n\nMultiple servers(\n3)\n\n\nBeStMan-gateway/Xrood\n\n\n\n\n\n\nSRM interface, No or Static Space Management Support, file replication\n\n\nMultiple servers(\n4)\n\n\nBeStMan-gateway/HDFS\n\n\n\n\n\n\nSRM interface, Dynamic Space Management Support, file replication, interface to tape backend\n\n\nMultiple servers (\n5)\n\n\ndcache\n\n\n\n\n\n\n\n\nVO Management Service\n\n\nA \nV\nirtual \nO\nganization \nM\nanagement \nS\nervice (VOMS) controls who is a member of your VO. Each VO needs to provide one VOMS. Please contact the VO Manager of VO to find out about the VOMS of your VO.\n\n\nRecommendations\n\n\nIn this section we outline \nimportant\n recommendations for setting up an OSG site. The recommendations are based on the knowledge of experienced system administrators and will help you avoid typical problems operating an OSG site from the beginning.\n\n\nShared File Systems\n\n\nWe recommend to use a dedicated server for hosting the shared file system. The expected load on the file server could be distributed further by providing a dedicated file server for \nHOME\n and \nOSG_DATA\n if possible. You should also consider to:\n\n\n\n\nuse a consistent mounting scheme for shared partitions when mounted on the gatekeeper with respect to all worker nodes\n\n\nuse a reasonable automatic cleanup procedure for \nHOME\n, \nOSG_DATA\n and \nOSG_WN_TMP\n\n\nnot use an automatic cleanup procedures for \nOSG_APP\n\n\nnot use quota for \nOSG_DATA\n\n\n\n\nNFS Warning\n\n\nNFS is known to be an \neasy\n but less adequate for large sites. If you want to use NFS you should read the chapter \n\"Optimizing NFS Performance\"\n in the \nhttp://nfs.sourceforge.net/nfs-howto/\n. Software partitions that can be locally installed, such as OSG_GRID, should be locally installed and not shared unless you have an enterprise-class NFS server.\n\n\nCompute and Storage Element\n\n\n\n\nprovide dedicated hardware for the Compute and the Storage Element\n\n\nuse as many cpu cores and main memory as possible\n\n\navoid running other grid services such as GUMS on the Compute and the Storage Element\n\n\navoid running a file server on the Compute and the Storage Element\n\n\n\n\nUse Job Manager managedfork instead of fork\n\n\nGRAM\n provides several jobmanagers that control how jobs will be executed on the gatekeeper and the worker nodes. By \ndefault jobmanager fork\n will be used to allow grid users to run maintenance jobs on the gatekeeper. Unfortunately fork doesn't provide a way to restrict the number of jobs that may be run simultaneously.\n\n\nBy choosing \nmanagedfork instead of fork\n during the setup process, you will be able to restrict the number of simultaneous running jobs on the gatekeeper. We strongly recommend to choose managedfork as the default jobmanager when configuring the Compute Element.\n\n\nRole based Authentication using VOMS Proxies\n\n\nVOMS proxies\n allow role based authentication. If your site will support VOMS proxies for authentication you will be able to restrict write access to shared file systems for grid users while providing write permissions to VO Administrators only. In this case we recommend to export \nOSG_APP\n read-only to the gatekeeper and all worker nodes for grid users and read-write for VO Managers only.\n\n\nExample Configurations\n\n\nThis section contains a few example that illustrate how the different elements contributing to an OSG site can be combined. Each \ngray\n box represents a physical resource or virtual machine that is required in the example.\n\n\nOSG Resource with Compute Element\n\n\n\n\nThe image above shows all elements of a compute element. \nedg-mkgridmap\n is used to create the \ngrid-mapfile\n and to keep it current. The \nshared filesystem\n is mounted on the compute element providing access to \nOSG_APP\n, \nOSG_DATA\n and \nHOME\n. \nGRAM\n and \nGridFTP\n use the \ngrid-mapfile\n to authenticate requests to run jobs and to transfer data.\n\n\n\n\nThe image above shows a \nWorker Node\n that is connected to the \nCompute Element\n above by a network.  The \nWorker Node Client\n has to be installed on each worker node.  The \nshared filesystem\n is mounted on the worker node providing access to \nOSG_APP\n, \nOSG_DATA\n and \nHOME\n.  \nNode Local Storage\n is used for \nOSG_WN_TMP\n and \nOSG_GRID\n. \nOSG_GRID\n points to the installation of the \nWorker Node Client\n.\n\n\nOSG Resource with joined Compute and Storage Element\n\n\n\n\nThe left side shows elements of a joined Compute and Storage Element.  The shared file system is mounted on the compute and storage element. The \nBeStMan\n storage solution provides the \nSRM Service\n allowing grid users to remotely access the shared file system.\n\n\n\n\nThe image above shows a \nWorker Node\n that is connected to the \nCompute Element\n above by a network. The \nWorker Node Client\n has to be installed on each worker node. The \nshared filesystem\n is mounted on the worker node providing access to \nOSG_APP\n, \nOSG_DATA\n and \nHOME\n. \nNode Local Storage\n is used for \nOSG_WN_TMP\n and \nOSG_GRID\n. \nOSG_GRID\n points to the installation of the \nWorker Node Client\n.\n\n\nOSG Resource with separate Compute and Storage Element\n\n\n\n\nThe image above shows the elements of an authorization host. \nGUMS\n provides a central web service for grid authorization requests from storage and compute elements.  It is not required but recommended to run GUMS outside of the compute and and storage element which are shown below.\n\n\n\n\nThe image above shows all elements of a compute element. \nGRAM\n and \nGridFTP\n query \nGUMS\n to authenticate requests from grid users to run jobs or transfer data. The \nshared filesystem\n is mounted on the compute element providing access to \nOSG_APP\n, \nOSG_DATA\n and \nHOME\n.\n\n\n\n\nThe image above shows all elements of a storage element. \nSRM\n and \nGridFTP\n query \nGUMS\n to authenticate requests from grid users to transfer data. The \nshared filesystem\n is mounted on the compute element providing access to \nOSG_APP\n, \nOSG_DATA\n and \nHOME\n. The \nBeStMan\n storage solution provides the \nSRM Service\n allowing grid users to remotely access the shared file system.\n\n\n\n\nThe image above shows a \nWorker Node\n that is connected to the \nCompute Element\n above by a network. The \nWorker Node Client\n has to be installed on each worker node. The \nshared filesystem\n is mounted on the worker node providing access to \nOSG_APP\n, \nOSG_DATA\n and \nHOME\n. \nNode Local Storage\n is used for \nOSG_WN_TMP\n and \nOSG_GRID\n. \nOSG_GRID\n points to the installation of the \nWorker Node Client\n.\n\n\nOSG Resource with Compute Element and dCache Storage Element\n\n\n\n\nOSG Compute and dCache Storage Element using GUMS\n\n\nThe left side shows all elements of a compute and a \ndCache\n storage element. On the right side a set of worker node are drawn which are distinct from \ndCache Pool Nodes\n. The worker nodes and dCache pool nodes are connected to the compute and storage element by a network. \nGRAM\n and \nGridFTP\n use \nGUMS\n to authenticate requests to run jobs and to transfer data.", 
            "title": "Site Planning"
        }, 
        {
            "location": "/site-planning/#site-planning", 
            "text": "Warning  This documentation is outdated and some of the listed technologies are no longer in use:   GRAM and the Globus Gatekeeper have been removed in favor of HTCondor-CE  Managed Fork has been removed along with GRAM  GUMS and EDG-Mkgridmap are deprecated in favor of  LCMAPS-VOMS authentication  BeStMan is deprecated in favor of HDFS and load-balanced GridFTP  NFSLite has been removed   Please be patient while we update the document to reflect current procedures and technologies.", 
            "title": "Site Planning"
        }, 
        {
            "location": "/site-planning/#about-this-document", 
            "text": "This document is for  System Administrators . The purpose of the document is to provide an overview about the different ways to setup an OSG site and to encourage you to plan your site before you continue to install the OSG software on your site.  After reading this document you should be able to identify the site elements needed to setup your OSG site and choose among different technology choices presented.", 
            "title": "About this Document"
        }, 
        {
            "location": "/site-planning/#background", 
            "text": "The goal for the Open Science Grid software stack is to provide a uniform computing and storage interface across many independently managed computing and storage clusters. Scientists, researchers, and students, organized as virtual organizations (VOs), are the consumers of the CPU cycles and storage.  Your site is encouraged to support as many OSG-registered VOs as possible, but you are not required to support all of them.  As the administrator responsible for deployment of the OSG software stack, your task is to make your existing computing and storage cluster available to and reliable for the VOs that you support. The OSG expects you to set up a gatekeeper node called a Compute Element (CE) on which the bulk of the OSG software gets installed. The end-user sends jobs into your cluster's batch system, your CE receives them and passes them out to Worker Nodes (WN) for execution. Some VOs and end-users require non-negligible amounts of data as input, or generate non-negligible amounts of data as output. They will need to store that data in a Storage Element (SE). A site is not required to provide both a CE and an SE.", 
            "title": "Background"
        }, 
        {
            "location": "/site-planning/#site-policies", 
            "text": "OSG expects you to clearly specify your site's policies regarding resource access. Please write them on a web page, make this page part of your site registration, and make it available via the GOC publishing tool  MyOSG  and the OSG information management system, OIM. We encourage you to allow all virtual organizations registered with the OSG at least \"opportunistic use\" of your resources. You may need to preempt those jobs when higher priority jobs come around. The end-users using the OSG generally prefer having access to your site subject to preemption over having no access at all.", 
            "title": "Site Policies"
        }, 
        {
            "location": "/site-planning/#osg-site-elements", 
            "text": "The OSG provides software and documentation to install and operate following services:     Element  Description      Authorization Service  enables grid users to authorize with your site using their grid or voms proxies    Compute Element  enables grid users to run jobs on your site    Worker Node Client  enables grid jobs running on worker nodes to access grid tools    Storage Element  enables grid users to store large amounts of data at your site    VO Management Service  provides functionality for VO Managers to manage the membership information of their users", 
            "title": "OSG Site Elements"
        }, 
        {
            "location": "/site-planning/#authorization-service", 
            "text": "Grid users will authorization with your site using their  grid or voms proxy . The OSG provides two different services that let you control the authorization process:     Service  Description  Advantages  Disadvantages      edg-mkgridmap  a simple program that contacts VOMS servers and creates a gridmap file  easy to install and maintain  does not support voms proxies    GUMS  a web service providing sophisticated controls of how users authorization  supports voms proxies  requires Tomcat to be run as a web service      Warning  A  VOMS Server  is not an element of your site. Each  Virtual Organization  operates a central VOMS Server to manage membership information of its grid users. Please contact the  VO Manager  for your virtual organization to obtain more details.", 
            "title": "Authorization Service"
        }, 
        {
            "location": "/site-planning/#compute-element", 
            "text": "A  Compute Element  allows grid users to run jobs on your site. It is software that provides following services when run on your  gatekeeper : The standard installation is based on HTCondor-CE with RSV for monitoring.  You must determine your security policy with regard to Unix ID management on the cluster. You may choose group accounts and/or dynamic accounts for all users.  You must choose the OS (Red Hat Enterprise Linux derivative), the batch system (Condor, PBS, LSF, SGE, and Slurm are presently supported), and the network architecture of your cluster. The default network assumption is public/private with NAT so you will need to advertise your architecture by changing some settings by hand if yours isn't like this. In addition, there are some configuration choices, including one that avoids all NFS exports from the CE to the compute cluster (NFS-lite).  The CE hosts information provider(s) and monitoring services, most of which are configured correctly by default. We require all OSG sites to deploy Gratia, the OSG accounting system. Your site thus sends accounting records to OSG about jobs run on your site and data transfers involving your site. Aggregated summaries of this information can be viewed via the  GRACC .     Service  Description  Comments      HTCondor-CE  HTCondor-CE, based on the HTCondor batch system, provide a public entry point to your local batch system. \nIt also allows grid users to  fork  jobs on your gatekeeper by default.  Required    RSV  R esource  S ervice  V alidation system which schedules execution of local  probes  of your CE (and SE), and reports the results up to the GOC. This is important for service availablity monitoring of OSG sites.  Required    Squid  Squid is a caching proxy for the Web that enables restricted access of worker nodes to the web.  Optional    Syslog-ng  A logging service that can be used to forward CE logfiles to the GOC for troubleshooting purposes.  Optional     Additionally two shared file systems are for grid users to install applications  OSG_APP  (required) and to save data  OSG_DATA  (optional). If available, both must be mounted on the gatekeeper and all worker nodes:     Shared Filesystem  Description  Recommended Size  Typical Size[TB]  Comments      HOME  space for grid user home directories  10GB for each VO  0.1 to 1  required, auto-cleanup    OSG_APP  space for grid users to install applications  10GB for each VO  0.1 to 1  required, no auto-cleanup    OSG_DATA  space for grid users to stage data  10GB for each VO and worker node  0.1 to 10  optional, no quota, auto-cleanup    OSG_WN_TMP  tmp space for users on worker nodes  2GB for each cpu core  0.1  required, auto-cleanup    OSG_GRID  location of the worker node client  10GB  0.1  optional/required (see  NOTE )      Note  If you install wn-client on each node via RPM all the client software is available in the default path. There is no need for OSG_GRID. The RPM installation creates  /etc/osg/wn-client/  with dummy setup files for compatibility with old jobs looking for a OSG_GRID. New jobs should source the setup file in OSG_GRID if this is defined; if not, they should expect all the client binaries in the default PATH.", 
            "title": "Compute Element"
        }, 
        {
            "location": "/site-planning/#worker-node-client", 
            "text": "The  Worker Node Client  is software installed on each worker node to give programs running on the worker nodes access to grid utilities. While it is technically optional, it is  strongly recommended  that you install it on the gatekeeper and all worker nodes. This can be done as a local installation, in which the software is installed individually on every worker node, or as a shared installation in which the software is installed on one machine that shares it via a global file system to all the worker nodes. All the site configurations below are showing a local installation of the worker node client.", 
            "title": "Worker Node Client"
        }, 
        {
            "location": "/site-planning/#storage-element", 
            "text": "A  Storage Element  provides grid users the possibility to read and write large amounts of data on your site using the  S torage  R esource  M anager ( SRM ). All Storage Element implementations in the OSG support the gsiftp protocol and full or partial SRM specification. The selection of storage suitable for your site varies on anticipated usage patterns, available hardware, the choice of underlying distributed storage, support for a tape-archival backend etc.  There are two types of storage element services provided by OSG which implement SRM v2. See pointers to instructions for these services:   BeStMan  - Sits in front of any POSIX filesystem. There is also a version which supports xrootd filesystems.  Hadoop  - Map-reduce based solution to aggregate off-the-shelf disks into a scalable reliable system.   An SE must run correctly configured Grid Information Providers, Gratia accounting and RSV probes.  These services are supported by the OSG Storage group. Please email   for installation and support questions for these services.     Storage Requirements  Min Hardware Requirements  OSG SE Solution      SRM interface, Dynamic Space Management Support  Server with local disk  BeStMan-fullmode    SRM interface, No or Static Space Management Support  Server with local disk or NFS  BeStMan-gateway    SRM interface, No or Static Space Management Support, jobs need root protocol to access data  Multiple servers( 3)  BeStMan-gateway/Xrood    SRM interface, No or Static Space Management Support, file replication  Multiple servers( 4)  BeStMan-gateway/HDFS    SRM interface, Dynamic Space Management Support, file replication, interface to tape backend  Multiple servers ( 5)  dcache", 
            "title": "Storage Element"
        }, 
        {
            "location": "/site-planning/#vo-management-service", 
            "text": "A  V irtual  O ganization  M anagement  S ervice (VOMS) controls who is a member of your VO. Each VO needs to provide one VOMS. Please contact the VO Manager of VO to find out about the VOMS of your VO.", 
            "title": "VO Management Service"
        }, 
        {
            "location": "/site-planning/#recommendations", 
            "text": "In this section we outline  important  recommendations for setting up an OSG site. The recommendations are based on the knowledge of experienced system administrators and will help you avoid typical problems operating an OSG site from the beginning.", 
            "title": "Recommendations"
        }, 
        {
            "location": "/site-planning/#shared-file-systems", 
            "text": "We recommend to use a dedicated server for hosting the shared file system. The expected load on the file server could be distributed further by providing a dedicated file server for  HOME  and  OSG_DATA  if possible. You should also consider to:   use a consistent mounting scheme for shared partitions when mounted on the gatekeeper with respect to all worker nodes  use a reasonable automatic cleanup procedure for  HOME ,  OSG_DATA  and  OSG_WN_TMP  not use an automatic cleanup procedures for  OSG_APP  not use quota for  OSG_DATA", 
            "title": "Shared File Systems"
        }, 
        {
            "location": "/site-planning/#nfs-warning", 
            "text": "NFS is known to be an  easy  but less adequate for large sites. If you want to use NFS you should read the chapter  \"Optimizing NFS Performance\"  in the  http://nfs.sourceforge.net/nfs-howto/ . Software partitions that can be locally installed, such as OSG_GRID, should be locally installed and not shared unless you have an enterprise-class NFS server.", 
            "title": "NFS Warning"
        }, 
        {
            "location": "/site-planning/#compute-and-storage-element", 
            "text": "provide dedicated hardware for the Compute and the Storage Element  use as many cpu cores and main memory as possible  avoid running other grid services such as GUMS on the Compute and the Storage Element  avoid running a file server on the Compute and the Storage Element", 
            "title": "Compute and Storage Element"
        }, 
        {
            "location": "/site-planning/#use-job-manager-managedfork-instead-of-fork", 
            "text": "GRAM  provides several jobmanagers that control how jobs will be executed on the gatekeeper and the worker nodes. By  default jobmanager fork  will be used to allow grid users to run maintenance jobs on the gatekeeper. Unfortunately fork doesn't provide a way to restrict the number of jobs that may be run simultaneously.  By choosing  managedfork instead of fork  during the setup process, you will be able to restrict the number of simultaneous running jobs on the gatekeeper. We strongly recommend to choose managedfork as the default jobmanager when configuring the Compute Element.", 
            "title": "Use Job Manager managedfork instead of fork"
        }, 
        {
            "location": "/site-planning/#role-based-authentication-using-voms-proxies", 
            "text": "VOMS proxies  allow role based authentication. If your site will support VOMS proxies for authentication you will be able to restrict write access to shared file systems for grid users while providing write permissions to VO Administrators only. In this case we recommend to export  OSG_APP  read-only to the gatekeeper and all worker nodes for grid users and read-write for VO Managers only.", 
            "title": "Role based Authentication using VOMS Proxies"
        }, 
        {
            "location": "/site-planning/#example-configurations", 
            "text": "This section contains a few example that illustrate how the different elements contributing to an OSG site can be combined. Each  gray  box represents a physical resource or virtual machine that is required in the example.", 
            "title": "Example Configurations"
        }, 
        {
            "location": "/site-planning/#osg-resource-with-compute-element", 
            "text": "The image above shows all elements of a compute element.  edg-mkgridmap  is used to create the  grid-mapfile  and to keep it current. The  shared filesystem  is mounted on the compute element providing access to  OSG_APP ,  OSG_DATA  and  HOME .  GRAM  and  GridFTP  use the  grid-mapfile  to authenticate requests to run jobs and to transfer data.   The image above shows a  Worker Node  that is connected to the  Compute Element  above by a network.  The  Worker Node Client  has to be installed on each worker node.  The  shared filesystem  is mounted on the worker node providing access to  OSG_APP ,  OSG_DATA  and  HOME .   Node Local Storage  is used for  OSG_WN_TMP  and  OSG_GRID .  OSG_GRID  points to the installation of the  Worker Node Client .", 
            "title": "OSG Resource with Compute Element"
        }, 
        {
            "location": "/site-planning/#osg-resource-with-joined-compute-and-storage-element", 
            "text": "The left side shows elements of a joined Compute and Storage Element.  The shared file system is mounted on the compute and storage element. The  BeStMan  storage solution provides the  SRM Service  allowing grid users to remotely access the shared file system.   The image above shows a  Worker Node  that is connected to the  Compute Element  above by a network. The  Worker Node Client  has to be installed on each worker node. The  shared filesystem  is mounted on the worker node providing access to  OSG_APP ,  OSG_DATA  and  HOME .  Node Local Storage  is used for  OSG_WN_TMP  and  OSG_GRID .  OSG_GRID  points to the installation of the  Worker Node Client .", 
            "title": "OSG Resource with joined Compute and Storage Element"
        }, 
        {
            "location": "/site-planning/#osg-resource-with-separate-compute-and-storage-element", 
            "text": "The image above shows the elements of an authorization host.  GUMS  provides a central web service for grid authorization requests from storage and compute elements.  It is not required but recommended to run GUMS outside of the compute and and storage element which are shown below.   The image above shows all elements of a compute element.  GRAM  and  GridFTP  query  GUMS  to authenticate requests from grid users to run jobs or transfer data. The  shared filesystem  is mounted on the compute element providing access to  OSG_APP ,  OSG_DATA  and  HOME .   The image above shows all elements of a storage element.  SRM  and  GridFTP  query  GUMS  to authenticate requests from grid users to transfer data. The  shared filesystem  is mounted on the compute element providing access to  OSG_APP ,  OSG_DATA  and  HOME . The  BeStMan  storage solution provides the  SRM Service  allowing grid users to remotely access the shared file system.   The image above shows a  Worker Node  that is connected to the  Compute Element  above by a network. The  Worker Node Client  has to be installed on each worker node. The  shared filesystem  is mounted on the worker node providing access to  OSG_APP ,  OSG_DATA  and  HOME .  Node Local Storage  is used for  OSG_WN_TMP  and  OSG_GRID .  OSG_GRID  points to the installation of the  Worker Node Client .", 
            "title": "OSG Resource with separate Compute and Storage Element"
        }, 
        {
            "location": "/site-planning/#osg-resource-with-compute-element-and-dcache-storage-element", 
            "text": "OSG Compute and dCache Storage Element using GUMS  The left side shows all elements of a compute and a  dCache  storage element. On the right side a set of worker node are drawn which are distinct from  dCache Pool Nodes . The worker nodes and dCache pool nodes are connected to the compute and storage element by a network.  GRAM  and  GridFTP  use  GUMS  to authenticate requests to run jobs and to transfer data.", 
            "title": "OSG Resource with Compute Element and dCache Storage Element"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/", 
            "text": "HTCondor-CE Overview\n\n\nAbout this Document\n\n\nThis document serves as an introduction to HTCondor-CE, how it works, and how it differs from a GRAM CE.\n\n\nDocument Requirements\n\n\nBefore continuing with this document, make sure that you are familiar with the following concepts:\n\n\n\n\nAn OSG site plan\n\n\nWhat is a batch system and which one will you use (\nHTCondor\n, PBS, LSF, SGE, or SLURM)?\n\n\nSecurity in the OSG via \nGSI\n (i.e., \nCertificate authorities\n, user and host \ncertificates\n, proxies)\n\n\n\n\n\n\nPilot jobs, frontends, and factories (i.e., \nGlideinWMS\n, AutoPyFactory)\n\n\n\n\nWhat is a Compute Element?\n\n\nAn OSG Compute Element (CE) is the entry point for the OSG to your local resources: a layer of software that you install on a machine that can submit jobs into your local batch system. At the heart of the CE is the \njob gateway\n software, which is responsible for handling incoming jobs, authorizing them, and delegating them to your batch system for execution. Historically, the OSG only had one option for a job gateway solution, Globus Toolkit\u2019s GRAM-based gatekeeper, but now offers the HTCondor-CE as an alternative.\n\n\nToday in OSG, most jobs that arrive at a CE (called \ngrid jobs\n) are \nnot\n end-user jobs, but rather pilot jobs submitted from factories. Successful pilot jobs create and make available an environment for actual end-user jobs to match and ultimately run within the pilot job container. Eventually pilot jobs remove themselves, typically after a period of inactivity.\n\n\nWhat is HTCondor-CE?\n\n\nHTCondor-CE is a special configuration of the HTCondor software designed to be a job gateway solution for the OSG. It is configured to use the \nJobRouter daemon\n to delegate jobs by transforming and submitting them to the site\u2019s batch system.\n\n\nHow is HTCondor-CE different from a GRAM CE?\n\n\nThe biggest difference you will see between an HTCondor-CE and a GRAM CE is in the way that jobs are submitted to your batch system; HTCondor-CE uses the built-in JobRouter daemon whereas GRAM CE uses jobmanager scripts written in Perl. Customizing your site\u2019s CE now requires editing configuration files instead of editing jobmanager scripts.\n\n\nListed below are some other benefits to switching to HTCondor-CE:\n\n\n\n\nScalability:\n HTCondor-CE is capable of supporting job workloads of large sites (see \nscale testing results\n)\n\n\nDebugging tools:\n HTCondor-CE offers \nmany tools to help troubleshoot\n issues with jobs\n\n\nRouting as configuration:\n HTCondor-CE\u2019s mechanism to transform and submit jobs is customized via configuration variables, which means that customizations will persist across upgrades and will not involve modification of software internals to route jobs\n\n\n\n\nHow Jobs Run\n\n\nOnce an incoming grid job is authorized, it is placed into HTCondor-CE\u2019s scheduler where the JobRouter creates a transformed copy (called the \nrouted job\n) and submits the copy to the batch system (called the \nbatch system job\n). After submission, HTCondor-CE monitors the batch system job and communicates its status to the original grid job, which in turn notifies the original submitter (e.g., job factory) of any updates. When the job completes, files are transferred along the same chain: from the batch system to the CE, then from the CE to the original submitter.\n\n\nOn HTCondor batch systems\n\n\nFor a site with an HTCondor \nbatch system\n, the JobRouter can use HTCondor protocols to place a transformed copy of the grid job directly into the batch system\u2019s scheduler, meaning that the routed and batch system jobs are one and the same. Thus, there are three representations of your job, each with its own ID (see diagram below):\n\n\n\n\nSubmit host: the HTCondor job ID in the original queue\n\n\nHTCondor-CE: the grid job\u2019s ID\n\n\nHTCondor batch system: the routed job\u2019s ID\n\n\n\n\n\n\nIn an HTCondor-CE/HTCondor setup, files are transferred from HTCondor-CE\u2019s spool directory to the batch system\u2019s spool directory using internal HTCondor protocols.\n\n\n\n\nNote\n\n\nThe JobRouter copies the job directly into the batch system and does not make use of \ncondor_submit\n. This means that if the HTCondor batch system is configured to add attributes to incoming jobs when they are submitted (i.e., \nSUBMIT_EXPRS\n), these attributes will not be added to the routed jobs.\n\n\n\n\nOn other batch systems\n\n\nFor non-HTCondor batch systems, the JobRouter transforms the grid job into a routed job on the CE and the routed job submits a job into the batch system via a process called the BLAHP. Thus, there are four representations of your job, each with its own ID (see diagram below):\n\n\n\n\nSubmit host: the HTCondor job ID in the original queue\n\n\nHTCondor-CE: the grid job\u2019s ID and the routed job\u2019s ID\n\n\nHTCondor batch system: the batch system\u2019s job ID\n\n\n\n\nAlthough the following figure specifies the PBS case, it applies to all non-HTCondor batch systems:\n\n\n\n\nWith non-HTCondor batch systems, HTCondor-CE cannot use internal HTCondor protocols to transfer files so its spool directory must be exported to a shared file system that is mounted on the batch system\u2019s worker nodes.\n\n\nOver SSH\n\n\nHTCondor-CE-Bosco is a special configuration of HTCondor-CE that can submit jobs to a remote cluster over SSH. The HTCondor-CE-Bosco provides a simple starting point for opportunistic resource owners that want to start contributing to the OSG with minimal effort: an organization will be able to accept OSG jobs by allowing SSH access to a submit node in their cluster.\n\n\n\n\nHTCondor-CE-Bosco is intended for small sites or as an introduction to the OSG. If your site intends to run thousands of OSG jobs, you will need to host a standard \nHTCondor-CE\n because HTCondor-CE-Bosco has not yet been optimized for such loads.\n\n\nHow the CE is Customized\n\n\nAside from the \nbasic configuration\n required in the CE installation, there are two main ways to customize your CE (if you decide any customization is required at all):\n\n\n\n\nDeciding which VOs are allowed to run at your site:\n The method of limiting the VOs that are allowed to run on your site has not changed between GRAM and HTCondor-CE\u2019s: select an authorization system, GUMS or edg-mkgridmap, and configure it accordingly.\n\n\nHow to filter and transform the grid jobs to be run on your batch system:\n Filtering and transforming grid jobs (i.e., setting site-specific attributes or resource limits), requires configuration of your site\u2019s job routes. For examples of common job routes, consult the \nJobRouter recipes\n page.\n\n\n\n\n\n\nNote\n\n\nIf you are running HTCondor as your batch system, you will have two HTCondor configurations side-by-side (one residing in \n/etc/condor/\n and the other in \n/etc/condor-ce\n) and will need to make sure to differentiate the two when editing any configuration.\n\n\n\n\nHow Security Works\n\n\nIn the OSG, security depends on a PKI infrastructure involving Certificate Authorities (CAs) where CAs sign and issue certifcates to users and hosts. When these users and hosts wish to communicate with each other, the identities of each party is confirmed by cross-checking their certificates with the signing CA and establishing trust.\n\n\nDue to the OSG's distributed nature, a user's job may end up at any number of sites, potentially needing to re-authenticate at multiple points. Instead of sending the user's certificate with the job for this re-authentication, trust can be delegated to a proxy that is generated from the user certificate, which is then attached to the job and expires after some set time for added security.\n\n\nIn its default configuration, HTCondor-CE uses GSI-based authentication and authorization (the same as Globus GRAM) to verify the certificate chain, which will work with existing GUMS servers or grid mapfiles. Additionally, it can be reconfigured to provide alternate authentication mechanisms such as Kerberos, SSL, shared secret, or even IP-based authentication. More information about authorization methods can be found \nhere\n.\n\n\nNext steps\n\n\nIf you're transitioning from a GRAM CE to HTCondor-CE, the process is the same as if you were setting up a completely new CE, whether you're installing it on a new machine or alongside your GRAM CE.\n\n\n\n\nSetting up \njob routes\n\n\nSubmitting\n jobs to HTCondor-CE\n\n\nTroubleshooting\n HTCondor-CE\n\n\nRegister the CE with OIM\n\n\nRegister with the OSG GlideinWMS factories and/or the ATLAS AutoPyFactory", 
            "title": "HTCondor-CE Overview"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#htcondor-ce-overview", 
            "text": "", 
            "title": "HTCondor-CE Overview"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#about-this-document", 
            "text": "This document serves as an introduction to HTCondor-CE, how it works, and how it differs from a GRAM CE.", 
            "title": "About this Document"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#document-requirements", 
            "text": "Before continuing with this document, make sure that you are familiar with the following concepts:   An OSG site plan  What is a batch system and which one will you use ( HTCondor , PBS, LSF, SGE, or SLURM)?  Security in the OSG via  GSI  (i.e.,  Certificate authorities , user and host  certificates , proxies)    Pilot jobs, frontends, and factories (i.e.,  GlideinWMS , AutoPyFactory)", 
            "title": "Document Requirements"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#what-is-a-compute-element", 
            "text": "An OSG Compute Element (CE) is the entry point for the OSG to your local resources: a layer of software that you install on a machine that can submit jobs into your local batch system. At the heart of the CE is the  job gateway  software, which is responsible for handling incoming jobs, authorizing them, and delegating them to your batch system for execution. Historically, the OSG only had one option for a job gateway solution, Globus Toolkit\u2019s GRAM-based gatekeeper, but now offers the HTCondor-CE as an alternative.  Today in OSG, most jobs that arrive at a CE (called  grid jobs ) are  not  end-user jobs, but rather pilot jobs submitted from factories. Successful pilot jobs create and make available an environment for actual end-user jobs to match and ultimately run within the pilot job container. Eventually pilot jobs remove themselves, typically after a period of inactivity.", 
            "title": "What is a Compute Element?"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#what-is-htcondor-ce", 
            "text": "HTCondor-CE is a special configuration of the HTCondor software designed to be a job gateway solution for the OSG. It is configured to use the  JobRouter daemon  to delegate jobs by transforming and submitting them to the site\u2019s batch system.", 
            "title": "What is HTCondor-CE?"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#how-is-htcondor-ce-different-from-a-gram-ce", 
            "text": "The biggest difference you will see between an HTCondor-CE and a GRAM CE is in the way that jobs are submitted to your batch system; HTCondor-CE uses the built-in JobRouter daemon whereas GRAM CE uses jobmanager scripts written in Perl. Customizing your site\u2019s CE now requires editing configuration files instead of editing jobmanager scripts.  Listed below are some other benefits to switching to HTCondor-CE:   Scalability:  HTCondor-CE is capable of supporting job workloads of large sites (see  scale testing results )  Debugging tools:  HTCondor-CE offers  many tools to help troubleshoot  issues with jobs  Routing as configuration:  HTCondor-CE\u2019s mechanism to transform and submit jobs is customized via configuration variables, which means that customizations will persist across upgrades and will not involve modification of software internals to route jobs", 
            "title": "How is HTCondor-CE different from a GRAM CE?"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#how-jobs-run", 
            "text": "Once an incoming grid job is authorized, it is placed into HTCondor-CE\u2019s scheduler where the JobRouter creates a transformed copy (called the  routed job ) and submits the copy to the batch system (called the  batch system job ). After submission, HTCondor-CE monitors the batch system job and communicates its status to the original grid job, which in turn notifies the original submitter (e.g., job factory) of any updates. When the job completes, files are transferred along the same chain: from the batch system to the CE, then from the CE to the original submitter.", 
            "title": "How Jobs Run"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#on-htcondor-batch-systems", 
            "text": "For a site with an HTCondor  batch system , the JobRouter can use HTCondor protocols to place a transformed copy of the grid job directly into the batch system\u2019s scheduler, meaning that the routed and batch system jobs are one and the same. Thus, there are three representations of your job, each with its own ID (see diagram below):   Submit host: the HTCondor job ID in the original queue  HTCondor-CE: the grid job\u2019s ID  HTCondor batch system: the routed job\u2019s ID    In an HTCondor-CE/HTCondor setup, files are transferred from HTCondor-CE\u2019s spool directory to the batch system\u2019s spool directory using internal HTCondor protocols.   Note  The JobRouter copies the job directly into the batch system and does not make use of  condor_submit . This means that if the HTCondor batch system is configured to add attributes to incoming jobs when they are submitted (i.e.,  SUBMIT_EXPRS ), these attributes will not be added to the routed jobs.", 
            "title": "On HTCondor batch systems"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#on-other-batch-systems", 
            "text": "For non-HTCondor batch systems, the JobRouter transforms the grid job into a routed job on the CE and the routed job submits a job into the batch system via a process called the BLAHP. Thus, there are four representations of your job, each with its own ID (see diagram below):   Submit host: the HTCondor job ID in the original queue  HTCondor-CE: the grid job\u2019s ID and the routed job\u2019s ID  HTCondor batch system: the batch system\u2019s job ID   Although the following figure specifies the PBS case, it applies to all non-HTCondor batch systems:   With non-HTCondor batch systems, HTCondor-CE cannot use internal HTCondor protocols to transfer files so its spool directory must be exported to a shared file system that is mounted on the batch system\u2019s worker nodes.", 
            "title": "On other batch systems"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#over-ssh", 
            "text": "HTCondor-CE-Bosco is a special configuration of HTCondor-CE that can submit jobs to a remote cluster over SSH. The HTCondor-CE-Bosco provides a simple starting point for opportunistic resource owners that want to start contributing to the OSG with minimal effort: an organization will be able to accept OSG jobs by allowing SSH access to a submit node in their cluster.   HTCondor-CE-Bosco is intended for small sites or as an introduction to the OSG. If your site intends to run thousands of OSG jobs, you will need to host a standard  HTCondor-CE  because HTCondor-CE-Bosco has not yet been optimized for such loads.", 
            "title": "Over SSH"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#how-the-ce-is-customized", 
            "text": "Aside from the  basic configuration  required in the CE installation, there are two main ways to customize your CE (if you decide any customization is required at all):   Deciding which VOs are allowed to run at your site:  The method of limiting the VOs that are allowed to run on your site has not changed between GRAM and HTCondor-CE\u2019s: select an authorization system, GUMS or edg-mkgridmap, and configure it accordingly.  How to filter and transform the grid jobs to be run on your batch system:  Filtering and transforming grid jobs (i.e., setting site-specific attributes or resource limits), requires configuration of your site\u2019s job routes. For examples of common job routes, consult the  JobRouter recipes  page.    Note  If you are running HTCondor as your batch system, you will have two HTCondor configurations side-by-side (one residing in  /etc/condor/  and the other in  /etc/condor-ce ) and will need to make sure to differentiate the two when editing any configuration.", 
            "title": "How the CE is Customized"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#how-security-works", 
            "text": "In the OSG, security depends on a PKI infrastructure involving Certificate Authorities (CAs) where CAs sign and issue certifcates to users and hosts. When these users and hosts wish to communicate with each other, the identities of each party is confirmed by cross-checking their certificates with the signing CA and establishing trust.  Due to the OSG's distributed nature, a user's job may end up at any number of sites, potentially needing to re-authenticate at multiple points. Instead of sending the user's certificate with the job for this re-authentication, trust can be delegated to a proxy that is generated from the user certificate, which is then attached to the job and expires after some set time for added security.  In its default configuration, HTCondor-CE uses GSI-based authentication and authorization (the same as Globus GRAM) to verify the certificate chain, which will work with existing GUMS servers or grid mapfiles. Additionally, it can be reconfigured to provide alternate authentication mechanisms such as Kerberos, SSL, shared secret, or even IP-based authentication. More information about authorization methods can be found  here .", 
            "title": "How Security Works"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#next-steps", 
            "text": "If you're transitioning from a GRAM CE to HTCondor-CE, the process is the same as if you were setting up a completely new CE, whether you're installing it on a new machine or alongside your GRAM CE.   Setting up  job routes  Submitting  jobs to HTCondor-CE  Troubleshooting  HTCondor-CE  Register the CE with OIM  Register with the OSG GlideinWMS factories and/or the ATLAS AutoPyFactory", 
            "title": "Next steps"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/", 
            "text": "Installing and Maintaining HTCondor-CE\n\n\nThe \nHTCondor-CE\n software is a \njob gateway\n for an OSG Compute Element (CE). As such, HTCondor-CE is the entry point for jobs coming from the OSG \u2014 it handles authorization and delegation of jobs to your local batch system. In OSG today, most CEs accept \npilot jobs\n from a factory, which in turn are able to accept and run end-user jobs.  See the \nHTCondor-CE Overview\n for a much more detailed introduction.\n\n\nUse this page to learn how to install, configure, run, test, and troubleshoot HTCondor-CE from the OSG software repositories.\n\n\nBefore Starting\n\n\nBefore starting the installation process, consider the following points (consulting \nthe Reference section below\n as needed):\n\n\n\n\nUser IDs:\n If they do not exist already, the installation will create the Linux users \ncondor\n (UID 4716) and \ngratia\n (UID 42401)\n\n\nService certificate:\n The HTCondor-CE service uses a host certificate at \n/etc/grid-security/hostcert.pem\n and an accompanying key at \n/etc/grid-security/hostkey.pem\n\n\nNetwork ports:\n The pilot factories must be able to contact your HTCondor-CE service on ports 9619 and 9620 for condor versions \n 8.3.2 (TCP)\n\n\nHost choice:\n HTCondor-CE should be installed on a host that already has the ability to submit jobs into your local cluster\n\n\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare the \nrequired Yum repositories\n\n\nInstall \nCA certificates\n\n\n\n\nInstalling HTCondor-CE\n\n\nAn HTCondor-CE installation consists of the job gateway (i.e., the HTCondor-CE job router) and other support software (e.g., GridFTP, a Gratia probe, authentication software). To simplify installation, OSG provides convenience RPMs that install all required software with a single command.\n\n\n\n\n\n\nClean yum cache:\n\n\nroot@host #\n yum clean all --enablerepo\n=\n*\n\n\n\n\n\n\n\n\n\nUpdate software:\n\n\nroot@host #\n yum update\n\n\n\n\n\nThis command will update \nall\n packages\n\n\n\n\n\n\nIf your batch system is already installed via non-RPM means and is in the following list, install the appropriate 'empty' RPM. Otherwise, skip to the next step.\n\n\n\n\n\n\n\n\nIf your batch system is\u2026\n\n\nThen run the following command\u2026\n\n\n\n\n\n\n\n\n\n\nHTCondor\n\n\nyum install empty-condor --enablerepo=osg-empty\n\n\n\n\n\n\nSLURM\n\n\nyum install empty-slurm --enablerepo=osg-empty\n\n\n\n\n\n\n\n\n\n\n\n\nIf your HTCondor batch system is already installed via non-OSG RPM means, add the line below to \n/etc/yum.repos.d/osg.repo\n. Otherwise, skip to the next step.\n\n\nexclude=condor empty-condor\n\n\n\n\n\n\n\n\n\nSelect the appropriate convenience RPM(s):\n\n\n\n\n\n\n\n\nIf your batch system is\u2026\n\n\nThen use the following package(s)\u2026\n\n\n\n\n\n\n\n\n\n\nHTCondor\n\n\nosg-ce-condor\n\n\n\n\n\n\nLSF\n\n\nosg-ce-lsf\n\n\n\n\n\n\nPBS\n\n\nosg-ce-pbs\n\n\n\n\n\n\nSGE\n\n\nosg-ce-sge\n\n\n\n\n\n\nSLURM\n\n\nosg-ce-slurm\n\n\n\n\n\n\n\n\n\n\n\n\nInstall the CE software:\n\n\nroot@host # yum install *PACKAGE(S)*\n\n\n\n\n\n\n\n\n\nConfiguring HTCondor-CE\n\n\nThere are a few required configuration steps to connect HTCondor-CE with your batch system and authentication method. For more advanced configuration, see the section on \noptional configurations\n.\n\n\nEnabling HTCondor-CE\n\n\nIf you are installing HTCondor-CE on a new host, the default configuration is correct and you can skip this step and continue onto \nConfiguring the batch system\n! However, if you are updating a host that used a Globus GRAM job gateway (aka the Globus gatekeeper), you must disable GRAM and enable the HTCondor job gateway. Edit the gateway configuration file \n/etc/osg/config.d/10-gateway.ini\n so that it reads:\n\n\ngram_gateway_enabled = False\nhtcondor_gateway_enabled = True\n\n\n\n\n\nConfiguring the batch system\n\n\nEnable your batch system by editing the \nenabled\n field in the \n/etc/osg/config.d/20-\nYOUR BATCH SYSTEM\n.ini\n\n\nenabled = \nTrue\n\n\n\n\n\n\nIf you are using HTCondor as your \nlocal batch system\n (i.e., in addition to your HTCondor-CE), skip to the \nconfiguring authentication\n section. For other batch systems (e.g., PBS, LSF, SGE, SLURM), keep reading.\n\n\nBatch systems other than HTCondor\n\n\nNon-HTCondor batch systems require additional configuration to support file transfer to your site's worker nodes.\n\n\nSharing the spool directory\n\n\nTo transfer files between the CE and the batch system, HTCondor-CE requires a shared file system. The current recommendation is to run a dedicated NFS server (whose installation is beyond the scope of this document) on the \nCE host\n. In this setup, HTCondor-CE writes to the local spool directory, the NFS server shares the directory, and each worker node mounts the directory in the same location as on the CE. For example, if your spool directory is \n/var/lib/condor-ce\n (the default), you must mount the shared directory to \n/var/lib/condor-ce\n on the worker nodes.\n\n\n\n\nNote\n\n\nIf you choose not to host the NFS server on your CE, you will need to turn off root squash so that the HTCondor-CE daemons can write to the spool directory.\n\n\n\n\nYou can control the value of the spool directory by setting \nSPOOL\n in \n/etc/condor-ce/config.d/99-local.conf\n (create this file if it doesn't exist). For example, the following sets the \nSPOOL\n directory to \n/home/condor\n:\n\n\nSPOOL = /home/condor\n\n\n\n\n\n\n\nNote\n\n\nThe shared spool directory must be readable and writeable by the \ncondor\n user for HTCondor-CE to function correctly.\n\n\n\n\nDisable worker node proxy renewal\n\n\nWorker node proxy renewal is not used by HTCondor-CE and leaving it on will cause some jobs to be held. Edit \n/etc/blah.config\n on the HTCondor-CE host and set the following values:\n\n\nblah_disable_wn_proxy_renewal=yes\nblah_delegate_renewed_proxies=no\nblah_disable_limited_proxy=yes\n\n\n\n\n\n\n\nNote\n\n\nThere should be no whitespace around the \n=\n.\n\n\n\n\nConfiguring authentication\n\n\nIn OSG 3.3, there are three methods to manage authentication for incoming jobs: the \nLCMAPS VOMS plugin\n, \nedg-mkgridmap\n and \nGUMS\n. edg-mkgridmap is easy to set up and maintain, and GUMS has more features and capabilities. The LCMAPS VOMS plugin is the new OSG-preferred authentication, offering the simplicity of edg-mkgridmap and many of GUMS' rich feature set. If you need to support \npool accounts\n, GUMS is the only software with that capability.\n\n\nIn OSG 3.4, the LCMAPS VOMS plugin is the only available authentication solution.\n\n\nAuthentication with the LCMAPS VOMS plugin\n\n\nTo configure your CE to use the LCMAPS VOMS plugin:\n\n\n\n\n\n\nIf you are using OSG 3.3, add the following line to \n/etc/sysconfig/condor-ce\n:\n\n\nexport LLGT_VOMS_ENABLE_CREDENTIAL_CHECK=1\n\n\n\n\n\n\n\n\n\nFollow the instructions in \nthe LCMAPS VOMS plugin installation and configuration document\n to prepare the LCMAPS VOMS plugin\n\n\n\n\n\n\n\n\nNote\n\n\nIf your local batch system is HTCondor, it will attempt to utilize the LCMAPS callouts if enabled in the \ncondor_mapfile\n. If this is not the desired behavior, set \nGSI_AUTHZ_CONF=/dev/null\n in the local HTCondor configuration.\n\n\n\n\nAuthentication with edg-mkgridmap\n\n\n\n\nWarning\n\n\nedg-mkgridmap is unavailable in OSG 3.4\n\n\n\n\nTo configure your CE to use edg-mkgridmap:\n\n\n\n\nFollow the configuration instructions in \nthe edg-mkgridmap document\n to define the VOs that your site accepts\n\n\nSet some critical gridmap attributes by editing the \n/etc/osg/config.d/10-misc.ini\n file on the HTCondor-CE host:\nauthorization_method = gridmap\n\n\n\n\n\n\n\n\n\nAuthentication with GUMS\n\n\n\n\nWarning\n\n\nGUMS is unavailable in OSG 3.4\n\n\n\n\n\n\nFollow the instructions in \nthe GUMS installation and configuration document\n to prepare GUMS\n\n\nSet some critical GUMS attributes by editing the \n/etc/osg/config.d/10-misc.ini\n file on the HTCondor-CE host:\nauthorization_method = xacml\ngums_host = \nYOUR GUMS HOSTNAME\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nIf your local batch system is HTCondor, it will attempt to utilize the LCMAPS callouts if enabled in the \ncondor_mapfile\n. If this is not the desired behavior, set \nGSI_AUTHZ_CONF=/dev/null\n in the local HTCondor configuration.\n\n\n\n\nConfiguring CE collector advertising\n\n\nTo split jobs between the various sites of the OSG, information about each site's capabilities are uploaded to a central collector. The job factories then query the central collector for idle resources and submit pilot jobs to the available sites. To advertise your site, you will need to enter some information about the worker nodes of your clusters.\n\n\nPlease see the \nSubcluster / Resource Entry configuration document\n about configuring the data that will be uploaded to the central collector.\n\n\nApplying configuration settings\n\n\nMaking changes to the OSG configuration files in the \n/etc/osg/config.d\n directory does not apply those settings to software automatically. Settings that are made outside of the OSG directory take effect immediately or at least when the relevant service is restarted. For the OSG settings, use the \nosg-configure\n tool to validate (to a limited extent) and apply the settings to the relevant software components. The \nosg-configure\n software is included automatically in an HTCondor-CE installation.\n\n\n\n\n\n\nMake all changes to \n.ini\n files in the \n/etc/osg/config.d\n directory\n\n\n\n\nNote\n\n\nThis document describes the critical settings for HTCondor-CE and related software. You may need to configure other software that is installed on your HTCondor-CE host, too.\n\n\n\n\n\n\n\n\nValidate the configuration settings\n\n\nroot@host #\n osg-configure -v\n\n\n\n\n\n\n\n\n\nFix any errors (at least) that \nosg-configure\n reports.\n\n\n\n\n\n\nOnce the validation command succeeds without errors, apply the configuration settings:\n\n\nroot@host # osg-configure -c\n\n\n\n\n\n\n\n\n\nGenerate a \nuser-vo-map\n file with your authentication set up (skip this step if you're using the LCMAPS VOMS plugin):\n\n\n\n\n\n\nIf you're using edg-mkgridmap, run the following:\n\n\nroot@host #\n edg-mkgridmap\n\n\n\n\n\n\n\n\n\nIf you're using GUMS, run the following:\n\n\nroot@host #\n gums-host-cron\n\n\n\n\n\n\n\n\n\n\n\n\n\nOptional configuration\n\n\nThe following configuration steps are optional and will likely not be required for setting up a small site. If you do not need any of the following special configurations, skip to \nthe section on using HTCondor-CE\n.\n\n\n\n\nTransforming and filtering jobs\n\n\nConfiguring for multiple network interfaces\n\n\nLimiting or disabling locally running jobs on the CE\n\n\nAccounting with multiple CEs or local user jobs\n\n\nHTCondor accounting groups\n\n\nInstalling the HTCondor-CE View\n\n\n\n\nTransforming and filtering jobs\n\n\nIf you need to modify or filter jobs, more information can be found in the \nJob Router Recipes\n document.\n\n\n\n\nNote\n\n\nIf you need to assign jobs to HTCondor accounting groups, refer to \nthis\n section.\n\n\n\n\nConfiguring for multiple network interfaces\n\n\nIf you have multiple network interfaces with different hostnames, the HTCondor-CE daemons need to know which hostname and interface to use when communicating to each other. Set \nNETWORK_HOSTNAME\n and \nNETWORK_INTERFACE\n to the hostname and IP address of your public interface, respectively, in \n/etc/condor-ce/config.d/99-local.conf\n directory with the line:\n\n\nNETWORK_HOSTNAME = \ncondorce.example.com\n\nNETWORK_INTERFACE = \n127.0.0.1\n\n\n\n\n\n\nReplacing \ncondorce.example.com\n text with your public interface\u2019s hostname and \n127.0.0.1\n with your public interface\u2019s IP address.\n\n\nLimiting or disabling locally running jobs on the CE\n\n\nIf you want to limit or disable jobs running locally on your CE, you will need to configure HTCondor-CE's local and scheduler universes. Local and scheduler universes are HTCondor-CE\u2019s analogue to GRAM\u2019s managed fork: they allow jobs to be run on the CE itself, mainly for remote troubleshooting. Pilot jobs will not run as local/scheduler universe jobs so leaving them enabled does NOT turn your CE into another worker node.\n\n\nThe two universes are effectively the same (scheduler universe launches a starter process for each job), so we will be configuring them in unison.\n\n\n\n\n\n\nTo change the default limit\n on the number of locally run jobs (the current default is 20), add the following to \n/etc/condor-ce/config.d/99-local.conf\n:\n\n\nSTART_LOCAL_UNIVERSE\n \n=\n TotalLocalJobsRunning + TotalSchedulerJobsRunning \n \nJOB-LIMIT\n\n\nSTART_SCHEDULER_UNIVERSE\n \n=\n \n$(\nSTART_LOCAL_UNIVERSE\n)\n\n\n\n\n\n\n\n\n\n\nTo only allow a specific user\n to start locally run jobs, add the following to \n/etc/condor-ce/config.d/99-local.conf\n:\n\n\nSTART_LOCAL_UNIVERSE\n \n=\n target.Owner \n=\n?\n=\n \nUSERNAME\n\n\nSTART_SCHEDULER_UNIVERSE\n \n=\n \n$(\nSTART_LOCAL_UNIVERSE\n)\n\n\n\n\n\n\n\n\n\n\nTo disable\n locally run jobs, add the following to \n/etc/condor-ce/config.d/99-local.conf\n:\n\n\nSTART_LOCAL_UNIVERSE\n \n=\n False\n\nSTART_SCHEDULER_UNIVERSE\n \n=\n \n$(\nSTART_LOCAL_UNIVERSE\n)\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nRSV requires the ability to start local universe jobs so if you are using RSV, you need to allow local universe jobs from the \nrsv\n user.\n\n\n\n\nAccounting with multiple CEs or local user jobs\n\n\n\n\nNote\n\n\nFor non-HTCondor batch systems only\n\n\n\n\nIf your site has multiple CEs or you have non-grid users submitting to the same local batch system, the OSG accounting software needs to be configured so that it doesn't over report the number of jobs. Use the following table to determine which file requires editing:\n\n\n\n\n\n\n\n\nIf your batch system is\u2026\n\n\nThen edit the following file on your CE(s)\u2026\n\n\n\n\n\n\n\n\n\n\nLSF\n\n\n/etc/gratia/pbs-lsf/ProbeConfig\n\n\n\n\n\n\nPBS\n\n\n/etc/gratia/pbs-lsf/ProbeConfig\n\n\n\n\n\n\nSGE\n\n\n/etc/gratia/sge/ProbeConfig\n\n\n\n\n\n\nSLURM\n\n\n/etc/gratia/slurm/ProbeConfig\n\n\n\n\n\n\n\n\nThen edit the value of \nSuppressNoDNRecords\n so that it reads:\n\n\nSuppressNoDNRecords=\n1\n\n\n\n\n\n\nHTCondor accounting groups\n\n\n\n\nNote\n\n\nFor HTCondor batch systems only\n\n\n\n\nIf you want to provide fairshare on a group basis, as opposed to a Unix user basis, you can use HTCondor accounting groups. They are independent of the Unix groups the user may already be in and are \ndocumented in the HTCondor manual\n. If you are using HTCondor accounting groups, you can map jobs from the CE into HTCondor accounting groups based on their UID, their DN, or their VOMS attributes.\n\n\n\n\n\n\nTo map UIDs to an accounting group,\n add entries to \n/etc/osg/uid_table.txt\n with the following form:\n\n\nuid GroupName\n\n\n\n\n\nThe following is an example \nuid_table.txt\n:\n\n\nuscms02 TestGroup\nosg     other.osgedu\n\n\n\n\n\n\n\n\n\nTo map DNs or VOMS attributes to an accounting group,\n add lines to \n/etc/osg/extattr_table.txt\n with the following form:\n\n\nSubjectOrAttribute\n GroupName\n\n\n\n\n\n\nThe \nSubjectOrAttribute\n can be a Perl regular expression. The following is an example \nextattr_table.txt\n:\n\n\ncmsprio cms.other.prio\ncms\\/Role=production cms.prod\n\\/DC=com\\/DC=DigiCert-Grid\\/O=Open\\ Science\\ Grid\\/OU=People\\/CN=Brian\\ Lin\\ 1047 osg.test\n.* other\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nEntries in \n/etc/osg/uid_table.txt\n are honored over \n/etc/osg/extattr_table.txt\n if a job would match to lines in both files.\n\n\n\n\nInstall and run the HTCondor-CE View\n\n\nThe HTCondor-CE View is an optional web interface to the status of your CE. To run the View,\n\n\n\n\n\n\nBegin by installing the package htcondor-ce-view:\n\n\nroot@host #\n yum install htcondor-ce-view\n\n\n\n\n\n\n\n\n\nNext, uncomment the \nDAEMON_LIST\n configuration located at \n/etc/condor-ce/config.d/05-ce-view.conf\n:\n\n\nDAEMON_LIST\n \n=\n \n$(\nDAEMON_LIST\n)\n, CEVIEW, GANGLIAD\n\n\n\n\n\n\n\n\n\nRestart the CE service:\n\n\nroot@host #\n service condor-ce restart\n\n\n\n\n\n\n\n\n\nVerify the service by entering your CE's hostname into your web browser\n\n\n\n\n\n\nThe website is served on port 80 by default. To change this default, edit the value of \nHTCONDORCE_VIEW_PORT\n in \n/etc/condor-ce/config.d/05-ce-view.conf\n.\n\n\nUsing HTCondor-CE\n\n\nAs a site administrator, there are a few ways in which you might use the HTCondor-CE:\n\n\n\n\nManaging the HTCondor-CE and associated services\n\n\nUsing HTCondor-CE administrative tools to monitor and maintain the job gateway\n\n\nUsing HTCondor-CE user tools to test gateway operations\n\n\n\n\nManaging HTCondor-CE and associated services\n\n\nIn addition to the HTCondor-CE job gateway service itself, there are a number of supporting services in your installation. The specific services are:\n\n\n\n\n\n\n\n\nSoftware\n\n\nService name\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nFetch CRL\n\n\nfetch-crl-boot\n and \nfetch-crl-cron\n\n\nSee \nCA documentation\n for more info\n\n\n\n\n\n\nGratia\n\n\ngratia-probes-cron\n\n\nAccounting software\n\n\n\n\n\n\nYour batch system\n\n\ncondor\n or \npbs_server\n or \u2026\n\n\n\n\n\n\n\n\nHTCondor-CE\n\n\ncondor-ce\n\n\n\n\n\n\n\n\n\n\nStart the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as \nroot\n):\n\n\n\n\n\n\n\n\nTo...\n\n\nOn EL6, run the command...\n\n\nOn EL7, run the command...\n\n\n\n\n\n\n\n\n\n\nStart a service\n\n\nservice \nSERVICE-NAME\n start\n\n\nsystemctl start \nSERVICE-NAME\n\n\n\n\n\n\nStop a  service\n\n\nservice \nSERVICE-NAME\n stop\n\n\nsystemctl stop \nSERVICE-NAME\n\n\n\n\n\n\nEnable a service to start on boot\n\n\nchkconfig \nSERVICE-NAME\n on\n\n\nsystemctl enable \nSERVICE-NAME\n\n\n\n\n\n\nDisable a service from starting on boot\n\n\nchkconfig \nSERVICE-NAME\n off\n\n\nsystemctl disable \nSERVICE-NAME\n\n\n\n\n\n\n\n\nUsing HTCondor-CE tools\n\n\nSome of the HTCondor-CE administrative and user tools are documented in \nthe HTCondor-CE troubleshooting guide\n.\n\n\nValidating HTCondor-CE\n\n\nThere are different ways to make sure that your HTCondor-CE host is working well:\n\n\n\n\nPerform automated validation by running \nRSV\n\n\nManually verify your HTCondor-CE using \nHTCondor-CE troubleshooting guide\n; useful tools include:\n\n\ncondor_ce_run\n\n\ncondor_ce_trace\n\n\ncondor_submit\n\n\n\n\n\n\n\n\nTroubleshooting HTCondor-CE\n\n\nFor information on how to troubleshoot your HTCondor-CE, please refer to \nthe HTCondor-CE troubleshooting guide\n.\n\n\nRegistering the CE\n\n\nTo be part of the OSG Production Grid, your CE must be registered in the \nOSG Information Management System\n (OIM). To register your resource:\n\n\n\n\nObtain, install, and verify your user certificate\n (which you may have done already)\n\n\nRegister your site and CE in OIM\n\n\n\n\nGetting Help\n\n\nTo get assistance, please use the \nthis page\n.\n\n\nReference\n\n\nHere are some other HTCondor-CE documents that might be helpful:\n\n\n\n\nHTCondor-CE overview and architecture\n\n\nConfiguring HTCondor-CE job routes\n\n\nThe HTCondor-CE troubleshooting guide\n\n\nSubmitting jobs to HTCondor-CE\n\n\n\n\nConfiguration\n\n\nThe following directories contain the configuration for HTCondor-CE. The directories are parsed in the order presented and thus configuration within the final directory will override configuration specified in the previous directories.\n\n\n\n\n\n\n\n\nLocation\n\n\nComment\n\n\n\n\n\n\n\n\n\n\n/usr/share/condor-ce/config.d/\n\n\nConfiguration defaults (overwritten on package updates)\n\n\n\n\n\n\n/etc/condor-ce/config.d/\n\n\nFiles in this directory are parsed in alphanumeric order (i.e., \n99-local.conf\n will override values in \n01-ce-auth.conf\n)\n\n\n\n\n\n\n\n\nFor a detailed order of the way configuration files are parsed, run the following command:\n\n\nuser@host $\n condor_ce_config_val -config\n\n\n\n\n\nUsers\n\n\nThe following users are needed by HTCondor-CE at all sites:\n\n\n\n\n\n\n\n\nUser\n\n\nComment\n\n\n\n\n\n\n\n\n\n\ncondor\n\n\nThe HTCondor-CE will be run as root, but perform most of its operations as the \ncondor\n user.\n\n\n\n\n\n\ngratia\n\n\nRuns the Gratia probes to collect accounting data\n\n\n\n\n\n\n\n\nCertificates\n\n\n\n\n\n\n\n\nFile\n\n\nUser that owns certificate\n\n\nPath to certificate\n\n\n\n\n\n\n\n\n\n\nHost certificate\n\n\nroot\n\n\n/etc/grid-security/hostcert.pem\n\n\n\n\n\n\nHost key\n\n\nroot\n\n\n/etc/grid-security/hostkey.pem\n\n\n\n\n\n\n\n\nFind instructions to request a host certificate \nhere\n.\n\n\nNetworking\n\n\n\n\n\n\n\n\nService Name\n\n\nProtocol\n\n\nPort Number\n\n\nInbound\n\n\nOutbound\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nHtcondor-CE\n\n\ntcp\n\n\n9619\n\n\nX\n\n\n\n\nHTCondor-CE shared port\n\n\n\n\n\n\n\n\nAllow inbound and outbound network connection to all internal site servers, such as GUMS and the batch system head-node only ephemeral outgoing ports are necessary.", 
            "title": "Install HTCondor-CE"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#installing-and-maintaining-htcondor-ce", 
            "text": "The  HTCondor-CE  software is a  job gateway  for an OSG Compute Element (CE). As such, HTCondor-CE is the entry point for jobs coming from the OSG \u2014 it handles authorization and delegation of jobs to your local batch system. In OSG today, most CEs accept  pilot jobs  from a factory, which in turn are able to accept and run end-user jobs.  See the  HTCondor-CE Overview  for a much more detailed introduction.  Use this page to learn how to install, configure, run, test, and troubleshoot HTCondor-CE from the OSG software repositories.", 
            "title": "Installing and Maintaining HTCondor-CE"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#before-starting", 
            "text": "Before starting the installation process, consider the following points (consulting  the Reference section below  as needed):   User IDs:  If they do not exist already, the installation will create the Linux users  condor  (UID 4716) and  gratia  (UID 42401)  Service certificate:  The HTCondor-CE service uses a host certificate at  /etc/grid-security/hostcert.pem  and an accompanying key at  /etc/grid-security/hostkey.pem  Network ports:  The pilot factories must be able to contact your HTCondor-CE service on ports 9619 and 9620 for condor versions   8.3.2 (TCP)  Host choice:  HTCondor-CE should be installed on a host that already has the ability to submit jobs into your local cluster   As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system  Obtain root access to the host  Prepare the  required Yum repositories  Install  CA certificates", 
            "title": "Before Starting"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#installing-htcondor-ce", 
            "text": "An HTCondor-CE installation consists of the job gateway (i.e., the HTCondor-CE job router) and other support software (e.g., GridFTP, a Gratia probe, authentication software). To simplify installation, OSG provides convenience RPMs that install all required software with a single command.    Clean yum cache:  root@host #  yum clean all --enablerepo = *    Update software:  root@host #  yum update  This command will update  all  packages    If your batch system is already installed via non-RPM means and is in the following list, install the appropriate 'empty' RPM. Otherwise, skip to the next step.     If your batch system is\u2026  Then run the following command\u2026      HTCondor  yum install empty-condor --enablerepo=osg-empty    SLURM  yum install empty-slurm --enablerepo=osg-empty       If your HTCondor batch system is already installed via non-OSG RPM means, add the line below to  /etc/yum.repos.d/osg.repo . Otherwise, skip to the next step.  exclude=condor empty-condor    Select the appropriate convenience RPM(s):     If your batch system is\u2026  Then use the following package(s)\u2026      HTCondor  osg-ce-condor    LSF  osg-ce-lsf    PBS  osg-ce-pbs    SGE  osg-ce-sge    SLURM  osg-ce-slurm       Install the CE software:  root@host # yum install *PACKAGE(S)*", 
            "title": "Installing HTCondor-CE"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#configuring-htcondor-ce", 
            "text": "There are a few required configuration steps to connect HTCondor-CE with your batch system and authentication method. For more advanced configuration, see the section on  optional configurations .", 
            "title": "Configuring HTCondor-CE"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#enabling-htcondor-ce", 
            "text": "If you are installing HTCondor-CE on a new host, the default configuration is correct and you can skip this step and continue onto  Configuring the batch system ! However, if you are updating a host that used a Globus GRAM job gateway (aka the Globus gatekeeper), you must disable GRAM and enable the HTCondor job gateway. Edit the gateway configuration file  /etc/osg/config.d/10-gateway.ini  so that it reads:  gram_gateway_enabled = False\nhtcondor_gateway_enabled = True", 
            "title": "Enabling HTCondor-CE"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#configuring-the-batch-system", 
            "text": "Enable your batch system by editing the  enabled  field in the  /etc/osg/config.d/20- YOUR BATCH SYSTEM .ini  enabled =  True   If you are using HTCondor as your  local batch system  (i.e., in addition to your HTCondor-CE), skip to the  configuring authentication  section. For other batch systems (e.g., PBS, LSF, SGE, SLURM), keep reading.", 
            "title": "Configuring the batch system"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#batch-systems-other-than-htcondor", 
            "text": "Non-HTCondor batch systems require additional configuration to support file transfer to your site's worker nodes.", 
            "title": "Batch systems other than HTCondor"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#sharing-the-spool-directory", 
            "text": "To transfer files between the CE and the batch system, HTCondor-CE requires a shared file system. The current recommendation is to run a dedicated NFS server (whose installation is beyond the scope of this document) on the  CE host . In this setup, HTCondor-CE writes to the local spool directory, the NFS server shares the directory, and each worker node mounts the directory in the same location as on the CE. For example, if your spool directory is  /var/lib/condor-ce  (the default), you must mount the shared directory to  /var/lib/condor-ce  on the worker nodes.   Note  If you choose not to host the NFS server on your CE, you will need to turn off root squash so that the HTCondor-CE daemons can write to the spool directory.   You can control the value of the spool directory by setting  SPOOL  in  /etc/condor-ce/config.d/99-local.conf  (create this file if it doesn't exist). For example, the following sets the  SPOOL  directory to  /home/condor :  SPOOL = /home/condor   Note  The shared spool directory must be readable and writeable by the  condor  user for HTCondor-CE to function correctly.", 
            "title": "Sharing the spool directory"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#disable-worker-node-proxy-renewal", 
            "text": "Worker node proxy renewal is not used by HTCondor-CE and leaving it on will cause some jobs to be held. Edit  /etc/blah.config  on the HTCondor-CE host and set the following values:  blah_disable_wn_proxy_renewal=yes\nblah_delegate_renewed_proxies=no\nblah_disable_limited_proxy=yes   Note  There should be no whitespace around the  = .", 
            "title": "Disable worker node proxy renewal"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#configuring-authentication", 
            "text": "In OSG 3.3, there are three methods to manage authentication for incoming jobs: the  LCMAPS VOMS plugin ,  edg-mkgridmap  and  GUMS . edg-mkgridmap is easy to set up and maintain, and GUMS has more features and capabilities. The LCMAPS VOMS plugin is the new OSG-preferred authentication, offering the simplicity of edg-mkgridmap and many of GUMS' rich feature set. If you need to support  pool accounts , GUMS is the only software with that capability.  In OSG 3.4, the LCMAPS VOMS plugin is the only available authentication solution.", 
            "title": "Configuring authentication"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#authentication-with-the-lcmaps-voms-plugin", 
            "text": "To configure your CE to use the LCMAPS VOMS plugin:    If you are using OSG 3.3, add the following line to  /etc/sysconfig/condor-ce :  export LLGT_VOMS_ENABLE_CREDENTIAL_CHECK=1    Follow the instructions in  the LCMAPS VOMS plugin installation and configuration document  to prepare the LCMAPS VOMS plugin     Note  If your local batch system is HTCondor, it will attempt to utilize the LCMAPS callouts if enabled in the  condor_mapfile . If this is not the desired behavior, set  GSI_AUTHZ_CONF=/dev/null  in the local HTCondor configuration.", 
            "title": "Authentication with the LCMAPS VOMS plugin"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#authentication-with-edg-mkgridmap", 
            "text": "Warning  edg-mkgridmap is unavailable in OSG 3.4   To configure your CE to use edg-mkgridmap:   Follow the configuration instructions in  the edg-mkgridmap document  to define the VOs that your site accepts  Set some critical gridmap attributes by editing the  /etc/osg/config.d/10-misc.ini  file on the HTCondor-CE host: authorization_method = gridmap", 
            "title": "Authentication with edg-mkgridmap"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#authentication-with-gums", 
            "text": "Warning  GUMS is unavailable in OSG 3.4    Follow the instructions in  the GUMS installation and configuration document  to prepare GUMS  Set some critical GUMS attributes by editing the  /etc/osg/config.d/10-misc.ini  file on the HTCondor-CE host: authorization_method = xacml\ngums_host =  YOUR GUMS HOSTNAME      Note  If your local batch system is HTCondor, it will attempt to utilize the LCMAPS callouts if enabled in the  condor_mapfile . If this is not the desired behavior, set  GSI_AUTHZ_CONF=/dev/null  in the local HTCondor configuration.", 
            "title": "Authentication with GUMS"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#configuring-ce-collector-advertising", 
            "text": "To split jobs between the various sites of the OSG, information about each site's capabilities are uploaded to a central collector. The job factories then query the central collector for idle resources and submit pilot jobs to the available sites. To advertise your site, you will need to enter some information about the worker nodes of your clusters.  Please see the  Subcluster / Resource Entry configuration document  about configuring the data that will be uploaded to the central collector.", 
            "title": "Configuring CE collector advertising"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#applying-configuration-settings", 
            "text": "Making changes to the OSG configuration files in the  /etc/osg/config.d  directory does not apply those settings to software automatically. Settings that are made outside of the OSG directory take effect immediately or at least when the relevant service is restarted. For the OSG settings, use the  osg-configure  tool to validate (to a limited extent) and apply the settings to the relevant software components. The  osg-configure  software is included automatically in an HTCondor-CE installation.    Make all changes to  .ini  files in the  /etc/osg/config.d  directory   Note  This document describes the critical settings for HTCondor-CE and related software. You may need to configure other software that is installed on your HTCondor-CE host, too.     Validate the configuration settings  root@host #  osg-configure -v    Fix any errors (at least) that  osg-configure  reports.    Once the validation command succeeds without errors, apply the configuration settings:  root@host # osg-configure -c    Generate a  user-vo-map  file with your authentication set up (skip this step if you're using the LCMAPS VOMS plugin):    If you're using edg-mkgridmap, run the following:  root@host #  edg-mkgridmap    If you're using GUMS, run the following:  root@host #  gums-host-cron", 
            "title": "Applying configuration settings"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#optional-configuration", 
            "text": "The following configuration steps are optional and will likely not be required for setting up a small site. If you do not need any of the following special configurations, skip to  the section on using HTCondor-CE .   Transforming and filtering jobs  Configuring for multiple network interfaces  Limiting or disabling locally running jobs on the CE  Accounting with multiple CEs or local user jobs  HTCondor accounting groups  Installing the HTCondor-CE View", 
            "title": "Optional configuration"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#transforming-and-filtering-jobs", 
            "text": "If you need to modify or filter jobs, more information can be found in the  Job Router Recipes  document.   Note  If you need to assign jobs to HTCondor accounting groups, refer to  this  section.", 
            "title": "Transforming and filtering jobs"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#configuring-for-multiple-network-interfaces", 
            "text": "If you have multiple network interfaces with different hostnames, the HTCondor-CE daemons need to know which hostname and interface to use when communicating to each other. Set  NETWORK_HOSTNAME  and  NETWORK_INTERFACE  to the hostname and IP address of your public interface, respectively, in  /etc/condor-ce/config.d/99-local.conf  directory with the line:  NETWORK_HOSTNAME =  condorce.example.com \nNETWORK_INTERFACE =  127.0.0.1   Replacing  condorce.example.com  text with your public interface\u2019s hostname and  127.0.0.1  with your public interface\u2019s IP address.", 
            "title": "Configuring for multiple network interfaces"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#limiting-or-disabling-locally-running-jobs-on-the-ce", 
            "text": "If you want to limit or disable jobs running locally on your CE, you will need to configure HTCondor-CE's local and scheduler universes. Local and scheduler universes are HTCondor-CE\u2019s analogue to GRAM\u2019s managed fork: they allow jobs to be run on the CE itself, mainly for remote troubleshooting. Pilot jobs will not run as local/scheduler universe jobs so leaving them enabled does NOT turn your CE into another worker node.  The two universes are effectively the same (scheduler universe launches a starter process for each job), so we will be configuring them in unison.    To change the default limit  on the number of locally run jobs (the current default is 20), add the following to  /etc/condor-ce/config.d/99-local.conf :  START_LOCAL_UNIVERSE   =  TotalLocalJobsRunning + TotalSchedulerJobsRunning    JOB-LIMIT  START_SCHEDULER_UNIVERSE   =   $( START_LOCAL_UNIVERSE )     To only allow a specific user  to start locally run jobs, add the following to  /etc/condor-ce/config.d/99-local.conf :  START_LOCAL_UNIVERSE   =  target.Owner  = ? =   USERNAME  START_SCHEDULER_UNIVERSE   =   $( START_LOCAL_UNIVERSE )     To disable  locally run jobs, add the following to  /etc/condor-ce/config.d/99-local.conf :  START_LOCAL_UNIVERSE   =  False START_SCHEDULER_UNIVERSE   =   $( START_LOCAL_UNIVERSE )      Note  RSV requires the ability to start local universe jobs so if you are using RSV, you need to allow local universe jobs from the  rsv  user.", 
            "title": "Limiting or disabling locally running jobs on the CE"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#accounting-with-multiple-ces-or-local-user-jobs", 
            "text": "Note  For non-HTCondor batch systems only   If your site has multiple CEs or you have non-grid users submitting to the same local batch system, the OSG accounting software needs to be configured so that it doesn't over report the number of jobs. Use the following table to determine which file requires editing:     If your batch system is\u2026  Then edit the following file on your CE(s)\u2026      LSF  /etc/gratia/pbs-lsf/ProbeConfig    PBS  /etc/gratia/pbs-lsf/ProbeConfig    SGE  /etc/gratia/sge/ProbeConfig    SLURM  /etc/gratia/slurm/ProbeConfig     Then edit the value of  SuppressNoDNRecords  so that it reads:  SuppressNoDNRecords= 1", 
            "title": "Accounting with multiple CEs or local user jobs"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#htcondor-accounting-groups", 
            "text": "Note  For HTCondor batch systems only   If you want to provide fairshare on a group basis, as opposed to a Unix user basis, you can use HTCondor accounting groups. They are independent of the Unix groups the user may already be in and are  documented in the HTCondor manual . If you are using HTCondor accounting groups, you can map jobs from the CE into HTCondor accounting groups based on their UID, their DN, or their VOMS attributes.    To map UIDs to an accounting group,  add entries to  /etc/osg/uid_table.txt  with the following form:  uid GroupName  The following is an example  uid_table.txt :  uscms02 TestGroup\nosg     other.osgedu    To map DNs or VOMS attributes to an accounting group,  add lines to  /etc/osg/extattr_table.txt  with the following form:  SubjectOrAttribute  GroupName   The  SubjectOrAttribute  can be a Perl regular expression. The following is an example  extattr_table.txt :  cmsprio cms.other.prio\ncms\\/Role=production cms.prod\n\\/DC=com\\/DC=DigiCert-Grid\\/O=Open\\ Science\\ Grid\\/OU=People\\/CN=Brian\\ Lin\\ 1047 osg.test\n.* other     Note  Entries in  /etc/osg/uid_table.txt  are honored over  /etc/osg/extattr_table.txt  if a job would match to lines in both files.", 
            "title": "HTCondor accounting groups"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#install-and-run-the-htcondor-ce-view", 
            "text": "The HTCondor-CE View is an optional web interface to the status of your CE. To run the View,    Begin by installing the package htcondor-ce-view:  root@host #  yum install htcondor-ce-view    Next, uncomment the  DAEMON_LIST  configuration located at  /etc/condor-ce/config.d/05-ce-view.conf :  DAEMON_LIST   =   $( DAEMON_LIST ) , CEVIEW, GANGLIAD    Restart the CE service:  root@host #  service condor-ce restart    Verify the service by entering your CE's hostname into your web browser    The website is served on port 80 by default. To change this default, edit the value of  HTCONDORCE_VIEW_PORT  in  /etc/condor-ce/config.d/05-ce-view.conf .", 
            "title": "Install and run the HTCondor-CE View"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#using-htcondor-ce", 
            "text": "As a site administrator, there are a few ways in which you might use the HTCondor-CE:   Managing the HTCondor-CE and associated services  Using HTCondor-CE administrative tools to monitor and maintain the job gateway  Using HTCondor-CE user tools to test gateway operations", 
            "title": "Using HTCondor-CE"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#managing-htcondor-ce-and-associated-services", 
            "text": "In addition to the HTCondor-CE job gateway service itself, there are a number of supporting services in your installation. The specific services are:     Software  Service name  Notes      Fetch CRL  fetch-crl-boot  and  fetch-crl-cron  See  CA documentation  for more info    Gratia  gratia-probes-cron  Accounting software    Your batch system  condor  or  pbs_server  or \u2026     HTCondor-CE  condor-ce      Start the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as  root ):     To...  On EL6, run the command...  On EL7, run the command...      Start a service  service  SERVICE-NAME  start  systemctl start  SERVICE-NAME    Stop a  service  service  SERVICE-NAME  stop  systemctl stop  SERVICE-NAME    Enable a service to start on boot  chkconfig  SERVICE-NAME  on  systemctl enable  SERVICE-NAME    Disable a service from starting on boot  chkconfig  SERVICE-NAME  off  systemctl disable  SERVICE-NAME", 
            "title": "Managing HTCondor-CE and associated services"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#using-htcondor-ce-tools", 
            "text": "Some of the HTCondor-CE administrative and user tools are documented in  the HTCondor-CE troubleshooting guide .", 
            "title": "Using HTCondor-CE tools"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#validating-htcondor-ce", 
            "text": "There are different ways to make sure that your HTCondor-CE host is working well:   Perform automated validation by running  RSV  Manually verify your HTCondor-CE using  HTCondor-CE troubleshooting guide ; useful tools include:  condor_ce_run  condor_ce_trace  condor_submit", 
            "title": "Validating HTCondor-CE"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#troubleshooting-htcondor-ce", 
            "text": "For information on how to troubleshoot your HTCondor-CE, please refer to  the HTCondor-CE troubleshooting guide .", 
            "title": "Troubleshooting HTCondor-CE"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#registering-the-ce", 
            "text": "To be part of the OSG Production Grid, your CE must be registered in the  OSG Information Management System  (OIM). To register your resource:   Obtain, install, and verify your user certificate  (which you may have done already)  Register your site and CE in OIM", 
            "title": "Registering the CE"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#getting-help", 
            "text": "To get assistance, please use the  this page .", 
            "title": "Getting Help"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#reference", 
            "text": "Here are some other HTCondor-CE documents that might be helpful:   HTCondor-CE overview and architecture  Configuring HTCondor-CE job routes  The HTCondor-CE troubleshooting guide  Submitting jobs to HTCondor-CE", 
            "title": "Reference"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#configuration", 
            "text": "The following directories contain the configuration for HTCondor-CE. The directories are parsed in the order presented and thus configuration within the final directory will override configuration specified in the previous directories.     Location  Comment      /usr/share/condor-ce/config.d/  Configuration defaults (overwritten on package updates)    /etc/condor-ce/config.d/  Files in this directory are parsed in alphanumeric order (i.e.,  99-local.conf  will override values in  01-ce-auth.conf )     For a detailed order of the way configuration files are parsed, run the following command:  user@host $  condor_ce_config_val -config", 
            "title": "Configuration"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#users", 
            "text": "The following users are needed by HTCondor-CE at all sites:     User  Comment      condor  The HTCondor-CE will be run as root, but perform most of its operations as the  condor  user.    gratia  Runs the Gratia probes to collect accounting data", 
            "title": "Users"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#certificates", 
            "text": "File  User that owns certificate  Path to certificate      Host certificate  root  /etc/grid-security/hostcert.pem    Host key  root  /etc/grid-security/hostkey.pem     Find instructions to request a host certificate  here .", 
            "title": "Certificates"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#networking", 
            "text": "Service Name  Protocol  Port Number  Inbound  Outbound  Comment      Htcondor-CE  tcp  9619  X   HTCondor-CE shared port     Allow inbound and outbound network connection to all internal site servers, such as GUMS and the batch system head-node only ephemeral outgoing ports are necessary.", 
            "title": "Networking"
        }, 
        {
            "location": "/compute-element/job-router-recipes/", 
            "text": "Writing Routes For HTCondor-CE\n\n\nThe \nJobRouter\n is at the heart of HTCondor-CE and allows admins to transform and direct jobs to specific batch systems. Customizations are made in the form of job routes where each route corresponds to a separate job transformation: If an incoming job matches a job route's requirements, the route creates a transformed job (referred to as the 'routed job') that is then submitted to the batch system. The CE package comes with default routes located in \n/etc/condor-ce/config.d/02-ce-*.conf\n that provide enough basic functionality for a small site.\n\n\nIf you have needs beyond delegating all incoming jobs to your batch system as they are, this document provides examples of common job routes and job route problems.\n\n\nQuirks and Pitfalls\n\n\n\n\nThe JobRouter matches jobs to routes in a round-robin fashion (for HTCondor versions \n 8.7.1). This means that if a job can match to multiple routes, it can be routed by any of them! So when writing job routes, make sure that they are exclusive to each other and that your jobs can only match to a single route.\n\n\nIf a value is set in \nJOB_ROUTER_DEFAULTS\n with \neval_set_\nvariable\n, override it by using \neval_set_\nvariable\n in the \nJOB_ROUTER_ENTRIES\n. Do this at your own risk as it may cause the CE to break.\n\n\nMake sure to run \ncondor_ce_reconfig\n after changing your routes, otherwise they will not take effect.\n\n\nBefore the last square bracket, make sure all lines end in a line continuation character (backslash). You can inspect the syntax of your routes with \ncondor_ce_config_val JOB_ROUTER_ENTRIES\n to see if HTCondor-CE has ingested them properly.\n\n\nDo \nnot\n set the job environment through the JobRouter. Instead, add any changes to the \n[Local Settings]\n section in \n/etc/osg/config.d/40-localsettings.ini\n and run osg-configure, as documented \nhere\n.\n\n\nHTCondor batch system only: Local universe jobs are excluded from any routing.\n\n\n\n\nHow Job Routes are Constructed\n\n\nEach job route\u2019s \nClassAd\n is constructed by combining each entry from the \nJOB_ROUTER_ENTRIES\n with the \nJOB_ROUTER_DEFAULTS\n. Attributes that are \nset_\n in \nJOB_ROUTER_ENTRIES\n will override those \nset_\n in \nJOB_ROUTER_DEFAULTS\n\n\nJOB_ROUTER_ENTRIES\n\n\nJOB_ROUTER_ENTRIES\n is a configuration variable whose default is set in \n/etc/condor-ce/config.d/02-ce-*.conf\n but may be overriden by the administrator in \n/etc/condor-ce/config.d/99-local.conf\n. This document outlines the many changes you can make to \nJOB_ROUTER_ENTRIES\n to fit your site\u2019s needs.\n\n\nJOB_ROUTER_DEFAULTS\n\n\nJOB_ROUTER_DEFAULTS\n is a python-generated configuration variable that sets default job route values that are required for the HTCondor-CE's functionality. To view its contents, run the following command:\n\n\nuser@host $\n condor_ce_config_val JOB_ROUTER_DEFAULTS \n|\n sed \ns/;/;\\n/g\n\n\n\n\n\n\n\n\nWarning\n\n\nIf a value is set in \nJOB_ROUTER_DEFAULTS\n with \neval_set_\nvariable\n, override it by using \neval_set_\nvariable\n in the \nJOB_ROUTER_ENTRIES\n. Do this at your own risk as it may cause the CE to break.\n\n\n\n\n\n\nWarning\n\n\nDo \nnot\n set the \nJOB_ROUTER_DEFAULTS\n configuration variable yourself. This will cause the CE to stop functioning.\n\n\n\n\nGeneric Routes\n\n\nThis section contains general information about job routes that can be used regardless of the type of batch system at your site. New routes should be placed in \n/etc/condor-ce/config.d/99-local.conf\n, not the original \n02-ce-*.conf\n.\n\n\nRequired fields\n\n\nThe minimum requirements for a route are that you specify the type of batch system that jobs should be routed to and a name for each route. Default routes can be found in \n/usr/share/condor-ce/config.d/02-ce-\nbatch system\n-defaults.conf\n, provided by the \nosg-ce-\nbatch system\n packages.\n\n\nBatch system\n\n\nEach route needs to indicate the type of batch system that jobs should be routed to. For HTCondor batch systems, the \nTargetUniverse\n attribute needs to be set to \n5\n or \n\"vanilla\"\n. For all other batch systems, the \nTargetUniverse\n attribute needs to be set to \n9\n or \n\"grid\"\n and the \nGridResource\n attribute needs to be set to \n\"batch \nbatch system\n\"\n (where \nbatch system\n can be one of \npbs\n (for both users of \npbs\n and \nSLURM\n), \nlsf\n, or \nsge\n).\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     \nTargetUniverse = 5; \\\n\n     name = \nRoute jobs to HTCondor\n; \\\n] \\\n[ \\\n     \nGridResource = \nbatch pbs\n; \\\n\n     \nTargetUniverse = 9; \\\n\n     name = \nRoute jobs to PBS\n; \\\n]\n\n\n\n\n\nRoute name\n\n\nTo identify routes, you will need to assign a name to the route with the \nname\n attribute:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     \nname = \nRoute jobs to HTCondor\n; \\\n\n]\n\n\n\n\n\nThe name of the route will be useful in debugging since it shows up in the output of \ncondor_ce_job_router_info\n, the \nJobRouterLog\n, and in the ClassAd of the routed job, which can be viewed with \ncondor_ce_q\n or \ncondor_ce_history\n.\n\n\nWriting multiple routes\n\n\nIf your batch system needs incoming jobs to be sorted (e.g. if different VO's need to go to separate queues), you will need to write multiple job routes. Each route is enclosed by square brackets and unless they're the last closing bracket, they need to be followed by the line continuation character. The following routes takes incoming jobs that have a \nqueue\n attribute set to \n\"analy\"\n and routes them to the site's HTCondor batch system. Any other jobs will be sent to that site's PBS batch system.\n\n\n\n\nNote\n\n\nFor versions of HTCondor \n 8.7.1, the JobRouter matches jobs to routes in a round-robin fashion. This means that if a job can match to multiple routes, it can be routed by any of them! So when writing job routes, make sure that they are exclusive to each other and that your jobs can only match to a single route.\n\n\n\n\nJOB_ROUTER_ENTRIES = \n[ \\\n\n     TargetUniverse = 5; \\\n     name = \nRoute jobs to HTCondor\n; \\\n     Requirements = (TARGET.queue =?= \nanaly\n); \\\n\n] \\\n[ \\\n\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nRoute jobs to PBS\n; \\\n     Requirements = (TARGET.queue =!= \nanaly\n); \\\n\n]\n\n\n\n\n\n\nWriting comments\n\n\nTo write comments you can use C-style comments, text enclosed by \n/* */\n. If the comment is at the end of a line, it still has to be followed by the line continuation character.\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name = \nC-style comments\n; \\\n     \n/* This is a comment */ \\\n\n]\n\n\n\n\n\nYou can also use \n#\n to comment out single lines:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name = \nHash comments\n; \\\n     \n# BrokenAttribute = \ncommented out\n; \\\n\n]\n\n\n\n\n\nSetting attributes for all routes\n\n\nTo set an attribute that will be applied to all routes, you will need to ensure that \nMERGE_JOB_ROUTER_DEFAULT_ADS\n is set to \nTrue\n (check the value with \ncondor_ce_config_val\n) and use the \nset_\n function in the \nJOB_ROUTER_DEFAULTS\n. The following configuration sets the \nPeriodic_Hold\n attribute for all routes:\n\n\n# Use the defaults generated by the condor_ce_router_defaults script.  To add\n\n\n# additional defaults, add additional lines of the form:\n\n\n#\n\n\n#   JOB_ROUTER_DEFAULTS = $(JOB_ROUTER_DEFAULTS) [set_foo = 1;]\n\n\n#\n\n\nMERGE_JOB_ROUTER_DEFAULT_ADS\n=\nTrue\n\nJOB_ROUTER_DEFAULTS\n \n=\n \n$(\nJOB_ROUTER_DEFAULTS\n)\n \n[\nset_Periodic_Hold\n \n=\n \n(\nNumJobStarts \n=\n \n1\n \n \nJobStatus\n \n==\n \n1\n)\n \n||\n NumJobStarts \n \n1\n;\n]\n\n\n\n\n\n\nFiltering jobs based on\u2026\n\n\nTo filter jobs, use the \nRequirements\n attribute. Jobs will evaluate against the ClassAd expression set in the \nRequirements\n and if the expression evaluates to \nTRUE\n, the route will match. More information on the syntax of ClassAd's can be found in the \nHTCondor manual\n. For an example on how incoming jobs interact with filtering in job routes, consult \nthis document\n.\n\n\nWhen setting requirements, you need to prefix job attributes that you are filtering with \nTARGET.\n so that the job route knows to compare the attribute of the incoming job rather than the route\u2019s own attribute. For example, if an incoming job has a \nqueue = \"analy\"\n attribute, then the following job route will not match:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name = \nFiltering by queue\n; \\\n     queue = \nnot-analy\n; \\\n     \nRequirements = (queue =?= \nanaly\n); \\\n\n]\n\n\n\n\n\nThis is because when evaluating the route requirement, the job route will compare its own \nqueue\n attribute to \"analy\" and see that it does not match. You can read more about comparing two ClassAds in the \nHTCondor manual\n.\n\n\n\n\nNote\n\n\nIf you have an HTCondor batch system, note the difference with \nset_requirements\n.\n\n\n\n\n\n\nNote\nFor versions of HTCondor \n 8.7.1, the JobRouter matches jobs to routes in a round-robin fashion. This means that if a job can match to multiple routes, it can be routed by any of them! So when writing job routes, make sure that they are exclusive to each other and that your jobs can only match to a single route.\n\n\n\n\n\n\nGlidein queue\n\n\nTo filter jobs based on their glidein queue attribute, your routes will need a \nRequirements\n expression using the incoming job's \nqueue\n attribute. The following entry routes jobs to the PBS queue if the incoming job (specified by \nTARGET\n) is an \nanaly\n (Analysis) glidein:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name = \nFiltering by queue\n; \\\n     \nRequirements = (TARGET.queue =?= \nanaly\n); \\\n\n]\n\n\n\n\n\nJob submitter\n\n\nTo filter jobs based on who submitted it, your routes will need a \nRequirements\n expression using the incoming job's \nOwner\n attribute. The following entry routes jobs to the HTCondor batch system iff the submitter is \nusatlas2\n:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name = \nFiltering by job submitter\n; \\\n     \nRequirements = (TARGET.Owner =?= \nusatlas2\n); \\\n\n]\n\n\n\n\n\nAlternatively, you can match based on regular expression. The following entry routes jobs to the PBS batch system iff the submitter's name begins with \nusatlas\n:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nFiltering by job submitter (regular expression)\n; \\\n     \nRequirements = regexp(\n^usatlas\n, TARGET.Owner); \\\n\n]\n\n\n\n\n\nVOMS attribute\n\n\nTo filter jobs based on the subject of the job's proxy, your routes will need a \nRequirements\n expression using the incoming job's \nx509UserProxyFirstFQAN\n attribute. The following entry routes jobs to the PBS batch system if the proxy subject contains \n/cms/Role=Pilot\n:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nFiltering by VOMS attribute (regex)\n; \\\n     \nRequirements = regexp(\n\\/cms\\/Role\\=pilot\n, TARGET.x509UserProxyFirstFQAN); \\\n\n]\n\n\n\n\n\nSetting a default\u2026\n\n\nThis section outlines how to set default job limits, memory, cores, queue, and maximum walltime. For an example on how users can override these defaults, consult \nthis document\n.\n\n\nMaximum number of jobs\n\n\nTo set a default limit to the maximum number of jobs per route, you can edit the configuration variable \nCONDORCE_MAX_JOBS\n in \n/etc/condor-ce/config.d/01-ce-router.conf\n:\n\n\nCONDORCE_MAX_JOBS = 10000\n\n\n\n\n\n\n\nNote\n\n\nThe above configuration  is to be placed directly into the HTCondor-CE configuration, not into a job route.\n\n\n\n\nMaximum memory\n\n\nTo set a default maximum memory for routed jobs, set the attribute \ndefault_maxMemory\n:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nRequest memory\n; \\\n     /* Set the requested memory to 1 GB */ \\\n     \nset_default_maxMemory = 1000; \\\n\n]\n\n\n\n\n\nNumber of cores to request\n\n\nTo set a default number of cores for routed jobs, set the attribute \ndefault_xcount\n:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nRequest CPU\n; \\\n     /* Set the requested cores to 8 */ \\\n     \nset_default_xcount = 8; \\\n\n]\n\n\n\n\n\nMaximum walltime\n\n\nTo set a default maximum walltime (in minutes) for routed jobs, set the attribute \ndefault_maxWallTime\n:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nSetting WallTime\n; \\\n     /* Set the max walltime to 1 hr */ \\\n     \nset_default_maxWallTime = 60; \\\n\n]\n\n\n\n\n\nEditing attributes\u2026\n\n\nThe following functions are operations that affect job attributes and are evaluated in the following order:\n\n\n\n\ncopy_*\n\n\ndelete_*\n\n\nset_*\n\n\neval_set_*\n\n\n\n\nAfter each job route\u2019s ClassAd is \nconstructed\n, the above operations are evaluated in order. For example, if the attribute \nfoo\n is set using \neval_set_foo\n in the \nJOB_ROUTER_DEFAULTS\n, you'll be unable to use \ndelete_foo\n to remote it from your jobs since the attribute is set using \neval_set_foo\n after the deletion occurs according to the order of operations. To get around this, we can take advantage of the fact that operations defined in \nJOB_ROUTER_DEFAULTS\n get overriden by the same operation in \nJOB_ROUTER_ENTRIES\n. So to 'delete' \nfoo\n, we would add \neval_set_foo = \"\"\n to the route in the \nJOB_ROUTER_ENTRIES\n, resulting in \nfoo\n being absent from the routed job.\n\n\nMore documentation can be found in the \nHTCondor manual\n.\n\n\nCopying attributes\n\n\nTo copy the value of an attribute of the incoming job to an attribute of the routed job, use \ncopy_\n. The following route copies the \nenvironment\n attribute of the incoming job and sets the attribute \nOriginal_Environment\n on the routed job to the same value:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nCopying attributes\n; \\\n     \ncopy_environment = \nOriginal_Environment\n; \\\n\n]\n\n\n\n\n\nRemoving attributes\n\n\nTo remove an attribute of the incoming job from the routed job, use \ndelete_\n. The following route removes the \nenvironment\n attribute from the routed job:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nCopying attributes\n; \\\n     \ndelete_environment = True; \\\n\n]\n\n\n\n\n\nSetting attributes\n\n\nTo set an attribute on the routed job, use \nset_\n. The following route sets the Job's \nRank\n attribute to 5:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nSetting an attribute\n; \\\n     \nset_Rank = 5; \\\n\n]\n\n\n\n\n\nSetting attributes with ClassAd expressions\n\n\nTo set an attribute to a ClassAd expression to be evaluated, use \nset_eval\n. The following route sets the \nExperiment\n attribute to \natlas.osguser\n if the Owner of the incoming job is \nosguser\n:\n\n\n\n\nNote\n\n\nIf a value is set in JOB_ROUTER_DEFAULTS with \neval_set_\nvariable\n, override it by using \neval_set_\nvariable\n in the \nJOB_ROUTER_ENTRIES\n.\n\n\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nSetting an attribute with a !ClassAd expression\n; \\\n     \neval_set_Experiment = strcat(\natlas.\n, Owner); \\\n\n]\n\n\n\n\n\nLimiting the number of...\n\n\nThis section outlines how to limit the number of total or idle jobs in a specific route (i.e., if this limit is reached, jobs will no longer be placed in this route).\n\n\n\n\nNote\n\n\nIf you are using an HTCondor batch system, limiting the number of jobs is not the preferred solution: HTCondor manages fair share on its own via \nuser priorities and group accounting\n.\n\n\n\n\nTotal jobs\n\n\nTo set a limit on the number of jobs for a specific route, set the \nMaxJobs\n attribute:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nLimit the total number of jobs to 100\n; \\\n     \nMaxJobs = 100; \\\n\n] \\\n[ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nLimit the total number of jobs to 75\n; \\\n     \nMaxJobs = 75; \\\n\n]\n\n\n\n\n\nIdle jobs\n\n\nTo set a limit on the number of idle jobs for a specific route, set the \nMaxIdleJobs\n attribute:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name = \nLimit the total number of idle jobs to 100\n; \\\n     \nMaxIdleJobs = 100; \\\n\n] \\\n[ \\\n     TargetUniverse = 5; \\\n     name = \nLimit the total number of idle jobs to 75\n; \\\n     \nMaxIdleJobs = 75; \\\n\n]\n\n\n\n\n\nDebugging routes\n\n\nTo help debug expressions in your routes, you can use the \ndebug()\n function. First, set the debug mode for the JobRouter by editing a file in \n/etc/condor-ce/config.d/\n to read\n\n\nJOB_ROUTER_DEBUG = D_FULLDEBUG\n\n\n\n\n\nThen wrap the problematic attribute in \ndebug()\n:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nDebugging a difficult !ClassAd expression\n; \\\n     \neval_set_Experiment = debug(strcat(\natlas\n, Name)); \\\n\n]\n\n\n\n\n\nYou will find the debugging output in \n/var/log/condor-ce/JobRouterLog\n.\n\n\nRoutes for HTCondor Batch Systems\n\n\nThis section contains information about job routes that can be used if you are running an HTCondor batch system at your site.\n\n\nSetting periodic hold, release or remove\n\n\nTo release, remove or put a job on hold if it meets certain criteria, use the \nPERIODIC_*\n family of attributes. By default, periodic expressions are evaluated once every 300 seconds but this can be changed by setting \nPERIODIC_EXPR_INTERVAL\n in your CE's configuration.\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name = \nSetting periodic statements\n; \\\n     \n/* Puts the routed job on hold if the job\ns been idle and has been started at least once or if the job has tried to start more than once */ \\\n     set_Periodic_Hold = (NumJobStarts \n= 1 \n JobStatus == 1) || NumJobStarts \n 1; \\\n     /* Remove routed jobs if their walltime is longer than 3 days and 5 minutes */ \\\n     set_Periodic_Remove = ( RemoteWallClockTime \n (3*24*60*60 + 5*60) ); \\\n     /* Release routed jobs if the condor_starter couldn\nt start the executable and \nVMGAHP_ERR_INTERNAL\n is in the HoldReason */ \\\n     set_Periodic_Release = HoldReasonCode == 6 \n regexp(\nVMGAHP_ERR_INTERNAL\n, HoldReason); \\\n\n]\n\n\n\n\n\nSetting routed job requirements\n\n\nIf you need to set requirements on your routed job, you will need to use \nset_Requirements\n instead of \nRequirements\n. The \nRequirements\n attribute filters jobs coming into your CE into different job routes whereas \nset_requirements\n will set conditions on the routed job that must be met by the worker node it lands on. For more information on requirements, consult the \nHTCondor manual\n.\n\n\nTo ensure that your job lands on a Linux machine in your pool:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     \nset_Requirements =  OpSys == \nLINUX\n; \\\n\n]\n\n\n\n\n\nSetting accounting groups\n\n\nTo assign jobs to an HTCondor accounting group to manage fair share on your local batch system, we recommend using \nUID and ExtAttr tables\n.\n\n\nRoutes for non-HTCondor Batch Systems\n\n\nThis section contains information about job routes that can be used if you are running a non-HTCondor batch system at your site.\n\n\nSetting a default batch queue\n\n\nTo set a default queue for routed jobs, set the attribute \ndefault_queue\n:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nSetting batch system queues\n; \\\n     \nset_default_queue = \nosg_queue\n; \\\n\n]\n\n\n\n\n\nSetting batch system directives\n\n\nTo write batch system directives that are not supported in the route examples above, you will need to edit the job submit script for your local batch system in \n/etc/blahp/\n (e.g., if your local batch system is PBS, edit \n/etc/blahp/pbs_local_submit_attributes.sh\n). This file is sourced during submit time and anything printed to stdout is appended to the batch system job submit script. ClassAd attributes can be passed from the routed job to the local submit attributes script via the \ndefault_remote_cerequirements\n attribute, which can take the following form:\n\n\ndefault_remote_cerequirements = \nfoo == X \n bar == \\\nY\\\n \n ...\n\n\n\n\n\n\nThis sets \nfoo\n to value \nX\n and \nbar\n to the string \nY\n (escaped double-quotes are required for string values) in the environment of the local submit attributes script. The following example sets the maximum walltime to 1 hour and the accounting group to the \nx509UserProxyFirstFQAN\n attribute of the job submitted to a PBS batch system\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nSetting job submit variables\n; \\\n     \nset_default_remote_cerequirements = strcat(\nWalltime == 3600 \n AccountingGroup == \\\n, x509UserProxyFirstFQAN, \n\\\n); \\\n\n]\n\n\n\n\n\nWith \n/etc/blahp/pbs_local_submit_attributes.sh\n containing.\n\n\n1\n2\n3\n#!/bin/bash\n\n\necho\n \n#PBS -l walltime=\n$Walltime\n\n\necho\n \n#PBS -A \n$AccountingGroup\n\n\n\n\n\n\n\nThis results in the following being appended to the script that gets submitted to your batch system:\n\n\n#PBS -l walltime=3600\n#PBS -A \nCE job\ns x509UserProxyFirstFQAN attribute\n\n\n\n\n\n\nReference\n\n\nHere are some other HTCondor-CE documents that might be helpful:\n\n\n\n\nHTCondor-CE overview and architecture\n\n\nInstalling HTCondor-CE\n\n\nThe HTCondor-CE troubleshooting guide\n\n\nSubmitting jobs to HTCondor-CE\n\n\n\n\nExample Configurations\n\n\nAGLT2's job routes\n\n\nAtlas AGLT2 is using an HTCondor batch system. Here are some things to note about their routes.\n\n\n\n\nSetting various HTCondor-specific attributes like \nRank\n, \nAccountingGroup\n, \nJobPrio\n and \nPeriodic_Remove\n (see the \nHTCondor manual\n for more). Some of these are site-specific like \nLastandFrac\n, \nIdleMP8Pressure\n, \nlocalQue\n, \nIsAnalyJob\n and \nJobMemoryLimit\n.\n\n\nThere is a difference between \nRequirements\n and \nset_requirements\n. The \nRequirements\n attribute matches jobs to specific routes while the \nset_requirements\n sets the \nRequirements\n attribute on the \nrouted\n job, which confines which machines that the routed job can land on.\n\n\n\n\nSource: \nhttps://www.aglt2.org/wiki/bin/view/AGLT2/CondorCE#The_JobRouter_configuration_file_content\n\n\nJOB_ROUTER_ENTRIES\n \n=\n \n\\\n\n/* Still to \ndo\n on all routes, get job requirements and add them here */ \n\\\n\n/* ***** Route no \n1\n ***** */ \n\\\n\n/* ***** Analysis queue ***** */ \n\\\n\n  \n[\n \n\\\n\n    \nGridResource\n \n=\n \ncondor localhost localhost\n;\n \n\\\n\n    \neval_set_GridResource\n \n=\n strcat\n(\ncondor \n, \n$(\nFULL_HOSTNAME\n)\n, \n \n$(\nJOB_ROUTER_SCHEDD2_POOL\n)\n)\n;\n \n\\\n\n    \nRequirements\n \n=\n target.queue\n==\nanaly\n;\n \n\\\n\n    \nName\n \n=\n \nAnalysis Queue\n;\n \n\\\n\n    \nTargetUniverse\n \n=\n \n5\n;\n \n\\\n\n    \neval_set_IdleMP8Pressure\n \n=\n \n$(\nIdleMP8Pressure\n)\n;\n \n\\\n\n    \neval_set_LastAndFrac\n \n=\n \n$(\nLastAndFrac\n)\n;\n \n\\\n\n    \nset_requirements\n \n=\n \n(\n \n(\n TARGET.TotalDisk \n=\n?\n=\n undefined \n)\n \n||\n \n(\n TARGET.TotalDisk \n=\n \n21000000\n \n)\n \n)\n \n \n(\n TARGET.Arch \n==\n \nX86_64\n \n)\n \n \n(\n TARGET.OpSys \n==\n \nLINUX\n \n)\n \n \n(\n TARGET.Disk \n=\n RequestDisk \n)\n \n \n(\n TARGET.Memory \n=\n RequestMemory \n)\n \n \n(\n TARGET.HasFileTransfer \n)\n \n \n(\nIfThenElse\n((\nOwner\n \n==\n \natlasconnect\n \n||\n \nOwner\n \n==\n \nmuoncal\n)\n,IfThenElse\n(\nIdleMP8Pressure,\n(\nTARGET.PARTITIONED \n=\n!\n=\n TRUE\n)\n,True\n)\n,IfThenElse\n(\nLastAndFrac,\n(\nTARGET.PARTITIONED \n=\n!\n=\n TRUE\n)\n,True\n)))\n;\n \n\\\n\n    \neval_set_AccountingGroup\n \n=\n strcat\n(\ngroup_gatekpr.prod.analy.\n,Owner\n)\n;\n \n\\\n\n    \nset_localQue\n \n=\n \nAnalysis\n;\n \n\\\n\n    \nset_IsAnalyJob\n \n=\n True\n;\n \n\\\n\n    \nset_JobPrio\n \n=\n \n5\n;\n \n\\\n\n    \nset_Rank\n \n=\n \n(\nSlotID + \n(\n64\n-TARGET.DETECTED_CORES\n))\n*1.0\n;\n \n\\\n\n    \nset_JobMemoryLimit\n \n=\n \n4194000\n;\n \n\\\n\n    \nset_Periodic_Remove\n \n=\n \n(\n \n(\n RemoteWallClockTime \n \n(\n3\n*24*60*60 + \n5\n*60\n)\n \n)\n \n||\n \n(\nImageSize \n JobMemoryLimit\n)\n \n)\n;\n \n\\\n\n  \n]\n \n\\\n\n/* ***** Route no \n2\n ***** */ \n\\\n\n/* ***** splitterNT queue ***** */ \n\\\n\n  \n[\n \n\\\n\n    \nGridResource\n \n=\n \ncondor localhost localhost\n;\n \n\\\n\n    \neval_set_GridResource\n \n=\n strcat\n(\ncondor \n, \n$(\nFULL_HOSTNAME\n)\n, \n \n$(\nJOB_ROUTER_SCHEDD2_POOL\n)\n)\n;\n \n\\\n\n    \nRequirements\n \n=\n target.queue \n==\n \nsplitterNT\n;\n \n\\\n\n    \nName\n \n=\n \nSplitter ntuple queue\n;\n \n\\\n\n    \nTargetUniverse\n \n=\n \n5\n;\n \n\\\n\n    \nset_requirements\n \n=\n \n(\n \n(\n TARGET.TotalDisk \n=\n?\n=\n undefined \n)\n \n||\n \n(\n TARGET.TotalDisk \n=\n \n21000000\n \n)\n \n)\n \n \n(\n TARGET.Arch \n==\n \nX86_64\n \n)\n \n \n(\n TARGET.OpSys \n==\n \nLINUX\n \n)\n \n \n(\n TARGET.Disk \n=\n RequestDisk \n)\n \n \n(\n TARGET.Memory \n=\n RequestMemory \n)\n \n \n(\n TARGET.HasFileTransfer \n)\n;\n \n\\\n\n    \neval_set_AccountingGroup\n \n=\n \ngroup_calibrate.muoncal\n;\n \n\\\n\n    \nset_localQue\n \n=\n \nSplitter\n;\n \n\\\n\n    \nset_IsAnalyJob\n \n=\n False\n;\n \n\\\n\n    \nset_JobPrio\n \n=\n \n10\n;\n \n\\\n\n    \nset_Rank\n \n=\n \n(\nSlotID + \n(\n64\n-TARGET.DETECTED_CORES\n))\n*1.0\n;\n \n\\\n\n    \nset_JobMemoryLimit\n \n=\n \n4194000\n;\n \n\\\n\n    \nset_Periodic_Remove\n \n=\n \n(\n \n(\n RemoteWallClockTime \n \n(\n3\n*24*60*60 + \n5\n*60\n)\n \n)\n \n||\n \n(\nImageSize \n JobMemoryLimit\n)\n \n)\n;\n \n\\\n\n  \n]\n \n\\\n\n/* ***** Route no \n3\n ***** */ \n\\\n\n/* ***** splitter queue ***** */ \n\\\n\n  \n[\n \n\\\n\n    \nGridResource\n \n=\n \ncondor localhost localhost\n;\n \n\\\n\n    \neval_set_GridResource\n \n=\n strcat\n(\ncondor \n, \n$(\nFULL_HOSTNAME\n)\n, \n \n$(\nJOB_ROUTER_SCHEDD2_POOL\n)\n)\n;\n \n\\\n\n    \nRequirements\n \n=\n target.queue \n==\n \nsplitter\n;\n \n\\\n\n    \nName\n \n=\n \nSplitter queue\n;\n \n\\\n\n    \nTargetUniverse\n \n=\n \n5\n;\n \n\\\n\n    \nset_requirements\n \n=\n \n(\n \n(\n TARGET.TotalDisk \n=\n?\n=\n undefined \n)\n \n||\n \n(\n TARGET.TotalDisk \n=\n \n21000000\n \n)\n \n)\n \n \n(\n TARGET.Arch \n==\n \nX86_64\n \n)\n \n \n(\n TARGET.OpSys \n==\n \nLINUX\n \n)\n \n \n(\n TARGET.Disk \n=\n RequestDisk \n)\n \n \n(\n TARGET.Memory \n=\n RequestMemory \n)\n \n \n(\n TARGET.HasFileTransfer \n)\n;\n \n\\\n\n    \neval_set_AccountingGroup\n \n=\n \ngroup_calibrate.muoncal\n;\n \n\\\n\n    \nset_localQue\n \n=\n \nSplitter\n;\n \n\\\n\n    \nset_IsAnalyJob\n \n=\n False\n;\n \n\\\n\n    \nset_JobPrio\n \n=\n \n15\n;\n \n\\\n\n    \nset_Rank\n \n=\n \n(\nSlotID + \n(\n64\n-TARGET.DETECTED_CORES\n))\n*1.0\n;\n \n\\\n\n    \nset_JobMemoryLimit\n \n=\n \n4194000\n;\n \n\\\n\n    \nset_Periodic_Remove\n \n=\n \n(\n \n(\n RemoteWallClockTime \n \n(\n3\n*24*60*60 + \n5\n*60\n)\n \n)\n \n||\n \n(\nImageSize \n JobMemoryLimit\n)\n \n)\n;\n \n\\\n\n  \n]\n \n\\\n\n/* ***** Route no \n4\n ***** */ \n\\\n\n/* ***** xrootd queue ***** */ \n\\\n\n  \n[\n \n\\\n\n    \nGridResource\n \n=\n \ncondor localhost localhost\n;\n \n\\\n\n    \neval_set_GridResource\n \n=\n strcat\n(\ncondor \n, \n$(\nFULL_HOSTNAME\n)\n, \n \n$(\nJOB_ROUTER_SCHEDD2_POOL\n)\n)\n;\n \n\\\n\n    \nRequirements\n \n=\n target.queue \n==\n \nxrootd\n;\n \n\\\n\n    \nName\n \n=\n \nXrootd queue\n;\n \n\\\n\n    \nTargetUniverse\n \n=\n \n5\n;\n \n\\\n\n    \nset_requirements\n \n=\n \n(\n \n(\n TARGET.TotalDisk \n=\n?\n=\n undefined \n)\n \n||\n \n(\n TARGET.TotalDisk \n=\n \n21000000\n \n)\n \n)\n \n \n(\n TARGET.Arch \n==\n \nX86_64\n \n)\n \n \n(\n TARGET.OpSys \n==\n \nLINUX\n \n)\n \n \n(\n TARGET.Disk \n=\n RequestDisk \n)\n \n \n(\n TARGET.Memory \n=\n RequestMemory \n)\n \n \n(\n TARGET.HasFileTransfer \n)\n;\n \n\\\n\n    \neval_set_AccountingGroup\n \n=\n strcat\n(\ngroup_gatekpr.prod.analy.\n,Owner\n)\n;\n \n\\\n\n    \nset_localQue\n \n=\n \nAnalysis\n;\n \n\\\n\n    \nset_IsAnalyJob\n \n=\n True\n;\n \n\\\n\n    \nset_JobPrio\n \n=\n \n35\n;\n \n\\\n\n    \nset_Rank\n \n=\n \n(\nSlotID + \n(\n64\n-TARGET.DETECTED_CORES\n))\n*1.0\n;\n \n\\\n\n    \nset_JobMemoryLimit\n \n=\n \n4194000\n;\n \n\\\n\n    \nset_Periodic_Remove\n \n=\n \n(\n \n(\n RemoteWallClockTime \n \n(\n3\n*24*60*60 + \n5\n*60\n)\n \n)\n \n||\n \n(\nImageSize \n JobMemoryLimit\n)\n \n)\n;\n \n\\\n\n  \n]\n \n\\\n\n/* ***** Route no \n5\n ***** */ \n\\\n\n/* ***** Tier3Test queue ***** */ \n\\\n\n  \n[\n \n\\\n\n    \nGridResource\n \n=\n \ncondor localhost localhost\n;\n \n\\\n\n    \neval_set_GridResource\n \n=\n strcat\n(\ncondor \n, \n$(\nFULL_HOSTNAME\n)\n, \n \n$(\nJOB_ROUTER_SCHEDD2_POOL\n)\n)\n;\n \n\\\n\n    \nRequirements\n \n=\n target.queue \n==\n \nTier3Test\n;\n \n\\\n\n    \nName\n \n=\n \nTier3 Test Queue\n;\n \n\\\n\n    \nTargetUniverse\n \n=\n \n5\n;\n \n\\\n\n    \nset_requirements\n \n=\n \n(\n \n(\n TARGET.TotalDisk \n=\n?\n=\n undefined \n)\n \n||\n \n(\n TARGET.TotalDisk \n=\n \n21000000\n \n)\n \n)\n \n \n(\n TARGET.Arch \n==\n \nX86_64\n \n)\n \n \n(\n TARGET.OpSys \n==\n \nLINUX\n \n)\n \n \n(\n TARGET.Disk \n=\n RequestDisk \n)\n \n \n(\n TARGET.Memory \n=\n RequestMemory \n)\n \n \n(\n TARGET.HasFileTransfer \n)\n \n \n(\n \nIS_TIER3_TEST_QUEUE\n \n=\n?\n=\n True \n)\n;\n \n\\\n\n    \neval_set_AccountingGroup\n \n=\n strcat\n(\ngroup_gatekpr.prod.analy.\n,Owner\n)\n;\n \n\\\n\n    \nset_localQue\n \n=\n \nTier3Test\n;\n \n\\\n\n    \nset_IsTier3TestJob\n \n=\n True\n;\n \n\\\n\n    \nset_IsAnalyJob\n \n=\n True\n;\n \n\\\n\n    \nset_JobPrio\n \n=\n \n20\n;\n \n\\\n\n    \nset_Rank\n \n=\n \n(\nSlotID + \n(\n64\n-TARGET.DETECTED_CORES\n))\n*1.0\n;\n \n\\\n\n    \nset_JobMemoryLimit\n \n=\n \n4194000\n;\n \n\\\n\n    \nset_Periodic_Remove\n \n=\n \n(\n \n(\n RemoteWallClockTime \n \n(\n3\n*24*60*60 + \n5\n*60\n)\n \n)\n \n||\n \n(\nImageSize \n JobMemoryLimit\n)\n \n)\n;\n \n\\\n\n  \n]\n \n\\\n\n/* ***** Route no \n6\n ***** */ \n\\\n\n/* ***** mp8 queue ***** */ \n\\\n\n  \n[\n \n\\\n\n    \nGridResource\n \n=\n \ncondor localhost localhost\n;\n \n\\\n\n    \neval_set_GridResource\n \n=\n strcat\n(\ncondor \n, \n$(\nFULL_HOSTNAME\n)\n, \n \n$(\nJOB_ROUTER_SCHEDD2_POOL\n)\n)\n;\n \n\\\n\n    \nRequirements\n \n=\n target.queue\n==\nmp8\n;\n \n\\\n\n    \nName\n \n=\n \nMCORE Queue\n;\n \n\\\n\n    \nTargetUniverse\n \n=\n \n5\n;\n \n\\\n\n    \nset_requirements\n \n=\n \n(\n \n(\n TARGET.TotalDisk \n=\n?\n=\n undefined \n)\n \n||\n \n(\n TARGET.TotalDisk \n=\n \n21000000\n \n)\n \n)\n \n \n(\n TARGET.Arch \n==\n \nX86_64\n \n)\n \n \n(\n TARGET.OpSys \n==\n \nLINUX\n \n)\n \n \n(\n TARGET.Disk \n=\n RequestDisk \n)\n \n \n(\n TARGET.Memory \n=\n RequestMemory \n)\n \n \n(\n TARGET.HasFileTransfer \n)\n \n \n((\n TARGET.Cpus \n==\n \n8\n \n TARGET.CPU_TYPE \n=\n?\n=\n \nmp8\n \n)\n \n||\n TARGET.PARTITIONED \n=\n?\n=\n True \n)\n;\n \n\\\n\n    \neval_set_AccountingGroup\n \n=\n strcat\n(\ngroup_gatekpr.prod.mcore.\n,Owner\n)\n;\n \n\\\n\n    \nset_localQue\n \n=\n \nMP8\n;\n \n\\\n\n    \nset_IsAnalyJob\n \n=\n False\n;\n \n\\\n\n    \nset_JobPrio\n \n=\n \n25\n;\n \n\\\n\n    \nset_Rank\n \n=\n \n0\n.0\n;\n \n\\\n\n    \neval_set_RequestCpus\n \n=\n \n8\n;\n \n\\\n\n    \nset_JobMemoryLimit\n \n=\n \n33552000\n;\n \n\\\n\n    \nset_Slot_Type\n \n=\n \nmp8\n;\n \n\\\n\n    \nset_Periodic_Remove\n \n=\n \n(\n \n(\n RemoteWallClockTime \n \n(\n3\n*24*60*60 + \n5\n*60\n)\n \n)\n \n||\n \n(\nImageSize \n JobMemoryLimit\n)\n \n)\n;\n \n\\\n\n  \n]\n \n\\\n\n/* ***** Route no \n7\n ***** */ \n\\\n\n/* ***** Installation queue, triggered by usatlas2 user ***** */ \n\\\n\n  \n[\n \n\\\n\n    \nGridResource\n \n=\n \ncondor localhost localhost\n;\n \n\\\n\n    \neval_set_GridResource\n \n=\n strcat\n(\ncondor \n, \n$(\nFULL_HOSTNAME\n)\n, \n \n$(\nJOB_ROUTER_SCHEDD2_POOL\n)\n)\n;\n \n\\\n\n    \nRequirements\n \n=\n target.queue is undefined \n target.Owner \n==\n \nusatlas2\n;\n \n\\\n\n    \nName\n \n=\n \nInstall Queue\n;\n \n\\\n\n    \nTargetUniverse\n \n=\n \n5\n;\n \n\\\n\n    \nset_requirements\n \n=\n \n(\n \n(\n TARGET.TotalDisk \n=\n?\n=\n undefined \n)\n \n||\n \n(\n TARGET.TotalDisk \n=\n \n21000000\n \n)\n \n)\n \n \n(\n TARGET.Arch \n==\n \nX86_64\n \n)\n \n \n(\n TARGET.OpSys \n==\n \nLINUX\n \n)\n \n \n(\n TARGET.Disk \n=\n RequestDisk \n)\n \n \n(\n TARGET.Memory \n=\n RequestMemory \n)\n \n \n(\n TARGET.HasFileTransfer \n)\n \n \n(\n TARGET.IS_INSTALL_QUE \n=\n?\n=\n True \n)\n \n \n(\nTARGET.AGLT2_SITE \n==\n \nUM\n \n)\n;\n \n\\\n\n    \neval_set_AccountingGroup\n \n=\n strcat\n(\ngroup_gatekpr.other.\n,Owner\n)\n;\n \n\\\n\n    \nset_localQue\n \n=\n \nDefault\n;\n \n\\\n\n    \nset_IsAnalyJob\n \n=\n False\n;\n \n\\\n\n    \nset_IsInstallJob\n \n=\n True\n;\n \n\\\n\n    \nset_JobPrio\n \n=\n \n15\n;\n \n\\\n\n    \nset_Rank\n \n=\n \n(\nSlotID + \n(\n64\n-TARGET.DETECTED_CORES\n))\n*1.0\n;\n \n\\\n\n    \nset_JobMemoryLimit\n \n=\n \n4194000\n;\n \n\\\n\n    \nset_Periodic_Remove\n \n=\n \n(\n \n(\n RemoteWallClockTime \n \n(\n3\n*24*60*60 + \n5\n*60\n)\n \n)\n \n||\n \n(\nImageSize \n JobMemoryLimit\n)\n \n)\n;\n \n\\\n\n  \n]\n \n\\\n\n/* ***** Route no \n8\n ***** */ \n\\\n\n/* ***** Default queue \nfor\n usatlas1 user ***** */ \n\\\n\n  \n[\n \n\\\n\n    \nGridResource\n \n=\n \ncondor localhost localhost\n;\n \n\\\n\n    \neval_set_GridResource\n \n=\n strcat\n(\ncondor \n, \n$(\nFULL_HOSTNAME\n)\n, \n \n$(\nJOB_ROUTER_SCHEDD2_POOL\n)\n)\n;\n \n\\\n\n    \nRequirements\n \n=\n target.queue is undefined \n regexp\n(\nusatlas1\n,target.Owner\n)\n;\n \n\\\n\n    \nName\n \n=\n \nATLAS Production Queue\n;\n \n\\\n\n    \nTargetUniverse\n \n=\n \n5\n;\n \n\\\n\n    \nset_requirements\n \n=\n \n(\n \n(\n TARGET.TotalDisk \n=\n?\n=\n undefined \n)\n \n||\n \n(\n TARGET.TotalDisk \n=\n \n21000000\n \n)\n \n)\n \n \n(\n TARGET.Arch \n==\n \nX86_64\n \n)\n \n \n(\n TARGET.OpSys \n==\n \nLINUX\n \n)\n \n \n(\n TARGET.Disk \n=\n RequestDisk \n)\n \n \n(\n TARGET.Memory \n=\n RequestMemory \n)\n \n \n(\n TARGET.HasFileTransfer \n)\n;\n \n\\\n\n    \neval_set_AccountingGroup\n \n=\n strcat\n(\ngroup_gatekpr.prod.prod.\n,Owner\n)\n;\n \n\\\n\n    \nset_localQue\n \n=\n \nDefault\n;\n \n\\\n\n    \nset_IsAnalyJob\n \n=\n False\n;\n \n\\\n\n    \nset_Rank\n \n=\n \n(\nSlotID + \n(\n64\n-TARGET.DETECTED_CORES\n))\n*1.0\n;\n \n\\\n\n    \nset_JobMemoryLimit\n \n=\n \n4194000\n;\n \n\\\n\n    \nset_Periodic_Remove\n \n=\n \n(\n \n(\n RemoteWallClockTime \n \n(\n3\n*24*60*60 + \n5\n*60\n)\n \n)\n \n||\n \n(\nImageSize \n JobMemoryLimit\n)\n \n)\n;\n \n\\\n\n  \n]\n \n\\\n\n/* ***** Route no \n9\n ***** */ \n\\\n\n/* ***** Default queue \nfor\n any other usatlas account ***** */ \n\\\n\n  \n[\n \n\\\n\n    \nGridResource\n \n=\n \ncondor localhost localhost\n;\n \n\\\n\n    \neval_set_GridResource\n \n=\n strcat\n(\ncondor \n, \n$(\nFULL_HOSTNAME\n)\n, \n \n$(\nJOB_ROUTER_SCHEDD2_POOL\n)\n)\n;\n \n\\\n\n    \nRequirements\n \n=\n target.queue is undefined \n \n(\nregexp\n(\nusatlas2\n,target.Owner\n)\n \n||\n regexp\n(\nusatlas3\n,target.Owner\n))\n;\n \n\\\n\n    \nName\n \n=\n \nOther ATLAS Production\n;\n \n\\\n\n    \nTargetUniverse\n \n=\n \n5\n;\n \n\\\n\n    \nset_requirements\n \n=\n \n(\n \n(\n TARGET.TotalDisk \n=\n?\n=\n undefined \n)\n \n||\n \n(\n TARGET.TotalDisk \n=\n \n21000000\n \n)\n \n)\n \n \n(\n TARGET.Arch \n==\n \nX86_64\n \n)\n \n \n(\n TARGET.OpSys \n==\n \nLINUX\n \n)\n \n \n(\n TARGET.Disk \n=\n RequestDisk \n)\n \n \n(\n TARGET.Memory \n=\n RequestMemory \n)\n \n \n(\n TARGET.HasFileTransfer \n)\n;\n \n\\\n\n    \neval_set_AccountingGroup\n \n=\n strcat\n(\ngroup_gatekpr.other.\n,Owner\n)\n;\n \n\\\n\n    \nset_localQue\n \n=\n \nDefault\n;\n \n\\\n\n    \nset_IsAnalyJob\n \n=\n False\n;\n \n\\\n\n    \nset_Rank\n \n=\n \n(\nSlotID + \n(\n64\n-TARGET.DETECTED_CORES\n))\n*1.0\n;\n \n\\\n\n    \nset_JobMemoryLimit\n \n=\n \n4194000\n;\n \n\\\n\n    \nset_Periodic_Remove\n \n=\n \n(\n \n(\n RemoteWallClockTime \n \n(\n3\n*24*60*60 + \n5\n*60\n)\n \n)\n \n||\n \n(\nImageSize \n JobMemoryLimit\n)\n \n)\n;\n \n\\\n\n  \n]\n \n\\\n\n/* ***** Route no \n10\n ***** */ \n\\\n\n/* ***** Anything \nelse\n. Set queue as Default and assign to other VOs  ***** */ \n\\\n\n  \n[\n \n\\\n\n    \nGridResource\n \n=\n \ncondor localhost localhost\n;\n \n\\\n\n    \neval_set_GridResource\n \n=\n strcat\n(\ncondor \n, \n$(\nFULL_HOSTNAME\n)\n, \n \n$(\nJOB_ROUTER_SCHEDD2_POOL\n)\n)\n;\n \n\\\n\n    \nRequirements\n \n=\n target.queue is undefined \n ifThenElse\n(\nregexp\n(\nusatlas\n,target.Owner\n)\n,false,true\n)\n;\n \n\\\n\n    \nName\n \n=\n \nOther Jobs\n;\n \n\\\n\n    \nTargetUniverse\n \n=\n \n5\n;\n \n\\\n\n    \nset_requirements\n \n=\n \n(\n \n(\n TARGET.TotalDisk \n=\n?\n=\n undefined \n)\n \n||\n \n(\n TARGET.TotalDisk \n=\n \n21000000\n \n)\n \n)\n \n \n(\n TARGET.Arch \n==\n \nX86_64\n \n)\n \n \n(\n TARGET.OpSys \n==\n \nLINUX\n \n)\n \n \n(\n TARGET.Disk \n=\n RequestDisk \n)\n \n \n(\n TARGET.Memory \n=\n RequestMemory \n)\n \n \n(\n TARGET.HasFileTransfer \n)\n;\n \n\\\n\n    \neval_set_AccountingGroup\n \n=\n strcat\n(\ngroup_VOgener.\n,Owner\n)\n;\n \n\\\n\n    \nset_localQue\n \n=\n \nDefault\n;\n \n\\\n\n    \nset_IsAnalyJob\n \n=\n False\n;\n \n\\\n\n    \nset_Rank\n \n=\n \n(\nSlotID + \n(\n64\n-TARGET.DETECTED_CORES\n))\n*1.0\n;\n \n\\\n\n    \nset_JobMemoryLimit\n \n=\n \n4194000\n;\n \n\\\n\n    \nset_Periodic_Remove\n \n=\n \n(\n \n(\n RemoteWallClockTime \n \n(\n3\n*24*60*60 + \n5\n*60\n)\n \n)\n \n||\n \n(\nImageSize \n JobMemoryLimit\n)\n \n)\n;\n \n\\\n\n  \n]\n\n\n\n\n\n\nBNL's job routes\n\n\nAtlas BNL T1, they are using an HTCondor batch system. Here are some things to note about their routes:\n\n\n\n\nSetting various HTCondor-specific attributes like \nJobLeaseDuration\n, \nRequirements\n and \nPeriodic_Hold\n (see the \nHTCondor manual\n for more). Some of these are site-specific like \nRACF_Group\n, \nExperiment\n, \nJob_Type\n and \nVO\n.\n\n\nJobs are split into different routes based on the \nGlideIn\n queue that they're in.\n\n\nThere is a difference between \nRequirements\n and \nset_requirements\n. The \nRequirements\n attribute matches \nincoming\n jobs to specific routes while the \nset_requirements\n sets the \nRequirements\n attribute on the \nrouted\n job, which confines which machines that the routed job can land on.\n\n\n\n\nSource: \nhttp://www.usatlas.bnl.gov/twiki/bin/view/Admins/HTCondorCE.html\n\n\n###############################################################################\n\n\n#\n\n\n# HTCondor-CE HTCondor batch system configuration file.\n\n\n#\n\n\n###############################################################################\n\n\n\n# Submit the job to the site Condor\n\n\n\nJOB_ROUTER_ENTRIES\n \n=\n \n\\\n\n   \n[\n \n\\\n\n     \nGridResource\n \n=\n \ncondor localhost localhost\n;\n \n\\\n\n     \neval_set_GridResource\n \n=\n strcat\n(\ncondor \n, \n$(\nFULL_HOSTNAME\n)\n, \n$(\nFULL_HOSTNAME\n)\n)\n;\n \n\\\n\n     \nTargetUniverse\n \n=\n \n5\n;\n \n\\\n\n     \nname\n \n=\n \nBNL_Condor_Pool_long\n;\n \n\\\n\n     \nRequirements\n \n=\n target.queue\n==\nanalysis.long\n;\n \n\\\n\n     \neval_set_RACF_Group\n \n=\n \nlong\n;\n \n\\\n\n     \nset_Experiment\n \n=\n \natlas\n;\n \n\\\n\n     \nset_requirements\n \n=\n \n(\n \n(\n \nArch\n \n==\n \nINTEL\n \n||\n \nArch\n \n==\n \nX86_64\n \n)\n \n \n(\n \nCPU_Experiment\n \n==\n \natlas\n \n)\n \n)\n \n \n(\n TARGET.OpSys \n==\n \nLINUX\n \n)\n \n \n(\n TARGET.Disk \n=\n RequestDisk \n)\n \n \n(\n TARGET.Memory \n=\n RequestMemory \n)\n \n \n(\n TARGET.HasFileTransfer \n)\n;\n \n\\\n\n     \nset_Job_Type\n \n=\n \ncas\n;\n \n\\\n\n     \nset_JobLeaseDuration\n \n=\n \n3600\n;\n \n\\\n\n     \nset_Periodic_Hold\n \n=\n \n(\nNumJobStarts \n=\n \n1\n \n \nJobStatus\n \n==\n \n1\n)\n \n||\n NumJobStarts \n \n1\n;\n \n\\\n\n     \neval_set_VO\n \n=\n x509UserProxyVOName\n;\n \n\\\n\n   \n]\n \n\\\n\n   \n[\n \n\\\n\n     \nGridResource\n \n=\n \ncondor localhost localhost\n;\n \n\\\n\n     \neval_set_GridResource\n \n=\n strcat\n(\ncondor \n, \n$(\nFULL_HOSTNAME\n)\n, \n$(\nFULL_HOSTNAME\n)\n)\n;\n \n\\\n\n     \nTargetUniverse\n \n=\n \n5\n;\n \n\\\n\n     \nname\n \n=\n \nBNL_Condor_Pool_short\n;\n \n\\\n\n     \nRequirements\n \n=\n target.queue\n==\nanalysis.short\n;\n \n\\\n\n     \neval_set_RACF_Group\n \n=\n \nshort\n;\n \n\\\n\n     \nset_Experiment\n \n=\n \natlas\n;\n \n\\\n\n     \nset_requirements\n \n=\n \n(\n \n(\n \nArch\n \n==\n \nINTEL\n \n||\n \nArch\n \n==\n \nX86_64\n \n)\n \n \n(\n \nCPU_Experiment\n \n==\n \natlas\n \n)\n \n)\n \n \n(\n TARGET.OpSys \n==\n \nLINUX\n \n)\n \n \n(\n TARGET.Disk \n=\n RequestDisk \n)\n \n \n(\n TARGET.Memory \n=\n RequestMemory \n)\n \n \n(\n TARGET.HasFileTransfer \n)\n;\n \n\\\n\n     \nset_Job_Type\n \n=\n \ncas\n;\n \n\\\n\n     \nset_JobLeaseDuration\n \n=\n \n3600\n;\n \n\\\n\n     \nset_Periodic_Hold\n \n=\n \n(\nNumJobStarts \n=\n \n1\n \n \nJobStatus\n \n==\n \n1\n)\n \n||\n NumJobStarts \n \n1\n;\n \n\\\n\n     \neval_set_VO\n \n=\n x509UserProxyVOName\n;\n \n\\\n\n   \n]\n \n\\\n\n   \n[\n \n\\\n\n     \nGridResource\n \n=\n \ncondor localhost localhost\n;\n \n\\\n\n     \neval_set_GridResource\n \n=\n strcat\n(\ncondor \n, \n$(\nFULL_HOSTNAME\n)\n, \n$(\nFULL_HOSTNAME\n)\n)\n;\n \n\\\n\n     \nTargetUniverse\n \n=\n \n5\n;\n \n\\\n\n     \nname\n \n=\n \nBNL_Condor_Pool_grid\n;\n \n\\\n\n     \nRequirements\n \n=\n target.queue\n==\ngrid\n;\n \n\\\n\n     \neval_set_RACF_Group\n \n=\n \ngrid\n;\n \n\\\n\n     \nset_Experiment\n \n=\n \natlas\n;\n \n\\\n\n     \nset_requirements\n \n=\n \n(\n \n(\n \nArch\n \n==\n \nINTEL\n \n||\n \nArch\n \n==\n \nX86_64\n \n)\n \n \n(\n \nCPU_Experiment\n \n==\n \natlas\n \n)\n \n)\n \n \n(\n TARGET.OpSys \n==\n \nLINUX\n \n)\n \n \n(\n TARGET.Disk \n=\n RequestDisk \n)\n \n \n(\n TARGET.Memory \n=\n RequestMemory \n)\n \n \n(\n TARGET.HasFileTransfer \n)\n;\n \n\\\n\n     \nset_Job_Type\n \n=\n \ncas\n;\n \n\\\n\n     \nset_JobLeaseDuration\n \n=\n \n3600\n;\n \n\\\n\n     \nset_Periodic_Hold\n \n=\n \n(\nNumJobStarts \n=\n \n1\n \n \nJobStatus\n \n==\n \n1\n)\n \n||\n NumJobStarts \n \n1\n;\n \n\\\n\n     \neval_set_VO\n \n=\n x509UserProxyVOName\n;\n \n\\\n\n   \n]\n \n\\\n\n   \n[\n \n\\\n\n     \nGridResource\n \n=\n \ncondor localhost localhost\n;\n \n\\\n\n     \neval_set_GridResource\n \n=\n strcat\n(\ncondor \n, \n$(\nFULL_HOSTNAME\n)\n, \n$(\nFULL_HOSTNAME\n)\n)\n;\n \n\\\n\n     \nTargetUniverse\n \n=\n \n5\n;\n \n\\\n\n     \nname\n \n=\n \nBNL_Condor_Pool\n;\n \n\\\n\n     \nRequirements\n \n=\n target.queue is undefined\n;\n \n\\\n\n     \neval_set_RACF_Group\n \n=\n \ngrid\n;\n \n\\\n\n     \nset_requirements\n \n=\n \n(\n \n(\n \nArch\n \n==\n \nINTEL\n \n||\n \nArch\n \n==\n \nX86_64\n \n)\n \n \n(\n \nCPU_Experiment\n \n==\n \nrcf\n \n)\n \n)\n \n \n(\n TARGET.OpSys \n==\n \nLINUX\n \n)\n \n \n(\n TARGET.Disk \n=\n RequestDisk \n)\n \n \n(\n TARGET.Memory \n=\n RequestMemory \n)\n \n \n(\n TARGET.HasFileTransfer \n)\n;\n \n\\\n\n     \nset_Experiment\n \n=\n \natlas\n;\n \n\\\n\n     \nset_Job_Type\n \n=\n \ncas\n;\n \n\\\n\n     \nset_JobLeaseDuration\n \n=\n \n3600\n;\n \n\\\n\n     \nset_Periodic_Hold\n \n=\n \n(\nNumJobStarts \n=\n \n1\n \n \nJobStatus\n \n==\n \n1\n)\n \n||\n NumJobStarts \n \n1\n;\n \n\\\n\n     \neval_set_VO\n \n=\n x509UserProxyVOName\n;\n \n\\\n\n   \n]", 
            "title": "Job Router Recipes"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#writing-routes-for-htcondor-ce", 
            "text": "The  JobRouter  is at the heart of HTCondor-CE and allows admins to transform and direct jobs to specific batch systems. Customizations are made in the form of job routes where each route corresponds to a separate job transformation: If an incoming job matches a job route's requirements, the route creates a transformed job (referred to as the 'routed job') that is then submitted to the batch system. The CE package comes with default routes located in  /etc/condor-ce/config.d/02-ce-*.conf  that provide enough basic functionality for a small site.  If you have needs beyond delegating all incoming jobs to your batch system as they are, this document provides examples of common job routes and job route problems.", 
            "title": "Writing Routes For HTCondor-CE"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#quirks-and-pitfalls", 
            "text": "The JobRouter matches jobs to routes in a round-robin fashion (for HTCondor versions   8.7.1). This means that if a job can match to multiple routes, it can be routed by any of them! So when writing job routes, make sure that they are exclusive to each other and that your jobs can only match to a single route.  If a value is set in  JOB_ROUTER_DEFAULTS  with  eval_set_ variable , override it by using  eval_set_ variable  in the  JOB_ROUTER_ENTRIES . Do this at your own risk as it may cause the CE to break.  Make sure to run  condor_ce_reconfig  after changing your routes, otherwise they will not take effect.  Before the last square bracket, make sure all lines end in a line continuation character (backslash). You can inspect the syntax of your routes with  condor_ce_config_val JOB_ROUTER_ENTRIES  to see if HTCondor-CE has ingested them properly.  Do  not  set the job environment through the JobRouter. Instead, add any changes to the  [Local Settings]  section in  /etc/osg/config.d/40-localsettings.ini  and run osg-configure, as documented  here .  HTCondor batch system only: Local universe jobs are excluded from any routing.", 
            "title": "Quirks and Pitfalls"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#how-job-routes-are-constructed", 
            "text": "Each job route\u2019s  ClassAd  is constructed by combining each entry from the  JOB_ROUTER_ENTRIES  with the  JOB_ROUTER_DEFAULTS . Attributes that are  set_  in  JOB_ROUTER_ENTRIES  will override those  set_  in  JOB_ROUTER_DEFAULTS", 
            "title": "How Job Routes are Constructed"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#job_router_entries", 
            "text": "JOB_ROUTER_ENTRIES  is a configuration variable whose default is set in  /etc/condor-ce/config.d/02-ce-*.conf  but may be overriden by the administrator in  /etc/condor-ce/config.d/99-local.conf . This document outlines the many changes you can make to  JOB_ROUTER_ENTRIES  to fit your site\u2019s needs.", 
            "title": "JOB_ROUTER_ENTRIES"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#job_router_defaults", 
            "text": "JOB_ROUTER_DEFAULTS  is a python-generated configuration variable that sets default job route values that are required for the HTCondor-CE's functionality. To view its contents, run the following command:  user@host $  condor_ce_config_val JOB_ROUTER_DEFAULTS  |  sed  s/;/;\\n/g    Warning  If a value is set in  JOB_ROUTER_DEFAULTS  with  eval_set_ variable , override it by using  eval_set_ variable  in the  JOB_ROUTER_ENTRIES . Do this at your own risk as it may cause the CE to break.    Warning  Do  not  set the  JOB_ROUTER_DEFAULTS  configuration variable yourself. This will cause the CE to stop functioning.", 
            "title": "JOB_ROUTER_DEFAULTS"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#generic-routes", 
            "text": "This section contains general information about job routes that can be used regardless of the type of batch system at your site. New routes should be placed in  /etc/condor-ce/config.d/99-local.conf , not the original  02-ce-*.conf .", 
            "title": "Generic Routes"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#required-fields", 
            "text": "The minimum requirements for a route are that you specify the type of batch system that jobs should be routed to and a name for each route. Default routes can be found in  /usr/share/condor-ce/config.d/02-ce- batch system -defaults.conf , provided by the  osg-ce- batch system  packages.", 
            "title": "Required fields"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#batch-system", 
            "text": "Each route needs to indicate the type of batch system that jobs should be routed to. For HTCondor batch systems, the  TargetUniverse  attribute needs to be set to  5  or  \"vanilla\" . For all other batch systems, the  TargetUniverse  attribute needs to be set to  9  or  \"grid\"  and the  GridResource  attribute needs to be set to  \"batch  batch system \"  (where  batch system  can be one of  pbs  (for both users of  pbs  and  SLURM ),  lsf , or  sge ).  JOB_ROUTER_ENTRIES = [ \\\n      TargetUniverse = 5; \\ \n     name =  Route jobs to HTCondor ; \\\n] \\\n[ \\\n      GridResource =  batch pbs ; \\ \n      TargetUniverse = 9; \\ \n     name =  Route jobs to PBS ; \\\n]", 
            "title": "Batch system"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#route-name", 
            "text": "To identify routes, you will need to assign a name to the route with the  name  attribute:  JOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n      name =  Route jobs to HTCondor ; \\ \n]  The name of the route will be useful in debugging since it shows up in the output of  condor_ce_job_router_info , the  JobRouterLog , and in the ClassAd of the routed job, which can be viewed with  condor_ce_q  or  condor_ce_history .", 
            "title": "Route name"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#writing-multiple-routes", 
            "text": "If your batch system needs incoming jobs to be sorted (e.g. if different VO's need to go to separate queues), you will need to write multiple job routes. Each route is enclosed by square brackets and unless they're the last closing bracket, they need to be followed by the line continuation character. The following routes takes incoming jobs that have a  queue  attribute set to  \"analy\"  and routes them to the site's HTCondor batch system. Any other jobs will be sent to that site's PBS batch system.   Note  For versions of HTCondor   8.7.1, the JobRouter matches jobs to routes in a round-robin fashion. This means that if a job can match to multiple routes, it can be routed by any of them! So when writing job routes, make sure that they are exclusive to each other and that your jobs can only match to a single route.   JOB_ROUTER_ENTRIES =  [ \\ \n     TargetUniverse = 5; \\\n     name =  Route jobs to HTCondor ; \\\n     Requirements = (TARGET.queue =?=  analy ); \\ ] \\\n[ \\ \n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Route jobs to PBS ; \\\n     Requirements = (TARGET.queue =!=  analy ); \\ ]", 
            "title": "Writing multiple routes"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#writing-comments", 
            "text": "To write comments you can use C-style comments, text enclosed by  /* */ . If the comment is at the end of a line, it still has to be followed by the line continuation character.  JOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name =  C-style comments ; \\\n      /* This is a comment */ \\ \n]  You can also use  #  to comment out single lines:  JOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name =  Hash comments ; \\\n      # BrokenAttribute =  commented out ; \\ \n]", 
            "title": "Writing comments"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#setting-attributes-for-all-routes", 
            "text": "To set an attribute that will be applied to all routes, you will need to ensure that  MERGE_JOB_ROUTER_DEFAULT_ADS  is set to  True  (check the value with  condor_ce_config_val ) and use the  set_  function in the  JOB_ROUTER_DEFAULTS . The following configuration sets the  Periodic_Hold  attribute for all routes:  # Use the defaults generated by the condor_ce_router_defaults script.  To add  # additional defaults, add additional lines of the form:  #  #   JOB_ROUTER_DEFAULTS = $(JOB_ROUTER_DEFAULTS) [set_foo = 1;]  #  MERGE_JOB_ROUTER_DEFAULT_ADS = True JOB_ROUTER_DEFAULTS   =   $( JOB_ROUTER_DEFAULTS )   [ set_Periodic_Hold   =   ( NumJobStarts  =   1     JobStatus   ==   1 )   ||  NumJobStarts    1 ; ]", 
            "title": "Setting attributes for all routes"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#filtering-jobs-based-on", 
            "text": "To filter jobs, use the  Requirements  attribute. Jobs will evaluate against the ClassAd expression set in the  Requirements  and if the expression evaluates to  TRUE , the route will match. More information on the syntax of ClassAd's can be found in the  HTCondor manual . For an example on how incoming jobs interact with filtering in job routes, consult  this document .  When setting requirements, you need to prefix job attributes that you are filtering with  TARGET.  so that the job route knows to compare the attribute of the incoming job rather than the route\u2019s own attribute. For example, if an incoming job has a  queue = \"analy\"  attribute, then the following job route will not match:  JOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name =  Filtering by queue ; \\\n     queue =  not-analy ; \\\n      Requirements = (queue =?=  analy ); \\ \n]  This is because when evaluating the route requirement, the job route will compare its own  queue  attribute to \"analy\" and see that it does not match. You can read more about comparing two ClassAds in the  HTCondor manual .   Note  If you have an HTCondor batch system, note the difference with  set_requirements .    Note For versions of HTCondor   8.7.1, the JobRouter matches jobs to routes in a round-robin fashion. This means that if a job can match to multiple routes, it can be routed by any of them! So when writing job routes, make sure that they are exclusive to each other and that your jobs can only match to a single route.", 
            "title": "Filtering jobs based on\u2026"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#glidein-queue", 
            "text": "To filter jobs based on their glidein queue attribute, your routes will need a  Requirements  expression using the incoming job's  queue  attribute. The following entry routes jobs to the PBS queue if the incoming job (specified by  TARGET ) is an  analy  (Analysis) glidein:  JOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name =  Filtering by queue ; \\\n      Requirements = (TARGET.queue =?=  analy ); \\ \n]", 
            "title": "Glidein queue"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#job-submitter", 
            "text": "To filter jobs based on who submitted it, your routes will need a  Requirements  expression using the incoming job's  Owner  attribute. The following entry routes jobs to the HTCondor batch system iff the submitter is  usatlas2 :  JOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name =  Filtering by job submitter ; \\\n      Requirements = (TARGET.Owner =?=  usatlas2 ); \\ \n]  Alternatively, you can match based on regular expression. The following entry routes jobs to the PBS batch system iff the submitter's name begins with  usatlas :  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Filtering by job submitter (regular expression) ; \\\n      Requirements = regexp( ^usatlas , TARGET.Owner); \\ \n]", 
            "title": "Job submitter"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#voms-attribute", 
            "text": "To filter jobs based on the subject of the job's proxy, your routes will need a  Requirements  expression using the incoming job's  x509UserProxyFirstFQAN  attribute. The following entry routes jobs to the PBS batch system if the proxy subject contains  /cms/Role=Pilot :  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Filtering by VOMS attribute (regex) ; \\\n      Requirements = regexp( \\/cms\\/Role\\=pilot , TARGET.x509UserProxyFirstFQAN); \\ \n]", 
            "title": "VOMS attribute"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#setting-a-default", 
            "text": "This section outlines how to set default job limits, memory, cores, queue, and maximum walltime. For an example on how users can override these defaults, consult  this document .", 
            "title": "Setting a default\u2026"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#maximum-number-of-jobs", 
            "text": "To set a default limit to the maximum number of jobs per route, you can edit the configuration variable  CONDORCE_MAX_JOBS  in  /etc/condor-ce/config.d/01-ce-router.conf :  CONDORCE_MAX_JOBS = 10000   Note  The above configuration  is to be placed directly into the HTCondor-CE configuration, not into a job route.", 
            "title": "Maximum number of jobs"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#maximum-memory", 
            "text": "To set a default maximum memory for routed jobs, set the attribute  default_maxMemory :  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Request memory ; \\\n     /* Set the requested memory to 1 GB */ \\\n      set_default_maxMemory = 1000; \\ \n]", 
            "title": "Maximum memory"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#number-of-cores-to-request", 
            "text": "To set a default number of cores for routed jobs, set the attribute  default_xcount :  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Request CPU ; \\\n     /* Set the requested cores to 8 */ \\\n      set_default_xcount = 8; \\ \n]", 
            "title": "Number of cores to request"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#maximum-walltime", 
            "text": "To set a default maximum walltime (in minutes) for routed jobs, set the attribute  default_maxWallTime :  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Setting WallTime ; \\\n     /* Set the max walltime to 1 hr */ \\\n      set_default_maxWallTime = 60; \\ \n]", 
            "title": "Maximum walltime"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#editing-attributes", 
            "text": "The following functions are operations that affect job attributes and are evaluated in the following order:   copy_*  delete_*  set_*  eval_set_*   After each job route\u2019s ClassAd is  constructed , the above operations are evaluated in order. For example, if the attribute  foo  is set using  eval_set_foo  in the  JOB_ROUTER_DEFAULTS , you'll be unable to use  delete_foo  to remote it from your jobs since the attribute is set using  eval_set_foo  after the deletion occurs according to the order of operations. To get around this, we can take advantage of the fact that operations defined in  JOB_ROUTER_DEFAULTS  get overriden by the same operation in  JOB_ROUTER_ENTRIES . So to 'delete'  foo , we would add  eval_set_foo = \"\"  to the route in the  JOB_ROUTER_ENTRIES , resulting in  foo  being absent from the routed job.  More documentation can be found in the  HTCondor manual .", 
            "title": "Editing attributes\u2026"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#copying-attributes", 
            "text": "To copy the value of an attribute of the incoming job to an attribute of the routed job, use  copy_ . The following route copies the  environment  attribute of the incoming job and sets the attribute  Original_Environment  on the routed job to the same value:  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Copying attributes ; \\\n      copy_environment =  Original_Environment ; \\ \n]", 
            "title": "Copying attributes"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#removing-attributes", 
            "text": "To remove an attribute of the incoming job from the routed job, use  delete_ . The following route removes the  environment  attribute from the routed job:  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Copying attributes ; \\\n      delete_environment = True; \\ \n]", 
            "title": "Removing attributes"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#setting-attributes", 
            "text": "To set an attribute on the routed job, use  set_ . The following route sets the Job's  Rank  attribute to 5:  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Setting an attribute ; \\\n      set_Rank = 5; \\ \n]", 
            "title": "Setting attributes"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#setting-attributes-with-classad-expressions", 
            "text": "To set an attribute to a ClassAd expression to be evaluated, use  set_eval . The following route sets the  Experiment  attribute to  atlas.osguser  if the Owner of the incoming job is  osguser :   Note  If a value is set in JOB_ROUTER_DEFAULTS with  eval_set_ variable , override it by using  eval_set_ variable  in the  JOB_ROUTER_ENTRIES .   JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Setting an attribute with a !ClassAd expression ; \\\n      eval_set_Experiment = strcat( atlas. , Owner); \\ \n]", 
            "title": "Setting attributes with ClassAd expressions"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#limiting-the-number-of", 
            "text": "This section outlines how to limit the number of total or idle jobs in a specific route (i.e., if this limit is reached, jobs will no longer be placed in this route).   Note  If you are using an HTCondor batch system, limiting the number of jobs is not the preferred solution: HTCondor manages fair share on its own via  user priorities and group accounting .", 
            "title": "Limiting the number of..."
        }, 
        {
            "location": "/compute-element/job-router-recipes/#total-jobs", 
            "text": "To set a limit on the number of jobs for a specific route, set the  MaxJobs  attribute:  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Limit the total number of jobs to 100 ; \\\n      MaxJobs = 100; \\ \n] \\\n[ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Limit the total number of jobs to 75 ; \\\n      MaxJobs = 75; \\ \n]", 
            "title": "Total jobs"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#idle-jobs", 
            "text": "To set a limit on the number of idle jobs for a specific route, set the  MaxIdleJobs  attribute:  JOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name =  Limit the total number of idle jobs to 100 ; \\\n      MaxIdleJobs = 100; \\ \n] \\\n[ \\\n     TargetUniverse = 5; \\\n     name =  Limit the total number of idle jobs to 75 ; \\\n      MaxIdleJobs = 75; \\ \n]", 
            "title": "Idle jobs"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#debugging-routes", 
            "text": "To help debug expressions in your routes, you can use the  debug()  function. First, set the debug mode for the JobRouter by editing a file in  /etc/condor-ce/config.d/  to read  JOB_ROUTER_DEBUG = D_FULLDEBUG  Then wrap the problematic attribute in  debug() :  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Debugging a difficult !ClassAd expression ; \\\n      eval_set_Experiment = debug(strcat( atlas , Name)); \\ \n]  You will find the debugging output in  /var/log/condor-ce/JobRouterLog .", 
            "title": "Debugging routes"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#routes-for-htcondor-batch-systems", 
            "text": "This section contains information about job routes that can be used if you are running an HTCondor batch system at your site.", 
            "title": "Routes for HTCondor Batch Systems"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#setting-periodic-hold-release-or-remove", 
            "text": "To release, remove or put a job on hold if it meets certain criteria, use the  PERIODIC_*  family of attributes. By default, periodic expressions are evaluated once every 300 seconds but this can be changed by setting  PERIODIC_EXPR_INTERVAL  in your CE's configuration.  JOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name =  Setting periodic statements ; \\\n      /* Puts the routed job on hold if the job s been idle and has been started at least once or if the job has tried to start more than once */ \\\n     set_Periodic_Hold = (NumJobStarts  = 1   JobStatus == 1) || NumJobStarts   1; \\\n     /* Remove routed jobs if their walltime is longer than 3 days and 5 minutes */ \\\n     set_Periodic_Remove = ( RemoteWallClockTime   (3*24*60*60 + 5*60) ); \\\n     /* Release routed jobs if the condor_starter couldn t start the executable and  VMGAHP_ERR_INTERNAL  is in the HoldReason */ \\\n     set_Periodic_Release = HoldReasonCode == 6   regexp( VMGAHP_ERR_INTERNAL , HoldReason); \\ \n]", 
            "title": "Setting periodic hold, release or remove"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#setting-routed-job-requirements", 
            "text": "If you need to set requirements on your routed job, you will need to use  set_Requirements  instead of  Requirements . The  Requirements  attribute filters jobs coming into your CE into different job routes whereas  set_requirements  will set conditions on the routed job that must be met by the worker node it lands on. For more information on requirements, consult the  HTCondor manual .  To ensure that your job lands on a Linux machine in your pool:  JOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n      set_Requirements =  OpSys ==  LINUX ; \\ \n]", 
            "title": "Setting routed job requirements"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#setting-accounting-groups", 
            "text": "To assign jobs to an HTCondor accounting group to manage fair share on your local batch system, we recommend using  UID and ExtAttr tables .", 
            "title": "Setting accounting groups"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#routes-for-non-htcondor-batch-systems", 
            "text": "This section contains information about job routes that can be used if you are running a non-HTCondor batch system at your site.", 
            "title": "Routes for non-HTCondor Batch Systems"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#setting-a-default-batch-queue", 
            "text": "To set a default queue for routed jobs, set the attribute  default_queue :  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Setting batch system queues ; \\\n      set_default_queue =  osg_queue ; \\ \n]", 
            "title": "Setting a default batch queue"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#setting-batch-system-directives", 
            "text": "To write batch system directives that are not supported in the route examples above, you will need to edit the job submit script for your local batch system in  /etc/blahp/  (e.g., if your local batch system is PBS, edit  /etc/blahp/pbs_local_submit_attributes.sh ). This file is sourced during submit time and anything printed to stdout is appended to the batch system job submit script. ClassAd attributes can be passed from the routed job to the local submit attributes script via the  default_remote_cerequirements  attribute, which can take the following form:  default_remote_cerequirements =  foo == X   bar == \\ Y\\    ...   This sets  foo  to value  X  and  bar  to the string  Y  (escaped double-quotes are required for string values) in the environment of the local submit attributes script. The following example sets the maximum walltime to 1 hour and the accounting group to the  x509UserProxyFirstFQAN  attribute of the job submitted to a PBS batch system  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Setting job submit variables ; \\\n      set_default_remote_cerequirements = strcat( Walltime == 3600   AccountingGroup == \\ , x509UserProxyFirstFQAN,  \\ ); \\ \n]  With  /etc/blahp/pbs_local_submit_attributes.sh  containing.  1\n2\n3 #!/bin/bash  echo   #PBS -l walltime= $Walltime  echo   #PBS -A  $AccountingGroup    This results in the following being appended to the script that gets submitted to your batch system:  #PBS -l walltime=3600\n#PBS -A  CE job s x509UserProxyFirstFQAN attribute", 
            "title": "Setting batch system directives"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#reference", 
            "text": "Here are some other HTCondor-CE documents that might be helpful:   HTCondor-CE overview and architecture  Installing HTCondor-CE  The HTCondor-CE troubleshooting guide  Submitting jobs to HTCondor-CE", 
            "title": "Reference"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#example-configurations", 
            "text": "", 
            "title": "Example Configurations"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#aglt2s-job-routes", 
            "text": "Atlas AGLT2 is using an HTCondor batch system. Here are some things to note about their routes.   Setting various HTCondor-specific attributes like  Rank ,  AccountingGroup ,  JobPrio  and  Periodic_Remove  (see the  HTCondor manual  for more). Some of these are site-specific like  LastandFrac ,  IdleMP8Pressure ,  localQue ,  IsAnalyJob  and  JobMemoryLimit .  There is a difference between  Requirements  and  set_requirements . The  Requirements  attribute matches jobs to specific routes while the  set_requirements  sets the  Requirements  attribute on the  routed  job, which confines which machines that the routed job can land on.   Source:  https://www.aglt2.org/wiki/bin/view/AGLT2/CondorCE#The_JobRouter_configuration_file_content  JOB_ROUTER_ENTRIES   =   \\ \n/* Still to  do  on all routes, get job requirements and add them here */  \\ \n/* ***** Route no  1  ***** */  \\ \n/* ***** Analysis queue ***** */  \\ \n   [   \\ \n     GridResource   =   condor localhost localhost ;   \\ \n     eval_set_GridResource   =  strcat ( condor  ,  $( FULL_HOSTNAME ) ,    $( JOB_ROUTER_SCHEDD2_POOL ) ) ;   \\ \n     Requirements   =  target.queue == analy ;   \\ \n     Name   =   Analysis Queue ;   \\ \n     TargetUniverse   =   5 ;   \\ \n     eval_set_IdleMP8Pressure   =   $( IdleMP8Pressure ) ;   \\ \n     eval_set_LastAndFrac   =   $( LastAndFrac ) ;   \\ \n     set_requirements   =   (   (  TARGET.TotalDisk  = ? =  undefined  )   ||   (  TARGET.TotalDisk  =   21000000   )   )     (  TARGET.Arch  ==   X86_64   )     (  TARGET.OpSys  ==   LINUX   )     (  TARGET.Disk  =  RequestDisk  )     (  TARGET.Memory  =  RequestMemory  )     (  TARGET.HasFileTransfer  )     ( IfThenElse (( Owner   ==   atlasconnect   ||   Owner   ==   muoncal ) ,IfThenElse ( IdleMP8Pressure, ( TARGET.PARTITIONED  = ! =  TRUE ) ,True ) ,IfThenElse ( LastAndFrac, ( TARGET.PARTITIONED  = ! =  TRUE ) ,True ))) ;   \\ \n     eval_set_AccountingGroup   =  strcat ( group_gatekpr.prod.analy. ,Owner ) ;   \\ \n     set_localQue   =   Analysis ;   \\ \n     set_IsAnalyJob   =  True ;   \\ \n     set_JobPrio   =   5 ;   \\ \n     set_Rank   =   ( SlotID +  ( 64 -TARGET.DETECTED_CORES )) *1.0 ;   \\ \n     set_JobMemoryLimit   =   4194000 ;   \\ \n     set_Periodic_Remove   =   (   (  RemoteWallClockTime    ( 3 *24*60*60 +  5 *60 )   )   ||   ( ImageSize   JobMemoryLimit )   ) ;   \\ \n   ]   \\ \n/* ***** Route no  2  ***** */  \\ \n/* ***** splitterNT queue ***** */  \\ \n   [   \\ \n     GridResource   =   condor localhost localhost ;   \\ \n     eval_set_GridResource   =  strcat ( condor  ,  $( FULL_HOSTNAME ) ,    $( JOB_ROUTER_SCHEDD2_POOL ) ) ;   \\ \n     Requirements   =  target.queue  ==   splitterNT ;   \\ \n     Name   =   Splitter ntuple queue ;   \\ \n     TargetUniverse   =   5 ;   \\ \n     set_requirements   =   (   (  TARGET.TotalDisk  = ? =  undefined  )   ||   (  TARGET.TotalDisk  =   21000000   )   )     (  TARGET.Arch  ==   X86_64   )     (  TARGET.OpSys  ==   LINUX   )     (  TARGET.Disk  =  RequestDisk  )     (  TARGET.Memory  =  RequestMemory  )     (  TARGET.HasFileTransfer  ) ;   \\ \n     eval_set_AccountingGroup   =   group_calibrate.muoncal ;   \\ \n     set_localQue   =   Splitter ;   \\ \n     set_IsAnalyJob   =  False ;   \\ \n     set_JobPrio   =   10 ;   \\ \n     set_Rank   =   ( SlotID +  ( 64 -TARGET.DETECTED_CORES )) *1.0 ;   \\ \n     set_JobMemoryLimit   =   4194000 ;   \\ \n     set_Periodic_Remove   =   (   (  RemoteWallClockTime    ( 3 *24*60*60 +  5 *60 )   )   ||   ( ImageSize   JobMemoryLimit )   ) ;   \\ \n   ]   \\ \n/* ***** Route no  3  ***** */  \\ \n/* ***** splitter queue ***** */  \\ \n   [   \\ \n     GridResource   =   condor localhost localhost ;   \\ \n     eval_set_GridResource   =  strcat ( condor  ,  $( FULL_HOSTNAME ) ,    $( JOB_ROUTER_SCHEDD2_POOL ) ) ;   \\ \n     Requirements   =  target.queue  ==   splitter ;   \\ \n     Name   =   Splitter queue ;   \\ \n     TargetUniverse   =   5 ;   \\ \n     set_requirements   =   (   (  TARGET.TotalDisk  = ? =  undefined  )   ||   (  TARGET.TotalDisk  =   21000000   )   )     (  TARGET.Arch  ==   X86_64   )     (  TARGET.OpSys  ==   LINUX   )     (  TARGET.Disk  =  RequestDisk  )     (  TARGET.Memory  =  RequestMemory  )     (  TARGET.HasFileTransfer  ) ;   \\ \n     eval_set_AccountingGroup   =   group_calibrate.muoncal ;   \\ \n     set_localQue   =   Splitter ;   \\ \n     set_IsAnalyJob   =  False ;   \\ \n     set_JobPrio   =   15 ;   \\ \n     set_Rank   =   ( SlotID +  ( 64 -TARGET.DETECTED_CORES )) *1.0 ;   \\ \n     set_JobMemoryLimit   =   4194000 ;   \\ \n     set_Periodic_Remove   =   (   (  RemoteWallClockTime    ( 3 *24*60*60 +  5 *60 )   )   ||   ( ImageSize   JobMemoryLimit )   ) ;   \\ \n   ]   \\ \n/* ***** Route no  4  ***** */  \\ \n/* ***** xrootd queue ***** */  \\ \n   [   \\ \n     GridResource   =   condor localhost localhost ;   \\ \n     eval_set_GridResource   =  strcat ( condor  ,  $( FULL_HOSTNAME ) ,    $( JOB_ROUTER_SCHEDD2_POOL ) ) ;   \\ \n     Requirements   =  target.queue  ==   xrootd ;   \\ \n     Name   =   Xrootd queue ;   \\ \n     TargetUniverse   =   5 ;   \\ \n     set_requirements   =   (   (  TARGET.TotalDisk  = ? =  undefined  )   ||   (  TARGET.TotalDisk  =   21000000   )   )     (  TARGET.Arch  ==   X86_64   )     (  TARGET.OpSys  ==   LINUX   )     (  TARGET.Disk  =  RequestDisk  )     (  TARGET.Memory  =  RequestMemory  )     (  TARGET.HasFileTransfer  ) ;   \\ \n     eval_set_AccountingGroup   =  strcat ( group_gatekpr.prod.analy. ,Owner ) ;   \\ \n     set_localQue   =   Analysis ;   \\ \n     set_IsAnalyJob   =  True ;   \\ \n     set_JobPrio   =   35 ;   \\ \n     set_Rank   =   ( SlotID +  ( 64 -TARGET.DETECTED_CORES )) *1.0 ;   \\ \n     set_JobMemoryLimit   =   4194000 ;   \\ \n     set_Periodic_Remove   =   (   (  RemoteWallClockTime    ( 3 *24*60*60 +  5 *60 )   )   ||   ( ImageSize   JobMemoryLimit )   ) ;   \\ \n   ]   \\ \n/* ***** Route no  5  ***** */  \\ \n/* ***** Tier3Test queue ***** */  \\ \n   [   \\ \n     GridResource   =   condor localhost localhost ;   \\ \n     eval_set_GridResource   =  strcat ( condor  ,  $( FULL_HOSTNAME ) ,    $( JOB_ROUTER_SCHEDD2_POOL ) ) ;   \\ \n     Requirements   =  target.queue  ==   Tier3Test ;   \\ \n     Name   =   Tier3 Test Queue ;   \\ \n     TargetUniverse   =   5 ;   \\ \n     set_requirements   =   (   (  TARGET.TotalDisk  = ? =  undefined  )   ||   (  TARGET.TotalDisk  =   21000000   )   )     (  TARGET.Arch  ==   X86_64   )     (  TARGET.OpSys  ==   LINUX   )     (  TARGET.Disk  =  RequestDisk  )     (  TARGET.Memory  =  RequestMemory  )     (  TARGET.HasFileTransfer  )     (   IS_TIER3_TEST_QUEUE   = ? =  True  ) ;   \\ \n     eval_set_AccountingGroup   =  strcat ( group_gatekpr.prod.analy. ,Owner ) ;   \\ \n     set_localQue   =   Tier3Test ;   \\ \n     set_IsTier3TestJob   =  True ;   \\ \n     set_IsAnalyJob   =  True ;   \\ \n     set_JobPrio   =   20 ;   \\ \n     set_Rank   =   ( SlotID +  ( 64 -TARGET.DETECTED_CORES )) *1.0 ;   \\ \n     set_JobMemoryLimit   =   4194000 ;   \\ \n     set_Periodic_Remove   =   (   (  RemoteWallClockTime    ( 3 *24*60*60 +  5 *60 )   )   ||   ( ImageSize   JobMemoryLimit )   ) ;   \\ \n   ]   \\ \n/* ***** Route no  6  ***** */  \\ \n/* ***** mp8 queue ***** */  \\ \n   [   \\ \n     GridResource   =   condor localhost localhost ;   \\ \n     eval_set_GridResource   =  strcat ( condor  ,  $( FULL_HOSTNAME ) ,    $( JOB_ROUTER_SCHEDD2_POOL ) ) ;   \\ \n     Requirements   =  target.queue == mp8 ;   \\ \n     Name   =   MCORE Queue ;   \\ \n     TargetUniverse   =   5 ;   \\ \n     set_requirements   =   (   (  TARGET.TotalDisk  = ? =  undefined  )   ||   (  TARGET.TotalDisk  =   21000000   )   )     (  TARGET.Arch  ==   X86_64   )     (  TARGET.OpSys  ==   LINUX   )     (  TARGET.Disk  =  RequestDisk  )     (  TARGET.Memory  =  RequestMemory  )     (  TARGET.HasFileTransfer  )     ((  TARGET.Cpus  ==   8    TARGET.CPU_TYPE  = ? =   mp8   )   ||  TARGET.PARTITIONED  = ? =  True  ) ;   \\ \n     eval_set_AccountingGroup   =  strcat ( group_gatekpr.prod.mcore. ,Owner ) ;   \\ \n     set_localQue   =   MP8 ;   \\ \n     set_IsAnalyJob   =  False ;   \\ \n     set_JobPrio   =   25 ;   \\ \n     set_Rank   =   0 .0 ;   \\ \n     eval_set_RequestCpus   =   8 ;   \\ \n     set_JobMemoryLimit   =   33552000 ;   \\ \n     set_Slot_Type   =   mp8 ;   \\ \n     set_Periodic_Remove   =   (   (  RemoteWallClockTime    ( 3 *24*60*60 +  5 *60 )   )   ||   ( ImageSize   JobMemoryLimit )   ) ;   \\ \n   ]   \\ \n/* ***** Route no  7  ***** */  \\ \n/* ***** Installation queue, triggered by usatlas2 user ***** */  \\ \n   [   \\ \n     GridResource   =   condor localhost localhost ;   \\ \n     eval_set_GridResource   =  strcat ( condor  ,  $( FULL_HOSTNAME ) ,    $( JOB_ROUTER_SCHEDD2_POOL ) ) ;   \\ \n     Requirements   =  target.queue is undefined   target.Owner  ==   usatlas2 ;   \\ \n     Name   =   Install Queue ;   \\ \n     TargetUniverse   =   5 ;   \\ \n     set_requirements   =   (   (  TARGET.TotalDisk  = ? =  undefined  )   ||   (  TARGET.TotalDisk  =   21000000   )   )     (  TARGET.Arch  ==   X86_64   )     (  TARGET.OpSys  ==   LINUX   )     (  TARGET.Disk  =  RequestDisk  )     (  TARGET.Memory  =  RequestMemory  )     (  TARGET.HasFileTransfer  )     (  TARGET.IS_INSTALL_QUE  = ? =  True  )     ( TARGET.AGLT2_SITE  ==   UM   ) ;   \\ \n     eval_set_AccountingGroup   =  strcat ( group_gatekpr.other. ,Owner ) ;   \\ \n     set_localQue   =   Default ;   \\ \n     set_IsAnalyJob   =  False ;   \\ \n     set_IsInstallJob   =  True ;   \\ \n     set_JobPrio   =   15 ;   \\ \n     set_Rank   =   ( SlotID +  ( 64 -TARGET.DETECTED_CORES )) *1.0 ;   \\ \n     set_JobMemoryLimit   =   4194000 ;   \\ \n     set_Periodic_Remove   =   (   (  RemoteWallClockTime    ( 3 *24*60*60 +  5 *60 )   )   ||   ( ImageSize   JobMemoryLimit )   ) ;   \\ \n   ]   \\ \n/* ***** Route no  8  ***** */  \\ \n/* ***** Default queue  for  usatlas1 user ***** */  \\ \n   [   \\ \n     GridResource   =   condor localhost localhost ;   \\ \n     eval_set_GridResource   =  strcat ( condor  ,  $( FULL_HOSTNAME ) ,    $( JOB_ROUTER_SCHEDD2_POOL ) ) ;   \\ \n     Requirements   =  target.queue is undefined   regexp ( usatlas1 ,target.Owner ) ;   \\ \n     Name   =   ATLAS Production Queue ;   \\ \n     TargetUniverse   =   5 ;   \\ \n     set_requirements   =   (   (  TARGET.TotalDisk  = ? =  undefined  )   ||   (  TARGET.TotalDisk  =   21000000   )   )     (  TARGET.Arch  ==   X86_64   )     (  TARGET.OpSys  ==   LINUX   )     (  TARGET.Disk  =  RequestDisk  )     (  TARGET.Memory  =  RequestMemory  )     (  TARGET.HasFileTransfer  ) ;   \\ \n     eval_set_AccountingGroup   =  strcat ( group_gatekpr.prod.prod. ,Owner ) ;   \\ \n     set_localQue   =   Default ;   \\ \n     set_IsAnalyJob   =  False ;   \\ \n     set_Rank   =   ( SlotID +  ( 64 -TARGET.DETECTED_CORES )) *1.0 ;   \\ \n     set_JobMemoryLimit   =   4194000 ;   \\ \n     set_Periodic_Remove   =   (   (  RemoteWallClockTime    ( 3 *24*60*60 +  5 *60 )   )   ||   ( ImageSize   JobMemoryLimit )   ) ;   \\ \n   ]   \\ \n/* ***** Route no  9  ***** */  \\ \n/* ***** Default queue  for  any other usatlas account ***** */  \\ \n   [   \\ \n     GridResource   =   condor localhost localhost ;   \\ \n     eval_set_GridResource   =  strcat ( condor  ,  $( FULL_HOSTNAME ) ,    $( JOB_ROUTER_SCHEDD2_POOL ) ) ;   \\ \n     Requirements   =  target.queue is undefined    ( regexp ( usatlas2 ,target.Owner )   ||  regexp ( usatlas3 ,target.Owner )) ;   \\ \n     Name   =   Other ATLAS Production ;   \\ \n     TargetUniverse   =   5 ;   \\ \n     set_requirements   =   (   (  TARGET.TotalDisk  = ? =  undefined  )   ||   (  TARGET.TotalDisk  =   21000000   )   )     (  TARGET.Arch  ==   X86_64   )     (  TARGET.OpSys  ==   LINUX   )     (  TARGET.Disk  =  RequestDisk  )     (  TARGET.Memory  =  RequestMemory  )     (  TARGET.HasFileTransfer  ) ;   \\ \n     eval_set_AccountingGroup   =  strcat ( group_gatekpr.other. ,Owner ) ;   \\ \n     set_localQue   =   Default ;   \\ \n     set_IsAnalyJob   =  False ;   \\ \n     set_Rank   =   ( SlotID +  ( 64 -TARGET.DETECTED_CORES )) *1.0 ;   \\ \n     set_JobMemoryLimit   =   4194000 ;   \\ \n     set_Periodic_Remove   =   (   (  RemoteWallClockTime    ( 3 *24*60*60 +  5 *60 )   )   ||   ( ImageSize   JobMemoryLimit )   ) ;   \\ \n   ]   \\ \n/* ***** Route no  10  ***** */  \\ \n/* ***** Anything  else . Set queue as Default and assign to other VOs  ***** */  \\ \n   [   \\ \n     GridResource   =   condor localhost localhost ;   \\ \n     eval_set_GridResource   =  strcat ( condor  ,  $( FULL_HOSTNAME ) ,    $( JOB_ROUTER_SCHEDD2_POOL ) ) ;   \\ \n     Requirements   =  target.queue is undefined   ifThenElse ( regexp ( usatlas ,target.Owner ) ,false,true ) ;   \\ \n     Name   =   Other Jobs ;   \\ \n     TargetUniverse   =   5 ;   \\ \n     set_requirements   =   (   (  TARGET.TotalDisk  = ? =  undefined  )   ||   (  TARGET.TotalDisk  =   21000000   )   )     (  TARGET.Arch  ==   X86_64   )     (  TARGET.OpSys  ==   LINUX   )     (  TARGET.Disk  =  RequestDisk  )     (  TARGET.Memory  =  RequestMemory  )     (  TARGET.HasFileTransfer  ) ;   \\ \n     eval_set_AccountingGroup   =  strcat ( group_VOgener. ,Owner ) ;   \\ \n     set_localQue   =   Default ;   \\ \n     set_IsAnalyJob   =  False ;   \\ \n     set_Rank   =   ( SlotID +  ( 64 -TARGET.DETECTED_CORES )) *1.0 ;   \\ \n     set_JobMemoryLimit   =   4194000 ;   \\ \n     set_Periodic_Remove   =   (   (  RemoteWallClockTime    ( 3 *24*60*60 +  5 *60 )   )   ||   ( ImageSize   JobMemoryLimit )   ) ;   \\ \n   ]", 
            "title": "AGLT2's job routes"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#bnls-job-routes", 
            "text": "Atlas BNL T1, they are using an HTCondor batch system. Here are some things to note about their routes:   Setting various HTCondor-specific attributes like  JobLeaseDuration ,  Requirements  and  Periodic_Hold  (see the  HTCondor manual  for more). Some of these are site-specific like  RACF_Group ,  Experiment ,  Job_Type  and  VO .  Jobs are split into different routes based on the  GlideIn  queue that they're in.  There is a difference between  Requirements  and  set_requirements . The  Requirements  attribute matches  incoming  jobs to specific routes while the  set_requirements  sets the  Requirements  attribute on the  routed  job, which confines which machines that the routed job can land on.   Source:  http://www.usatlas.bnl.gov/twiki/bin/view/Admins/HTCondorCE.html  ###############################################################################  #  # HTCondor-CE HTCondor batch system configuration file.  #  ###############################################################################  # Submit the job to the site Condor  JOB_ROUTER_ENTRIES   =   \\ \n    [   \\ \n      GridResource   =   condor localhost localhost ;   \\ \n      eval_set_GridResource   =  strcat ( condor  ,  $( FULL_HOSTNAME ) ,  $( FULL_HOSTNAME ) ) ;   \\ \n      TargetUniverse   =   5 ;   \\ \n      name   =   BNL_Condor_Pool_long ;   \\ \n      Requirements   =  target.queue == analysis.long ;   \\ \n      eval_set_RACF_Group   =   long ;   \\ \n      set_Experiment   =   atlas ;   \\ \n      set_requirements   =   (   (   Arch   ==   INTEL   ||   Arch   ==   X86_64   )     (   CPU_Experiment   ==   atlas   )   )     (  TARGET.OpSys  ==   LINUX   )     (  TARGET.Disk  =  RequestDisk  )     (  TARGET.Memory  =  RequestMemory  )     (  TARGET.HasFileTransfer  ) ;   \\ \n      set_Job_Type   =   cas ;   \\ \n      set_JobLeaseDuration   =   3600 ;   \\ \n      set_Periodic_Hold   =   ( NumJobStarts  =   1     JobStatus   ==   1 )   ||  NumJobStarts    1 ;   \\ \n      eval_set_VO   =  x509UserProxyVOName ;   \\ \n    ]   \\ \n    [   \\ \n      GridResource   =   condor localhost localhost ;   \\ \n      eval_set_GridResource   =  strcat ( condor  ,  $( FULL_HOSTNAME ) ,  $( FULL_HOSTNAME ) ) ;   \\ \n      TargetUniverse   =   5 ;   \\ \n      name   =   BNL_Condor_Pool_short ;   \\ \n      Requirements   =  target.queue == analysis.short ;   \\ \n      eval_set_RACF_Group   =   short ;   \\ \n      set_Experiment   =   atlas ;   \\ \n      set_requirements   =   (   (   Arch   ==   INTEL   ||   Arch   ==   X86_64   )     (   CPU_Experiment   ==   atlas   )   )     (  TARGET.OpSys  ==   LINUX   )     (  TARGET.Disk  =  RequestDisk  )     (  TARGET.Memory  =  RequestMemory  )     (  TARGET.HasFileTransfer  ) ;   \\ \n      set_Job_Type   =   cas ;   \\ \n      set_JobLeaseDuration   =   3600 ;   \\ \n      set_Periodic_Hold   =   ( NumJobStarts  =   1     JobStatus   ==   1 )   ||  NumJobStarts    1 ;   \\ \n      eval_set_VO   =  x509UserProxyVOName ;   \\ \n    ]   \\ \n    [   \\ \n      GridResource   =   condor localhost localhost ;   \\ \n      eval_set_GridResource   =  strcat ( condor  ,  $( FULL_HOSTNAME ) ,  $( FULL_HOSTNAME ) ) ;   \\ \n      TargetUniverse   =   5 ;   \\ \n      name   =   BNL_Condor_Pool_grid ;   \\ \n      Requirements   =  target.queue == grid ;   \\ \n      eval_set_RACF_Group   =   grid ;   \\ \n      set_Experiment   =   atlas ;   \\ \n      set_requirements   =   (   (   Arch   ==   INTEL   ||   Arch   ==   X86_64   )     (   CPU_Experiment   ==   atlas   )   )     (  TARGET.OpSys  ==   LINUX   )     (  TARGET.Disk  =  RequestDisk  )     (  TARGET.Memory  =  RequestMemory  )     (  TARGET.HasFileTransfer  ) ;   \\ \n      set_Job_Type   =   cas ;   \\ \n      set_JobLeaseDuration   =   3600 ;   \\ \n      set_Periodic_Hold   =   ( NumJobStarts  =   1     JobStatus   ==   1 )   ||  NumJobStarts    1 ;   \\ \n      eval_set_VO   =  x509UserProxyVOName ;   \\ \n    ]   \\ \n    [   \\ \n      GridResource   =   condor localhost localhost ;   \\ \n      eval_set_GridResource   =  strcat ( condor  ,  $( FULL_HOSTNAME ) ,  $( FULL_HOSTNAME ) ) ;   \\ \n      TargetUniverse   =   5 ;   \\ \n      name   =   BNL_Condor_Pool ;   \\ \n      Requirements   =  target.queue is undefined ;   \\ \n      eval_set_RACF_Group   =   grid ;   \\ \n      set_requirements   =   (   (   Arch   ==   INTEL   ||   Arch   ==   X86_64   )     (   CPU_Experiment   ==   rcf   )   )     (  TARGET.OpSys  ==   LINUX   )     (  TARGET.Disk  =  RequestDisk  )     (  TARGET.Memory  =  RequestMemory  )     (  TARGET.HasFileTransfer  ) ;   \\ \n      set_Experiment   =   atlas ;   \\ \n      set_Job_Type   =   cas ;   \\ \n      set_JobLeaseDuration   =   3600 ;   \\ \n      set_Periodic_Hold   =   ( NumJobStarts  =   1     JobStatus   ==   1 )   ||  NumJobStarts    1 ;   \\ \n      eval_set_VO   =  x509UserProxyVOName ;   \\ \n    ]", 
            "title": "BNL's job routes"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/", 
            "text": "HTCondor-CE Troubleshooting Guide\n\n\nIn this document, you will find a collection of files and commands to help troubleshoot HTCondor-CE along with a list of common issues with suggested troubleshooting steps.\n\n\nKnown Issues\n\n\nSUBMIT_EXPRS are not applied to jobs on the local HTCondor\n\n\nIf you are adding attributes to jobs submitted to your HTCondor pool with \nSUBMIT_EXPRS\n, these will \nnot\n be applied to jobs that are entering your pool from the HTCondor-CE. To get around this, you will want to add the attributes to your \njob routes\n. If the CE is the only entry point for jobs into your pool, you can get rid of \nSUBMIT_EXPRS\n on your backend. Otherwise, you will have to maintain your list of attributes both in your list of routes and in your \nSUBMIT_EXPRS\n.\n\n\nGeneral Troubleshooting Items\n\n\nMaking sure packages are up-to-date\n\n\nIt is important to make sure that the HTCondor-CE and related RPMs are up-to-date.\n\n\nroot@host #\n yum update \nhtcondor-ce*\n blahp condor\n\n\n\n\n\nIf you just want to see the packages to update, but do not want to perform the update now, answer \nN\n at the prompt.\n\n\nVerify package contents\n\n\nIf the contents of your HTCondor-CE packages have been changed, the CE may cease to function properly. To verify the contents of your packages (ignoring changes to configuration files):\n\n\nuser@host $\n rpm -q --verify htcondor-ce htcondor-ce-client blahp \n|\n awk \n$2 != \nc\n {print $0}\n\n\n\n\n\n\nIf the verification command returns output, this means that your packages have been changed. To fix this, you can reinstall the packages:\n\n\nuser@host $\n yum reinstall htcondor-ce htcondor-ce-client blahp\n\n\n\n\n\n\n\nNote\n\n\nThe reinstall command may place original versions of configuration files alongside the versions that you have modified. If this is the case, the reinstall command will notify you that the original versions will have an \n.rpmnew\n suffix. Further inspection of these files may be required as to whether or not you need to merge them into your current configuration.\n\n\n\n\nVerify clocks are synchronized\n\n\nLike all GSI-based authentication, HTCondor-CE is sensitive to time skews. Make sure the clock on your CE is synchronized using a utility such as \nntpd\n. Additionally, HTCondor itself is sensitive to time skews on the NFS server. If you see empty stdout / err being returned to the submitter, verify there is no NFS server time skew.\n\n\nHTCondor-CE Troubleshooting Items\n\n\nThis section contains common issues you may encounter using HTCondor-CE and next actions to take when you do. Before troubleshooting, we recommend increasing the log level:\n\n\n\n\n\n\nWrite the following into \n/etc/condor-ce/config.d/99-local.conf\n to increase the log level for all daemons:\n\n\nALL_DEBUG = D_FULLDEBUG\n\n\n\n\n\n\n\n\n\nEnsure that the configuration is in place:\n\n\nroot@host # condor_ce_reconfig\n\n\n\n\n\n\n\n\n\nReproduce the issue\n\n\n\n\n\n\n\n\nNote\n\n\nBefore spending any time on troubleshooting, you should ensure that the state of configuration is as expected by running \ncondor_ce_reconfig\n.\n\n\n\n\nDaemons fail to start\n\n\nIf there are errors in your configuration of HTCondor-CE, this may cause some of its required daemons to fail to startup. Check the following subsections in order:\n\n\nSymptoms\n\n\nDaemon startup failure may manifest in many ways, the following are few symptoms of the problem.\n\n\n\n\n\n\nThe service fails to start:\n\n\nroot@host #\n service condor-ce start\n\nStarting Condor-CE daemons: [ FAIL ]\n\n\n\n\n\n\n\n\n\n\ncondor_ce_q\n fails with a lengthy error message:\n\n\nuser@host $\n condor_ce_q\n\nError:\n\n\n\nExtra Info: You probably saw this error because the condor_schedd is not running\n\n\non the machine you are trying to query. If the condor_schedd is not running, the\n\n\nCondor system will not be able to find an address and port to connect to and\n\n\nsatisfy this request. Please make sure the Condor daemons are running and try\n\n\nagain.\n\n\n\nExtra Info: If the condor_schedd is running on the machine you are trying to\n\n\nquery and you still see the error, the most likely cause is that you have setup\n\n\na personal Condor, you have not defined SCHEDD_NAME in your condor_config file,\n\n\nand something is wrong with your SCHEDD_ADDRESS_FILE setting. You must define\n\n\neither or both of those settings in your config file, or you must use the -name\n\n\noption to condor_q. Please see the Condor manual for details on SCHEDD_NAME and\n\n\nSCHEDD_ADDRESS_FILE.\n\n\n\n\n\n\n\n\n\n\nNext actions\n\n\n\n\nIf the MasterLog is filled with \nERROR:SECMAN...TCP connection to collector...failed\n:\n This is likely due to a misconfiguration for a host with multiple network interfaces. Verify that you have followed the instructions in \nthis\n section of the install guide.\n\n\nIf the MasterLog is filled with \nDC_AUTHENTICATE\n errors:\n The HTCondor-CE daemons use the host certificate to authenticate with each other. Verify that your host certificate\u2019s DN matches one of the regular expressions found in \n/etc/condor-ce/condor_mapfile\n.\n\n\nIf the SchedLog is filled with \nCan\u2019t find address for negotiator\n:\n You can ignore this error! The negotiator daemon is used in HTCondor batch systems to match jobs with resources but since HTCondor-CE does not manage any resources directly, it does not run one.\n\n\n\n\nJobs fail to submit to the CE\n\n\nIf a user is having issues submitting jobs to the CE and you've ruled out general connectivity or firewalls as the culprit, then you may have encountered an authentication or authorization issue. You may see error messages like the following in your \nSchedLog\n:\n\n\n08/30/16 16:52:56 DC_AUTHENTICATE: required authentication of 72.33.0.189 failed: AUTHENTICATE:1003:Failed to authenticate with any method|AUTHENTICATE:1004:Failed to authenticate using GSI|GSI:5002:Failed to authenticate because the remote (client) side was not able to acquire its credentials.|AUTHENTICATE:1004:Failed to authenticate using FS|FS:1004:Unable to lstat(/tmp/FS_XXXZpUlYa)\n08/30/16 16:53:12 PERMISSION DENIED to gsi@unmapped from host 72.33.0.189 for command 60021 (DC_NOP_WRITE), access level WRITE: reason: WRITE authorization policy contains no matching ALLOW entry for this request; identifiers used for this host: 72.33.0.189,dyn-72-33-0-189.uwnet.wisc.edu, hostname size = 1, original ip address = 72.33.0.189\n08/30/16 16:53:12 DC_AUTHENTICATE: Command not authorized, done!\n\n\n\n\n\nNext actions\n\n\n\n\nCheck GUMS or grid-mapfile\n and ensure that the user's DN is known to your \nauthentication method\n\n\nCheck for lcmaps errors\n in \n/var/log/messages\n\n\nIf you do not see helpful error messages in \n/var/log/messages\n,\n adjust the debug level by adding \nexport LCMAPS_DEBUG_LEVEL=5\n to \n/etc/sysconfig/condor-ce\n, restarting the condor-ce service, and checking \n/var/log/messages\n for errors again.\n\n\n\n\nJobs stay idle on the CE\n\n\nCheck the following subsections in order, but note that jobs may take several minutes or longer to run if the CE is busy.\n\n\nIdle jobs on CE: Is the job router handling the incoming job?\n\n\nJobs on the CE will be put on hold if they do not match any job routes after 30 minutes, but you can check a few things if you suspect that the jobs are not being matched. Check if the JobRouter sees a job before that by looking at the \njob router log\n and looking for the text \nsrc=\nJOB-ID\n\u2026claimed job\n.\n\n\nNext actions\n\n\nUse \ncondor_ce_job_router_info\n to see why your idle job does not match any routes\n\n\nIdle jobs on CE: Verify correct operation between the CE and your local batch system\n\n\nFor HTCondor batch systems\n\n\nHTCondor-CE submits jobs directly to an HTCondor batch system via the JobRouter, so any issues with the CE/local batch system interaction will appear in the \nJobRouterLog\n.\n\n\nNext actions\n\n\n\n\nCheck the \nJobRouterLog\n for failures.\n\n\nVerify that the local HTCondor is functional.\n\n\nUse \ncondor_ce_config_val\n to verify that the \nJOB_ROUTER_SCHEDD2_NAME\n, \nJOB_ROUTER_SCHEDD2_POOL\n, and \nJOB_ROUTER_SCHEDD2_SPOOL\n configuration variables are set to the hostname of your CE, the hostname and port of your local HTCondor\u2019s collector, and the location of your local HTCondor\u2019s spool directory, respectively.\n\n\nUse \ncondor_config_val QUEUE_SUPER_USER_MAY_IMPERSONATE\n and verify that it is set to \n.*\n.\n\n\n\n\nFor non-HTCondor batch systems\n\n\nHTCondor-CE submits jobs to a non-HTCondor batch system via the Gridmanager, so any issues with the CE/local batch system interaction will appear in the \nGridmanagerLog\n. Look for \ngm state change\u2026\n lines to figure out where the issures are occuring.\n\n\nNext actions\n\n\n\n\nIf you see failures in the GridmanagerLog during job submission:\n Save the submit files by adding the appropriate entry to \nblah.config\n and submit it \nmanually\n to the batch system. If that succeeds, make sure that the BLAHP knows where your binaries are located by setting the \nbatch system\n_binpath\n in \n/etc/blah.config\n.\n\n\n\n\nIf you see failures in the GridmanagerLog during queries for job status:\n Query the resultant job with your batch system tools from the CE. If you can, the BLAHP uses scripts to query for status in \n/usr/libexec/blahp/\nbatch system\n_status.sh\n (e.g., \n/usr/libexec/blahp/lsf_status.sh\n) that take the argument \nbatch system/YYYMMDD/job ID\n (e.g., \nlsf/20141008/65053\n). Run the appropriate status script for your batch system and upon success, you should see the following output:\n\n\nroot@host #\n /usr/libexec/blahp/lsf_status.sh lsf/20141008/65053\n\n[ BatchjobId = \n894862\n; JobStatus = 4; ExitCode = 0; WorkerNode = \natl-prod08\n ]\n\n\n\n\n\n\nIf the script fails, \nrequest help\n from the OSG.\n\n\n\n\n\n\nIdle jobs on CE: Make sure the underlying batch system can run jobs\n\n\nHTCondor-CE communicates directly with an HTCondor batch system schedd, so if jobs are not running, examine the \nSchedLog\n and diagnose the problem from there. For other batch systems, the BLAHP is used to submit jobs using your batch system\u2019s job submission binaries, whose location is specified in \n/etc/blah.config\n.\n\n\nProcedure\n\n\n\n\nManually create and submit a simple job (e.g., one that runs \nsleep\n)\n\n\nCheck for errors in the submission itself\n\n\nWatch the job in the batch system queue (e.g., using \ncondor_q\n)\n\n\nIf the job does not run, check for errors on the batch system\n\n\n\n\nNext actions\n\n\nIf the underlying batch system does not run a simple manual job, it will probably not run a job coming from HTCondor-CE. Once you can run simple manual jobs on your batch system, try submitting to the HTCondor-CE again.\n\n\nIdle jobs on CE: Verify ability to change permissions on key files\n\n\nHTCondor-CE needs the ability to write and chown files in its \nspool\n directory and if it cannot, jobs will not run at all. Spool permission errors can appear in the \nSchedLog\n and the \nJobRouterLog\n.\n\n\nSymptoms\n\n\n09/17/14 14:45:42 Error: Unable to chown \n/var/lib/condor-ce/spool/1/0/cluster1.proc0.subproc0/env\n from 12345 to 54321\n\n\n\n\n\nNext actions\n\n\n\n\nAs root, try to change ownership of the file or directory in question. If the file does not exist, a parent directory may have improper permissions.\n\n\nVerify that there aren't any underlying file system issues in the specified location\n\n\n\n\nJobs stay idle on a remote host submitting to the CE\n\n\nIf you are submitting your job from a separate submit host to the CE, it stays idle in the queue forever, and you do not see a resultant job in the CE's queue, this means that your job cannot contact the CE for submission or it is not authorized to run there. Note that jobs may take several minutes or longer if the CE is busy.\n\n\nRemote idle jobs: Can you contact the CE?\n\n\nTo check basic connectivity to a CE, use \ncondor_ce_ping\n:\n\n\nSymptoms\n\n\nuser@host $\n condor_ping -verbose -name condorce.example.com -pool condorce.example.com:9619 WRITE\n\nERROR: couldn\nt locate condorce.example.com!\n\n\n\n\n\n\nNext actions\n\n\n\n\nMake sure that the HTCondor-CE daemons are running with \ncondor_ce_status\n.\n\n\nVerify that your CE is reachable from your submit host, replacing \ncondorce.example.com\n with the hostname of your CE:\nuser@host $\n ping condorce.example.com\n\n\n\n\n\n\n\n\n\nRemote idle jobs: Are you authorized to run jobs on the CE?\n\n\nThe CE will only accept jobs from users that authenticate via LCMAPS, grid mapfile, or GUMS. You can use \ncondor_ce_ping\n to check if you are authorized and what user your proxy is being mapped to.\n\n\nSymptoms\n\n\nuser@host $\n condor_ping -verbose -name condorce.example.com -pool condorce.example.com:9619 WRITE\n\nRemote Version:              $CondorVersion: 8.0.7 Sep 24 2014 $\n\n\nLocal  Version:              $CondorVersion: 8.0.7 Sep 24 2014 $\n\n\nSession ID:                  condorce:3343:1412790611:0\n\n\nInstruction:                 WRITE\n\n\nCommand:                     60021\n\n\nEncryption:                  none\n\n\nIntegrity:                   MD5\n\n\nAuthenticated using:         GSI\n\n\nAll authentication methods:  GSI\n\n\nRemote Mapping:              gsi@unmapped\n\n\nAuthorized:                  FALSE\n\n\n\n\n\n\nNotice the failures in the above message: \nRemote Mapping: gsi@unmapped\n and \nAuthorized: FALSE\n\n\nNext actions\n\n\n\n\nVerify that an \nauthentication method\n is set up on the CE\n\n\nVerify that your user DN is mapped to an existing system user\n\n\n\n\nJobs go on hold\n\n\nJobs will be put on held with a \nHoldReason\n attribute that can be inspected with \ncondor_ce_q\n:\n\n\nuser@host $\n condor_ce_q -l \nJOB-ID\n -attr HoldReason\n\nHoldReason = \nCE job in status 5 put on hold by SYSTEM_PERIODIC_HOLD due to non-existent route or entry in JOB_ROUTER_ENTRIES.\n\n\n\n\n\n\nHeld jobs: Missing/expired user proxy\n\n\nHTCondor-CE requires a valid user proxy for each job that is submitted. You can check the status of your proxy with the following\n\n\nuser@host $\n voms-proxy-info -all\n\n\n\n\n\nNext actions\n\n\nEnsure that the owner of the job generates their proxy with \nvoms-proxy-init\n.\n\n\nHeld jobs: Invalid job universe\n\n\nThe HTCondor-CE only accepts jobs that have \nuniverse\n in their submit files set to \nvanilla\n, \nstandard\n, \nlocal\n, or \nscheduler\n. These universes also have corresponding integer values that can be found in the \nHTCondor manual\n.\n\n\nNext actions\n\n\n\n\nEnsure jobs submitted locally, from the CE host, are submitted with \nuniverse = vanilla\n\n\n\n\nEnsure jobs submitted from a remote submit point are submitted with:\n\n\nuniverse = grid\ngrid_resource = condor condorce.example.com condorce.example.com:9619\n\n\n\n\n\nreplacing \ncondorce.example.com\n with the hostname of the CE.\n\n\n\n\n\n\nHeld jobs: Non-existent route or entry in JOB_ROUTER_ENTRIES\n\n\nJobs on the CE will be put on hold if they do not match any job routes within 30 minutes.\n\n\nNext actions\n\n\nUse \ncondor_ce_job_router_info\n to see why your idle job does not match any routes.\n\n\nIdentifying the corresponding job ID on the local batch system\n\n\nWhen troubleshooting interactions between your CE and your local batch system, you will need to associate the CE job ID and the resultant job ID on the batch system. The methods for finding the resultant job ID differs between batch systems.\n\n\nHTCondor batch systems\n\n\n\n\n\n\nTo inspect the CE\u2019s job ad, use \ncondor_ce_q\n or \ncondor_ce_history\n:\n\n\n\n\n\n\nUse \ncondor_ce_q\n if the job is still in the CE\u2019s queue:\n\n\nuser@host $\n condor_ce_q \nJOB-ID\n -af RoutedToJobId\n\n\n\n\n\n\n\n\n\nUse \ncondor_ce_history\n if the job has left the CE\u2019s queue:\n\n\nuser@host $\n condor_ce_history \nJOB-ID\n -af RoutedToJobId\n\n\n\n\n\n\n\n\n\n\n\n\n\nParse the \nJobRouterLog\n for the CE\u2019s job ID.\n\n\n\n\n\n\nNon-HTCondor batch systems\n\n\nWhen HTCondor-CE records the corresponding batch system job ID, it is written in the form \nBATCH-SYSTEM\n/\nDATE\n/\nJOB ID\n:\n\n\nlsf/20141206/482046\n\n\n\n\n\n\n\n\n\nTo inspect the CE\u2019s job ad, use \ncondor_ce_q\n:\n\n\nuser@host $\n condor_ce_q \nJOB-ID\n -af GridJobId\n\n\n\n\n\n\n\n\n\nParse the \nGridmanagerLog\n for the CE\u2019s job ID.\n\n\n\n\n\n\nJobs removed from the local HTCondor pool become resubmitted (HTCondor batch systems only)\n\n\nBy design, HTCondor-CE will resubmit jobs that have been removed from the underlying HTCondor pool. Therefore, to remove misbehaving jobs, they will need to be removed on the CE level following these steps:\n\n\n\n\nIdentify the misbehaving job ID in your batch system queue\n\n\n\n\nFind the job's corresponding CE job ID:\n\n\nuser@host $\n condor_q \nJOB-ID\n -af RoutedFromJobId\n\n\n\n\n\n\n\n\n\nUse \ncondor_ce_rm\n to remove the CE job from the queue\n\n\n\n\n\n\nMissing HTCondor tools\n\n\nMost of the HTCondor-CE tools are just wrappers around existing HTCondor tools that load the CE-specific config. If you are trying to use HTCondor-CE tools and you see the following error:\n\n\nuser@host $\n condor_ce_job_router_info\n\n/usr/bin/condor_ce_job_router_info: line 6: exec: condor_job_router_info: not found\n\n\n\n\n\n\nThis means that the \ncondor_job_router_info\n (note this is not the CE version), is not in your \nPATH\n.\n\n\nNext Actions\n\n\n\n\nEither the condor RPM is missing or there are some other issues with it (try \nrpm --verify condor\n).\n\n\nYou have installed HTCondor in a non-standard location that is not in your \nPATH\n.\n\n\nThe \ncondor_job_router_info\n tool itself wasn't available until Condor-8.2.3-1.1 (available in osg-upcoming).\n\n\n\n\nHTCondor-CE Troubleshooting Tools\n\n\nHTCondor-CE has its own separate set of of the HTCondor tools with \nce\n in the name (i.e., \ncondor_ce_submit\n vs \ncondor_submit\n). Some of the the commands are only for the CE (e.g., \ncondor_ce_run\n and \ncondor_ce_trace\n) but many of them are just HTCondor commands configured to interact with the CE (e.g., \ncondor_ce_q\n, \ncondor_ce_status\n). It is important to differentiate the two: \ncondor_ce_config_val\n will provide configuration values for your HTCondor-CE while \ncondor_config_val\n will provide configuration values for your HTCondor batch system. If you are not running an HTCondor batch system, the non-CE commands will return errors.\n\n\ncondor_ce_trace\n\n\nUsage\n\n\ncondor_ce_trace\n is a useful tool for testing end-to-end job submission. It contacts both the CE\u2019s Schedd and Collector daemons to verify your permission to submit to the CE, displays the submit script that it submits to the CE, and tracks the resultant job.\n\n\n\n\nNote\n\n\nYou must have generated a proxy (e.g., \nvoms-proxy-init\n) and your DN must be added to your \nchosen authentication method\n.\n\n\n\n\nuser@host $\n condor_ce_trace condorce.example.com\n\n\n\n\n\nReplacing the \ncondorce.example.com\n with the hostname of the CE. If you are familiar with the output of condor commands, the command also takes a \n--debug\n option that displays verbose condor output.\n\n\nTroubleshooting\n\n\n\n\nIf the command fails with \u201cFailed ping\u2026\u201d:\n Make sure that the HTCondor-CE daemons are running on the CE\n\n\nIf you see \u201cgsi@unmapped\u201d in the \u201cRemote Mapping\u201d line:\n Either your credentials are not mapped on the CE or authentication is not set up at all. To set up authentication, refer to our \ninstallation document\n.\n\n\nIf the job submits but does not complete:\n Look at the status of the job and perform the relevant \ntroubleshooting steps\n.\n\n\n\n\ncondor_ce_run\n\n\nUsage\n\n\nSimilar to \nglobus-job-run\n, \ncondor_ce_run\n is a tool that submits a simple job to your CE, so it is useful for quickly submitting jobs through your CE. To submit a job to the CE and run the \nenv\n command on the remote batch system:\n\n\n\n\nNote\n\n\nYou must have generated a proxy (e.g., \nvoms-proxy-init\n) and your DN must be added to your \nchosen authentication method\n.\n\n\n\n\nuser@host $\n condor_ce_run -r condorce.example.com:9619 /bin/env\n\n\n\n\n\nReplacing the \ncondorce.example.com\n with the hostname of the CE. If you are troubleshooting an HTCondor-CE that you do not have a login for and the CE accepts local universe jobs, you can run commands locally on the CE with \ncondor_ce_run\n with the \n-l\n option. The following example outputs the JobRouterLog of the CE in question:\n\n\nuser@host $\n condor_ce_run -lr condorce.example.com:9619 cat /var/log/condor-ce/JobRouterLog\n\n\n\n\n\nReplacing the \ncondorce.example.com\n text with the hostname of the CE. To disable this feature on your CE, consult \nthis\n section of the install documentation.\n\n\nTroubleshooting\n\n\n\n\nIf you do not see any results:\n \ncondor_ce_run\n does not display results until the job completes on the CE, which may take several minutes or longer if the CE is busy. In the meantime, can use \ncondor_ce_q\n in a separate terminal to track the job on the CE. If you never see any results, use \ncondor_ce_trace\n to pinpoint errors.\n\n\nIf you see an error message that begins with \u201cFailed to\u2026\u201d:\n Check connectivity to the CE with \ncondor_ce_trace\n or \ncondor_ce_ping\n\n\n\n\ncondor_ce_submit\n\n\nSee the \nsubmitting to HTCondor-CE\n document for details.\n\n\ncondor_ce_ping\n\n\nUsage\n\n\nUse the following \ncondor_ce_ping\n command to test your ability to submit jobs to an HTCondor-CE, replacing \ncondorce.example.com\n with the hostname of your CE:\n\n\nuser@host $\n condor_ce_ping -verbose -name condorce.example.com -pool condorce.example.com:9619 WRITE\n\n\n\n\n\nThe following shows successful output where I am able to submit jobs (\nAuthorized: TRUE\n) as the glow user (\nRemote Mapping: glow@users.opensciencegrid.org\n):\n\n\nRemote Version:              $CondorVersion: 8.0.7 Sep 24 2014 $\n\n\nLocal  Version:              $CondorVersion: 8.0.7 Sep 24 2014 $\n\n\nSession ID:                  condorce:27407:1412286981:3\n\n\nInstruction:                 WRITE\n\n\nCommand:                     60021\n\n\nEncryption:                  none\n\n\nIntegrity:                   MD5\n\n\nAuthenticated using:         GSI\n\n\nAll authentication methods:  GSI\n\n\nRemote Mapping:              glow@users.opensciencegrid.org\n\n\nAuthorized:                  TRUE\n\n\n\n\n\n\n\n\nNote\n\n\nIf you run the \ncondor_ce_ping\n command on the CE that you are testing, omit the \n-name\n and \n-pool\n options. \ncondor_ce_ping\n takes the same arguments as \ncondor_ping\n and is documented in the \nHTCondor manual\n.\n\n\n\n\nTroubleshooting\n\n\n\n\n\n\nIf you see \u201cERROR: couldn\u2019t locate (null)\u201d\n, that means the HTCondor-CE schedd (the daemon that schedules jobs) cannot be reached. To track down the issue, increase debugging levels on the CE:\n\n\nMASTER_DEBUG = D_FULLDEBUG\nSCHEDD_DEBUG = D_FULLDEBUG\n\n\n\n\n\nThen look in the \nMasterLog\n and \nSchedLog\n for any errors.\n\n\n\n\n\n\nIf you see \u201cgsi@unmapped\u201d in the \u201cRemote Mapping\u201d line\n, this means that either your credentials are not mapped on the CE or that authentication is not set up at all. To set up authentication, refer to our \ninstallation document\n.\n\n\n\n\n\n\ncondor_ce_q\n\n\nUsage\n\n\ncondor_ce_q\n can display job status or specific job attributes for jobs that are still in the CE\u2019s queue. To list jobs that are queued on a CE:\n\n\nuser@host $\n condor_ce_q -name condorce.example.com -pool condorce.example.com:9619\n\n\n\n\n\nTo inspect the full ClassAd for a specific job, specify the \n-l\n flag and the job ID:\n\n\nuser@host $\n condor_ce_q -name condorce.example.com -pool condorce.example.com:9619 -l \nJOB-ID\n\n\n\n\n\n\n\n\nNote\n\n\nIf you run the \ncondor_ce_q\n command on the CE that you are testing, omit the \n-name\n and \n-pool\n options. \ncondor_ce_q\n takes the same arguments as \ncondor_q\n and is documented in the \nHTCondor manual\n.\n\n\n\n\nTroubleshooting\n\n\nIf the jobs that you are submiting to a CE are not completing, \ncondor_ce_q\n can tell you the status of your jobs.\n\n\n\n\n\n\nIf the schedd is not running:\n You will see a lengthy message about being unable to contact the schedd. To track down the issue, increase the debugging levels on the CE with:\n\n\nMASTER_DEBUG = D_FULLDEBUG\nSCHEDD_DEBUG = D_FULLDEBUG\n\n\n\n\n\nTo apply these changes, reconfigure HTCondor-CE:\n\n\nroot@host #\n condor_ce_reconfig\n\n\n\n\n\nThen look in the \nMasterLog\n and \nSchedLog\n on the CE for any errors.\n\n\n\n\n\n\nIf there are issues with contacting the collector:\n You will see the following message:\n\n\nuser@host $\n condor_ce_q -pool ce1.accre.vanderbilt.edu -name ce1.accre.vanderbilt.edu\n\n\n-- Failed to fetch ads from: \n129.59.197.223:9620?sock`33630_8b33_4\n : ce1.accre.vanderbilt.edu\n\n\n\n\n\n\nThis may be due to network issues or bad HTCondor daemon permissions. To fix the latter issue, ensure that the \nALLOW_READ\n configuration value is not set:\n\n\nuser@host $\n condor_ce_config_val -v ALLOW_READ\n\nNot defined: ALLOW_READ\n\n\n\n\n\n\nIf it is defined, remove it from the file that is returned in the output.\n\n\n\n\n\n\nIf a job is held:\n There should be an accompanying \nHoldReason\n that will tell you why it is being held. The \nHoldReason\n is in the job\u2019s ClassAd, so you can use the long form of \ncondor_ce_q\n to extract its value:\n\n\nuser@host $\n condor_ce_q -name condorce.example.com -pool condorce.example.com:9619 -l \nJob ID\n \n|\n grep HoldReason\n\n\n\n\n\n\n\n\n\nIf a job is idle:\n The most common cause is that it is not matching any routes in the CE\u2019s job router. To find out whether this is the case, use the \ncondor_ce_job_router_info\n.\n\n\n\n\n\n\ncondor_ce_history\n\n\nUsage\n\n\ncondor_ce_history\n can display job status or specific job attributes for jobs that have that have left the CE\u2019s queue. To list jobs that have run on the CE:\n\n\nuser@host $\n condor_ce_history -name condorce.example.com -pool condorce.example.com:9619\n\n\n\n\n\nTo inspect the full ClassAd for a specific job, specify the \n-l\n flag and the job ID:\n\n\nuser@host $\n condor_ce_history -name condorce.example.com -pool condorce.example.com:9619 -l \nJob ID\n\n\n\n\n\n\n\n\nNote\n\n\nIf you run the \ncondor_ce_history\n command on the CE that you are testing, omit the \n-name\n and \n-pool\n options. \ncondor_ce_history\n takes the same arguments as \ncondor_history\n and is documented in the \nHTCondor manual\n.\n\n\n\n\ncondor_ce_job_router_info\n\n\nUsage\n\n\nUse the \ncondor_ce_job_router_info\n command to help troubleshoot your routes and how jobs will match to them. To see all of your routes (the output is long because it combines your routes with the \nJOB_ROUTER_DEFAULTS\n configuration variable):\n\n\nroot@host #\n condor_ce_job_router_info -config\n\n\n\n\n\nTo see how the job router is handling a job that is currently in the CE\u2019s queue, analyze the output of \ncondor_ce_q\n (replace the \nJOB-ID\n with the job ID that you are interested in):\n\n\nroot@host #\n condor_ce_q -l \nJOB-ID\n \n|\n condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads -\n\n\n\n\n\nTo inspect a job that has already left the queue, use \ncondor_ce_history\n instead of \ncondor_ce_q\n:\n\n\nroot@host #\n condor_ce_history -l \nJOB-ID\n \n|\n condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads -\n\n\n\n\n\n\n\nNote\n\n\nIf the proxy for the job has expired, the job will not match any routes. To work around this constraint:\n\n\n\n\nroot@host #\n condor_ce_history -l \nJOB-ID\n \n|\n sed \ns/^\\(x509UserProxyExpiration\\) = .*/\\1 = `date +%s --date \n+1 sec\n`/\n \n|\n condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads -\n\n\n\n\n\nAlternatively, you can provide a file containing a job\u2019s ClassAd as the input and edit attributes within that file:\n\n\nroot@host #\n condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads \nJOBAD-FILE\n\n\n\n\n\n\nTroubleshooting\n\n\n\n\nIf the job does not match any route:\n You can identify this case when you see \n0 candidate jobs found\n in the \ncondor_job_router_info\n output. This message means that, when compared to your job\u2019s ClassAd, the Umbrella constraint does not evaluate to \ntrue\n. When troubleshooting, look at all of the expressions prior to the \ntarget.ProcId \n= 0\n expression, because it and everything following it is logic that the job router added so that routed jobs do not get routed again.\n\n\n\n\nIf your job matches more than one route:\n the tool will tell you by showing all matching routes after the job ID:\n\n\nChecking Job src=162,0 against all routes\nRoute Matches: Local_PBS\nRoute Matches: Condor_Test\n\n\n\n\n\nTo troubleshoot why this is occuring, look at the combined Requirements expressions for all routes and compare it to the job\u2019s ClassAd provided. The combined Requirements expression is \nhighlighted below\n:\n\n\nUmbrella constraint: ((target.x509userproxysubject =!= UNDEFINED) \n\n(target.x509UserProxyExpiration =!= UNDEFINED) \n\n(time() \n target.x509UserProxyExpiration) \n\n(target.JobUniverse =?= 5 || target.JobUniverse =?= 1)) \n\n\n( (target.osgTestPBS is true) || (true) )\n \n\n(target.ProcId \n= 0 \n target.JobStatus == 1 \n\n(target.StageInStart is undefined || target.StageInFinish isnt undefined) \n\ntarget.Managed isnt \nScheddDone\n \n\ntarget.Managed isnt \nExtenal\n \n\ntarget.Owner isnt Undefined \n\ntarget.RoutedBy isnt \nhtcondor-ce\n)\n\n\n\n\n\nBoth routes evaluate to \ntrue\n for the job\u2019s ClassAd because it contained \nosgTestPBS = true\n. Make sure your routes are mutually exclusive, otherwise you may have jobs routed incorrectly! See the \njob route configuration page\n for more details.\n\n\n\n\n\n\nIf it is unclear why jobs are matching a route:\n wrap the route's requirements expression in \ndebug()\n and check the \nJobRouterLog\n for more information.\n\n\n\n\n\n\ncondor_ce_router_q\n\n\nUsage\n\n\nIf you have multiple job routes and many jobs, \ncondor_ce_router_q\n is a useful tool to see how jobs are being routed and their statuses:\n\n\nuser@host $\n condor_ce_router_q\n\n\n\n\n\ncondor_ce_router_q\n takes the same options as \ncondor_router_q\n and \ncondor_q\n and is documented in the \nHTCondor manual\n\n\ncondor_ce_status\n\n\nUsage\n\n\nTo see the daemons running on a CE, you can run the following:\n\n\nuser@host $\n condor_ce_status -any -name condorce.example.com -pool condorce.example.com:9619\n\n\n\n\n\nReplacing \ncondorce.example.com\n  with the hostname of the CE.\n\n\n\n\nNote\n\n\nIf you run the \ncondor_ce_status\n command on the CE that you are testing, omit the \n-name\n and \n-pool\n options. \ncondor_ce_status\n takes the same arguments as \ncondor_status\n and is documented in the \nHTCondor manual\n.\n\n\n\n\nTroubleshooting\n\n\nTo list the daemons that are configured to run:\n\n\nuser@host $\n condor_ce_config_val -v DAEMON_LIST\n\nDAEMON_LIST: MASTER COLLECTOR SCHEDD JOB_ROUTER, SHARED_PORT, SHARED_PORT\n\n\n  Defined in \n/etc/condor-ce/config.d/03-ce-shared-port.conf\n, line 9.\n\n\n\n\n\n\nIf you do not see these daemons in the output of \ncondor_ce_status\n, check the \nMaster log\n for errors.\n\n\ncondor_ce_config_val\n\n\nUsage\n\n\nTo see the value of configuration variables and where they are set, use \ncondor_ce_config_val\n. Primarily, This tool is used with the other troubleshooting tools to make sure your configuration is set properly. To see the value of a single variable and where it is set:\n\n\nuser@host $\n condor_ce_config_val -v \nCONFIGURATION-VARIABLE\n\n\n\n\n\n\nTo see a list of all configuration variables and their values:\n\n\nuser@host $\n condor_ce_config_val -dump\n\n\n\n\n\nTo see a list of all the files that are used to create your configuration and the order that they are parsed, use the following command:\n\n\nuser@host $\n condor_ce_config_val -config\n\n\n\n\n\ncondor_ce_config_val\n takes the same arguments as \ncondor_config_val\n and is documented in the \nHTCondor manual\n.\n\n\ncondor_ce_reconfig\n\n\nUsage\n\n\nTo ensure that your configuration changes have taken effect, run \ncondor_ce_reconfig\n.\n\n\nuser@host $\n condor_ce_reconfig\n\n\n\n\n\ncondor_ce_{on,off,restart}\n\n\nUsage\n\n\nTo turn on/off/restart HTCondor-CE daemons, use the following commands:\n\n\nroot@host #\n condor_ce_on\n\nroot@host #\n condor_ce_off\n\nroot@host #\n condor_ce_restart\n\n\n\n\n\nThe HTCondor-CE service uses the previous commands with default values. Using these commands directly gives you more fine-grained control over the behavior of HTCondor-CE's on/off/restart:\n\n\n\n\n\n\nIf you have installed a new version of HTCondor-CE and want to restart the CE under the new version, run the following command:\n\n\nroot@host #\n condor_ce_restart -fast\n\n\n\n\n\nThis will cause HTCondor-CE to restart and quickly reconnect to all running jobs.\n\n\n\n\n\n\nIf you need to stop running new jobs, run the following:\n\n\nroot@host #\n condor_ce_off -peaceful\n\n\n\n\n\nThis will cause HTCondor-CE to accept new jobs without starting them and will wait for currently running jobs to complete before shutting down.\n\n\n\n\n\n\nHTCondor-CE Troubleshooting Data\n\n\nThe following files are located on the CE host.\n\n\nMasterLog\n\n\nThe HTCondor-CE master log tracks status of all of the other HTCondor daemons and thus contains valuable information if they fail to start.\n\n\n\n\nLocation: \n/var/log/condor-ce/MasterLog\n\n\nKey contents: Start-up, shut-down, and communication with other HTCondor daemons\n\n\n\n\nIncreasing the debug level:\n\n\n\n\n\n\nSet the following value in \n/etc/condor-ce/config.d/99-local.conf\n on the CE host:\n\n\nMASTER_DEBUG = D_FULLDEBUG\n\n\n\n\n\n\n\n\n\nTo apply these changes, reconfigure HTCondor-CE:\n\n\nroot@host #\n condor_ce_reconfig\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat to look for:\n\n\nSuccessful daemon start-up. The following line shows that the Collector daemon started successfully:\n\n\n10/07/14 14:20:27 Started DaemonCore process \n/usr/sbin/condor_collector -f -port 9619\n, pid and pgroup = 7318\n\n\n\n\n\nSchedLog\n\n\nThe HTCondor-CE schedd log contains information on all jobs that are submitted to the CE. It contains valuable information when trying to troubleshoot authentication issues.\n\n\n\n\nLocation: \n/var/log/condor-ce/SchedLog\n\n\nKey contents:\n\n\nEvery job submitted to the CE\n\n\nUser authorization events\n\n\n\n\n\n\n\n\nIncreasing the debug level:\n\n\n\n\n\n\nSet the following value in \n/etc/condor-ce/config.d/99-local.conf\n on the CE host:\n\n\nSCHEDD_DEBUG = D_FULLDEBUG\n\n\n\n\n\n\n\n\n\nTo apply these changes, reconfigure HTCondor-CE:\n\n\nroot@host #\n condor_ce_reconfig\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat to look for\n\n\n\n\n\n\nJob is submitted to the CE queue:\n\n\n10/07/14 16:52:17 Submitting new job 234.0\n\n\n\n\n\nIn this example, the ID of the submitted job is \n234.0\n.\n\n\n\n\n\n\nJob owner is authorized and mapped:\n\n\n10/07/14 16:52:17 Command=QMGMT_WRITE_CMD, peer=\n131.225.154.68:42262\n\n10/07/14 16:52:17 AuthMethod=GSI, AuthId=/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Brian Lin 1047,\n                  /GLOW/Role=NULL/Capability=NULL, \nCondorId=glow@users.opensciencegrid.org\n\n\n\n\n\n\nIn this example, the job is authorized with the job\u2019s proxy subject using GSI and is mapped to the \nglow\n user.\n\n\n\n\n\n\nUser job submission fails\n due to improper authentication or authorization:\n\n\n08/30/16 16:52:56 DC_AUTHENTICATE: required authentication of 72.33.0.189\n                  failed: AUTHENTICATE:1003:Failed to authenticate with any\n                  method|AUTHENTICATE:1004:Failed to authenticate using GSI|GSI:5002:Failed to\n                  authenticate because the remote (client) side was not able to acquire its\n                  credentials.|AUTHENTICATE:1004:Failed to authenticate using FS|FS:1004:Unable to\n                  lstat(/tmp/FS_XXXZpUlYa)\n08/30/16 16:53:12 PERMISSION DENIED to \ngsi@unmapped\n\n                  from host 72.33.0.189 for command 60021 (DC_NOP_WRITE), access level WRITE:\n                  reason: WRITE authorization policy contains no matching ALLOW entry for this\n                  request; identifiers used for this host:\n                  72.33.0.189,dyn-72-33-0-189.uwnet.wisc.edu, hostname size = 1, original ip\n                  address = 72.33.0.189\n08/30/16 16:53:12 DC_AUTHENTICATE: Command not authorized, done\n\n\n\n\n\n\n\n\n\nMissing negotiator:\n\n\n10/18/14 17:32:21 Can\nt find address for negotiator\n10/18/14 17:32:21 Failed to send RESCHEDULE to unknown daemon:\n\n\n\n\n\nSince HTCondor-CE does not manage any resources, it does not run a negotiator daemon by default and this error message is expected. In the same vein, you may see messages that there are 0 worker nodes:\n\n\n06/23/15 11:15:03 Number of Active Workers 0\n\n\n\n\n\n\n\n\n\nCorrupted \njob_queue.log\n:\n\n\n02/07/17 10:55:49 WARNING: Encountered corrupt log record _654 (byte offset 5046225)\n02/07/17 10:55:49 103 1354325.0 PeriodicRemove ( StageInFinish \n 0 ) 105\n02/07/17 10:55:49 Lines following corrupt log record _654 (up to 3):\n02/07/17 10:55:49 103 1346101.0 RemoteWallClockTime 116668.000000\n02/07/17 10:55:49 104 1346101.0 WallClockCheckpoint\n02/07/17 10:55:49 104 1346101.0 ShadowBday\n02/07/17 10:55:49 ERROR \nError: corrupt log record _654 (byte offset 5046225) occurred inside closed transaction,\n                  recovery failed\n at line 1080 in file /builddir/build/BUILD/condor-8.4.8/src/condor_utils/classad_log.cpp\n\n\n\n\n\nThis means \n/var/lib/condor-ce/spool/job_queue.log\n has been corrupted and you will need to hand-remove the offending record by searching for the text specified after the \nLines following corrupt log record...\n line. The most common culprit of the corruption is that the disk containing the \njob_queue.log\n has filled up. To avoid this problem, you can change the location of \njob_queue.log\n by setting \nJOB_QUEUE_LOG\n in \n/etc/condor-ce/config.d/\n to a path, preferably one on a large SSD.\n\n\n\n\n\n\nJobRouterLog\n\n\nThe HTCondor-CE job router log produced by the job router itself and thus contains valuable information when trying to troubleshoot issues with job routing.\n\n\n\n\nLocation: \n/var/log/condor-ce/JobRouterLog\n\n\nKey contents:\n\n\nEvery attempt to route a job\n\n\nRouting success messages\n\n\nJob attribute changes, based on chosen route\n\n\nJob submission errors to an HTCondor batch system\n\n\nCorresponding job IDs on an HTCondor batch system\n\n\n\n\n\n\n\n\nIncreasing the debug level:\n\n\n\n\n\n\nSet the following value in \n/etc/condor-ce/config.d/99-local.conf\n on the CE host:\n\n\nJOB_ROUTER_DEBUG = D_FULLDEBUG\n\n\n\n\n\n\n\n\n\nApply these changes, reconfigure HTCondor-CE:\n\n\nroot@host #\n condor_ce_reconfig\n\n\n\n\n\n\n\n\n\n\n\n\n\nKnown Errors\n\n\n\n\n\n\nIf you have \nD_FULLDEBUG\n turned on for the job router, you will see errors like the following:\n\n\n06/12/15 14:00:28 HOOK_UPDATE_JOB_INFO not configured.\n\n\n\n\n\nYou can safely ignore these.\n\n\n\n\n\n\nWhat to look for\n\n\n\n\n\n\nJob is considered for routing:\n\n\n09/17/14 15:00:56 JobRouter (src=86.0,route=Local_LSF): found candidate job\n\n\n\n\n\nIn parentheses are the original HTCondor-CE job ID (e.g., \n86.0\n) and the route (e.g., \nLocal_LSF\n).\n\n\n\n\n\n\nJob is successfully routed:\n\n\n09/17/14 15:00:57 JobRouter (src=86.0,route=Local_LSF): claimed job\n\n\n\n\n\n\n\n\n\nFinding the corresponding job ID on your HTCondor batch system:\n\n\n09/17/14 15:00:57 JobRouter (src=86.0,dest=205.0,route=Local_Condor): claimed job\n\n\n\n\n\nIn parentheses are the original HTCondor-CE job ID (e.g., \n86.0\n) and the resultant job ID on the HTCondor batch system (e.g., \n205.0\n)\n\n\n\n\n\n\nIf your job is not routed, there will not be any evidence of it within the log itself. To investigate why your jobs are not being considered for routing, use the \ncondor_ce_job_router_info\n\n\n\n\nHTCondor batch systems only\n: The following error occurs when the job router daemon cannot submit the routed job:\n10/19/14 13:09:15 Can\nt resolve collector condorce.example.com; skipping\n10/19/14 13:09:15 ERROR (pool condorce.example.com) Can\nt find address of schedd\n10/19/14 13:09:15 JobRouter failure (src=5.0,route=Local_Condor): failed to submit job\n\n\n\n\n\n\n\n\n\nGridmanagerLog\n\n\nThe HTCondor-CE grid manager log tracks the submission and status of jobs on non-HTCondor batch systems. It contains valuable information when trying to troubleshoot jobs that have been routed but failed to complete. Details on how to read the Gridmanager log can be found on the \nHTCondor Wiki\n.\n\n\n\n\nLocation: \n/var/log/condor-ce/GridmanagerLog.\nJOB-OWNER\n\n\nKey contents:\n\n\nEvery attempt to submit a job to a batch system or other grid resource\n\n\nStatus updates of submitted jobs\n\n\nCorresponding job IDs on non-HTCondor batch systems\n\n\n\n\n\n\n\n\nIncreasing the debug level:\n\n\n\n\n\n\nSet the following value in \n/etc/condor-ce/config.d/99-local.conf\n on the CE host:\n\n\nMAX_GRIDMANAGER_LOG = 6h\nMAX_NUM_GRIDMANAGER_LOG = 8\nGRIDMANAGER_DEBUG = D_FULLDEBUG\n\n\n\n\n\n\n\n\n\nTo apply these changes, reconfigure HTCondor-CE:\n\n\nroot@host #\n condor_ce_reconfig\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat to look for\n\n\n\n\n\n\nJob is submitted to the batch system:\n\n\n09/17/14 09:51:34 [12997] (85.0) gm state change: GM_SUBMIT_SAVE -\n GM_SUBMITTED\n\n\n\n\n\nEvery state change the Gridmanager tracks should have the job ID in parentheses (i.e.=(85.0)).\n\n\n\n\n\n\nJob status being updated:\n\n\n09/17/14 15:07:24 [25543] (87.0) gm state change: GM_SUBMITTED -\n GM_POLL_ACTIVE\n09/17/14 15:07:24 [25543] GAHP[25563] \n- \nBLAH_JOB_STATUS 3 lsf/20140917/482046\n\n09/17/14 15:07:24 [25543] GAHP[25563] -\n \nS\n\n09/17/14 15:07:25 [25543] GAHP[25563] \n- \nRESULTS\n\n09/17/14 15:07:25 [25543] GAHP[25563] -\n \nR\n\n09/17/14 15:07:25 [25543] GAHP[25563] -\n \nS\n \n1\n\n09/17/14 15:07:25 [25543] GAHP[25563] -\n \n3\n \n0\n \nNo Error\n \n4\n \n[ BatchjobId = \n482046\n; JobStatus = 4; ExitCode = 0; WorkerNode = \natl-prod08\n ]\n\n\n\n\n\n\nThe first line tells us that the Gridmanager is initiating a status update and the following lines are the results. The most interesting line is the second highlighted section that notes the job ID on the batch system and its status. If there are errors querying the job on the batch system, they will appear here.\n\n\n\n\n\n\nFinding the corresponding job ID on your non-HTCondor batch system:\n\n\n09/17/14 15:07:24 [25543] (87.0) gm state change: GM_SUBMITTED -\n GM_POLL_ACTIVE\n09/17/14 15:07:24 [25543] GAHP[25563] \n- \nBLAH_JOB_STATUS 3 lsf/20140917/482046\n\n\n\n\n\n\nOn the first line, after the timestamp and PID of the Gridmanager process, you will find the CE\u2019s job ID in parentheses, \n(87.0)\n. At the end of the second line, you will find the batch system, date, and batch system job id separated by slashes, \nlsf/20140917/482046\n.\n\n\n\n\n\n\nJob completion on the batch system:\n\n\n09/17/14 15:07:25 [25543] (87.0) gm state change: GM_TRANSFER_OUTPUT -\n GM_DONE_SAVE\n\n\n\n\n\n\n\n\n\nSharedPortLog\n\n\nThe HTCondor-CE shared port log keeps track of all connections to all of the HTCondor-CE daemons other than the collector. This log is a good place to check if experiencing connectivity issues with HTCondor-CE. More information can be found \nhere\n.\n\n\n\n\nLocation: \n/var/log/condor-ce/SharedPortLog\n\n\nKey contents: Every attempt to connect to HTCondor-CE (except collector queries)\n\n\n\n\nIncreasing the debug level:\n\n\n\n\n\n\nSet the following value in \n/etc/condor-ce/config.d/99-local.conf\n on the CE host:\n\n\nSHARED_PORT_DEBUG = D_FULLDEBUG\n\n\n\n\n\n\n\n\n\nTo apply these changes, reconfigure HTCondor-CE:\n\n\nroot@host #\n condor_ce_reconfig\n\n\n\n\n\n\n\n\n\n\n\n\n\nMessages log\n\n\nThe messages file can include output from lcmaps, which handles mapping of X.509 proxies to Unix usernames. If there are issues with the \nauthentication setup\n, the errors may appear here.\n\n\n\n\nLocation: \n/var/log/messages\n\n\nKey contents: User authentication\n\n\n\n\nWhat to look for\n\n\nA user is mapped:\n\n\nOct 6 10:35:32 osgserv06 htondor-ce-llgt[12147]: Callout to \nLCMAPS\n returned local user (service condor): \nosgglow01\n\n\n\n\n\n\nBLAHP Configuration File\n\n\nHTCondor-CE uses the BLAHP to submit jobs to your local non-HTCondor batch system using your batch system's client tools.\n\n\n\n\nLocation: \n/etc/blah.config\n\n\nKey contents:\n\n\nLocations of the batch system's client binaries and logs\n\n\nLocation to save files that are submitted to the local batch system\n\n\n\n\n\n\n\n\nYou can also tell the BLAHP to save the files that are being submitted to the local batch system to \nDIR-NAME\n by adding the following line:\n\n\nblah_debug_save_submit_info=\nDIR_NAME\n\n\n\n\n\n\nThe BLAHP will then create a directory with the format \nbl_*\n for each submission to the local jobmanager with the submit file and proxy used.\n\n\n\n\nNote\n\n\nWhitespace is important so do not put any spaces around the = sign. In addition, the directory must be created and HTCondor-CE should have sufficient permissions to create directories within \nDIR_NAME\n.\n\n\n\n\nGetting Help\n\n\nIf you are still experiencing issues after using this document, please let us know!\n\n\n\n\nGather basic HTCondor-CE and related information (versions, relevant configuration, problem description, etc.)\n\n\n\n\nGather system information:\n\n\nroot@host # osg-system-profiler\n\n\n\n\n\n\n\n\n\nStart a support request using \na web interface\n or by email to \n\n\n\n\nDescribe issue and expected or desired behavior\n\n\nInclude basic HTCondor-CE and related information\n\n\nAttach the osg-system-profiler output\n\n\n\n\n\n\n\n\nReference\n\n\nHere are some other HTCondor-CE documents that might be helpful:\n\n\n\n\nHTCondor-CE overview and architecture\n\n\nInstalling HTCondor-CE\n\n\nConfiguring HTCondor-CE job routes\n\n\nSubmitting jobs to HTCondor-CE", 
            "title": "Troubleshooting HTCondor-CE"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#htcondor-ce-troubleshooting-guide", 
            "text": "In this document, you will find a collection of files and commands to help troubleshoot HTCondor-CE along with a list of common issues with suggested troubleshooting steps.", 
            "title": "HTCondor-CE Troubleshooting Guide"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#known-issues", 
            "text": "", 
            "title": "Known Issues"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#submit_exprs-are-not-applied-to-jobs-on-the-local-htcondor", 
            "text": "If you are adding attributes to jobs submitted to your HTCondor pool with  SUBMIT_EXPRS , these will  not  be applied to jobs that are entering your pool from the HTCondor-CE. To get around this, you will want to add the attributes to your  job routes . If the CE is the only entry point for jobs into your pool, you can get rid of  SUBMIT_EXPRS  on your backend. Otherwise, you will have to maintain your list of attributes both in your list of routes and in your  SUBMIT_EXPRS .", 
            "title": "SUBMIT_EXPRS are not applied to jobs on the local HTCondor"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#general-troubleshooting-items", 
            "text": "", 
            "title": "General Troubleshooting Items"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#making-sure-packages-are-up-to-date", 
            "text": "It is important to make sure that the HTCondor-CE and related RPMs are up-to-date.  root@host #  yum update  htcondor-ce*  blahp condor  If you just want to see the packages to update, but do not want to perform the update now, answer  N  at the prompt.", 
            "title": "Making sure packages are up-to-date"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#verify-package-contents", 
            "text": "If the contents of your HTCondor-CE packages have been changed, the CE may cease to function properly. To verify the contents of your packages (ignoring changes to configuration files):  user@host $  rpm -q --verify htcondor-ce htcondor-ce-client blahp  |  awk  $2 !=  c  {print $0}   If the verification command returns output, this means that your packages have been changed. To fix this, you can reinstall the packages:  user@host $  yum reinstall htcondor-ce htcondor-ce-client blahp   Note  The reinstall command may place original versions of configuration files alongside the versions that you have modified. If this is the case, the reinstall command will notify you that the original versions will have an  .rpmnew  suffix. Further inspection of these files may be required as to whether or not you need to merge them into your current configuration.", 
            "title": "Verify package contents"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#verify-clocks-are-synchronized", 
            "text": "Like all GSI-based authentication, HTCondor-CE is sensitive to time skews. Make sure the clock on your CE is synchronized using a utility such as  ntpd . Additionally, HTCondor itself is sensitive to time skews on the NFS server. If you see empty stdout / err being returned to the submitter, verify there is no NFS server time skew.", 
            "title": "Verify clocks are synchronized"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#htcondor-ce-troubleshooting-items", 
            "text": "This section contains common issues you may encounter using HTCondor-CE and next actions to take when you do. Before troubleshooting, we recommend increasing the log level:    Write the following into  /etc/condor-ce/config.d/99-local.conf  to increase the log level for all daemons:  ALL_DEBUG = D_FULLDEBUG    Ensure that the configuration is in place:  root@host # condor_ce_reconfig    Reproduce the issue     Note  Before spending any time on troubleshooting, you should ensure that the state of configuration is as expected by running  condor_ce_reconfig .", 
            "title": "HTCondor-CE Troubleshooting Items"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#daemons-fail-to-start", 
            "text": "If there are errors in your configuration of HTCondor-CE, this may cause some of its required daemons to fail to startup. Check the following subsections in order:  Symptoms  Daemon startup failure may manifest in many ways, the following are few symptoms of the problem.    The service fails to start:  root@host #  service condor-ce start Starting Condor-CE daemons: [ FAIL ]     condor_ce_q  fails with a lengthy error message:  user@host $  condor_ce_q Error:  Extra Info: You probably saw this error because the condor_schedd is not running  on the machine you are trying to query. If the condor_schedd is not running, the  Condor system will not be able to find an address and port to connect to and  satisfy this request. Please make sure the Condor daemons are running and try  again.  Extra Info: If the condor_schedd is running on the machine you are trying to  query and you still see the error, the most likely cause is that you have setup  a personal Condor, you have not defined SCHEDD_NAME in your condor_config file,  and something is wrong with your SCHEDD_ADDRESS_FILE setting. You must define  either or both of those settings in your config file, or you must use the -name  option to condor_q. Please see the Condor manual for details on SCHEDD_NAME and  SCHEDD_ADDRESS_FILE.     Next actions   If the MasterLog is filled with  ERROR:SECMAN...TCP connection to collector...failed :  This is likely due to a misconfiguration for a host with multiple network interfaces. Verify that you have followed the instructions in  this  section of the install guide.  If the MasterLog is filled with  DC_AUTHENTICATE  errors:  The HTCondor-CE daemons use the host certificate to authenticate with each other. Verify that your host certificate\u2019s DN matches one of the regular expressions found in  /etc/condor-ce/condor_mapfile .  If the SchedLog is filled with  Can\u2019t find address for negotiator :  You can ignore this error! The negotiator daemon is used in HTCondor batch systems to match jobs with resources but since HTCondor-CE does not manage any resources directly, it does not run one.", 
            "title": "Daemons fail to start"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#jobs-fail-to-submit-to-the-ce", 
            "text": "If a user is having issues submitting jobs to the CE and you've ruled out general connectivity or firewalls as the culprit, then you may have encountered an authentication or authorization issue. You may see error messages like the following in your  SchedLog :  08/30/16 16:52:56 DC_AUTHENTICATE: required authentication of 72.33.0.189 failed: AUTHENTICATE:1003:Failed to authenticate with any method|AUTHENTICATE:1004:Failed to authenticate using GSI|GSI:5002:Failed to authenticate because the remote (client) side was not able to acquire its credentials.|AUTHENTICATE:1004:Failed to authenticate using FS|FS:1004:Unable to lstat(/tmp/FS_XXXZpUlYa)\n08/30/16 16:53:12 PERMISSION DENIED to gsi@unmapped from host 72.33.0.189 for command 60021 (DC_NOP_WRITE), access level WRITE: reason: WRITE authorization policy contains no matching ALLOW entry for this request; identifiers used for this host: 72.33.0.189,dyn-72-33-0-189.uwnet.wisc.edu, hostname size = 1, original ip address = 72.33.0.189\n08/30/16 16:53:12 DC_AUTHENTICATE: Command not authorized, done!  Next actions   Check GUMS or grid-mapfile  and ensure that the user's DN is known to your  authentication method  Check for lcmaps errors  in  /var/log/messages  If you do not see helpful error messages in  /var/log/messages ,  adjust the debug level by adding  export LCMAPS_DEBUG_LEVEL=5  to  /etc/sysconfig/condor-ce , restarting the condor-ce service, and checking  /var/log/messages  for errors again.", 
            "title": "Jobs fail to submit to the CE"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#jobs-stay-idle-on-the-ce", 
            "text": "Check the following subsections in order, but note that jobs may take several minutes or longer to run if the CE is busy.", 
            "title": "Jobs stay idle on the CE"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#idle-jobs-on-ce-is-the-job-router-handling-the-incoming-job", 
            "text": "Jobs on the CE will be put on hold if they do not match any job routes after 30 minutes, but you can check a few things if you suspect that the jobs are not being matched. Check if the JobRouter sees a job before that by looking at the  job router log  and looking for the text  src= JOB-ID \u2026claimed job .  Next actions  Use  condor_ce_job_router_info  to see why your idle job does not match any routes", 
            "title": "Idle jobs on CE: Is the job router handling the incoming job?"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#idle-jobs-on-ce-verify-correct-operation-between-the-ce-and-your-local-batch-system", 
            "text": "", 
            "title": "Idle jobs on CE: Verify correct operation between the CE and your local batch system"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#for-htcondor-batch-systems", 
            "text": "HTCondor-CE submits jobs directly to an HTCondor batch system via the JobRouter, so any issues with the CE/local batch system interaction will appear in the  JobRouterLog .  Next actions   Check the  JobRouterLog  for failures.  Verify that the local HTCondor is functional.  Use  condor_ce_config_val  to verify that the  JOB_ROUTER_SCHEDD2_NAME ,  JOB_ROUTER_SCHEDD2_POOL , and  JOB_ROUTER_SCHEDD2_SPOOL  configuration variables are set to the hostname of your CE, the hostname and port of your local HTCondor\u2019s collector, and the location of your local HTCondor\u2019s spool directory, respectively.  Use  condor_config_val QUEUE_SUPER_USER_MAY_IMPERSONATE  and verify that it is set to  .* .", 
            "title": "For HTCondor batch systems"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#for-non-htcondor-batch-systems", 
            "text": "HTCondor-CE submits jobs to a non-HTCondor batch system via the Gridmanager, so any issues with the CE/local batch system interaction will appear in the  GridmanagerLog . Look for  gm state change\u2026  lines to figure out where the issures are occuring.  Next actions   If you see failures in the GridmanagerLog during job submission:  Save the submit files by adding the appropriate entry to  blah.config  and submit it  manually  to the batch system. If that succeeds, make sure that the BLAHP knows where your binaries are located by setting the  batch system _binpath  in  /etc/blah.config .   If you see failures in the GridmanagerLog during queries for job status:  Query the resultant job with your batch system tools from the CE. If you can, the BLAHP uses scripts to query for status in  /usr/libexec/blahp/ batch system _status.sh  (e.g.,  /usr/libexec/blahp/lsf_status.sh ) that take the argument  batch system/YYYMMDD/job ID  (e.g.,  lsf/20141008/65053 ). Run the appropriate status script for your batch system and upon success, you should see the following output:  root@host #  /usr/libexec/blahp/lsf_status.sh lsf/20141008/65053 [ BatchjobId =  894862 ; JobStatus = 4; ExitCode = 0; WorkerNode =  atl-prod08  ]   If the script fails,  request help  from the OSG.", 
            "title": "For non-HTCondor batch systems"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#idle-jobs-on-ce-make-sure-the-underlying-batch-system-can-run-jobs", 
            "text": "HTCondor-CE communicates directly with an HTCondor batch system schedd, so if jobs are not running, examine the  SchedLog  and diagnose the problem from there. For other batch systems, the BLAHP is used to submit jobs using your batch system\u2019s job submission binaries, whose location is specified in  /etc/blah.config .  Procedure   Manually create and submit a simple job (e.g., one that runs  sleep )  Check for errors in the submission itself  Watch the job in the batch system queue (e.g., using  condor_q )  If the job does not run, check for errors on the batch system   Next actions  If the underlying batch system does not run a simple manual job, it will probably not run a job coming from HTCondor-CE. Once you can run simple manual jobs on your batch system, try submitting to the HTCondor-CE again.", 
            "title": "Idle jobs on CE: Make sure the underlying batch system can run jobs"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#idle-jobs-on-ce-verify-ability-to-change-permissions-on-key-files", 
            "text": "HTCondor-CE needs the ability to write and chown files in its  spool  directory and if it cannot, jobs will not run at all. Spool permission errors can appear in the  SchedLog  and the  JobRouterLog .  Symptoms  09/17/14 14:45:42 Error: Unable to chown  /var/lib/condor-ce/spool/1/0/cluster1.proc0.subproc0/env  from 12345 to 54321  Next actions   As root, try to change ownership of the file or directory in question. If the file does not exist, a parent directory may have improper permissions.  Verify that there aren't any underlying file system issues in the specified location", 
            "title": "Idle jobs on CE: Verify ability to change permissions on key files"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#jobs-stay-idle-on-a-remote-host-submitting-to-the-ce", 
            "text": "If you are submitting your job from a separate submit host to the CE, it stays idle in the queue forever, and you do not see a resultant job in the CE's queue, this means that your job cannot contact the CE for submission or it is not authorized to run there. Note that jobs may take several minutes or longer if the CE is busy.", 
            "title": "Jobs stay idle on a remote host submitting to the CE"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#remote-idle-jobs-can-you-contact-the-ce", 
            "text": "To check basic connectivity to a CE, use  condor_ce_ping :  Symptoms  user@host $  condor_ping -verbose -name condorce.example.com -pool condorce.example.com:9619 WRITE ERROR: couldn t locate condorce.example.com!   Next actions   Make sure that the HTCondor-CE daemons are running with  condor_ce_status .  Verify that your CE is reachable from your submit host, replacing  condorce.example.com  with the hostname of your CE: user@host $  ping condorce.example.com", 
            "title": "Remote idle jobs: Can you contact the CE?"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#remote-idle-jobs-are-you-authorized-to-run-jobs-on-the-ce", 
            "text": "The CE will only accept jobs from users that authenticate via LCMAPS, grid mapfile, or GUMS. You can use  condor_ce_ping  to check if you are authorized and what user your proxy is being mapped to.  Symptoms  user@host $  condor_ping -verbose -name condorce.example.com -pool condorce.example.com:9619 WRITE Remote Version:              $CondorVersion: 8.0.7 Sep 24 2014 $  Local  Version:              $CondorVersion: 8.0.7 Sep 24 2014 $  Session ID:                  condorce:3343:1412790611:0  Instruction:                 WRITE  Command:                     60021  Encryption:                  none  Integrity:                   MD5  Authenticated using:         GSI  All authentication methods:  GSI  Remote Mapping:              gsi@unmapped  Authorized:                  FALSE   Notice the failures in the above message:  Remote Mapping: gsi@unmapped  and  Authorized: FALSE  Next actions   Verify that an  authentication method  is set up on the CE  Verify that your user DN is mapped to an existing system user", 
            "title": "Remote idle jobs: Are you authorized to run jobs on the CE?"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#jobs-go-on-hold", 
            "text": "Jobs will be put on held with a  HoldReason  attribute that can be inspected with  condor_ce_q :  user@host $  condor_ce_q -l  JOB-ID  -attr HoldReason HoldReason =  CE job in status 5 put on hold by SYSTEM_PERIODIC_HOLD due to non-existent route or entry in JOB_ROUTER_ENTRIES.", 
            "title": "Jobs go on hold"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#held-jobs-missingexpired-user-proxy", 
            "text": "HTCondor-CE requires a valid user proxy for each job that is submitted. You can check the status of your proxy with the following  user@host $  voms-proxy-info -all  Next actions  Ensure that the owner of the job generates their proxy with  voms-proxy-init .", 
            "title": "Held jobs: Missing/expired user proxy"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#held-jobs-invalid-job-universe", 
            "text": "The HTCondor-CE only accepts jobs that have  universe  in their submit files set to  vanilla ,  standard ,  local , or  scheduler . These universes also have corresponding integer values that can be found in the  HTCondor manual .  Next actions   Ensure jobs submitted locally, from the CE host, are submitted with  universe = vanilla   Ensure jobs submitted from a remote submit point are submitted with:  universe = grid\ngrid_resource = condor condorce.example.com condorce.example.com:9619  replacing  condorce.example.com  with the hostname of the CE.", 
            "title": "Held jobs: Invalid job universe"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#held-jobs-non-existent-route-or-entry-in-job_router_entries", 
            "text": "Jobs on the CE will be put on hold if they do not match any job routes within 30 minutes.  Next actions  Use  condor_ce_job_router_info  to see why your idle job does not match any routes.", 
            "title": "Held jobs: Non-existent route or entry in JOB_ROUTER_ENTRIES"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#identifying-the-corresponding-job-id-on-the-local-batch-system", 
            "text": "When troubleshooting interactions between your CE and your local batch system, you will need to associate the CE job ID and the resultant job ID on the batch system. The methods for finding the resultant job ID differs between batch systems.", 
            "title": "Identifying the corresponding job ID on the local batch system"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#htcondor-batch-systems", 
            "text": "To inspect the CE\u2019s job ad, use  condor_ce_q  or  condor_ce_history :    Use  condor_ce_q  if the job is still in the CE\u2019s queue:  user@host $  condor_ce_q  JOB-ID  -af RoutedToJobId    Use  condor_ce_history  if the job has left the CE\u2019s queue:  user@host $  condor_ce_history  JOB-ID  -af RoutedToJobId      Parse the  JobRouterLog  for the CE\u2019s job ID.", 
            "title": "HTCondor batch systems"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#non-htcondor-batch-systems", 
            "text": "When HTCondor-CE records the corresponding batch system job ID, it is written in the form  BATCH-SYSTEM / DATE / JOB ID :  lsf/20141206/482046    To inspect the CE\u2019s job ad, use  condor_ce_q :  user@host $  condor_ce_q  JOB-ID  -af GridJobId    Parse the  GridmanagerLog  for the CE\u2019s job ID.", 
            "title": "Non-HTCondor batch systems"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#jobs-removed-from-the-local-htcondor-pool-become-resubmitted-htcondor-batch-systems-only", 
            "text": "By design, HTCondor-CE will resubmit jobs that have been removed from the underlying HTCondor pool. Therefore, to remove misbehaving jobs, they will need to be removed on the CE level following these steps:   Identify the misbehaving job ID in your batch system queue   Find the job's corresponding CE job ID:  user@host $  condor_q  JOB-ID  -af RoutedFromJobId    Use  condor_ce_rm  to remove the CE job from the queue", 
            "title": "Jobs removed from the local HTCondor pool become resubmitted (HTCondor batch systems only)"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#missing-htcondor-tools", 
            "text": "Most of the HTCondor-CE tools are just wrappers around existing HTCondor tools that load the CE-specific config. If you are trying to use HTCondor-CE tools and you see the following error:  user@host $  condor_ce_job_router_info /usr/bin/condor_ce_job_router_info: line 6: exec: condor_job_router_info: not found   This means that the  condor_job_router_info  (note this is not the CE version), is not in your  PATH .  Next Actions   Either the condor RPM is missing or there are some other issues with it (try  rpm --verify condor ).  You have installed HTCondor in a non-standard location that is not in your  PATH .  The  condor_job_router_info  tool itself wasn't available until Condor-8.2.3-1.1 (available in osg-upcoming).", 
            "title": "Missing HTCondor tools"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#htcondor-ce-troubleshooting-tools", 
            "text": "HTCondor-CE has its own separate set of of the HTCondor tools with  ce  in the name (i.e.,  condor_ce_submit  vs  condor_submit ). Some of the the commands are only for the CE (e.g.,  condor_ce_run  and  condor_ce_trace ) but many of them are just HTCondor commands configured to interact with the CE (e.g.,  condor_ce_q ,  condor_ce_status ). It is important to differentiate the two:  condor_ce_config_val  will provide configuration values for your HTCondor-CE while  condor_config_val  will provide configuration values for your HTCondor batch system. If you are not running an HTCondor batch system, the non-CE commands will return errors.", 
            "title": "HTCondor-CE Troubleshooting Tools"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_trace", 
            "text": "", 
            "title": "condor_ce_trace"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#usage", 
            "text": "condor_ce_trace  is a useful tool for testing end-to-end job submission. It contacts both the CE\u2019s Schedd and Collector daemons to verify your permission to submit to the CE, displays the submit script that it submits to the CE, and tracks the resultant job.   Note  You must have generated a proxy (e.g.,  voms-proxy-init ) and your DN must be added to your  chosen authentication method .   user@host $  condor_ce_trace condorce.example.com  Replacing the  condorce.example.com  with the hostname of the CE. If you are familiar with the output of condor commands, the command also takes a  --debug  option that displays verbose condor output.", 
            "title": "Usage"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#troubleshooting", 
            "text": "If the command fails with \u201cFailed ping\u2026\u201d:  Make sure that the HTCondor-CE daemons are running on the CE  If you see \u201cgsi@unmapped\u201d in the \u201cRemote Mapping\u201d line:  Either your credentials are not mapped on the CE or authentication is not set up at all. To set up authentication, refer to our  installation document .  If the job submits but does not complete:  Look at the status of the job and perform the relevant  troubleshooting steps .", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_run", 
            "text": "", 
            "title": "condor_ce_run"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#usage_1", 
            "text": "Similar to  globus-job-run ,  condor_ce_run  is a tool that submits a simple job to your CE, so it is useful for quickly submitting jobs through your CE. To submit a job to the CE and run the  env  command on the remote batch system:   Note  You must have generated a proxy (e.g.,  voms-proxy-init ) and your DN must be added to your  chosen authentication method .   user@host $  condor_ce_run -r condorce.example.com:9619 /bin/env  Replacing the  condorce.example.com  with the hostname of the CE. If you are troubleshooting an HTCondor-CE that you do not have a login for and the CE accepts local universe jobs, you can run commands locally on the CE with  condor_ce_run  with the  -l  option. The following example outputs the JobRouterLog of the CE in question:  user@host $  condor_ce_run -lr condorce.example.com:9619 cat /var/log/condor-ce/JobRouterLog  Replacing the  condorce.example.com  text with the hostname of the CE. To disable this feature on your CE, consult  this  section of the install documentation.", 
            "title": "Usage"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#troubleshooting_1", 
            "text": "If you do not see any results:   condor_ce_run  does not display results until the job completes on the CE, which may take several minutes or longer if the CE is busy. In the meantime, can use  condor_ce_q  in a separate terminal to track the job on the CE. If you never see any results, use  condor_ce_trace  to pinpoint errors.  If you see an error message that begins with \u201cFailed to\u2026\u201d:  Check connectivity to the CE with  condor_ce_trace  or  condor_ce_ping", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_submit", 
            "text": "See the  submitting to HTCondor-CE  document for details.", 
            "title": "condor_ce_submit"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_ping", 
            "text": "", 
            "title": "condor_ce_ping"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#usage_2", 
            "text": "Use the following  condor_ce_ping  command to test your ability to submit jobs to an HTCondor-CE, replacing  condorce.example.com  with the hostname of your CE:  user@host $  condor_ce_ping -verbose -name condorce.example.com -pool condorce.example.com:9619 WRITE  The following shows successful output where I am able to submit jobs ( Authorized: TRUE ) as the glow user ( Remote Mapping: glow@users.opensciencegrid.org ):  Remote Version:              $CondorVersion: 8.0.7 Sep 24 2014 $  Local  Version:              $CondorVersion: 8.0.7 Sep 24 2014 $  Session ID:                  condorce:27407:1412286981:3  Instruction:                 WRITE  Command:                     60021  Encryption:                  none  Integrity:                   MD5  Authenticated using:         GSI  All authentication methods:  GSI  Remote Mapping:              glow@users.opensciencegrid.org  Authorized:                  TRUE    Note  If you run the  condor_ce_ping  command on the CE that you are testing, omit the  -name  and  -pool  options.  condor_ce_ping  takes the same arguments as  condor_ping  and is documented in the  HTCondor manual .", 
            "title": "Usage"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#troubleshooting_2", 
            "text": "If you see \u201cERROR: couldn\u2019t locate (null)\u201d , that means the HTCondor-CE schedd (the daemon that schedules jobs) cannot be reached. To track down the issue, increase debugging levels on the CE:  MASTER_DEBUG = D_FULLDEBUG\nSCHEDD_DEBUG = D_FULLDEBUG  Then look in the  MasterLog  and  SchedLog  for any errors.    If you see \u201cgsi@unmapped\u201d in the \u201cRemote Mapping\u201d line , this means that either your credentials are not mapped on the CE or that authentication is not set up at all. To set up authentication, refer to our  installation document .", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_q", 
            "text": "", 
            "title": "condor_ce_q"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#usage_3", 
            "text": "condor_ce_q  can display job status or specific job attributes for jobs that are still in the CE\u2019s queue. To list jobs that are queued on a CE:  user@host $  condor_ce_q -name condorce.example.com -pool condorce.example.com:9619  To inspect the full ClassAd for a specific job, specify the  -l  flag and the job ID:  user@host $  condor_ce_q -name condorce.example.com -pool condorce.example.com:9619 -l  JOB-ID    Note  If you run the  condor_ce_q  command on the CE that you are testing, omit the  -name  and  -pool  options.  condor_ce_q  takes the same arguments as  condor_q  and is documented in the  HTCondor manual .", 
            "title": "Usage"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#troubleshooting_3", 
            "text": "If the jobs that you are submiting to a CE are not completing,  condor_ce_q  can tell you the status of your jobs.    If the schedd is not running:  You will see a lengthy message about being unable to contact the schedd. To track down the issue, increase the debugging levels on the CE with:  MASTER_DEBUG = D_FULLDEBUG\nSCHEDD_DEBUG = D_FULLDEBUG  To apply these changes, reconfigure HTCondor-CE:  root@host #  condor_ce_reconfig  Then look in the  MasterLog  and  SchedLog  on the CE for any errors.    If there are issues with contacting the collector:  You will see the following message:  user@host $  condor_ce_q -pool ce1.accre.vanderbilt.edu -name ce1.accre.vanderbilt.edu -- Failed to fetch ads from:  129.59.197.223:9620?sock`33630_8b33_4  : ce1.accre.vanderbilt.edu   This may be due to network issues or bad HTCondor daemon permissions. To fix the latter issue, ensure that the  ALLOW_READ  configuration value is not set:  user@host $  condor_ce_config_val -v ALLOW_READ Not defined: ALLOW_READ   If it is defined, remove it from the file that is returned in the output.    If a job is held:  There should be an accompanying  HoldReason  that will tell you why it is being held. The  HoldReason  is in the job\u2019s ClassAd, so you can use the long form of  condor_ce_q  to extract its value:  user@host $  condor_ce_q -name condorce.example.com -pool condorce.example.com:9619 -l  Job ID   |  grep HoldReason    If a job is idle:  The most common cause is that it is not matching any routes in the CE\u2019s job router. To find out whether this is the case, use the  condor_ce_job_router_info .", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_history", 
            "text": "", 
            "title": "condor_ce_history"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#usage_4", 
            "text": "condor_ce_history  can display job status or specific job attributes for jobs that have that have left the CE\u2019s queue. To list jobs that have run on the CE:  user@host $  condor_ce_history -name condorce.example.com -pool condorce.example.com:9619  To inspect the full ClassAd for a specific job, specify the  -l  flag and the job ID:  user@host $  condor_ce_history -name condorce.example.com -pool condorce.example.com:9619 -l  Job ID    Note  If you run the  condor_ce_history  command on the CE that you are testing, omit the  -name  and  -pool  options.  condor_ce_history  takes the same arguments as  condor_history  and is documented in the  HTCondor manual .", 
            "title": "Usage"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_job_router_info", 
            "text": "", 
            "title": "condor_ce_job_router_info"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#usage_5", 
            "text": "Use the  condor_ce_job_router_info  command to help troubleshoot your routes and how jobs will match to them. To see all of your routes (the output is long because it combines your routes with the  JOB_ROUTER_DEFAULTS  configuration variable):  root@host #  condor_ce_job_router_info -config  To see how the job router is handling a job that is currently in the CE\u2019s queue, analyze the output of  condor_ce_q  (replace the  JOB-ID  with the job ID that you are interested in):  root@host #  condor_ce_q -l  JOB-ID   |  condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads -  To inspect a job that has already left the queue, use  condor_ce_history  instead of  condor_ce_q :  root@host #  condor_ce_history -l  JOB-ID   |  condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads -   Note  If the proxy for the job has expired, the job will not match any routes. To work around this constraint:   root@host #  condor_ce_history -l  JOB-ID   |  sed  s/^\\(x509UserProxyExpiration\\) = .*/\\1 = `date +%s --date  +1 sec `/   |  condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads -  Alternatively, you can provide a file containing a job\u2019s ClassAd as the input and edit attributes within that file:  root@host #  condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads  JOBAD-FILE", 
            "title": "Usage"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#troubleshooting_4", 
            "text": "If the job does not match any route:  You can identify this case when you see  0 candidate jobs found  in the  condor_job_router_info  output. This message means that, when compared to your job\u2019s ClassAd, the Umbrella constraint does not evaluate to  true . When troubleshooting, look at all of the expressions prior to the  target.ProcId  = 0  expression, because it and everything following it is logic that the job router added so that routed jobs do not get routed again.   If your job matches more than one route:  the tool will tell you by showing all matching routes after the job ID:  Checking Job src=162,0 against all routes\nRoute Matches: Local_PBS\nRoute Matches: Condor_Test  To troubleshoot why this is occuring, look at the combined Requirements expressions for all routes and compare it to the job\u2019s ClassAd provided. The combined Requirements expression is  highlighted below :  Umbrella constraint: ((target.x509userproxysubject =!= UNDEFINED)  \n(target.x509UserProxyExpiration =!= UNDEFINED)  \n(time()   target.x509UserProxyExpiration)  \n(target.JobUniverse =?= 5 || target.JobUniverse =?= 1))   ( (target.osgTestPBS is true) || (true) )   \n(target.ProcId  = 0   target.JobStatus == 1  \n(target.StageInStart is undefined || target.StageInFinish isnt undefined)  \ntarget.Managed isnt  ScheddDone   \ntarget.Managed isnt  Extenal   \ntarget.Owner isnt Undefined  \ntarget.RoutedBy isnt  htcondor-ce )  Both routes evaluate to  true  for the job\u2019s ClassAd because it contained  osgTestPBS = true . Make sure your routes are mutually exclusive, otherwise you may have jobs routed incorrectly! See the  job route configuration page  for more details.    If it is unclear why jobs are matching a route:  wrap the route's requirements expression in  debug()  and check the  JobRouterLog  for more information.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_router_q", 
            "text": "", 
            "title": "condor_ce_router_q"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#usage_6", 
            "text": "If you have multiple job routes and many jobs,  condor_ce_router_q  is a useful tool to see how jobs are being routed and their statuses:  user@host $  condor_ce_router_q  condor_ce_router_q  takes the same options as  condor_router_q  and  condor_q  and is documented in the  HTCondor manual", 
            "title": "Usage"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_status", 
            "text": "", 
            "title": "condor_ce_status"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#usage_7", 
            "text": "To see the daemons running on a CE, you can run the following:  user@host $  condor_ce_status -any -name condorce.example.com -pool condorce.example.com:9619  Replacing  condorce.example.com   with the hostname of the CE.   Note  If you run the  condor_ce_status  command on the CE that you are testing, omit the  -name  and  -pool  options.  condor_ce_status  takes the same arguments as  condor_status  and is documented in the  HTCondor manual .", 
            "title": "Usage"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#troubleshooting_5", 
            "text": "To list the daemons that are configured to run:  user@host $  condor_ce_config_val -v DAEMON_LIST DAEMON_LIST: MASTER COLLECTOR SCHEDD JOB_ROUTER, SHARED_PORT, SHARED_PORT    Defined in  /etc/condor-ce/config.d/03-ce-shared-port.conf , line 9.   If you do not see these daemons in the output of  condor_ce_status , check the  Master log  for errors.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_config_val", 
            "text": "", 
            "title": "condor_ce_config_val"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#usage_8", 
            "text": "To see the value of configuration variables and where they are set, use  condor_ce_config_val . Primarily, This tool is used with the other troubleshooting tools to make sure your configuration is set properly. To see the value of a single variable and where it is set:  user@host $  condor_ce_config_val -v  CONFIGURATION-VARIABLE   To see a list of all configuration variables and their values:  user@host $  condor_ce_config_val -dump  To see a list of all the files that are used to create your configuration and the order that they are parsed, use the following command:  user@host $  condor_ce_config_val -config  condor_ce_config_val  takes the same arguments as  condor_config_val  and is documented in the  HTCondor manual .", 
            "title": "Usage"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_reconfig", 
            "text": "", 
            "title": "condor_ce_reconfig"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#usage_9", 
            "text": "To ensure that your configuration changes have taken effect, run  condor_ce_reconfig .  user@host $  condor_ce_reconfig", 
            "title": "Usage"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_onoffrestart", 
            "text": "", 
            "title": "condor_ce_{on,off,restart}"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#usage_10", 
            "text": "To turn on/off/restart HTCondor-CE daemons, use the following commands:  root@host #  condor_ce_on root@host #  condor_ce_off root@host #  condor_ce_restart  The HTCondor-CE service uses the previous commands with default values. Using these commands directly gives you more fine-grained control over the behavior of HTCondor-CE's on/off/restart:    If you have installed a new version of HTCondor-CE and want to restart the CE under the new version, run the following command:  root@host #  condor_ce_restart -fast  This will cause HTCondor-CE to restart and quickly reconnect to all running jobs.    If you need to stop running new jobs, run the following:  root@host #  condor_ce_off -peaceful  This will cause HTCondor-CE to accept new jobs without starting them and will wait for currently running jobs to complete before shutting down.", 
            "title": "Usage"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#htcondor-ce-troubleshooting-data", 
            "text": "The following files are located on the CE host.", 
            "title": "HTCondor-CE Troubleshooting Data"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#masterlog", 
            "text": "The HTCondor-CE master log tracks status of all of the other HTCondor daemons and thus contains valuable information if they fail to start.   Location:  /var/log/condor-ce/MasterLog  Key contents: Start-up, shut-down, and communication with other HTCondor daemons   Increasing the debug level:    Set the following value in  /etc/condor-ce/config.d/99-local.conf  on the CE host:  MASTER_DEBUG = D_FULLDEBUG    To apply these changes, reconfigure HTCondor-CE:  root@host #  condor_ce_reconfig", 
            "title": "MasterLog"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#what-to-look-for", 
            "text": "Successful daemon start-up. The following line shows that the Collector daemon started successfully:  10/07/14 14:20:27 Started DaemonCore process  /usr/sbin/condor_collector -f -port 9619 , pid and pgroup = 7318", 
            "title": "What to look for:"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#schedlog", 
            "text": "The HTCondor-CE schedd log contains information on all jobs that are submitted to the CE. It contains valuable information when trying to troubleshoot authentication issues.   Location:  /var/log/condor-ce/SchedLog  Key contents:  Every job submitted to the CE  User authorization events     Increasing the debug level:    Set the following value in  /etc/condor-ce/config.d/99-local.conf  on the CE host:  SCHEDD_DEBUG = D_FULLDEBUG    To apply these changes, reconfigure HTCondor-CE:  root@host #  condor_ce_reconfig", 
            "title": "SchedLog"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#what-to-look-for_1", 
            "text": "Job is submitted to the CE queue:  10/07/14 16:52:17 Submitting new job 234.0  In this example, the ID of the submitted job is  234.0 .    Job owner is authorized and mapped:  10/07/14 16:52:17 Command=QMGMT_WRITE_CMD, peer= 131.225.154.68:42262 \n10/07/14 16:52:17 AuthMethod=GSI, AuthId=/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=People/CN=Brian Lin 1047,\n                  /GLOW/Role=NULL/Capability=NULL,  CondorId=glow@users.opensciencegrid.org   In this example, the job is authorized with the job\u2019s proxy subject using GSI and is mapped to the  glow  user.    User job submission fails  due to improper authentication or authorization:  08/30/16 16:52:56 DC_AUTHENTICATE: required authentication of 72.33.0.189\n                  failed: AUTHENTICATE:1003:Failed to authenticate with any\n                  method|AUTHENTICATE:1004:Failed to authenticate using GSI|GSI:5002:Failed to\n                  authenticate because the remote (client) side was not able to acquire its\n                  credentials.|AUTHENTICATE:1004:Failed to authenticate using FS|FS:1004:Unable to\n                  lstat(/tmp/FS_XXXZpUlYa)\n08/30/16 16:53:12 PERMISSION DENIED to  gsi@unmapped \n                  from host 72.33.0.189 for command 60021 (DC_NOP_WRITE), access level WRITE:\n                  reason: WRITE authorization policy contains no matching ALLOW entry for this\n                  request; identifiers used for this host:\n                  72.33.0.189,dyn-72-33-0-189.uwnet.wisc.edu, hostname size = 1, original ip\n                  address = 72.33.0.189\n08/30/16 16:53:12 DC_AUTHENTICATE: Command not authorized, done    Missing negotiator:  10/18/14 17:32:21 Can t find address for negotiator\n10/18/14 17:32:21 Failed to send RESCHEDULE to unknown daemon:  Since HTCondor-CE does not manage any resources, it does not run a negotiator daemon by default and this error message is expected. In the same vein, you may see messages that there are 0 worker nodes:  06/23/15 11:15:03 Number of Active Workers 0    Corrupted  job_queue.log :  02/07/17 10:55:49 WARNING: Encountered corrupt log record _654 (byte offset 5046225)\n02/07/17 10:55:49 103 1354325.0 PeriodicRemove ( StageInFinish   0 ) 105\n02/07/17 10:55:49 Lines following corrupt log record _654 (up to 3):\n02/07/17 10:55:49 103 1346101.0 RemoteWallClockTime 116668.000000\n02/07/17 10:55:49 104 1346101.0 WallClockCheckpoint\n02/07/17 10:55:49 104 1346101.0 ShadowBday\n02/07/17 10:55:49 ERROR  Error: corrupt log record _654 (byte offset 5046225) occurred inside closed transaction,\n                  recovery failed  at line 1080 in file /builddir/build/BUILD/condor-8.4.8/src/condor_utils/classad_log.cpp  This means  /var/lib/condor-ce/spool/job_queue.log  has been corrupted and you will need to hand-remove the offending record by searching for the text specified after the  Lines following corrupt log record...  line. The most common culprit of the corruption is that the disk containing the  job_queue.log  has filled up. To avoid this problem, you can change the location of  job_queue.log  by setting  JOB_QUEUE_LOG  in  /etc/condor-ce/config.d/  to a path, preferably one on a large SSD.", 
            "title": "What to look for"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#jobrouterlog", 
            "text": "The HTCondor-CE job router log produced by the job router itself and thus contains valuable information when trying to troubleshoot issues with job routing.   Location:  /var/log/condor-ce/JobRouterLog  Key contents:  Every attempt to route a job  Routing success messages  Job attribute changes, based on chosen route  Job submission errors to an HTCondor batch system  Corresponding job IDs on an HTCondor batch system     Increasing the debug level:    Set the following value in  /etc/condor-ce/config.d/99-local.conf  on the CE host:  JOB_ROUTER_DEBUG = D_FULLDEBUG    Apply these changes, reconfigure HTCondor-CE:  root@host #  condor_ce_reconfig", 
            "title": "JobRouterLog"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#known-errors", 
            "text": "If you have  D_FULLDEBUG  turned on for the job router, you will see errors like the following:  06/12/15 14:00:28 HOOK_UPDATE_JOB_INFO not configured.  You can safely ignore these.", 
            "title": "Known Errors"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#what-to-look-for_2", 
            "text": "Job is considered for routing:  09/17/14 15:00:56 JobRouter (src=86.0,route=Local_LSF): found candidate job  In parentheses are the original HTCondor-CE job ID (e.g.,  86.0 ) and the route (e.g.,  Local_LSF ).    Job is successfully routed:  09/17/14 15:00:57 JobRouter (src=86.0,route=Local_LSF): claimed job    Finding the corresponding job ID on your HTCondor batch system:  09/17/14 15:00:57 JobRouter (src=86.0,dest=205.0,route=Local_Condor): claimed job  In parentheses are the original HTCondor-CE job ID (e.g.,  86.0 ) and the resultant job ID on the HTCondor batch system (e.g.,  205.0 )    If your job is not routed, there will not be any evidence of it within the log itself. To investigate why your jobs are not being considered for routing, use the  condor_ce_job_router_info   HTCondor batch systems only : The following error occurs when the job router daemon cannot submit the routed job: 10/19/14 13:09:15 Can t resolve collector condorce.example.com; skipping\n10/19/14 13:09:15 ERROR (pool condorce.example.com) Can t find address of schedd\n10/19/14 13:09:15 JobRouter failure (src=5.0,route=Local_Condor): failed to submit job", 
            "title": "What to look for"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#gridmanagerlog", 
            "text": "The HTCondor-CE grid manager log tracks the submission and status of jobs on non-HTCondor batch systems. It contains valuable information when trying to troubleshoot jobs that have been routed but failed to complete. Details on how to read the Gridmanager log can be found on the  HTCondor Wiki .   Location:  /var/log/condor-ce/GridmanagerLog. JOB-OWNER  Key contents:  Every attempt to submit a job to a batch system or other grid resource  Status updates of submitted jobs  Corresponding job IDs on non-HTCondor batch systems     Increasing the debug level:    Set the following value in  /etc/condor-ce/config.d/99-local.conf  on the CE host:  MAX_GRIDMANAGER_LOG = 6h\nMAX_NUM_GRIDMANAGER_LOG = 8\nGRIDMANAGER_DEBUG = D_FULLDEBUG    To apply these changes, reconfigure HTCondor-CE:  root@host #  condor_ce_reconfig", 
            "title": "GridmanagerLog"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#what-to-look-for_3", 
            "text": "Job is submitted to the batch system:  09/17/14 09:51:34 [12997] (85.0) gm state change: GM_SUBMIT_SAVE -  GM_SUBMITTED  Every state change the Gridmanager tracks should have the job ID in parentheses (i.e.=(85.0)).    Job status being updated:  09/17/14 15:07:24 [25543] (87.0) gm state change: GM_SUBMITTED -  GM_POLL_ACTIVE\n09/17/14 15:07:24 [25543] GAHP[25563]  -  BLAH_JOB_STATUS 3 lsf/20140917/482046 \n09/17/14 15:07:24 [25543] GAHP[25563] -   S \n09/17/14 15:07:25 [25543] GAHP[25563]  -  RESULTS \n09/17/14 15:07:25 [25543] GAHP[25563] -   R \n09/17/14 15:07:25 [25543] GAHP[25563] -   S   1 \n09/17/14 15:07:25 [25543] GAHP[25563] -   3   0   No Error   4   [ BatchjobId =  482046 ; JobStatus = 4; ExitCode = 0; WorkerNode =  atl-prod08  ]   The first line tells us that the Gridmanager is initiating a status update and the following lines are the results. The most interesting line is the second highlighted section that notes the job ID on the batch system and its status. If there are errors querying the job on the batch system, they will appear here.    Finding the corresponding job ID on your non-HTCondor batch system:  09/17/14 15:07:24 [25543] (87.0) gm state change: GM_SUBMITTED -  GM_POLL_ACTIVE\n09/17/14 15:07:24 [25543] GAHP[25563]  -  BLAH_JOB_STATUS 3 lsf/20140917/482046   On the first line, after the timestamp and PID of the Gridmanager process, you will find the CE\u2019s job ID in parentheses,  (87.0) . At the end of the second line, you will find the batch system, date, and batch system job id separated by slashes,  lsf/20140917/482046 .    Job completion on the batch system:  09/17/14 15:07:25 [25543] (87.0) gm state change: GM_TRANSFER_OUTPUT -  GM_DONE_SAVE", 
            "title": "What to look for"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#sharedportlog", 
            "text": "The HTCondor-CE shared port log keeps track of all connections to all of the HTCondor-CE daemons other than the collector. This log is a good place to check if experiencing connectivity issues with HTCondor-CE. More information can be found  here .   Location:  /var/log/condor-ce/SharedPortLog  Key contents: Every attempt to connect to HTCondor-CE (except collector queries)   Increasing the debug level:    Set the following value in  /etc/condor-ce/config.d/99-local.conf  on the CE host:  SHARED_PORT_DEBUG = D_FULLDEBUG    To apply these changes, reconfigure HTCondor-CE:  root@host #  condor_ce_reconfig", 
            "title": "SharedPortLog"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#messages-log", 
            "text": "The messages file can include output from lcmaps, which handles mapping of X.509 proxies to Unix usernames. If there are issues with the  authentication setup , the errors may appear here.   Location:  /var/log/messages  Key contents: User authentication", 
            "title": "Messages log"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#what-to-look-for_4", 
            "text": "A user is mapped:  Oct 6 10:35:32 osgserv06 htondor-ce-llgt[12147]: Callout to  LCMAPS  returned local user (service condor):  osgglow01", 
            "title": "What to look for"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#blahp-configuration-file", 
            "text": "HTCondor-CE uses the BLAHP to submit jobs to your local non-HTCondor batch system using your batch system's client tools.   Location:  /etc/blah.config  Key contents:  Locations of the batch system's client binaries and logs  Location to save files that are submitted to the local batch system     You can also tell the BLAHP to save the files that are being submitted to the local batch system to  DIR-NAME  by adding the following line:  blah_debug_save_submit_info= DIR_NAME   The BLAHP will then create a directory with the format  bl_*  for each submission to the local jobmanager with the submit file and proxy used.   Note  Whitespace is important so do not put any spaces around the = sign. In addition, the directory must be created and HTCondor-CE should have sufficient permissions to create directories within  DIR_NAME .", 
            "title": "BLAHP Configuration File"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#getting-help", 
            "text": "If you are still experiencing issues after using this document, please let us know!   Gather basic HTCondor-CE and related information (versions, relevant configuration, problem description, etc.)   Gather system information:  root@host # osg-system-profiler    Start a support request using  a web interface  or by email to    Describe issue and expected or desired behavior  Include basic HTCondor-CE and related information  Attach the osg-system-profiler output", 
            "title": "Getting Help"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#reference", 
            "text": "Here are some other HTCondor-CE documents that might be helpful:   HTCondor-CE overview and architecture  Installing HTCondor-CE  Configuring HTCondor-CE job routes  Submitting jobs to HTCondor-CE", 
            "title": "Reference"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/", 
            "text": "Submitting Jobs to an HTCondor-CE\n\n\nThis document outlines methods of manual submission to an HTCondor-CE. It is intended for site administrators wishing to verify the functionality of their HTCondor-CE installation and developers writing software to submit jobs to an HTCondor-CE (e.g., pilot jobs).\n\n\n\n\nNote\n\n\nMost incoming jobs are pilots from factories and that manual submission does not reflect the standard method that jobs are submitted to OSG CE\u2019s.\n\n\n\n\nSubmitting Jobs...\n\n\nThere are two main methods for submitting files to an HTCondor-CE: using the tools bundled with the \nhtcondor-ce-client\n package and using the \ncondor_submit\n command with a submit file. Both methods will test end-to-end job submission but the former method is simpler while the latter will walk you through writing your own submit file.\n\n\nBefore attempting to submit jobs, you will need to generate a proxy from a user certificate before running any jobs. To generate a proxy, run the following command on the host you plan on submitting from:\n\n\nuser@host $\n voms-proxy-init\n\n\n\n\n\nUsing HTCondor-CE tools\n\n\nThere are two HTCondor-CE tools that allow users to test the functionality of their HTCondor-CE: \ncondor_ce_trace\n and \ncondor_ce_run\n. The former is the preferred tool as it provides useful feedback if failure occurs while the latter is simply an automated submission tool. These commands may be run from any host that has \nhtcondor-ce-client\n installed, which you may wish to do if you are testing availability of your CE from an external source.\n\n\ncondor_ce_trace\n\n\ncondor_ce_trace\n is a Python script that uses HTCondor's Python bindings to run diagnostics, including job submission, against your HTCondor-CE. To submit a job with \ncondor_ce_trace\n, run the following command:\n\n\nuser@host $\n condor_ce_trace --debug condorce.example.com\n\n\n\n\n\nReplacing \ncondorce.example.com\n with the hostname of the CE you wish to test. On success, you will see \nJob status: Completed\n and the environment of the job on the worker node it landed on. If you do not get the expected output, refer to the \ntroubleshooting guide\n.\n\n\nRequesting resources\n\n\ncondor_ce_trace\n doesn't make any specific resource requests so its jobs are only given the default resources by the CE. To request specific resources (or other job attributes), you can specify the \n--attribute\n option on the command line:\n\n\nuser@host $\n condor_ce_trace --debug --attribute\n=\n+resource1=value1\n...--attribute\n=\n+resourceN=valueN\n condorce.example.com\n\n\n\n\n\nTo submit a job that requests 4 cores, 4 GB of RAM, a wall clock time of 2 hours, and the 'osg' queue, run the following command:\n\n\nuser@host $\n condor_ce_trace --debug --attribute\n=\n+xcount=4\n --attribute\n=\n+maxMemory=4000\n --attribute\n=\n+maxWallTime=120\n --attribute\n=\n+remote_queue=osg\n condorce.example.com\n\n\n\n\n\nFor a list of other attributes that can be set with the \n--attribute\n option, consult the \njob attributes\n section.\n\n\ncondor_ce_run\n\n\ncondor_ce_run\n is a Python script that calls \ncondor_submit\n on a generated submit file and tracks its progress with \ncondor_q\n. To submit a job with \ncondor_ce_run\n, run the following command:\n\n\nuser@host $\n condor_ce_run -r condorce.example.com:9619 /bin/env\n\n\n\n\n\nReplacing \ncondorce.example.com\n with the hostname of the CE you wish to test. The command will not return any output until it completes: When it does you will see the environment of the job on the worker noded it landed on. If you do not get the expected output, refer to the \ntroubleshooting guide\n.\n\n\nUsing a submit file...\n\n\nIf you are familiar with HTCondor, submitting a job to an HTCondor-CE using a submit file follows the same procedure as submitting a job to an HTCondor batch system: Write a submit file and use \ncondor_submit\n (or in one of our cases, \ncondor_ce_submit\n) to submit the job. This is by virtue of the fact that HTCondor-CE is just a special configuration of HTCondor. The major differences occur in the specific attributes for the submit files outlined below.\n\n\nFrom the CE host\n\n\nThis method uses \ncondor_ce_submit\n to submit directly to an HTCondor-CE. The only reason we use \ncondor_ce_submit\n in this case is to take advantage of the already running daemons on the CE host.\n\n\n\n\n\n\nWrite a submit file, \nce_test.sub\n:\n\n\n# Required for local HTCondor-CE submission\nuniverse = vanilla\nuse_x509userproxy = true\n+Owner = undefined\n\n# Files\nexecutable = \nce_test.sh\n\noutput = ce_test.out\nerror = ce_test.err\nlog = ce_test.log\n\n# File transfer behavior\nShouldTransferFiles = YES\nWhenToTransferOutput = ON_EXIT\n\n# Optional resource requests\n#+xcount = 4            # Request 4 cores\n#+maxMemory = 4000      # Request 4GB of RAM\n#+maxWallTime = 120     # Request 2 hrs of wall clock time\n#+remote_queue = \nosg\n  # Request the OSG queue\n\n# Run job once\nqueue\n\n\n\n\n\nReplacing \nce_test.sh\n with the path to the executable you wish to run.\n\n\n\n\n\n\nYou can use any executable you choose for the \nexecutable\n field. If you don't have one in mind, you may use the following example test script:\n\n\n1\n2\n3\n4\n5\n#!/bin/bash\n\n\ndate\nhostname\nenv\n\n\n\n\n\n\n\n\n\n\nMark the test script as executable:\n\n\nuser@host $\n chmod +x ce_test.sh\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubmit the job:\n\n\nuser@host $\n condor_ce_submit ce_test.sub\n\n\n\n\n\n\n\n\n\nFrom another host\n\n\nFor this method, you will need a functional HTCondor submit node. If you do not have one readily available, you can install the \ncondor\n package from the OSG repository to get a simple submit node:\n\n\n\n\n\n\nInstall HTCondor:\n\n\nroot@host #\n yum install condor\n\n\n\n\n\n\n\n\n\nStart the \ncondor\n service:\n\n\nroot@host $\n service condor start\n\n\n\n\n\n\n\n\n\nOnce the \ncondor\n service is running, write a submit file and submit your job:\n\n\n\n\n\n\nWrite a submit file, \nce_test.sub\n:\n\n\n# Required for remote HTCondor-CE submission\nuniverse = grid\nuse_x509userproxy = true\ngrid_resource = condor \ncondorce.example.com condorce.example.com\n:9619\n\n# Files\nexecutable = \nce_test.sh\n\noutput = ce_test.out\nerror = ce_test.err\nlog = ce_test.log\n\n# File transfer behavior\nShouldTransferFiles = YES\nWhenToTransferOutput = ON_EXIT\n\n# Optional resource requests\n#+xcount = 4            # Request 4 cores\n#+maxMemory = 4000      # Request 4GB of RAM\n#+maxWallTime = 120     # Request 2 hrs of wall clock time\n#+remote_queue = \nosg\n  # Request the OSG queue\n\n# Run job once\nqueue\n\n\n\n\n\n\n\nNote\n\n\nThe \ngrid_resource\n line should start with \ncondor\n and is not related to which batch system you are using.\n\n\n\n\n\n\n\n\nYou can use any executable you choose for the \nexecutable\n field. If you don't have one in mind, you may use the following example test script:\n\n\n1\n2\n3\n4\n5\n#!/bin/bash\n\n\ndate\nhostname\nenv\n\n\n\n\n\n\n\n\n\n\nMark the test script as executable:\n\n\nuser@host $\n chmod +x ce_test.sh\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubmit the job:\n\n\nuser@host $\n condor_submit ce_test.sub\n\n\n\n\n\n\n\n\n\nTracking job progress\n\n\nWhen the job completes, stdout will be placed into \nce_test.out\n, stderr will be placed into \nce_test.err\n, and HTCondor logging information will be placed in \nce_test.log\n. You can track job progress by looking at the condor queue by running the following command on the CE host:\n\n\nuser@host $\n condor_ce_q\n\n\n\n\n\nUsing the following table to determine job status:\n\n\n\n\n\n\n\n\nThis value in the \nST\n column...\n\n\nMeans that the job is...\n\n\n\n\n\n\n\n\n\n\nI\n\n\nidle\n\n\n\n\n\n\nC\n\n\ncomplete\n\n\n\n\n\n\nX\n\n\nbeing removed\n\n\n\n\n\n\nH\n\n\nheld\n\n\n\n\n\n\n\n\ntransferring input\n\n\n\n\n\n\n\n\ntransferring output\n\n\n\n\n\n\n\n\nHow Job Routes Affect Your Job\n\n\nUpon successful submission of your job, the Job Router takes control of your job by matching it to routes and submitting a transformed job to your batch system.\n\n\nMatching\n\n\nFirst, the Job Router checks if your job \nmatches any routes\n. It does this by checking the routes \nRequirements\n expression against the job and selecting the first match. If your job does not match any routes, the job will be put on hold and eventually removed from the CE queue without completing.\n\n\n\n\nNote\n\n\nFor versions of HTCondor \n 8.7.1, the JobRouter matches jobs to routes in a round-robin fashion. This means that if a job can match to multiple routes, it can be routed by any of them! So when writing job routes, make sure that they are exclusive to each other and that your jobs can only match to a single route.\n\n\n\n\nExamples\n\n\nThe following three routes only perform filtering and submission of routed jobs to an HTCondor batch system. The only differences are in the types of jobs that they match:\n\n\n\n\nRoute 1:\n Matches jobs whose attribute \nfoo\n is equal to \nbar\n.\n\n\nRoute 2:\n Matches jobs whose attribute \nfoo\n is equal to \nbaz\n.\n\n\nRoute 3:\n Matches jobs whose attribute \nfoo\n is neither equal to \nbar\n nor \nbaz\n.\n\n\n\n\n\n\nNote\n\n\nSetting a custom attribute for job submission requires the \n+\n prefix in your submit file but it is unnecessary in the job routes.\n\n\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name = \nRoute 1\n; \\\n     Requirements = (TARGET.foo =?= \nbar\n); \\\n] \\\n[ \\\n     TargetUniverse = 5; \\\n     name = \nRoute 2\n; \\\n     Requirements = (TARGET.foo =?= \nbaz\n); \\\n] \\\n[ \\\n     TargetUniverse = 5; \\\n     name = \nRoute 3\n; \\\n     Requirements = (TARGET.foo =!= \nbar\n) \n (TARGET.foo =!= \nbaz\n); \\\n]\n\n\n\n\n\nIf a user could submitted their job with \n+foo=bar\n, the job would match \nRoute 1\n.\n\n\nRoute defaults\n\n\nRoute defaults\n can be set for batch system queue, maximum memory, number of cores to request, and maximum walltime. The submitting user can override any of these by setting the corresponding \nattribute\n in their job.\n\n\nExamples\n\n\nThe following route takes all incoming jobs and submits them to an HTCondor batch system requesting 1GB of memory.\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name = \nRoute 1\n; \\\n     set_default_maxMemory = 1000; \\\n]\n\n\n\n\n\nA user could submit their job with the attribute \n+maxMemory=2000\n and that job would be submitted requesting 2GB memory instead of the default of 1GB.\n\n\nTroubleshooting Your Jobs\n\n\nTroubleshooting\n\n\nAll interactions between \ncondor_submit\n and the CE will be recorded in the file specified by the \nlog\n attribute in your submit file. This includes acknowledgement of the job in your local queue, connection to the CE, and a record of job completion:\n\n\n000 (786.000.000) 12/09 16:49:55 Job submitted from host: \n131.225.154.68:53134\n\n...\n027 (786.000.000) 12/09 16:50:09 Job submitted to grid resource\n    GridResource: condor condorce.example.com condorce.example.com:9619\n    GridJobId: condor condorce.example.com condorce.example.com:9619 796.0\n...\n005 (786.000.000) 12/09 16:52:19 Job terminated.\n        (1) Normal termination (return value 0)\n                Usr 0 00:00:00, Sys 0 00:00:00  -  Run Remote Usage\n                Usr 0 00:00:00, Sys 0 00:00:00  -  Run Local Usage\n                Usr 0 00:00:00, Sys 0 00:00:00  -  Total Remote Usage\n                Usr 0 00:00:00, Sys 0 00:00:00  -  Total Local Usage\n        0  -  Run Bytes Sent By Job\n        0  -  Run Bytes Received By Job\n        0  -  Total Bytes Sent By Job\n        0  -  Total Bytes Received By Job\n\n\n\n\n\nIf there are issues contacting the CE, you will see error messages about a 'Down Globus Resource':\n\n\n020 (788.000.000) 12/09 16:56:17 Detected Down Globus Resource\n    RM-Contact: fermicloud133.fnal.gov\n...\n026 (788.000.000) 12/09 16:56:17 Detected Down Grid Resource\n    GridResource: condor condorce.example.com condorce.example.com:9619\n\n\n\n\n\nThis indicates a communication issue with your CE that can be diagnosed with \ncondor_ce_ping\n.\n\n\nReference\n\n\nHere are some other HTCondor-CE documents that might be helpful:\n\n\n\n\nHTCondor-CE overview and architecture\n\n\nInstalling HTCondor-CE\n\n\nConfiguring HTCondor-CE job routes\n\n\nThe HTCondor-CE troubleshooting guide\n\n\n\n\nJob attributes\n\n\nThe following table is a reference of job attributes that can be included in HTCondor submit files and their GlobusRSL equivalents. A more comprehensive list of submit file attributes specific to HTCondor-CE can be found in the \nHTCondor manual\n.\n\n\n\n\n\n\n\n\nHTCondor Attribute\n\n\nGlobus RSL\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\narguments\n\n\narguments\n\n\nArguments that will be provided to the executable for the job.\n\n\n\n\n\n\nerror\n\n\nstderr\n\n\nPath to the file on the client machine that stores stderr from the job.\n\n\n\n\n\n\nexecutable\n\n\nexecutable\n\n\nPath to the file on the client machine that the job will execute.\n\n\n\n\n\n\ninput\n\n\nstdin\n\n\nPath to the file on the client machine that stores input to be piped into the stdin of the job.\n\n\n\n\n\n\n+maxMemory\n\n\nmaxMemory\n\n\nThe amount of memory in MB that you wish to allocate to the job.\n\n\n\n\n\n\n+maxWallTime\n\n\nmaxWallTime\n\n\nThe maximum walltime (in minutes) the job is allowed to run before it is removed.\n\n\n\n\n\n\noutput\n\n\nstdout\n\n\nPath to the file on the client machine that stores stdout from the job.\n\n\n\n\n\n\n+remote_queue\n\n\nqueue\n\n\nAssign job to the target queue in the scheduler. Note that the queue name should be in quotes.\n\n\n\n\n\n\ntransfer_input_files\n\n\nfile_stage_in\n\n\nA comma-delimited list of all the files and directories to be transferred into the working directory for the job, before the job is started.\n\n\n\n\n\n\ntransfer_output_files\n\n\ntransfer_output_files\n\n\nA comma-delimited list of all the files and directories to be transferred back to the client, after the job completes.\n\n\n\n\n\n\n+xcount\n\n\nxcount\n\n\nThe number of cores to allocate for the job.\n\n\n\n\n\n\n\n\nIf you are setting an attribute to a string value, make sure enclose the string in double-quotes (\n\"\n), otherwise HTCondor-CE will try to find an attribute by that name.", 
            "title": "Submitting to HTCondor-CE"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#submitting-jobs-to-an-htcondor-ce", 
            "text": "This document outlines methods of manual submission to an HTCondor-CE. It is intended for site administrators wishing to verify the functionality of their HTCondor-CE installation and developers writing software to submit jobs to an HTCondor-CE (e.g., pilot jobs).   Note  Most incoming jobs are pilots from factories and that manual submission does not reflect the standard method that jobs are submitted to OSG CE\u2019s.", 
            "title": "Submitting Jobs to an HTCondor-CE"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#submitting-jobs", 
            "text": "There are two main methods for submitting files to an HTCondor-CE: using the tools bundled with the  htcondor-ce-client  package and using the  condor_submit  command with a submit file. Both methods will test end-to-end job submission but the former method is simpler while the latter will walk you through writing your own submit file.  Before attempting to submit jobs, you will need to generate a proxy from a user certificate before running any jobs. To generate a proxy, run the following command on the host you plan on submitting from:  user@host $  voms-proxy-init", 
            "title": "Submitting Jobs..."
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#using-htcondor-ce-tools", 
            "text": "There are two HTCondor-CE tools that allow users to test the functionality of their HTCondor-CE:  condor_ce_trace  and  condor_ce_run . The former is the preferred tool as it provides useful feedback if failure occurs while the latter is simply an automated submission tool. These commands may be run from any host that has  htcondor-ce-client  installed, which you may wish to do if you are testing availability of your CE from an external source.", 
            "title": "Using HTCondor-CE tools"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#condor_ce_trace", 
            "text": "condor_ce_trace  is a Python script that uses HTCondor's Python bindings to run diagnostics, including job submission, against your HTCondor-CE. To submit a job with  condor_ce_trace , run the following command:  user@host $  condor_ce_trace --debug condorce.example.com  Replacing  condorce.example.com  with the hostname of the CE you wish to test. On success, you will see  Job status: Completed  and the environment of the job on the worker node it landed on. If you do not get the expected output, refer to the  troubleshooting guide .", 
            "title": "condor_ce_trace"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#requesting-resources", 
            "text": "condor_ce_trace  doesn't make any specific resource requests so its jobs are only given the default resources by the CE. To request specific resources (or other job attributes), you can specify the  --attribute  option on the command line:  user@host $  condor_ce_trace --debug --attribute = +resource1=value1 ...--attribute = +resourceN=valueN  condorce.example.com  To submit a job that requests 4 cores, 4 GB of RAM, a wall clock time of 2 hours, and the 'osg' queue, run the following command:  user@host $  condor_ce_trace --debug --attribute = +xcount=4  --attribute = +maxMemory=4000  --attribute = +maxWallTime=120  --attribute = +remote_queue=osg  condorce.example.com  For a list of other attributes that can be set with the  --attribute  option, consult the  job attributes  section.", 
            "title": "Requesting resources"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#condor_ce_run", 
            "text": "condor_ce_run  is a Python script that calls  condor_submit  on a generated submit file and tracks its progress with  condor_q . To submit a job with  condor_ce_run , run the following command:  user@host $  condor_ce_run -r condorce.example.com:9619 /bin/env  Replacing  condorce.example.com  with the hostname of the CE you wish to test. The command will not return any output until it completes: When it does you will see the environment of the job on the worker noded it landed on. If you do not get the expected output, refer to the  troubleshooting guide .", 
            "title": "condor_ce_run"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#using-a-submit-file", 
            "text": "If you are familiar with HTCondor, submitting a job to an HTCondor-CE using a submit file follows the same procedure as submitting a job to an HTCondor batch system: Write a submit file and use  condor_submit  (or in one of our cases,  condor_ce_submit ) to submit the job. This is by virtue of the fact that HTCondor-CE is just a special configuration of HTCondor. The major differences occur in the specific attributes for the submit files outlined below.", 
            "title": "Using a submit file..."
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#from-the-ce-host", 
            "text": "This method uses  condor_ce_submit  to submit directly to an HTCondor-CE. The only reason we use  condor_ce_submit  in this case is to take advantage of the already running daemons on the CE host.    Write a submit file,  ce_test.sub :  # Required for local HTCondor-CE submission\nuniverse = vanilla\nuse_x509userproxy = true\n+Owner = undefined\n\n# Files\nexecutable =  ce_test.sh \noutput = ce_test.out\nerror = ce_test.err\nlog = ce_test.log\n\n# File transfer behavior\nShouldTransferFiles = YES\nWhenToTransferOutput = ON_EXIT\n\n# Optional resource requests\n#+xcount = 4            # Request 4 cores\n#+maxMemory = 4000      # Request 4GB of RAM\n#+maxWallTime = 120     # Request 2 hrs of wall clock time\n#+remote_queue =  osg   # Request the OSG queue\n\n# Run job once\nqueue  Replacing  ce_test.sh  with the path to the executable you wish to run.    You can use any executable you choose for the  executable  field. If you don't have one in mind, you may use the following example test script:  1\n2\n3\n4\n5 #!/bin/bash \n\ndate\nhostname\nenv     Mark the test script as executable:  user@host $  chmod +x ce_test.sh      Submit the job:  user@host $  condor_ce_submit ce_test.sub", 
            "title": "From the CE host"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#from-another-host", 
            "text": "For this method, you will need a functional HTCondor submit node. If you do not have one readily available, you can install the  condor  package from the OSG repository to get a simple submit node:    Install HTCondor:  root@host #  yum install condor    Start the  condor  service:  root@host $  service condor start    Once the  condor  service is running, write a submit file and submit your job:    Write a submit file,  ce_test.sub :  # Required for remote HTCondor-CE submission\nuniverse = grid\nuse_x509userproxy = true\ngrid_resource = condor  condorce.example.com condorce.example.com :9619\n\n# Files\nexecutable =  ce_test.sh \noutput = ce_test.out\nerror = ce_test.err\nlog = ce_test.log\n\n# File transfer behavior\nShouldTransferFiles = YES\nWhenToTransferOutput = ON_EXIT\n\n# Optional resource requests\n#+xcount = 4            # Request 4 cores\n#+maxMemory = 4000      # Request 4GB of RAM\n#+maxWallTime = 120     # Request 2 hrs of wall clock time\n#+remote_queue =  osg   # Request the OSG queue\n\n# Run job once\nqueue   Note  The  grid_resource  line should start with  condor  and is not related to which batch system you are using.     You can use any executable you choose for the  executable  field. If you don't have one in mind, you may use the following example test script:  1\n2\n3\n4\n5 #!/bin/bash \n\ndate\nhostname\nenv     Mark the test script as executable:  user@host $  chmod +x ce_test.sh      Submit the job:  user@host $  condor_submit ce_test.sub", 
            "title": "From another host"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#tracking-job-progress", 
            "text": "When the job completes, stdout will be placed into  ce_test.out , stderr will be placed into  ce_test.err , and HTCondor logging information will be placed in  ce_test.log . You can track job progress by looking at the condor queue by running the following command on the CE host:  user@host $  condor_ce_q  Using the following table to determine job status:     This value in the  ST  column...  Means that the job is...      I  idle    C  complete    X  being removed    H  held     transferring input     transferring output", 
            "title": "Tracking job progress"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#how-job-routes-affect-your-job", 
            "text": "Upon successful submission of your job, the Job Router takes control of your job by matching it to routes and submitting a transformed job to your batch system.", 
            "title": "How Job Routes Affect Your Job"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#matching", 
            "text": "First, the Job Router checks if your job  matches any routes . It does this by checking the routes  Requirements  expression against the job and selecting the first match. If your job does not match any routes, the job will be put on hold and eventually removed from the CE queue without completing.   Note  For versions of HTCondor   8.7.1, the JobRouter matches jobs to routes in a round-robin fashion. This means that if a job can match to multiple routes, it can be routed by any of them! So when writing job routes, make sure that they are exclusive to each other and that your jobs can only match to a single route.   Examples  The following three routes only perform filtering and submission of routed jobs to an HTCondor batch system. The only differences are in the types of jobs that they match:   Route 1:  Matches jobs whose attribute  foo  is equal to  bar .  Route 2:  Matches jobs whose attribute  foo  is equal to  baz .  Route 3:  Matches jobs whose attribute  foo  is neither equal to  bar  nor  baz .    Note  Setting a custom attribute for job submission requires the  +  prefix in your submit file but it is unnecessary in the job routes.   JOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name =  Route 1 ; \\\n     Requirements = (TARGET.foo =?=  bar ); \\\n] \\\n[ \\\n     TargetUniverse = 5; \\\n     name =  Route 2 ; \\\n     Requirements = (TARGET.foo =?=  baz ); \\\n] \\\n[ \\\n     TargetUniverse = 5; \\\n     name =  Route 3 ; \\\n     Requirements = (TARGET.foo =!=  bar )   (TARGET.foo =!=  baz ); \\\n]  If a user could submitted their job with  +foo=bar , the job would match  Route 1 .", 
            "title": "Matching"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#route-defaults", 
            "text": "Route defaults  can be set for batch system queue, maximum memory, number of cores to request, and maximum walltime. The submitting user can override any of these by setting the corresponding  attribute  in their job.  Examples  The following route takes all incoming jobs and submits them to an HTCondor batch system requesting 1GB of memory.  JOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name =  Route 1 ; \\\n     set_default_maxMemory = 1000; \\\n]  A user could submit their job with the attribute  +maxMemory=2000  and that job would be submitted requesting 2GB memory instead of the default of 1GB.", 
            "title": "Route defaults"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#troubleshooting-your-jobs", 
            "text": "", 
            "title": "Troubleshooting Your Jobs"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#troubleshooting", 
            "text": "All interactions between  condor_submit  and the CE will be recorded in the file specified by the  log  attribute in your submit file. This includes acknowledgement of the job in your local queue, connection to the CE, and a record of job completion:  000 (786.000.000) 12/09 16:49:55 Job submitted from host:  131.225.154.68:53134 \n...\n027 (786.000.000) 12/09 16:50:09 Job submitted to grid resource\n    GridResource: condor condorce.example.com condorce.example.com:9619\n    GridJobId: condor condorce.example.com condorce.example.com:9619 796.0\n...\n005 (786.000.000) 12/09 16:52:19 Job terminated.\n        (1) Normal termination (return value 0)\n                Usr 0 00:00:00, Sys 0 00:00:00  -  Run Remote Usage\n                Usr 0 00:00:00, Sys 0 00:00:00  -  Run Local Usage\n                Usr 0 00:00:00, Sys 0 00:00:00  -  Total Remote Usage\n                Usr 0 00:00:00, Sys 0 00:00:00  -  Total Local Usage\n        0  -  Run Bytes Sent By Job\n        0  -  Run Bytes Received By Job\n        0  -  Total Bytes Sent By Job\n        0  -  Total Bytes Received By Job  If there are issues contacting the CE, you will see error messages about a 'Down Globus Resource':  020 (788.000.000) 12/09 16:56:17 Detected Down Globus Resource\n    RM-Contact: fermicloud133.fnal.gov\n...\n026 (788.000.000) 12/09 16:56:17 Detected Down Grid Resource\n    GridResource: condor condorce.example.com condorce.example.com:9619  This indicates a communication issue with your CE that can be diagnosed with  condor_ce_ping .", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#reference", 
            "text": "Here are some other HTCondor-CE documents that might be helpful:   HTCondor-CE overview and architecture  Installing HTCondor-CE  Configuring HTCondor-CE job routes  The HTCondor-CE troubleshooting guide", 
            "title": "Reference"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#job-attributes", 
            "text": "The following table is a reference of job attributes that can be included in HTCondor submit files and their GlobusRSL equivalents. A more comprehensive list of submit file attributes specific to HTCondor-CE can be found in the  HTCondor manual .     HTCondor Attribute  Globus RSL  Summary      arguments  arguments  Arguments that will be provided to the executable for the job.    error  stderr  Path to the file on the client machine that stores stderr from the job.    executable  executable  Path to the file on the client machine that the job will execute.    input  stdin  Path to the file on the client machine that stores input to be piped into the stdin of the job.    +maxMemory  maxMemory  The amount of memory in MB that you wish to allocate to the job.    +maxWallTime  maxWallTime  The maximum walltime (in minutes) the job is allowed to run before it is removed.    output  stdout  Path to the file on the client machine that stores stdout from the job.    +remote_queue  queue  Assign job to the target queue in the scheduler. Note that the queue name should be in quotes.    transfer_input_files  file_stage_in  A comma-delimited list of all the files and directories to be transferred into the working directory for the job, before the job is started.    transfer_output_files  transfer_output_files  A comma-delimited list of all the files and directories to be transferred back to the client, after the job completes.    +xcount  xcount  The number of cores to allocate for the job.     If you are setting an attribute to a string value, make sure enclose the string in double-quotes ( \" ), otherwise HTCondor-CE will try to find an attribute by that name.", 
            "title": "Job attributes"
        }, 
        {
            "location": "/worker-node/using-wn/", 
            "text": "Introduction\n\n\nThe Worker Node Client is a collection of useful software components that is expected to be on every OSG worker node. In addition, a job running on a worker node can access a handful of environment variables that can be used to locate resources.\n\n\nThis page describes how to initialize the environment of your job to correctly access the execution and data areas from the worker node.\n\n\nThe OSG provides no scientific software dependencies or software build tools on the worker node; you are expected to bring along all application-level dependencies yourself (preferred; most portable) or utilize CVMFS. Sites are not required to provide any specific tools (\ngcc\n, \nlapack\n, \nblas\n, etc.) beyond the ones in the OSG worker node client and the base OS.\n\n\nIf you would like to test the minimal OS environment that jobs can expect, you can test out your scientific software in \nthe OSG Docker image\n.\n\n\nCommon software available on worker nodes.\n\n\nThe OSG worker node environment contains the following software:\n\n\n\n\nThe supported set of CA certificates (located in \n$X509_CERT_DIR\n after the environment is set up)\n\n\nProxy management tools:\n\n\nCreate proxies: \nvoms-proxy-init\n and \ngrid-proxy-init\n\n\nShow proxy info: \nvoms-proxy-info\n and \ngrid-proxy-info\n\n\nDestroy the current proxy: \nvoms-proxy-destroy\n and \ngrid-proxy-destroy\n\n\n\n\n\n\nData transfer tools:\n\n\nHTTP/plain FTP protocol tools (via system dependencies):\n\n\nwget\n and \ncurl\n: standard tools for downloading files with HTTP and FTP\n\n\n\n\n\n\nTransfer clients\n\n\nGFAL\n-based client (\ngfal-copy\n and others).  GFAL supports SRM, GridFTP, and HTTP protocols.\n\n\nGlobus GridFTP client (\nglobus-url-copy\n)\n\n\n\n\n\n\n\n\n\n\nMyProxy client tools\n\n\n\n\nAt some sites, these tools may not be available at the pilot launch.  To setup the environment, do the following:\n\n\nuser@host $\n \nsource\n \n$OSG_GRID\n/setup.sh\n\n\n\n\n\nThis should be done by a pilot job, not by the end-user payload.\n\n\nThe Worker Node Environment\n\n\nThe following table outlines the various important directories and information in the worker node environment.\nA job running on an OSG worker node can refer to each directory using the corresponding environment variable.\n\n\n\n\n\n\n\n\nEnvironment Variable\n\n\nPurpose\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n$X509_CERT_DIR\n\n\nLocation of the CA certificates\n\n\nIf not defined, defaults to \n/etc/grid-security/certificates\n.\n\n\n\n\n\n\n$OSG_WN_TMP\n\n\nTemporary storage area in which your job(s) run\n\n\nLocal to each worker node (recommended size: 10 GB/job). Create a directory under this as your work area. Site administrators should use common batch-system capabilities to create a temporary, per-job directory that is clearned up after each job is run. Alternatively, administrators should ensure that this directory is purged periodically.\n\n\n\n\n\n\n$_CONDOR_SCRATCH_DIR\n\n\nSuggested temporary storage for glideinWMS-based payloads.\n\n\nUsers should prefer this environment variable over \n$OSG_WN_TMP\n if running inside glideinWMS.\n\n\n\n\n\n\n$OSG_SQUID_LOCATION\n, \nhttp_proxy\n\n\nLocation of a HTTP caching proxy server\n\n\nUtilize this service for downloading files via HTTP for cache-friendly workflows.\n\n\n\n\n\n\n$OSG_GRID\n\n\nLocation of additional environment variables.\n\n\nPilots should source \n$OSG_GRID/setup.sh\n in order to guarantee the environment contains the worker node binaries in \n$PATH\n.\n\n\n\n\n\n\n$OSG_SITE_NAME\n\n\nName of the OSG resource where the worker node is located.\n\n\n\n\n\n\n\n\n$OSG_HOSTNAME\n\n\nHostname of the CE where this pilot was launched.\n\n\n\n\n\n\n\n\n\n\nBe careful with using \n$OSG_WN_TMP\n; at some sites, this directory might be shared with other VOs. We recommend creating a new sub-directory as a precaution:\n\n\nmkdir -p \n$OSG_WN_TMP\n/MYVO\n\nexport\n \nmydir\n=\n`\nmktemp -d -t MYVO\n`\n\n\ncd\n \n$mydir\n\n\n# Run the rest of your application\n\nrm -rf \n$mydir\n\n\n\n\n\n\nThis advice applies to maintainers of \npilot\n software; end-users should contact their VO for advice (often, this will be either \n$TMPDIR\n or \n$_CONDOR_SCRATCH_DIR\n).\nThe pilot should utilize \n$TMPDIR\n to communicate the location of temporary storage to payloads.\n\n\nA significant number of sites use the batch system to make an independent directory for each user job, and change \n$OSG_WN_TMP\n on the fly to point to this directory.\n\n\nThere is no way to know in advance how much scratch disk space any given worker node has available; recall, what disk space is available may be shared among a number of job slots.", 
            "title": "Worker Node Overview"
        }, 
        {
            "location": "/worker-node/using-wn/#introduction", 
            "text": "The Worker Node Client is a collection of useful software components that is expected to be on every OSG worker node. In addition, a job running on a worker node can access a handful of environment variables that can be used to locate resources.  This page describes how to initialize the environment of your job to correctly access the execution and data areas from the worker node.  The OSG provides no scientific software dependencies or software build tools on the worker node; you are expected to bring along all application-level dependencies yourself (preferred; most portable) or utilize CVMFS. Sites are not required to provide any specific tools ( gcc ,  lapack ,  blas , etc.) beyond the ones in the OSG worker node client and the base OS.  If you would like to test the minimal OS environment that jobs can expect, you can test out your scientific software in  the OSG Docker image .", 
            "title": "Introduction"
        }, 
        {
            "location": "/worker-node/using-wn/#common-software-available-on-worker-nodes", 
            "text": "The OSG worker node environment contains the following software:   The supported set of CA certificates (located in  $X509_CERT_DIR  after the environment is set up)  Proxy management tools:  Create proxies:  voms-proxy-init  and  grid-proxy-init  Show proxy info:  voms-proxy-info  and  grid-proxy-info  Destroy the current proxy:  voms-proxy-destroy  and  grid-proxy-destroy    Data transfer tools:  HTTP/plain FTP protocol tools (via system dependencies):  wget  and  curl : standard tools for downloading files with HTTP and FTP    Transfer clients  GFAL -based client ( gfal-copy  and others).  GFAL supports SRM, GridFTP, and HTTP protocols.  Globus GridFTP client ( globus-url-copy )      MyProxy client tools   At some sites, these tools may not be available at the pilot launch.  To setup the environment, do the following:  user@host $   source   $OSG_GRID /setup.sh  This should be done by a pilot job, not by the end-user payload.", 
            "title": "Common software available on worker nodes."
        }, 
        {
            "location": "/worker-node/using-wn/#the-worker-node-environment", 
            "text": "The following table outlines the various important directories and information in the worker node environment.\nA job running on an OSG worker node can refer to each directory using the corresponding environment variable.     Environment Variable  Purpose  Notes      $X509_CERT_DIR  Location of the CA certificates  If not defined, defaults to  /etc/grid-security/certificates .    $OSG_WN_TMP  Temporary storage area in which your job(s) run  Local to each worker node (recommended size: 10 GB/job). Create a directory under this as your work area. Site administrators should use common batch-system capabilities to create a temporary, per-job directory that is clearned up after each job is run. Alternatively, administrators should ensure that this directory is purged periodically.    $_CONDOR_SCRATCH_DIR  Suggested temporary storage for glideinWMS-based payloads.  Users should prefer this environment variable over  $OSG_WN_TMP  if running inside glideinWMS.    $OSG_SQUID_LOCATION ,  http_proxy  Location of a HTTP caching proxy server  Utilize this service for downloading files via HTTP for cache-friendly workflows.    $OSG_GRID  Location of additional environment variables.  Pilots should source  $OSG_GRID/setup.sh  in order to guarantee the environment contains the worker node binaries in  $PATH .    $OSG_SITE_NAME  Name of the OSG resource where the worker node is located.     $OSG_HOSTNAME  Hostname of the CE where this pilot was launched.      Be careful with using  $OSG_WN_TMP ; at some sites, this directory might be shared with other VOs. We recommend creating a new sub-directory as a precaution:  mkdir -p  $OSG_WN_TMP /MYVO export   mydir = ` mktemp -d -t MYVO `  cd   $mydir  # Run the rest of your application \nrm -rf  $mydir   This advice applies to maintainers of  pilot  software; end-users should contact their VO for advice (often, this will be either  $TMPDIR  or  $_CONDOR_SCRATCH_DIR ).\nThe pilot should utilize  $TMPDIR  to communicate the location of temporary storage to payloads.  A significant number of sites use the batch system to make an independent directory for each user job, and change  $OSG_WN_TMP  on the fly to point to this directory.  There is no way to know in advance how much scratch disk space any given worker node has available; recall, what disk space is available may be shared among a number of job slots.", 
            "title": "The Worker Node Environment"
        }, 
        {
            "location": "/worker-node/install-wn/", 
            "text": "Installing and Using the Worker Node Client From RPMs\n\n\nAbout This Guide\n\n\nThe \nOSG Worker Node Client\n is a collection of software components that is expected to be added to every worker node that can run OSG jobs. It provides a common environment and a minimal set of common tools that all OSG jobs can expect to use. See the reference section below for contents of the Worker Node Client.\n\n\nIt is possible to install the Worker Node Client software in a variety of ways, depending on what works best for distributing and managing software at your site:\n\n\n\n\nInstall using RPMs and Yum (this guide) - useful when managing your worker nodes with a tool (e.g., Puppet, Chef) that can automate RPM installs\n\n\nInstall using a tarball\n - useful when installing onto a shared filesystem for distribution to worker nodes\n\n\nUse from OASIS\n - useful when worker nodes already mount \nCVMFS\n\n\n\n\nThis document is intended to guide system administrators through the process of configuring a site to make the Worker Node Client software available from an RPM.\n\n\nBefore Starting\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare \nthe required Yum repositories\n\n\nInstall \nCA certificates\n\n\n\n\nInstall the Worker Node Client\n\n\nInstall the Worker Node Client RPM:\n\n\nroot@host #\n yum install osg-wn-client\n\n\n\n\n\nServices\n\n\nFetch-CRL is the only service required to support the WN Client.\n\n\n\n\n\n\n\n\nSoftware\n\n\nService name\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nFetch CRL\n\n\nfetch-crl-boot\n and \nfetch-crl-cron\n\n\nSee \nCA documentation\n for more info\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nfetch-crl-boot\n will begin fetching CRLS, which can take a few minutes and fail on transient errors. You can add configuration to ignore these transient errors in \n/etc/fetch-crl.conf\n:\n\n\nnoerrors\n\n\n\n\n\n\n\nAs a reminder, here are common service commands (all run as \nroot\n):\n\n\n\n\n\n\n\n\nTo \u2026\n\n\nRun the command \u2026\n\n\n\n\n\n\n\n\n\n\nStart a service\n\n\nservice SERVICE-NAME start\n\n\n\n\n\n\nStop a service\n\n\nservice SERVICE-NAME stop\n\n\n\n\n\n\nEnable a service to start during boot\n\n\nchkconfig SERVICE-NAME on\n\n\n\n\n\n\nDisable a service from starting during boot\n\n\nchkconfig SERVICE-NAME off\n\n\n\n\n\n\n\n\nValidating the Worker Node Client\n\n\nTo verify functionality of the worker node client, you will need to submit a test job against your CE and verify the job's output.\n\n\n\n\nSubmit a job that executes the \nenv\n command (e.g. Run \ncondor_ce_trace\n with the \n-d\n flag from your HTCondor CE)\n\n\nVerify that the value of \nOSG_GRID\n is set to \n/etc/osg/wn-client\n\n\n\n\nHow to get Help?\n\n\nTo get assistance please use this \nHelp Procedure\n.\n\n\nReference\n\n\nPlease see the documentation on using \nyum and RPM\n, \nthe best practices\n for using yum to install software, and using \nyum repositories\n.\n\n\nWorker node contents\n\n\nThe worker node may be updated from time to time. As of OSG 3.3.21 in February 2017, the OSG worker node client contains:\n\n\n\n\nOSG Certificates\n\n\ncurl\n\n\nFetch CRL\n\n\nFTS client\n\n\ngfal2\n\n\nglobus-url-copy (GridFTP client)\n\n\nglobus-xio-udt-driver\n\n\nldapsearch\n\n\nMyProxy\n\n\nosg-system-profiler\n\n\nosg-version\n\n\nUberFTP\n\n\nvo-client (includes /etc/vomses file)\n\n\nVOMS client\n\n\nwget\n\n\nxrdcp\n\n\n\n\nTo see the currently installed version of the worker node package, run the following command:\n\n\nroot@host # rpm -q --requires osg-wn-client\n\n\n\n\n\nClick \nhere\n for more details on using RPM to see what was installed.", 
            "title": "Worker Node (RPM install)"
        }, 
        {
            "location": "/worker-node/install-wn/#installing-and-using-the-worker-node-client-from-rpms", 
            "text": "", 
            "title": "Installing and Using the Worker Node Client From RPMs"
        }, 
        {
            "location": "/worker-node/install-wn/#about-this-guide", 
            "text": "The  OSG Worker Node Client  is a collection of software components that is expected to be added to every worker node that can run OSG jobs. It provides a common environment and a minimal set of common tools that all OSG jobs can expect to use. See the reference section below for contents of the Worker Node Client.  It is possible to install the Worker Node Client software in a variety of ways, depending on what works best for distributing and managing software at your site:   Install using RPMs and Yum (this guide) - useful when managing your worker nodes with a tool (e.g., Puppet, Chef) that can automate RPM installs  Install using a tarball  - useful when installing onto a shared filesystem for distribution to worker nodes  Use from OASIS  - useful when worker nodes already mount  CVMFS   This document is intended to guide system administrators through the process of configuring a site to make the Worker Node Client software available from an RPM.", 
            "title": "About This Guide"
        }, 
        {
            "location": "/worker-node/install-wn/#before-starting", 
            "text": "As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system  Obtain root access to the host  Prepare  the required Yum repositories  Install  CA certificates", 
            "title": "Before Starting"
        }, 
        {
            "location": "/worker-node/install-wn/#install-the-worker-node-client", 
            "text": "Install the Worker Node Client RPM:  root@host #  yum install osg-wn-client", 
            "title": "Install the Worker Node Client"
        }, 
        {
            "location": "/worker-node/install-wn/#services", 
            "text": "Fetch-CRL is the only service required to support the WN Client.     Software  Service name  Notes      Fetch CRL  fetch-crl-boot  and  fetch-crl-cron  See  CA documentation  for more info      Note  fetch-crl-boot  will begin fetching CRLS, which can take a few minutes and fail on transient errors. You can add configuration to ignore these transient errors in  /etc/fetch-crl.conf :  noerrors   As a reminder, here are common service commands (all run as  root ):     To \u2026  Run the command \u2026      Start a service  service SERVICE-NAME start    Stop a service  service SERVICE-NAME stop    Enable a service to start during boot  chkconfig SERVICE-NAME on    Disable a service from starting during boot  chkconfig SERVICE-NAME off", 
            "title": "Services"
        }, 
        {
            "location": "/worker-node/install-wn/#validating-the-worker-node-client", 
            "text": "To verify functionality of the worker node client, you will need to submit a test job against your CE and verify the job's output.   Submit a job that executes the  env  command (e.g. Run  condor_ce_trace  with the  -d  flag from your HTCondor CE)  Verify that the value of  OSG_GRID  is set to  /etc/osg/wn-client", 
            "title": "Validating the Worker Node Client"
        }, 
        {
            "location": "/worker-node/install-wn/#how-to-get-help", 
            "text": "To get assistance please use this  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/worker-node/install-wn/#reference", 
            "text": "Please see the documentation on using  yum and RPM ,  the best practices  for using yum to install software, and using  yum repositories .", 
            "title": "Reference"
        }, 
        {
            "location": "/worker-node/install-wn/#worker-node-contents", 
            "text": "The worker node may be updated from time to time. As of OSG 3.3.21 in February 2017, the OSG worker node client contains:   OSG Certificates  curl  Fetch CRL  FTS client  gfal2  globus-url-copy (GridFTP client)  globus-xio-udt-driver  ldapsearch  MyProxy  osg-system-profiler  osg-version  UberFTP  vo-client (includes /etc/vomses file)  VOMS client  wget  xrdcp   To see the currently installed version of the worker node package, run the following command:  root@host # rpm -q --requires osg-wn-client  Click  here  for more details on using RPM to see what was installed.", 
            "title": "Worker node contents"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/", 
            "text": "Installing and Using the Worker Node Client from Tarballs\n\n\nIntroduction\n\n\nThis document is intended to guide users through the process of installing the worker node software and configuring the installed worker node software. Contents of the worker node client can be found \nhere\n.  Although this document is oriented to system administrators, any unprivileged user may install and use the client.\n\n\nIf you are installing the OSG worker node client following these instruction, remember to configure the \ngrid_dir\n option on your CE - see \nbelow\n.\n\n\nAbout This Guide\n\n\nThe \nOSG Worker Node Client\n is a collection of software components that is expected to be added to every worker node that can run OSG jobs. It provides a common environment and a minimal set of common tools that all OSG jobs can expect to use.\n\n\nIt is possible to install the Worker Node Client software in a variety of ways, depending on what works best for distributing and managing software at your site:  This guide is useful when installing onto a shared filesystem for distribution to worker nodes.  Other options include installing \nvia RPMs\n or providing the client \nvia OASIS (CVMFS)\n.\n\n\nBefore starting, ensure the host has \na supported operating system\n.\n\n\nDownload, Installation and Configuration\n\n\nDownload the WN Client\n\n\nPlease pick the \nosg-wn-client\n tarball that is appropriate for your distribution and architecture. You will find them in \nhttps://repo.opensciencegrid.org/tarball-install/\n .\n\n\nThe latest available the tarballs for OSG 3.3 are:\n\n\n\n\nBinaries for 32-bit RHEL6\n\n\nBinaries for 64-bit RHEL6\n\n\nBinaries for RHEL7\n\n\n\n\nFor OSG 3.4:\n\n\n\n\nBinaries for 64-bit RHEL6\n\n\nBinaries for RHEL7\n\n\n\n\nInstall the WN Client\n\n\n\n\nUnpack the tarball.\n\n\nMove the directory that was created to where you want the tarball client to be.\n\n\nRun \nosg-post-install\n (\nPATH_TO_CLIENT\n/osg/osg-post-install\n) to fix the directories in the installation.\n\n\nSource the setup \nsource \nPATH_TO_CLIENT\n/setup.sh\n (or \nsetup.csh\n depending on the shell).\n\n\nDownload and set up CA certificiates using \nosg-ca-manage\n (See the \nCA management documentation\n for the available options).\n\n\nDownload CRLs using \nfetch-crl\n.\n\n\n\n\n\n\nWarning\n\n\nOnce \nosg-post-install\n is run to relocate the install, it cannot be run again.  You will need to unpack a fresh copy.\n\n\n\n\nExample installation (in \n/home/user/test-install\n, the \nPATH_TO_CLIENT\n/\n is \n/home/user/test-install/osg-wn-client\n ):\n\n\nroot@host #\n mkdir /home/user/test-install\n\nroot@host #\n \ncd\n /home/user/test-install\n\nroot@host #\n wget http://repo.opensciencegrid.org/tarball-install/3.4/osg-wn-client-latest.el6.x86_64.tar.gz\n\nroot@host #\n tar xzf osg-wn-client-latest.el6.x86_64.tar.gz\n\nroot@host #\n \ncd\n osg-wn-client\n\nroot@host #\n ./osg/osg-post-install\n\nroot@host #\n \nsource\n setup.sh\n\nroot@host #\n osg-ca-manage setupCA --url osg\n\nroot@host #\n fetch-crl\n\n\n\n\n\nConfigure the CE\n\n\nUsing the wn-client software installed from the tarball will require a few changes on the compute element so that the resource's configuration can be correctly reported.\n\n\nSet \ngrid_dir\n in the \nStorage\n section of your OSG-Configure configs: \nCE configuration instructions\n. \ngrid_dir\n is used as the \n$OSG_GRID\n environment variable in running jobs - see the \nworker node environment document\n. Pilot jobs source \n$OSG_GRID/setup.sh\n before performing any work. The value set for \ngrid_dir\n must be the path of the wn-client installation directory. This is the path returned by \necho $OSG_LOCATION\n once you source the setup file created by this installation.\n\n\nServices\n\n\nThe WN client is a collection of client programs that do not require service startup or shutdown. The only services are \nosg-update-certs\n that keeps the CA certificates up-to-date and \nfetch-crl\n that keeps the CRLs up-to-date. Following the instructions below you'll add the services to your crontab that will take care to run them periodically until you remove them.\n\n\nAuto-updating certificates and CRLs\n\n\nYou must create cron jobs to run \nfetch-crl\n and \nosg-update-certs\n to update your CRLs and certificates automatically.\n\n\nHere is what they should look like. (Note: fill in \nOSG_LOCATION\n with the full path of your tarball install, including \nosg-wn-client\n that is created by the tarball).\n\n\n# Cron job to update certs.\n# Runs every hour by default, though does not update certs until they\nre at\n# least 24 hours old.  There is a random sleep time for up to 45 minutes (2700\n# seconds) to avoid overloading cert servers.\n10 * * * *   ( . \nOSG_LOCATION\n/setup.sh \n osg-update-certs --random-sleep 2700 --called-from-cron )\n\n\n\n\n\n# Cron job to update CRLs\n# Runs every 6 hours at, 45 minutes +/- 3 minutes.\n42 */6 * * *   ( . \nOSG_LOCATION\n/setup.sh \n fetch-crl -q -r 360 )\n\n\n\n\n\nYou might want to configure proxy settings in \n$OSG_LOCATION/etc/fetch-crl.conf\n.\n\n\nStarting and Enabling Services\n\n\nTo start the services you must edit your cron with \ncrontab -e\n and add the lines above.\n\n\nStopping and Disabling Services\n\n\nTo stop the services you must edit your cron with \ncrontab -e\n and remove or comment the lines above.\n\n\nValiding the Worker Node Client\n\n\nTo verify functionality of the worker node client, you will need to submit a test job against your CE and verify the job's output.\n\n\n\n\nSubmit a job that executes the \nenv\n command (e.g. Run \ncondor_ce_trace\n with the \n-d\n flag from your HTCondor CE)\n\n\nVerify that the value of \n$OSG_GRID\n is set to the directory of your worker node client installation\n\n\n\n\nHow to get Help?\n\n\nTo get assistance please use this \nHelp Procedure\n.", 
            "title": "Worker Node (tarball install)"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#installing-and-using-the-worker-node-client-from-tarballs", 
            "text": "", 
            "title": "Installing and Using the Worker Node Client from Tarballs"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#introduction", 
            "text": "This document is intended to guide users through the process of installing the worker node software and configuring the installed worker node software. Contents of the worker node client can be found  here .  Although this document is oriented to system administrators, any unprivileged user may install and use the client.  If you are installing the OSG worker node client following these instruction, remember to configure the  grid_dir  option on your CE - see  below .", 
            "title": "Introduction"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#about-this-guide", 
            "text": "The  OSG Worker Node Client  is a collection of software components that is expected to be added to every worker node that can run OSG jobs. It provides a common environment and a minimal set of common tools that all OSG jobs can expect to use.  It is possible to install the Worker Node Client software in a variety of ways, depending on what works best for distributing and managing software at your site:  This guide is useful when installing onto a shared filesystem for distribution to worker nodes.  Other options include installing  via RPMs  or providing the client  via OASIS (CVMFS) .  Before starting, ensure the host has  a supported operating system .", 
            "title": "About This Guide"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#download-installation-and-configuration", 
            "text": "", 
            "title": "Download, Installation and Configuration"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#download-the-wn-client", 
            "text": "Please pick the  osg-wn-client  tarball that is appropriate for your distribution and architecture. You will find them in  https://repo.opensciencegrid.org/tarball-install/  .  The latest available the tarballs for OSG 3.3 are:   Binaries for 32-bit RHEL6  Binaries for 64-bit RHEL6  Binaries for RHEL7   For OSG 3.4:   Binaries for 64-bit RHEL6  Binaries for RHEL7", 
            "title": "Download the WN Client"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#install-the-wn-client", 
            "text": "Unpack the tarball.  Move the directory that was created to where you want the tarball client to be.  Run  osg-post-install  ( PATH_TO_CLIENT /osg/osg-post-install ) to fix the directories in the installation.  Source the setup  source  PATH_TO_CLIENT /setup.sh  (or  setup.csh  depending on the shell).  Download and set up CA certificiates using  osg-ca-manage  (See the  CA management documentation  for the available options).  Download CRLs using  fetch-crl .    Warning  Once  osg-post-install  is run to relocate the install, it cannot be run again.  You will need to unpack a fresh copy.   Example installation (in  /home/user/test-install , the  PATH_TO_CLIENT /  is  /home/user/test-install/osg-wn-client  ):  root@host #  mkdir /home/user/test-install root@host #   cd  /home/user/test-install root@host #  wget http://repo.opensciencegrid.org/tarball-install/3.4/osg-wn-client-latest.el6.x86_64.tar.gz root@host #  tar xzf osg-wn-client-latest.el6.x86_64.tar.gz root@host #   cd  osg-wn-client root@host #  ./osg/osg-post-install root@host #   source  setup.sh root@host #  osg-ca-manage setupCA --url osg root@host #  fetch-crl", 
            "title": "Install the WN Client"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#configure-the-ce", 
            "text": "Using the wn-client software installed from the tarball will require a few changes on the compute element so that the resource's configuration can be correctly reported.  Set  grid_dir  in the  Storage  section of your OSG-Configure configs:  CE configuration instructions .  grid_dir  is used as the  $OSG_GRID  environment variable in running jobs - see the  worker node environment document . Pilot jobs source  $OSG_GRID/setup.sh  before performing any work. The value set for  grid_dir  must be the path of the wn-client installation directory. This is the path returned by  echo $OSG_LOCATION  once you source the setup file created by this installation.", 
            "title": "Configure the CE"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#services", 
            "text": "The WN client is a collection of client programs that do not require service startup or shutdown. The only services are  osg-update-certs  that keeps the CA certificates up-to-date and  fetch-crl  that keeps the CRLs up-to-date. Following the instructions below you'll add the services to your crontab that will take care to run them periodically until you remove them.", 
            "title": "Services"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#auto-updating-certificates-and-crls", 
            "text": "You must create cron jobs to run  fetch-crl  and  osg-update-certs  to update your CRLs and certificates automatically.  Here is what they should look like. (Note: fill in  OSG_LOCATION  with the full path of your tarball install, including  osg-wn-client  that is created by the tarball).  # Cron job to update certs.\n# Runs every hour by default, though does not update certs until they re at\n# least 24 hours old.  There is a random sleep time for up to 45 minutes (2700\n# seconds) to avoid overloading cert servers.\n10 * * * *   ( .  OSG_LOCATION /setup.sh   osg-update-certs --random-sleep 2700 --called-from-cron )  # Cron job to update CRLs\n# Runs every 6 hours at, 45 minutes +/- 3 minutes.\n42 */6 * * *   ( .  OSG_LOCATION /setup.sh   fetch-crl -q -r 360 )  You might want to configure proxy settings in  $OSG_LOCATION/etc/fetch-crl.conf .", 
            "title": "Auto-updating certificates and CRLs"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#starting-and-enabling-services", 
            "text": "To start the services you must edit your cron with  crontab -e  and add the lines above.", 
            "title": "Starting and Enabling Services"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#stopping-and-disabling-services", 
            "text": "To stop the services you must edit your cron with  crontab -e  and remove or comment the lines above.", 
            "title": "Stopping and Disabling Services"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#validing-the-worker-node-client", 
            "text": "To verify functionality of the worker node client, you will need to submit a test job against your CE and verify the job's output.   Submit a job that executes the  env  command (e.g. Run  condor_ce_trace  with the  -d  flag from your HTCondor CE)  Verify that the value of  $OSG_GRID  is set to the directory of your worker node client installation", 
            "title": "Validing the Worker Node Client"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#how-to-get-help", 
            "text": "To get assistance please use this  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/worker-node/install-wn-oasis/", 
            "text": "Configuring a Site to Use the Worker Node Client Software From OASIS\n\n\nAbout This Guide\n\n\nThe \nOSG Worker Node Client\n is a collection of software components that is expected to be added to every worker node that can run OSG jobs. It provides a common environment and a minimal set of common tools that all OSG jobs can expect to use.\n\n\nIt is possible to install the Worker Node Client software in a variety of ways, depending on what works best for distributing and managing software at your site:\n\n\n\n\nUse the Worker Node Client software from OASIS (this guide) - useful when \nCVMFS\n is already mounted on your worker nodes\n\n\nInstall using RPMs and Yum\n - useful when managing your worker nodes with a tool (e.g., Puppet, Chef) that can automate RPM installs\n\n\nInstall using a tarball\n - useful when installing onto a shared filesystem for distribution to worker nodes\n\n\n\n\nThis document is intended to guide system administrators through the process of configuring a site to make the Worker Node Client software available from OASIS.\n\n\nBefore Starting\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\n\n\nAlso note that, once configured to use OASIS, each use of Worker Node Client software will cause that software and its associated files to be downloaded from a CVMFS server or your local cache thereof. This may result in extra network activity, especially if Worker Node Client tools are used heavily.\n\n\nConfiguring Your Site to Use the Worker Node Client From OASIS\n\n\nBelow are the one-time steps that you must perform to configure your site to use the Worker Node Client software from OASIS.\n\n\nOn every worker node, \ninstall and configure CVMFS\n\n\nDetermine the OASIS path to the Worker Node Client software for your worker nodes:\n\n\n\n\n\n\n\n\nWorker Node OS\n\n\nUse\u2026\n\n\n\n\n\n\n\n\n\n\nEL\u00a06 (32-bit)\n\n\n/cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el6-i386\n\n\n\n\n\n\nEL\u00a06 (64-bit)\n\n\n/cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el6-x86_64\n\n\n\n\n\n\nEL\u00a07 (64-bit)\n\n\n/cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el7-x86_64\n\n\n\n\n\n\n\n\nOn the CE, in the \n/etc/osg/config.d/10-storage.ini\n file, set the \ngrid_dir\n configuration setting to the path from the previous step.\n\n\nFor more information, see the \nOSG worker node environment documentation\n and the \nCE configuration instructions\n.\n\n\nOnce you finish making changes to configuration files on your CE, validate, fix, and apply the configuration:\n\n\nroot@host #\n osg-configure -v\n\nroot@host #\n osg-configure -c\n\n\n\n\n\nValidating the Worker Node Client\n\n\nTo verify functionality of the worker node client, you will need to submit a test job against your CE and verify the job's output.\n\n\n\n\nSubmit a job that executes the \nenv\n command (e.g. Run \ncondor_ce_trace\n with the \n-d\n flag from your HTCondor CE)\n\n\nVerify that the value of \nOSG_GRID\n is set to the directory of your WN Client installation\n\n\n\n\nManually Using the Worker Node Client From OASIS\n\n\nIf you must log onto a worker node and use the Worker Node Client software directly during your login session, consult the following table for the command to set up your environment:\n\n\n\n\n\n\n\n\nWorker Node OS\n\n\nRun the following command\u2026\n\n\n\n\n\n\n\n\n\n\nEL\u00a06 (32-bit)\n\n\nsource /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el6-i386/setup.sh\n\n\n\n\n\n\nEL\u00a06 (64-bit)\n\n\nsource /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el6-x86_64/setup.sh\n\n\n\n\n\n\nEL\u00a07 (64-bit)\n\n\nsource /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el7-x86_64/setup.sh\n\n\n\n\n\n\n\n\nTroubleshooting\n\n\nSome possible issues that may come up:\n\n\n\n\n\n\nA missing softlink to the CA certs directory. To check this, run:\n\n\nuser@host $\n ls -l /cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.3/current/el6-x86_64/etc/grid-security/\n\n\n\n\n\nand check that \ncertificates\n is linked to somewhere. The fix is to yum update the \noasis-config\n package to version 4 or higher. A known workaround is to run:\n\n\nuser@host $\n \nexport\n \nX509_CERT_DIR\n=\n/cvmfs/oasis.opensciencegrid.org/mis/certificates\n\n\n\n\n\nbefore any commands.\n\n\n\n\n\n\nHow to get Help?\n\n\nTo get assistance please use this \nHelp Procedure\n.", 
            "title": "Worker Node (OASIS install)"
        }, 
        {
            "location": "/worker-node/install-wn-oasis/#configuring-a-site-to-use-the-worker-node-client-software-from-oasis", 
            "text": "", 
            "title": "Configuring a Site to Use the Worker Node Client Software From OASIS"
        }, 
        {
            "location": "/worker-node/install-wn-oasis/#about-this-guide", 
            "text": "The  OSG Worker Node Client  is a collection of software components that is expected to be added to every worker node that can run OSG jobs. It provides a common environment and a minimal set of common tools that all OSG jobs can expect to use.  It is possible to install the Worker Node Client software in a variety of ways, depending on what works best for distributing and managing software at your site:   Use the Worker Node Client software from OASIS (this guide) - useful when  CVMFS  is already mounted on your worker nodes  Install using RPMs and Yum  - useful when managing your worker nodes with a tool (e.g., Puppet, Chef) that can automate RPM installs  Install using a tarball  - useful when installing onto a shared filesystem for distribution to worker nodes   This document is intended to guide system administrators through the process of configuring a site to make the Worker Node Client software available from OASIS.", 
            "title": "About This Guide"
        }, 
        {
            "location": "/worker-node/install-wn-oasis/#before-starting", 
            "text": "As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system   Also note that, once configured to use OASIS, each use of Worker Node Client software will cause that software and its associated files to be downloaded from a CVMFS server or your local cache thereof. This may result in extra network activity, especially if Worker Node Client tools are used heavily.", 
            "title": "Before Starting"
        }, 
        {
            "location": "/worker-node/install-wn-oasis/#configuring-your-site-to-use-the-worker-node-client-from-oasis", 
            "text": "Below are the one-time steps that you must perform to configure your site to use the Worker Node Client software from OASIS.  On every worker node,  install and configure CVMFS  Determine the OASIS path to the Worker Node Client software for your worker nodes:     Worker Node OS  Use\u2026      EL\u00a06 (32-bit)  /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el6-i386    EL\u00a06 (64-bit)  /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el6-x86_64    EL\u00a07 (64-bit)  /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el7-x86_64     On the CE, in the  /etc/osg/config.d/10-storage.ini  file, set the  grid_dir  configuration setting to the path from the previous step.  For more information, see the  OSG worker node environment documentation  and the  CE configuration instructions .  Once you finish making changes to configuration files on your CE, validate, fix, and apply the configuration:  root@host #  osg-configure -v root@host #  osg-configure -c", 
            "title": "Configuring Your Site to Use the Worker Node Client From OASIS"
        }, 
        {
            "location": "/worker-node/install-wn-oasis/#validating-the-worker-node-client", 
            "text": "To verify functionality of the worker node client, you will need to submit a test job against your CE and verify the job's output.   Submit a job that executes the  env  command (e.g. Run  condor_ce_trace  with the  -d  flag from your HTCondor CE)  Verify that the value of  OSG_GRID  is set to the directory of your WN Client installation", 
            "title": "Validating the Worker Node Client"
        }, 
        {
            "location": "/worker-node/install-wn-oasis/#manually-using-the-worker-node-client-from-oasis", 
            "text": "If you must log onto a worker node and use the Worker Node Client software directly during your login session, consult the following table for the command to set up your environment:     Worker Node OS  Run the following command\u2026      EL\u00a06 (32-bit)  source /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el6-i386/setup.sh    EL\u00a06 (64-bit)  source /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el6-x86_64/setup.sh    EL\u00a07 (64-bit)  source /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el7-x86_64/setup.sh", 
            "title": "Manually Using the Worker Node Client From OASIS"
        }, 
        {
            "location": "/worker-node/install-wn-oasis/#troubleshooting", 
            "text": "Some possible issues that may come up:    A missing softlink to the CA certs directory. To check this, run:  user@host $  ls -l /cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.3/current/el6-x86_64/etc/grid-security/  and check that  certificates  is linked to somewhere. The fix is to yum update the  oasis-config  package to version 4 or higher. A known workaround is to run:  user@host $   export   X509_CERT_DIR = /cvmfs/oasis.opensciencegrid.org/mis/certificates  before any commands.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/worker-node/install-wn-oasis/#how-to-get-help", 
            "text": "To get assistance please use this  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/worker-node/install-cvmfs/", 
            "text": "Install CVMFS\n\n\nHere we describe how to install the\n\nCVMFS\n (Cern-VM file system) client.\nThis document is intended for system administrators who wish to\ninstall this client to have access to files distributed by CVMFS\nservers via HTTP.\n\n\n\n\nApplicable versions\n\n\nThe applicable software versions for this document are OSG Version \n= 3.4.3.\nThe version of CVMFS installed should be \n= 2.4.1\n\n\n\n\nBefore Starting\n\n\nBefore starting the installation process, consider the following points (consulting \nthe Reference section below\n as needed):\n\n\n\n\nUser IDs:\n If it does not exist already, the installation will create the \ncvmfs\n Linux user\n\n\nGroup IDs:\n If they do not exist already, the installation will create the Linux groups \ncvmfs\n and \nfuse\n\n\nNetwork ports:\n You will need network access to a local squid server such as the \nsquid distributed by OSG\n. The squid will need out-bound access to cvmfs stratum 1 servers.\n\n\nHost choice:\n -  Sufficient (~20GB+20%) cache space reserved, preferably in a separate filesystem (details \nbelow\n)\n\n\nFUSE\n: CVMFS requires FUSE\n\n\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare the \nrequired Yum repositories\n\n\n\n\nInstalling CVMFS\n\n\nThe following will install CVMFS from the OSG yum repository. It will also install fuse and autofs if you do not have them, and it will install the configuration for the OSG CVMFS distribution which is called OASIS. To simplify installation, OSG provides convenience RPMs that install all required software with a single command.\n\n\n\n\n\n\nClean yum cache:\n\n\nroot@host #\n yum clean all --enablerepo\n=\n*\n\n\n\n\n\n\n\n\n\nUpdate software:\n\n\nroot@host #\n yum update\n\n\n\n\n\nThis command will update \nall\n packages\n\n\n\n\n\n\nInstall CVMFS software:\n\n\nroot@host #\n yum install osg-oasis\n\n\n\n\n\n\n\n\n\nSetup of FUSE and automount\n\n\nFUSE and automount are required for CVMFS and the steps to configure them are different on EL6 vs EL7. Follow the section that is appropriate for your host's OS:\n\n\n\n\nFor EL6 hosts\n\n\nFor EL7 hosts\n\n\n\n\nFor EL6 hosts\n\n\n\n\n\n\nCreate or edit \n/etc/fuse.conf\n so that it contains:\n\n\nuser_allow_other\n\n\n\n\n\n\n\n\n\nCreate or edit \n/etc/auto.master\n to have the following contents to enable automounting:\n\n\n/cvmfs /etc/auto.cvmfs\n\n\n\n\n\n\n\n\n\nRestart autofs to make the change take effect:\n\n\nroot@host #\n service autofs restart\n\n\n\n\n\n\n\n\n\nFor EL7 hosts\n\n\n\n\n\n\nAdd the following to \n/etc/auto.master.d/cvmfs.autofs\n:\n\n\n/cvmfs /etc/auto.cvmfs\n\n\n\n\n\n\n\n\n\nRestart autofs to make the change take effect:\n\n\nroot@host #\n systemctl restart autofs\n\n\n\n\n\n\n\n\n\nConfiguring CVMFS\n\n\nCreate or edit \n/etc/cvmfs/default.local\n, a file that controls the\nCVMFS configuration. Below is a sample configuration, but please note\nthat you will need to \nedit the parts in \nred\n. In\nparticular, the \nCVMFS_HTTP_PROXY\n line below must be edited for your\nsite.\n\n\nCVMFS_REPOSITORIES=\n`echo $((echo oasis.opensciencegrid.org;echo cms.cern.ch;ls /cvmfs)|sort -u)|tr \n \n ,`\n\nCVMFS_QUOTA_LIMIT=\n20000\n\nCVMFS_HTTP_PROXY=\nhttp://squid.example.com:3128\n\n\n\n\n\n\nCVMFS by default allows any repository to be mounted, no matter what\nthe setting of \nCVMFS_REPOSITORIES\n is; that variable is used by support\ntools such as \ncvmfs_config\n and \ncvmfs_talk\n when they need to know a\nlist of repositories.  The recommended \nCVMFS_REPOSITORIES\n setting\nabove is so that those tools will use two common repositories plus any\nadditional that have been mounted.  You may want to choose a different\nset of always-known repositories.\n\n\nSet up a list of CVMFS HTTP proxies to retrieve from in\n\nCVMFS_HTTP_PROXY\n. If you do not have any squid at your site follow\nthe instructions to \ninstall squid from OSG\n.\nVertical bars separating proxies means to load balance between them\nand try them all before continuing. A semicolon between proxies means\nto try that one only after the previous ones have failed. A special\nproxy called DIRECT can be placed last in the list to indicate\ndirectly connecting to servers if all other proxies fail. A DIRECT\nproxy is acceptable for small sites but discouraged for large sites\nbecause of the potential load that could be put upon globally shared\nservers.\n\n\nSet up the cache limit in \nCVMFS_QUOTA_LIMIT\n (in MegaBytes). The\nrecommended value for most applications is 20000 MB. This is the\ncombined limit for all but the osgstorage.org repositories. This cache\nwill be stored in \n/var/lib/cvmfs\n by default; to override the\nlocation, set \nCVMFS_CACHE_BASE\n in \n/etc/cvmfs/default.local\n. Note\nthat an additional 1000 MB is allocated for a separate osgstorage.org\nrepositories cache in \n$CVMFS_CACHE_BASE/osgstorage\n. To be safe, make\nsure that at least 20% more than \n$CVMFS_QUOTA_LIMIT\n + 1000 MB of space\nstays available for CVMFS in that filesystem. This is very important,\nsince if that space is not available it can cause many I/O errors and\napplication crashes. Many system administrators choose to put the\ncache space in a separate filesystem, which is a good way to manage\nit.\n\n\n\n\nWarning\n\n\nIf you use SELinux and change \nCVMFS_CACHE_BASE\n, then the\nnew cache directory must be labelled with SELinux type\n\ncvmfs_cache_t\n. This can be done by executing the following command:\n\n\nuser@host $\n chcon -R -t cvmfs_cache_t \n$CVMFS_CACHE_BASE\n\n\n\n\n\n\n\n\nValidating CVMFS\n\n\nAfter CVMFS is installed, you should be able to see the \n/cvmfs\n\ndirectory. But note that it will initially appear to be empty:\n\n\nuser@host $\n ls /cvmfs\n\nuser@host $\n\n\n\n\n\n\nDirectories within \n/cvmfs\n will not be mounted until you examine them. For instance:\n\n\nuser@host $\n ls /cvmfs\n\nuser@host $\n ls -l /cvmfs/atlas.cern.ch\n\ntotal 1\n\n\ndrwxr-xr-x 8 cvmfs cvmfs 3 Apr 13 14:50 repo\n\n\nuser@host $\n ls -l /cvmfs/oasis.opensciencegrid.org/cmssoft\n\ntotal 1\n\n\nlrwxrwxrwx 1 cvmfs cvmfs 18 May 13  2015 cms -\n /cvmfs/cms.cern.ch\n\n\nuser@host $\n ls -l /cvmfs/glast.egi.eu\n\ntotal 5\n\n\ndrwxr-xr-x 9 cvmfs cvmfs 4096 Feb  7  2014 glast\n\n\nuser@host $\n ls -l /cvmfs/nova.osgstorage.org\n\ntotal 6\n\n\nlrwxrwxrwx 1 cvmfs cvmfs   43 Jun 14  2016 analysis -\n pnfs/fnal.gov/usr/nova/persistent/analysis/\n\n\nlrwxrwxrwx 1 cvmfs cvmfs   32 Jan 19 11:40 flux -\n pnfs/fnal.gov/usr/nova/data/flux\n\n\ndrwxr-xr-x 3 cvmfs cvmfs 4096 Jan 19 11:39 pnfs\n\n\nuser@host $\n ls /cvmfs\n\natlas.cern.ch                   glast.egi.eu         oasis.opensciencegrid.org\n\n\nconfig-osg.opensciencegrid.org  nova.osgstorage.org\n\n\n\n\n\n\nTroubleshooting problems\n\n\nIf no directories exist under \n/cvmfs/\n, you can try the following\nsteps to debug:\n\n\n\n\nMount it manually with \nmkdir -p /mnt/cvmfs\n and then\n  \nmount -t cvmfs REPOSITORYNAME /mnt/cvmfs\n where REPOSITORYNAME is\n  the repository, for example config-osg.opensciencegrid.org (which is\n  the best one to try first because other repositories require it to\n  be mounted).  If this works, then CVMFS is working, but there is a\n  problem with automount.\n\n\nIf that doesn't work and doesn't give any explanatory errors, try\n   \ncvmfs_config chksetup\n or \ncvmfs_config showconfig REPOSITORYNAME\n\n   to verify your setup.\n\n\nIf chksetup reports access problems to proxies, it may be caused by\n  access control settings in the squids.\n\n\nIf you have changed settings in \n/etc/cvmfs/default.local\n, and they\n  do not seem to be taking effect, note that there are other\n  configuration files that can override the settings. See the comments\n  at the beginning of \n/etc/cvmfs/default.conf\n regarding the order in\n  which configuration files are evaluated and look for old files that\n  may have been left from a previous installation.\n\n\nMore things to try are in the\n  \nupstream documentation\n.\n\n\n\n\nStarting and Stopping services\n\n\nOnce it is set up, CVMFS is always automatically started when one of\nthe repositories are accessed; there are no system services to start.\n\n\nCVMFS can be stopped via:\n\n\nroot@host #\n cvmfs_config umount\n\nUnmounting /cvmfs/config-osg.opensciencegrid.org: OK\n\n\nUnmounting /cvmfs/atlas.cern.ch: OK\n\n\nUnmounting /cvmfs/oasis.opensciencegrid.org: OK\n\n\nUnmounting /cvmfs/glast.egi.eu: OK\n\n\nUnmounting /cvmfs/nova.osgstorage.org: OK\n\n\n\n\n\n\nHow to get Help?\n\n\nIf you cannot resolve the problem, there are several ways to receive help:\n\n\n\n\nFor bug reporting and OSG-specific issues, see our \nhelp procedure\n\n\nFor community support and best-effort software team support contact\n   \n.\n\n\nFor general CERN VM FileSystem support contact \n.\n\n\n\n\nReferences\n\n\n\n\nhttp://cernvm.cern.ch/portal/filesystem/techinformation\n\n\nhttps://cvmfs.readthedocs.io/en/latest/\n\n\n\n\nUsers and Groups\n\n\nThis installation will create one user unless it already exists\n\n\n\n\n\n\n\n\nUser\n\n\nComment\n\n\n\n\n\n\n\n\n\n\ncvmfs\n\n\nCernVM-FS service account\n\n\n\n\n\n\n\n\nThe installation will also create a cvmfs group and default the cvmfs\nuser to that group. In addition, if the fuse rpm is not for some\nreason already installed, installing cvmfs will also install fuse and\nthat will create another group:\n\n\n\n\n\n\n\n\nGroup\n\n\nComment\n\n\nGroup members\n\n\n\n\n\n\n\n\n\n\ncvmfs\n\n\nCernVM-FS service account\n\n\nnone\n\n\n\n\n\n\nfuse\n\n\nFUSE service account\n\n\ncvmfs", 
            "title": "CVMFS"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#install-cvmfs", 
            "text": "Here we describe how to install the CVMFS  (Cern-VM file system) client.\nThis document is intended for system administrators who wish to\ninstall this client to have access to files distributed by CVMFS\nservers via HTTP.   Applicable versions  The applicable software versions for this document are OSG Version  = 3.4.3.\nThe version of CVMFS installed should be  = 2.4.1", 
            "title": "Install CVMFS"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#before-starting", 
            "text": "Before starting the installation process, consider the following points (consulting  the Reference section below  as needed):   User IDs:  If it does not exist already, the installation will create the  cvmfs  Linux user  Group IDs:  If they do not exist already, the installation will create the Linux groups  cvmfs  and  fuse  Network ports:  You will need network access to a local squid server such as the  squid distributed by OSG . The squid will need out-bound access to cvmfs stratum 1 servers.  Host choice:  -  Sufficient (~20GB+20%) cache space reserved, preferably in a separate filesystem (details  below )  FUSE : CVMFS requires FUSE   As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system  Obtain root access to the host  Prepare the  required Yum repositories", 
            "title": "Before Starting"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#installing-cvmfs", 
            "text": "The following will install CVMFS from the OSG yum repository. It will also install fuse and autofs if you do not have them, and it will install the configuration for the OSG CVMFS distribution which is called OASIS. To simplify installation, OSG provides convenience RPMs that install all required software with a single command.    Clean yum cache:  root@host #  yum clean all --enablerepo = *    Update software:  root@host #  yum update  This command will update  all  packages    Install CVMFS software:  root@host #  yum install osg-oasis", 
            "title": "Installing CVMFS"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#setup-of-fuse-and-automount", 
            "text": "FUSE and automount are required for CVMFS and the steps to configure them are different on EL6 vs EL7. Follow the section that is appropriate for your host's OS:   For EL6 hosts  For EL7 hosts", 
            "title": "Setup of FUSE and automount"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#for-el6-hosts", 
            "text": "Create or edit  /etc/fuse.conf  so that it contains:  user_allow_other    Create or edit  /etc/auto.master  to have the following contents to enable automounting:  /cvmfs /etc/auto.cvmfs    Restart autofs to make the change take effect:  root@host #  service autofs restart", 
            "title": "For EL6 hosts"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#for-el7-hosts", 
            "text": "Add the following to  /etc/auto.master.d/cvmfs.autofs :  /cvmfs /etc/auto.cvmfs    Restart autofs to make the change take effect:  root@host #  systemctl restart autofs", 
            "title": "For EL7 hosts"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#configuring-cvmfs", 
            "text": "Create or edit  /etc/cvmfs/default.local , a file that controls the\nCVMFS configuration. Below is a sample configuration, but please note\nthat you will need to  edit the parts in  red . In\nparticular, the  CVMFS_HTTP_PROXY  line below must be edited for your\nsite.  CVMFS_REPOSITORIES= `echo $((echo oasis.opensciencegrid.org;echo cms.cern.ch;ls /cvmfs)|sort -u)|tr     ,` \nCVMFS_QUOTA_LIMIT= 20000 \nCVMFS_HTTP_PROXY= http://squid.example.com:3128   CVMFS by default allows any repository to be mounted, no matter what\nthe setting of  CVMFS_REPOSITORIES  is; that variable is used by support\ntools such as  cvmfs_config  and  cvmfs_talk  when they need to know a\nlist of repositories.  The recommended  CVMFS_REPOSITORIES  setting\nabove is so that those tools will use two common repositories plus any\nadditional that have been mounted.  You may want to choose a different\nset of always-known repositories.  Set up a list of CVMFS HTTP proxies to retrieve from in CVMFS_HTTP_PROXY . If you do not have any squid at your site follow\nthe instructions to  install squid from OSG .\nVertical bars separating proxies means to load balance between them\nand try them all before continuing. A semicolon between proxies means\nto try that one only after the previous ones have failed. A special\nproxy called DIRECT can be placed last in the list to indicate\ndirectly connecting to servers if all other proxies fail. A DIRECT\nproxy is acceptable for small sites but discouraged for large sites\nbecause of the potential load that could be put upon globally shared\nservers.  Set up the cache limit in  CVMFS_QUOTA_LIMIT  (in MegaBytes). The\nrecommended value for most applications is 20000 MB. This is the\ncombined limit for all but the osgstorage.org repositories. This cache\nwill be stored in  /var/lib/cvmfs  by default; to override the\nlocation, set  CVMFS_CACHE_BASE  in  /etc/cvmfs/default.local . Note\nthat an additional 1000 MB is allocated for a separate osgstorage.org\nrepositories cache in  $CVMFS_CACHE_BASE/osgstorage . To be safe, make\nsure that at least 20% more than  $CVMFS_QUOTA_LIMIT  + 1000 MB of space\nstays available for CVMFS in that filesystem. This is very important,\nsince if that space is not available it can cause many I/O errors and\napplication crashes. Many system administrators choose to put the\ncache space in a separate filesystem, which is a good way to manage\nit.   Warning  If you use SELinux and change  CVMFS_CACHE_BASE , then the\nnew cache directory must be labelled with SELinux type cvmfs_cache_t . This can be done by executing the following command:  user@host $  chcon -R -t cvmfs_cache_t  $CVMFS_CACHE_BASE", 
            "title": "Configuring CVMFS"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#validating-cvmfs", 
            "text": "After CVMFS is installed, you should be able to see the  /cvmfs \ndirectory. But note that it will initially appear to be empty:  user@host $  ls /cvmfs user@host $   Directories within  /cvmfs  will not be mounted until you examine them. For instance:  user@host $  ls /cvmfs user@host $  ls -l /cvmfs/atlas.cern.ch total 1  drwxr-xr-x 8 cvmfs cvmfs 3 Apr 13 14:50 repo  user@host $  ls -l /cvmfs/oasis.opensciencegrid.org/cmssoft total 1  lrwxrwxrwx 1 cvmfs cvmfs 18 May 13  2015 cms -  /cvmfs/cms.cern.ch  user@host $  ls -l /cvmfs/glast.egi.eu total 5  drwxr-xr-x 9 cvmfs cvmfs 4096 Feb  7  2014 glast  user@host $  ls -l /cvmfs/nova.osgstorage.org total 6  lrwxrwxrwx 1 cvmfs cvmfs   43 Jun 14  2016 analysis -  pnfs/fnal.gov/usr/nova/persistent/analysis/  lrwxrwxrwx 1 cvmfs cvmfs   32 Jan 19 11:40 flux -  pnfs/fnal.gov/usr/nova/data/flux  drwxr-xr-x 3 cvmfs cvmfs 4096 Jan 19 11:39 pnfs  user@host $  ls /cvmfs atlas.cern.ch                   glast.egi.eu         oasis.opensciencegrid.org  config-osg.opensciencegrid.org  nova.osgstorage.org", 
            "title": "Validating CVMFS"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#troubleshooting-problems", 
            "text": "If no directories exist under  /cvmfs/ , you can try the following\nsteps to debug:   Mount it manually with  mkdir -p /mnt/cvmfs  and then\n   mount -t cvmfs REPOSITORYNAME /mnt/cvmfs  where REPOSITORYNAME is\n  the repository, for example config-osg.opensciencegrid.org (which is\n  the best one to try first because other repositories require it to\n  be mounted).  If this works, then CVMFS is working, but there is a\n  problem with automount.  If that doesn't work and doesn't give any explanatory errors, try\n    cvmfs_config chksetup  or  cvmfs_config showconfig REPOSITORYNAME \n   to verify your setup.  If chksetup reports access problems to proxies, it may be caused by\n  access control settings in the squids.  If you have changed settings in  /etc/cvmfs/default.local , and they\n  do not seem to be taking effect, note that there are other\n  configuration files that can override the settings. See the comments\n  at the beginning of  /etc/cvmfs/default.conf  regarding the order in\n  which configuration files are evaluated and look for old files that\n  may have been left from a previous installation.  More things to try are in the\n   upstream documentation .", 
            "title": "Troubleshooting problems"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#starting-and-stopping-services", 
            "text": "Once it is set up, CVMFS is always automatically started when one of\nthe repositories are accessed; there are no system services to start.  CVMFS can be stopped via:  root@host #  cvmfs_config umount Unmounting /cvmfs/config-osg.opensciencegrid.org: OK  Unmounting /cvmfs/atlas.cern.ch: OK  Unmounting /cvmfs/oasis.opensciencegrid.org: OK  Unmounting /cvmfs/glast.egi.eu: OK  Unmounting /cvmfs/nova.osgstorage.org: OK", 
            "title": "Starting and Stopping services"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#how-to-get-help", 
            "text": "If you cannot resolve the problem, there are several ways to receive help:   For bug reporting and OSG-specific issues, see our  help procedure  For community support and best-effort software team support contact\n    .  For general CERN VM FileSystem support contact  .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#references", 
            "text": "http://cernvm.cern.ch/portal/filesystem/techinformation  https://cvmfs.readthedocs.io/en/latest/", 
            "title": "References"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#users-and-groups", 
            "text": "This installation will create one user unless it already exists     User  Comment      cvmfs  CernVM-FS service account     The installation will also create a cvmfs group and default the cvmfs\nuser to that group. In addition, if the fuse rpm is not for some\nreason already installed, installing cvmfs will also install fuse and\nthat will create another group:     Group  Comment  Group members      cvmfs  CernVM-FS service account  none    fuse  FUSE service account  cvmfs", 
            "title": "Users and Groups"
        }, 
        {
            "location": "/worker-node/install-singularity/", 
            "text": "Install Singularity\n\n\nSingularity\n is a tool that creates\ndocker-like process containers but without giving extra privileges to\nunprivileged users.  It is used by grid pilot jobs (which are\nsubmitted by per-VO grid workload management systems) to isolate user\njobs from the pilot's files and processes and from other users' files\nand processes.  It also supplies a chroot environment in order to run\nuser jobs in different operating system images under one Linux kernel.\n\n\nFor operating system kernels older than the one released for\nRed Hat Enterprise Linux (RHEL) 7.4,\nsingularity needs to use kernel capabilities that are only accessible\nto the root user, so it has to be installed with setuid-root\nexecutables.  Securing setuid-root programs is difficult, but singularity\nkeeps that privileged code to a\n\nminimum\n to keep the\nvulnerability low.  Beginning with the kernel released with RHEL 7.4,\nthere is a new\n\ntechnology preview feature\n\nto allow unprivileged bind mounts in user name spaces, which allows\nsingularity to run as an unprivileged user.  The OSG has installed\nsingularity in \ncvmfs\n, so if you have a RHEL 7.4 kernel\nor later you\ncan avoid installing singularity at all and reduce vulnerability even\nfurther.  The RHEL 7.4 kernel (version 3.10.0-693) is available as a\nsecurity update for all RHEL 7 based versions, even on systems that\nhave not updated to RHEL 7.4.\n\n\nThe document is intended for system administrators who wish to either\ninstall singularity or enable it to be run as an unprivileged user.\n\n\n\n\nApplicable versions\n\n\nThe applicable software versions for this document are OSG Version \n= 3.4.3.\nThe version of singularity installed should be \n= 2.3.1\n\n\n\n\nBefore Starting\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nIf you're installing singularity, prepare the \nrequired Yum repositories\n\n\n\n\nUsing Singularity via CVMFS (EL 7 only)\n\n\nIf the operating system is an EL 7 variant, and has been updated to EL\n7.4 or the 7.4 kernel (3.10.0-693 or greater), you can skip\ninstallation altogether and use singularity through CVMFS:\n\n\n\n\nSet the \nnamespace.unpriv_enable=1\n boot option.  The easiest way\n    to do this is to add it in \n/etc/sysconfig/grub\n to the end of the\n    GRUB_CMDLINE_LINUX variable, before the ending double-quote.\n\n\n\n\nUpdate the grub configuration:\n\n\nroot@host #\n grub2-mkconfig -o /boot/grub2/grub.cfg\n\n\n\n\n\n\n\n\n\nSet a sysctl option as follows:\n\n\nroot@host #\n \necho\n \nuser.max_user_namespaces = 15000\n \n\\\n\n    \n /etc/sysctl.d/90-max_user_namespaces.conf\n\n\n\n\n\n\n\n\n\nReboot\n\n\n\n\nIf you haven't yet installed \ncvmfs\n, do so.\n\n\nLog in as an ordinary unprivileged user and verify that singularity\n    works:\nuser@host $\n /cvmfs/oasis.opensciencegrid.org/mis/singularity/el7-x86_64/bin/singularity \n\\\n\n        \nexec\n -C -H \n$HOME\n:/srv /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo:el6 \n\\\n\n        ps -ef\n\nWARNING: Container does not have an exec helper script, calling \ncat\n directly\n\n\nUID        PID  PPID  C STIME TTY          TIME CMD\n\n\nuser         1     0  0 21:34 ?        00:00:00 ps -ef\n\n\n\n\n\n\n\n\n\n\nInstalling singularity\n\n\nTo install singularity, make sure that your host is up to date before installing the required packages:\n\n\n\n\n\n\nClean yum cache:\n\n\nroot@host #\n yum clean all --enablerepo\n=\n*\n\n\n\n\n\n\n\n\n\nUpdate software:\n\n\nroot@host #\n yum update\n\n\n\n\n\nThis command will update \nall\n packages\n\n\n\n\n\n\nThe singularity packages are split into two parts, choose the command that corresponds to your host:\n\n\n\n\n\n\nIf you are installing singularity on a worker node, where images do not need to be created of a manipulated, install just the smaller part to limit the amount of setuid-root code that is installed:\n\n\nroot@host #\n yum install singularity-runtime\n\n\n\n\n\n\n\n\n\nIf you want a full singularity installation, run the following command:\n\n\nroot@host #\n yum install singularity\n\n\n\n\n\n\n\n\n\n\n\n\n\nConfiguring singularity\n\n\nThe default configuration of singularity is sufficient.  If you want\nto see what options are available, see \n/etc/singularity/singularity.conf\n.\n\n\nValidating singularity\n\n\nAfter singularity is installed, as an ordinary user run the following\ncommand to verify it:\n\n\nuser@host $\n singlarity \nexec\n -C -H \n$HOME\n:/srv /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo:el6 ps -ef\n\nWARNING: Container does not have an exec helper script, calling \ncat\n directly\n\n\nUID        PID  PPID  C STIME TTY          TIME CMD\n\n\nuser         1     0  0 21:34 ?        00:00:00 ps -ef\n\n\n\n\n\n\nStarting and Stopping services\n\n\nsingularity has no services to start or stop.", 
            "title": "Singularity"
        }, 
        {
            "location": "/worker-node/install-singularity/#install-singularity", 
            "text": "Singularity  is a tool that creates\ndocker-like process containers but without giving extra privileges to\nunprivileged users.  It is used by grid pilot jobs (which are\nsubmitted by per-VO grid workload management systems) to isolate user\njobs from the pilot's files and processes and from other users' files\nand processes.  It also supplies a chroot environment in order to run\nuser jobs in different operating system images under one Linux kernel.  For operating system kernels older than the one released for\nRed Hat Enterprise Linux (RHEL) 7.4,\nsingularity needs to use kernel capabilities that are only accessible\nto the root user, so it has to be installed with setuid-root\nexecutables.  Securing setuid-root programs is difficult, but singularity\nkeeps that privileged code to a minimum  to keep the\nvulnerability low.  Beginning with the kernel released with RHEL 7.4,\nthere is a new technology preview feature \nto allow unprivileged bind mounts in user name spaces, which allows\nsingularity to run as an unprivileged user.  The OSG has installed\nsingularity in  cvmfs , so if you have a RHEL 7.4 kernel\nor later you\ncan avoid installing singularity at all and reduce vulnerability even\nfurther.  The RHEL 7.4 kernel (version 3.10.0-693) is available as a\nsecurity update for all RHEL 7 based versions, even on systems that\nhave not updated to RHEL 7.4.  The document is intended for system administrators who wish to either\ninstall singularity or enable it to be run as an unprivileged user.   Applicable versions  The applicable software versions for this document are OSG Version  = 3.4.3.\nThe version of singularity installed should be  = 2.3.1", 
            "title": "Install Singularity"
        }, 
        {
            "location": "/worker-node/install-singularity/#before-starting", 
            "text": "As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system  Obtain root access to the host  If you're installing singularity, prepare the  required Yum repositories", 
            "title": "Before Starting"
        }, 
        {
            "location": "/worker-node/install-singularity/#using-singularity-via-cvmfs-el-7-only", 
            "text": "If the operating system is an EL 7 variant, and has been updated to EL\n7.4 or the 7.4 kernel (3.10.0-693 or greater), you can skip\ninstallation altogether and use singularity through CVMFS:   Set the  namespace.unpriv_enable=1  boot option.  The easiest way\n    to do this is to add it in  /etc/sysconfig/grub  to the end of the\n    GRUB_CMDLINE_LINUX variable, before the ending double-quote.   Update the grub configuration:  root@host #  grub2-mkconfig -o /boot/grub2/grub.cfg    Set a sysctl option as follows:  root@host #   echo   user.max_user_namespaces = 15000   \\ \n      /etc/sysctl.d/90-max_user_namespaces.conf    Reboot   If you haven't yet installed  cvmfs , do so.  Log in as an ordinary unprivileged user and verify that singularity\n    works: user@host $  /cvmfs/oasis.opensciencegrid.org/mis/singularity/el7-x86_64/bin/singularity  \\ \n         exec  -C -H  $HOME :/srv /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo:el6  \\ \n        ps -ef WARNING: Container does not have an exec helper script, calling  cat  directly  UID        PID  PPID  C STIME TTY          TIME CMD  user         1     0  0 21:34 ?        00:00:00 ps -ef", 
            "title": "Using Singularity via CVMFS (EL 7 only)"
        }, 
        {
            "location": "/worker-node/install-singularity/#installing-singularity", 
            "text": "To install singularity, make sure that your host is up to date before installing the required packages:    Clean yum cache:  root@host #  yum clean all --enablerepo = *    Update software:  root@host #  yum update  This command will update  all  packages    The singularity packages are split into two parts, choose the command that corresponds to your host:    If you are installing singularity on a worker node, where images do not need to be created of a manipulated, install just the smaller part to limit the amount of setuid-root code that is installed:  root@host #  yum install singularity-runtime    If you want a full singularity installation, run the following command:  root@host #  yum install singularity", 
            "title": "Installing singularity"
        }, 
        {
            "location": "/worker-node/install-singularity/#configuring-singularity", 
            "text": "The default configuration of singularity is sufficient.  If you want\nto see what options are available, see  /etc/singularity/singularity.conf .", 
            "title": "Configuring singularity"
        }, 
        {
            "location": "/worker-node/install-singularity/#validating-singularity", 
            "text": "After singularity is installed, as an ordinary user run the following\ncommand to verify it:  user@host $  singlarity  exec  -C -H  $HOME :/srv /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo:el6 ps -ef WARNING: Container does not have an exec helper script, calling  cat  directly  UID        PID  PPID  C STIME TTY          TIME CMD  user         1     0  0 21:34 ?        00:00:00 ps -ef", 
            "title": "Validating singularity"
        }, 
        {
            "location": "/worker-node/install-singularity/#starting-and-stopping-services", 
            "text": "singularity has no services to start or stop.", 
            "title": "Starting and Stopping services"
        }, 
        {
            "location": "/data/frontier-squid/", 
            "text": "Install the Frontier Squid HTTP Caching Proxy\n\n\nFrontier Squid is a distribution of the well-known \nsquid HTTP caching\nproxy software\n that is optimized for use with\napplications on the Worldwide LHC Computing Grid (WLCG). It has\n\nmany advantages\n\nover regular squid for common grid applications, especially Frontier\nand CVMFS. The OSG distribution of frontier-squid is a straight rebuild of the\nupstream frontier-squid package for the convenience of OSG users.\n\n\nThis document is intended for System Administrators who are installing\n\nfrontier-squid\n, the OSG distribution of the Frontier Squid software.\n\n\n\n\nApplicable versions\n\n\nThe applicable software versions for this document are OSG Version \n= 3.4.0.\nThe version of frontier-squid installed should be \n= 3.5.24-3.1.\nWhen using an OSG Version \n 3.4.0 and a frontier-squid version in the\n2.7STABLE9 series, refer to the\n\nold upstream install documentation\n\ninstead of the current links included below. There are some\nincompatibilities between the two versions, so if you are upgrading\nfrom a 2.7STABLE9 version to a 3.5 version, see the\n\nupstream documentation on upgrading\n.\n\n\n\n\nFrontier Squid is Recommended\n\n\nOSG recommends that all sites run a caching proxy for HTTP and HTTPS\nto help reduce bandwidth and improve throughput. To that end, Compute\nElement (CE) installations include Frontier Squid automatically. We\nencourage all sites to configure and use this service, as described\nbelow.\n\n\nFor large sites that expect heavy load on the proxy, it may be best to\nrun the proxy on its own host. In that case, the Frontier Squid\nsoftware still will be installed on the CE, but it need not be\nenabled. Instead, install your proxy service on the separate host and\nthen configure the CE host to refer to the proxy on that host.\n\n\nThe \nosg-configure\n configuration tool (version 1.0.45 and later)\nwarns users who have not added the proxy location to their CE\nconfiguration. In the future, a proxy will be required and\nosg-configure will fail if the proxy location is not set.\n\n\nBefore Starting\n\n\nBefore starting the installation process, consider the following points (consulting \nthe Reference section below\n as needed):\n\n\n\n\nUser IDs:\n If it does not exist already, the installation will create the \nsquid\n Linux user\n\n\nNetwork ports:\n Frontier squid communicates on ports 3128 (TCP) and 3401 (UDP)\n\n\nHost choice:\n If you will be supporting the Frontier application at your site, review the\n\nupstream documentation Hardware considerations section\n to determine how to size your equipment.\n\n\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare the \nrequired Yum repositories\n\n\n\n\nInstalling Frontier Squid\n\n\nTo install Frontier Squid, make sure that your host is up to date before installing the required packages:\n\n\n\n\n\n\nClean yum cache:\n\n\nroot@host #\n yum clean all --enablerepo\n=\n*\n\n\n\n\n\n\n\n\n\nUpdate software:\n\n\nroot@host #\n yum update\n\n\n\n\n\nThis command will update \nall\n packages\n\n\n\n\n\n\nInstall Frontier Squid:\n\n\nroot@host #\n yum install frontier-squid\n\n\n\n\n\n\n\n\n\nConfiguring Frontier Squid\n\n\nConfiguring the Frontier Squid Service\n\n\nTo configure the Frontier Squid service itself:\n\n\n\n\nFollow the\n    \nConfiguration section of the upstream Frontier Squid documentation\n.\n\n\nEnable, start, and test the service (as described below).\n\n\nEnable WLCG monitoring as described in the\n    \nupstream documentation on enabling monitoring\n\n    and\n    \nregister the squid in OIM\n.\n\n\n\n\n\n\nNote\n\n\nAn important difference between the standard Squid software and\nthe Frontier Squid variant is that Frontier Squid changes are in\n\n/etc/squid/customize.sh\n instead of \n/etc/squid/squid.conf\n.\n\n\n\n\nConfiguring the OSG CE\n\n\nTo configure the OSG Compute Element (CE) to know about your Frontier Squid service:\n\n\n\n\n\n\nOn your CE host, edit \n/etc/osg/config.d/01-squid.ini\n\n\n\n\nMake sure that \nenabled\n is set to \nTrue\n\n\nSet \nlocation\n to the hostname and port of your Frontier Squid\n    service (e.g., \nmy.squid.host.edu:3128\n)\n\n\nLeave the other settings at \nDEFAULT\n unless you have specific\n    reasons to change them\n\n\n\n\n\n\n\n\nRun \nosg-configure\n to propagate the changes on your CE.\n\n\n\n\n\n\n\n\nNote\n\n\nYou may want to finish other CE configuration tasks before running\n\nosg-configure\n. Just be sure to run it once before starting CE\nservices.\n\n\n\n\nUsing Frontier-Squid\n\n\nStart the frontier-squid service and enable it to start at boot time. As a reminder, here are common service commands (all run as \nroot\n):\n\n\n\n\n\n\n\n\nTo...\n\n\nOn EL6, run the command...\n\n\nOn EL7, run the command...\n\n\n\n\n\n\n\n\n\n\nStart a service\n\n\nservice frontier-squid start\n\n\nsystemctl start frontier-squid\n\n\n\n\n\n\nStop a  service\n\n\nservice frontier-squid stop\n\n\nsystemctl stop frontier-squid\n\n\n\n\n\n\nEnable a service to start on boot\n\n\nchkconfig frontier-squid on\n\n\nsystemctl enable frontier-squid\n\n\n\n\n\n\nDisable a service from starting on boot\n\n\nchkconfig frontier-squid off\n\n\nsystemctl disable frontier-squid\n\n\n\n\n\n\n\n\nValidating Frontier Squid\n\n\nAs any user on another computer, do the following (where\n\nyoursquid.your.domain\n is the fully qualified domain name of your\nsquid server):\n\n\nuser@host $\n \nexport\n \nhttp_proxy\n=\nhttp://\nyoursquid.your.domain\n:3128\n\nuser@host $\n wget -qdO/dev/null http://frontier.cern.ch \n2\n1\n|\ngrep X-Cache\n\nX-Cache: MISS from \nyoursquid.your.domain\n\n\nuser@host $\n wget -qdO/dev/null http://frontier.cern.ch \n2\n1\n|\ngrep X-Cache\n\nX-Cache: HIT from \nyoursquid.your.domain\n\n\n\n\n\n\nIf the grep doesn't print anything, try removing it from the pipeline\nto see if errors are obvious. If the second try says MISS again,\nsomething is probably wrong with the squid cache writes. Look at the squid\n\naccess.log file\n\nto try to see what's wrong.\n\n\nIf your squid will be supporting the Frontier application, it is also\ngood to do the test in the\n\nupstream documentation Testing the installation section\n.\n\n\nReference\n\n\nUsers\n\n\nThe frontier-squid installation will create one user account unless it\nalready exists.\n\n\n\n\n\n\n\n\nUser\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nsquid\n\n\nReduced privilege user that the squid process runs under. Set the default gid of the \"squid\" user to be a group that is also called \"squid\".\n\n\n\n\n\n\n\n\nThe package can instead use another user name of your choice if you\ncreate a configuration file before installation. Details are in the\n\nupstream documentation Preparation section\n.\n\n\nNetworking\n\n\n\n\n\n\n\n\nService Name\n\n\nProtocol\n\n\nPort Number\n\n\nInbound\n\n\nOutbound\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nSquid\n\n\ntcp\n\n\n3128\n\n\n\u2713\n\n\n\u2713\n\n\nAlso limited in squid ACLs. Both in and outbound must not be wide open to internet simultaneously\n\n\n\n\n\n\nSquid monitor\n\n\nudp\n\n\n3401\n\n\n\u2713\n\n\n\n\nAlso limited in squid ACLs. Should be limited to monitoring server addresses\n\n\n\n\n\n\n\n\nThe addresses of the WLCG monitoring servers for use in firewalls are\nlisted in the\n\nupstream documentation Enabling monitoring section\n.\n\n\nFrontier Squid Log Files\n\n\nLog file contents are explained in the\n\nupstream documentation Log file contents section\n.", 
            "title": "Install HTTP Cache"
        }, 
        {
            "location": "/data/frontier-squid/#install-the-frontier-squid-http-caching-proxy", 
            "text": "Frontier Squid is a distribution of the well-known  squid HTTP caching\nproxy software  that is optimized for use with\napplications on the Worldwide LHC Computing Grid (WLCG). It has many advantages \nover regular squid for common grid applications, especially Frontier\nand CVMFS. The OSG distribution of frontier-squid is a straight rebuild of the\nupstream frontier-squid package for the convenience of OSG users.  This document is intended for System Administrators who are installing frontier-squid , the OSG distribution of the Frontier Squid software.   Applicable versions  The applicable software versions for this document are OSG Version  = 3.4.0.\nThe version of frontier-squid installed should be  = 3.5.24-3.1.\nWhen using an OSG Version   3.4.0 and a frontier-squid version in the\n2.7STABLE9 series, refer to the old upstream install documentation \ninstead of the current links included below. There are some\nincompatibilities between the two versions, so if you are upgrading\nfrom a 2.7STABLE9 version to a 3.5 version, see the upstream documentation on upgrading .", 
            "title": "Install the Frontier Squid HTTP Caching Proxy"
        }, 
        {
            "location": "/data/frontier-squid/#frontier-squid-is-recommended", 
            "text": "OSG recommends that all sites run a caching proxy for HTTP and HTTPS\nto help reduce bandwidth and improve throughput. To that end, Compute\nElement (CE) installations include Frontier Squid automatically. We\nencourage all sites to configure and use this service, as described\nbelow.  For large sites that expect heavy load on the proxy, it may be best to\nrun the proxy on its own host. In that case, the Frontier Squid\nsoftware still will be installed on the CE, but it need not be\nenabled. Instead, install your proxy service on the separate host and\nthen configure the CE host to refer to the proxy on that host.  The  osg-configure  configuration tool (version 1.0.45 and later)\nwarns users who have not added the proxy location to their CE\nconfiguration. In the future, a proxy will be required and\nosg-configure will fail if the proxy location is not set.", 
            "title": "Frontier Squid is Recommended"
        }, 
        {
            "location": "/data/frontier-squid/#before-starting", 
            "text": "Before starting the installation process, consider the following points (consulting  the Reference section below  as needed):   User IDs:  If it does not exist already, the installation will create the  squid  Linux user  Network ports:  Frontier squid communicates on ports 3128 (TCP) and 3401 (UDP)  Host choice:  If you will be supporting the Frontier application at your site, review the upstream documentation Hardware considerations section  to determine how to size your equipment.   As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system  Obtain root access to the host  Prepare the  required Yum repositories", 
            "title": "Before Starting"
        }, 
        {
            "location": "/data/frontier-squid/#installing-frontier-squid", 
            "text": "To install Frontier Squid, make sure that your host is up to date before installing the required packages:    Clean yum cache:  root@host #  yum clean all --enablerepo = *    Update software:  root@host #  yum update  This command will update  all  packages    Install Frontier Squid:  root@host #  yum install frontier-squid", 
            "title": "Installing Frontier Squid"
        }, 
        {
            "location": "/data/frontier-squid/#configuring-frontier-squid", 
            "text": "", 
            "title": "Configuring Frontier Squid"
        }, 
        {
            "location": "/data/frontier-squid/#configuring-the-frontier-squid-service", 
            "text": "To configure the Frontier Squid service itself:   Follow the\n     Configuration section of the upstream Frontier Squid documentation .  Enable, start, and test the service (as described below).  Enable WLCG monitoring as described in the\n     upstream documentation on enabling monitoring \n    and\n     register the squid in OIM .    Note  An important difference between the standard Squid software and\nthe Frontier Squid variant is that Frontier Squid changes are in /etc/squid/customize.sh  instead of  /etc/squid/squid.conf .", 
            "title": "Configuring the Frontier Squid Service"
        }, 
        {
            "location": "/data/frontier-squid/#configuring-the-osg-ce", 
            "text": "To configure the OSG Compute Element (CE) to know about your Frontier Squid service:    On your CE host, edit  /etc/osg/config.d/01-squid.ini   Make sure that  enabled  is set to  True  Set  location  to the hostname and port of your Frontier Squid\n    service (e.g.,  my.squid.host.edu:3128 )  Leave the other settings at  DEFAULT  unless you have specific\n    reasons to change them     Run  osg-configure  to propagate the changes on your CE.     Note  You may want to finish other CE configuration tasks before running osg-configure . Just be sure to run it once before starting CE\nservices.", 
            "title": "Configuring the OSG CE"
        }, 
        {
            "location": "/data/frontier-squid/#using-frontier-squid", 
            "text": "Start the frontier-squid service and enable it to start at boot time. As a reminder, here are common service commands (all run as  root ):     To...  On EL6, run the command...  On EL7, run the command...      Start a service  service frontier-squid start  systemctl start frontier-squid    Stop a  service  service frontier-squid stop  systemctl stop frontier-squid    Enable a service to start on boot  chkconfig frontier-squid on  systemctl enable frontier-squid    Disable a service from starting on boot  chkconfig frontier-squid off  systemctl disable frontier-squid", 
            "title": "Using Frontier-Squid"
        }, 
        {
            "location": "/data/frontier-squid/#validating-frontier-squid", 
            "text": "As any user on another computer, do the following (where yoursquid.your.domain  is the fully qualified domain name of your\nsquid server):  user@host $   export   http_proxy = http:// yoursquid.your.domain :3128 user@host $  wget -qdO/dev/null http://frontier.cern.ch  2 1 | grep X-Cache X-Cache: MISS from  yoursquid.your.domain  user@host $  wget -qdO/dev/null http://frontier.cern.ch  2 1 | grep X-Cache X-Cache: HIT from  yoursquid.your.domain   If the grep doesn't print anything, try removing it from the pipeline\nto see if errors are obvious. If the second try says MISS again,\nsomething is probably wrong with the squid cache writes. Look at the squid access.log file \nto try to see what's wrong.  If your squid will be supporting the Frontier application, it is also\ngood to do the test in the upstream documentation Testing the installation section .", 
            "title": "Validating Frontier Squid"
        }, 
        {
            "location": "/data/frontier-squid/#reference", 
            "text": "", 
            "title": "Reference"
        }, 
        {
            "location": "/data/frontier-squid/#users", 
            "text": "The frontier-squid installation will create one user account unless it\nalready exists.     User  Comment      squid  Reduced privilege user that the squid process runs under. Set the default gid of the \"squid\" user to be a group that is also called \"squid\".     The package can instead use another user name of your choice if you\ncreate a configuration file before installation. Details are in the upstream documentation Preparation section .", 
            "title": "Users"
        }, 
        {
            "location": "/data/frontier-squid/#networking", 
            "text": "Service Name  Protocol  Port Number  Inbound  Outbound  Comment      Squid  tcp  3128  \u2713  \u2713  Also limited in squid ACLs. Both in and outbound must not be wide open to internet simultaneously    Squid monitor  udp  3401  \u2713   Also limited in squid ACLs. Should be limited to monitoring server addresses     The addresses of the WLCG monitoring servers for use in firewalls are\nlisted in the upstream documentation Enabling monitoring section .", 
            "title": "Networking"
        }, 
        {
            "location": "/data/frontier-squid/#frontier-squid-log-files", 
            "text": "Log file contents are explained in the upstream documentation Log file contents section .", 
            "title": "Frontier Squid Log Files"
        }, 
        {
            "location": "/data/update-oasis/", 
            "text": "Updating Software on OASIS\n\n\nOASIS is the OSG Application Software Installation Service. It is the recommended method to install software on the Open Science Grid. It is implemented using CernVM FileSystem (CVMFS) technology.\n\n\n\n\nNote\n\n\nThe applicable software versions for this document are \nosg-version 3.1.13\n (or higher)\n\n\n\n\nThis document is a step by step explanation of how a Virtual Organization (VO) Software Adminstrator can enable the OASIS service and use it to publish and update software on OSG Worker Nodes under /cvmfs/oasis.opensciencegrid.org. For information on how to configure a client for OASIS see the \nOSG CVMFS Installation documentation\n. For information on hosting your own opensciencegrid.org repository see \nthis document\n.\n\n\nRequirements\n\n\nTo begin the process to distribute software on OASIS using the service hosted at the OSG GOC, you must:\n\n\n\n\nhave a personal grid certificate. The process for getting one is detailed \nhere\n.\n\n\nregister that certificate in \nOIM\n. In order to register, the certificate has to be installed in your browser. If you are then not registered, clicking \"Login\" on the page will prompt you to register.\n\n\nbe associated with a VO that is registered in OIM. The list of registered VOs appears \nhere\n.\n\n\nregister your certificate in the VOMS for your Virtual Organization. Click on the VO name that you're associated with in the above list and in the VO page click on the \n\"Membership Services URL\"\n to register with that VO VOMS.\n\n\n\n\nHow to use OASIS\n\n\nEnable OASIS\n\n\nWhen you are ready to distribute your software with OASIS, submit a \nGOC ticket\n with a request to enable OASIS for your VO. In your request, please specify your VO and provide a list of people who will install and administer the VO software in OASIS.\n\n\nThe GOC will enable OASIS for your VO in \nOIM\n and add your list of administrators to the \"OASIS Managers\" list (which is near the bottom of the page of information about each VO in OIM). oasis-login will then grant access to the people who are listed as OASIS managers. Any time the list is to be modified, submit another GOC ticket.\n\n\nLog in with GSISSH\n\n\nThe next step is to generate a proxy and log into \noasis-login.opensciencegrid.org\n with \ngsissh\n. These commands should be run on a computer that has the \nOSG worker node client\n software. First make sure that your grid certificate is installed in \n~/.globus/usercred.p12\n on that computer and that it is mode 600, then run these commands:\n\n\nuser@host $\n voms-proxy-init -voms \nVO\n\n\nuser@host $\n gsissh -o \nGSSAPIDelegateCredentials\n=\nyes oasis-login.opensciencegrid.org\n\n\n\n\n\nIn case the user can be mapped to more than one account, specify it explicitly in a command like this\n\n\nuser@host $\n gsissh -o \nGSSAPIDelegateCredentials\n=\nyes ouser.\nVO\n@oasis-login.opensciencegrid.org\n\n\n\n\n\nInstead of putting \n-o GSSAPIDelegateCredentials\nyes\n/verbatim\n on the command line, you can put it in your =~/.ssh/config\n like this:\n\n\nHost oasis-login.opensciencegrid.org\n\n\n    GSSAPIDelegateCredentials yes\n\n\n\n\n\n\nInstall and update software\n\n\nOnce you log in, you can add/modify/remove content on a staging area at \n/stage/oasis/$VO\n where $VO is the name of the VO represented by the manager.\n\n\nFiles here are visible to both \noasis-login\n and the Stratum 0 server (oasis.opensciencegrid.org).  There is a symbolic link at \n/cvmfs/oasis.opensciencegrid.org/$VO\n that points to the same staging area.  \n\n\nRequest an oasis publish with this command:\n\n\nuser@host $\n osg-oasis-update\n\n\n\n\n\nThis command queues a process to sync the content of OASIS with the content of \n/stage/oasis/$VO\n\n\nosg-oasis-update\n returns immediately, but only one update can run at a time (across all VOs); your request may be queued behind a different VO. If you encounter severe delays before the update is finished being published (more than 4 hours), please file a GOC ticket.\n\n\nLimitations on repository content\n\n\nAlthough CVMFS provides a POSIX filesystem, it does not work well with all types of content. Content in OASIS is expected to adhere to the \nCVMFS repository content limitations\n so please review those guidelines carefully.\n\n\nTesting\n\n\nAfter \nosg-oasis-update\n completes and the changes have been propagated to the CVMFS stratum 1 servers (typically between 0 and 60 minutes, but possibly longer if the servers are busy with updates of other repositories) then the changes can be visible under \n/cvmfs/oasis.opensciencegrid.org\n on a computer that has the \nCVMFS client installed\n. A client normally only checks for updates if at least an hour has passed since it last checked, but people who have superuser access on the client machine can force it to check again with\n\n\nroot@host #\n cvmfs_talk -i oasis.opensciencegrid.org remount\n\n\n\n\n\nThis can be done while the filesystem is mounted (despite what the name sounds like it does not do umount/mount). If the filesystem is not mounted, it will automatically check for new updates the next time it is mounted.\n\n\nIn order to find out if an update has reached the CVMFS stratum 1 server, you can find out the latest \nosg-oasis-update\n time seen by the stratum 1 most favored by your CVMFS client with the following long command on your client machine:\n\n\nuser@host $\n date -d \n1970-1-1 GMT + \n$(\nwget -qO- \n$(\nattr -qg host /cvmfs/oasis.opensciencegrid.org\n)\n/.cvmfspublished \n|\n \n\\\n\n                                                            cat -v\n|\nsed -n \n/^T/{s/^T//p;q;}\n)\n sec\n\n\n\n\n\n\nReferences\n\n\nCERN CVMFS home page", 
            "title": "Update OASIS"
        }, 
        {
            "location": "/data/update-oasis/#updating-software-on-oasis", 
            "text": "OASIS is the OSG Application Software Installation Service. It is the recommended method to install software on the Open Science Grid. It is implemented using CernVM FileSystem (CVMFS) technology.   Note  The applicable software versions for this document are  osg-version 3.1.13  (or higher)   This document is a step by step explanation of how a Virtual Organization (VO) Software Adminstrator can enable the OASIS service and use it to publish and update software on OSG Worker Nodes under /cvmfs/oasis.opensciencegrid.org. For information on how to configure a client for OASIS see the  OSG CVMFS Installation documentation . For information on hosting your own opensciencegrid.org repository see  this document .", 
            "title": "Updating Software on OASIS"
        }, 
        {
            "location": "/data/update-oasis/#requirements", 
            "text": "To begin the process to distribute software on OASIS using the service hosted at the OSG GOC, you must:   have a personal grid certificate. The process for getting one is detailed  here .  register that certificate in  OIM . In order to register, the certificate has to be installed in your browser. If you are then not registered, clicking \"Login\" on the page will prompt you to register.  be associated with a VO that is registered in OIM. The list of registered VOs appears  here .  register your certificate in the VOMS for your Virtual Organization. Click on the VO name that you're associated with in the above list and in the VO page click on the  \"Membership Services URL\"  to register with that VO VOMS.", 
            "title": "Requirements"
        }, 
        {
            "location": "/data/update-oasis/#how-to-use-oasis", 
            "text": "", 
            "title": "How to use OASIS"
        }, 
        {
            "location": "/data/update-oasis/#enable-oasis", 
            "text": "When you are ready to distribute your software with OASIS, submit a  GOC ticket  with a request to enable OASIS for your VO. In your request, please specify your VO and provide a list of people who will install and administer the VO software in OASIS.  The GOC will enable OASIS for your VO in  OIM  and add your list of administrators to the \"OASIS Managers\" list (which is near the bottom of the page of information about each VO in OIM). oasis-login will then grant access to the people who are listed as OASIS managers. Any time the list is to be modified, submit another GOC ticket.", 
            "title": "Enable OASIS"
        }, 
        {
            "location": "/data/update-oasis/#log-in-with-gsissh", 
            "text": "The next step is to generate a proxy and log into  oasis-login.opensciencegrid.org  with  gsissh . These commands should be run on a computer that has the  OSG worker node client  software. First make sure that your grid certificate is installed in  ~/.globus/usercred.p12  on that computer and that it is mode 600, then run these commands:  user@host $  voms-proxy-init -voms  VO  user@host $  gsissh -o  GSSAPIDelegateCredentials = yes oasis-login.opensciencegrid.org  In case the user can be mapped to more than one account, specify it explicitly in a command like this  user@host $  gsissh -o  GSSAPIDelegateCredentials = yes ouser. VO @oasis-login.opensciencegrid.org  Instead of putting  -o GSSAPIDelegateCredentials yes /verbatim  on the command line, you can put it in your =~/.ssh/config  like this:  Host oasis-login.opensciencegrid.org      GSSAPIDelegateCredentials yes", 
            "title": "Log in with GSISSH"
        }, 
        {
            "location": "/data/update-oasis/#install-and-update-software", 
            "text": "Once you log in, you can add/modify/remove content on a staging area at  /stage/oasis/$VO  where $VO is the name of the VO represented by the manager.  Files here are visible to both  oasis-login  and the Stratum 0 server (oasis.opensciencegrid.org).  There is a symbolic link at  /cvmfs/oasis.opensciencegrid.org/$VO  that points to the same staging area.    Request an oasis publish with this command:  user@host $  osg-oasis-update  This command queues a process to sync the content of OASIS with the content of  /stage/oasis/$VO  osg-oasis-update  returns immediately, but only one update can run at a time (across all VOs); your request may be queued behind a different VO. If you encounter severe delays before the update is finished being published (more than 4 hours), please file a GOC ticket.", 
            "title": "Install and update software"
        }, 
        {
            "location": "/data/update-oasis/#limitations-on-repository-content", 
            "text": "Although CVMFS provides a POSIX filesystem, it does not work well with all types of content. Content in OASIS is expected to adhere to the  CVMFS repository content limitations  so please review those guidelines carefully.", 
            "title": "Limitations on repository content"
        }, 
        {
            "location": "/data/update-oasis/#testing", 
            "text": "After  osg-oasis-update  completes and the changes have been propagated to the CVMFS stratum 1 servers (typically between 0 and 60 minutes, but possibly longer if the servers are busy with updates of other repositories) then the changes can be visible under  /cvmfs/oasis.opensciencegrid.org  on a computer that has the  CVMFS client installed . A client normally only checks for updates if at least an hour has passed since it last checked, but people who have superuser access on the client machine can force it to check again with  root@host #  cvmfs_talk -i oasis.opensciencegrid.org remount  This can be done while the filesystem is mounted (despite what the name sounds like it does not do umount/mount). If the filesystem is not mounted, it will automatically check for new updates the next time it is mounted.  In order to find out if an update has reached the CVMFS stratum 1 server, you can find out the latest  osg-oasis-update  time seen by the stratum 1 most favored by your CVMFS client with the following long command on your client machine:  user@host $  date -d  1970-1-1 GMT +  $( wget -qO-  $( attr -qg host /cvmfs/oasis.opensciencegrid.org ) /.cvmfspublished  |   \\ \n                                                            cat -v | sed -n  /^T/{s/^T//p;q;} )  sec", 
            "title": "Testing"
        }, 
        {
            "location": "/data/update-oasis/#references", 
            "text": "CERN CVMFS home page", 
            "title": "References"
        }, 
        {
            "location": "/data/external-oasis-repos/", 
            "text": "Procedures for handling OASIS externally-hosted CVMFS repositories\n\n\nTechnical information contained here is believed to be reliable but administrative procedures, particularly as concerns organizational entities outside the OSG, should be considered unapproved proposals.\n\n\nAbout OASIS\n\n\nOASIS is the OSG Application Software Installation Service. It is the recommended method to install software on the Open Science Grid. It is implemented using CernVM FileSystem (CVMFS) technology.\n\n\nAbout this Document\n\n\nThis document describes the detailed procedures for dealing with CVMFS repositories that are part of the OASIS system but are not hosted at the OSG Global Operations Center (GOC). Rather, they are hosted at the home institution of a Virtual Organization (VO). The procedures include the steps performed by both the GOC and by the repository service administrator. If you are only interested in using an OASIS repository that is hosted at the GOC, see \nthis document\n instead.\n\n\nPolicies\n\n\nThese are the policies regarding OSG CVMFS repositories:\n\n\n\n\nThe GOC will not host VO-specific CVMFS repositories. This means it will not use its machines to publish files to a repository that is dedicated to a VO. The GOC hosts one and only one repository and that is oasis.opensciencegrid.org.\n\n\nThe GOC will replicate an external repository of a VO or provide access to use oasis.opensciencegrid.org per the existing mechanisms but not both for a given VO.\n\n\nQuotas on size of repos may be implemented at the discretion of the GOC. A minimum of 100 GB per VO/repo is guaranteed, larger limits may be requested.\n\n\nThe fully qualified repository names of CVMFS repositories distributed to the OSG may be in any domain, but only those in the opensciencegrid.org domain may be requested to be exported to the European grid EGI at this time. There must be one secured master key for all the repositories in a domain. (Although note that the OSG OASIS servers replace that key with their own when distributing non-opensciencegrid.org repositories to OSG sites.)\n\n\n\n\nRequirements: setting up a CVMFS repository server\n\n\nThis document doesn't cover requirements for the GOC computers since those are already set up. The requirements for a CVMFS repository server are that it have installed cvmfs-server and cvmfs client, both version 2.2.2 or greater; version 2.3.3 or greater is recommended on EL6 and required on EL7. This requires using aufs or OverlayFS: aufs requires a modified kernel from CERN for Redhat EL6-based systems, and OverlayFS requires at least EL7.3; the latter uses a standard kernel so it is highly recommended. Also an apache httpd server needs to be enabled. NOTE: multiple cvmfs repositories can be hosted on the same machine.\n\n\n/srv/cvmfs\n will hold all the published data, so make sure it is large enough to hold all the repositories hosted on the machine. \n/var/spool/cvmfs\n will hold files during publish, so it should be large enough to hold the most amount of data that might be attempted to be published at once. In addition, on EL7 and cvmfs-server-2.4.1, \n/var/spool/cvmfs\n needs to be of filesystem type 'ext3' or 'ext4', or the \ncvmfs_server\n command will not allow it. There is a variable to override the filesystem type check, but other filesystem types are not guaranteed to work (xfs created with \nftype=1\n should work with \nCVMFS_DONT_CHECK_OVERLAYFS_VERSION=yes\n).\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare the \nrequired Yum repositories\n\n\n\n\nThis is the procedure for installing on a Redhat EL6-based system:\n\n\nroot@host #\n rpm -i https://cvmrepo.web.cern.ch/cvmrepo/yum/cvmfs-release-latest.noarch.rpm \n\nroot@host #\n yum install --enablerepo\n=\ncernvm-kernel --disablerepo\n=\ncernvm kernel aufs2-util cvmfs-server.x86_64 cvmfs.x86_64 cvmfs-config-osg \n\nroot@host #\n \necho\n \ncvmfs_server mount -a\n \n/etc/rc.local \n\nroot@host #\n reboot\n\n\n\n\n\nThis is the procedure for installing on a Redhat EL7-based system:\n\n\nroot@host #\n yum install cvmfs-server osg-oasis \n\nroot@host #\n \necho\n \ncvmfs_server mount -a\n \n/etc/rc.local\n\nroot@host #\n chmod +x /etc/rc.local\n\n\n\n\n\nIn addition, apache should listen on port 8000, have KeepAlive enabled, and be started. Use commands like these:\n\n\nroot@host #\n \necho\n Listen \n8000\n \n/etc/httpd/conf.d/cvmfs.conf \n\nroot@host #\n \necho\n KeepAlive on \n/etc/httpd/conf.d/cvmfs.conf \n\nroot@host #\n chkconfig httpd on \n\nroot@host #\n service httpd start\n\n\n\n\n\nMake sure that port 8000 is available to the internet through any firewalls.\n\n\nProcedure to add an externally-hosted repository to OASIS\n\n\n\n\n\n\nA VO representative who will have responsibility for the contents of the repository chooses a name for the repository. This name should be the name of the VO or be derived from it (or a project in the VO for the case of the OSG VO), and in a domain that has a secured master key. The recommended domain name for a new repository that originates at an OSG site is opensciencegrid.org. The full name used as an example in this document is \nrepo.domain.name\n. Note: the VO representative's name will be registered in OIM as an OASIS manager for the VO, and names can be added or changed later with GOC tickets. If there is more than one repository for the VO, all the OASIS managers are assumed to be contacts for all the repositories.\n\n\n\n\n\n\nUsing whatever mechanism is appropriate at their institution, the VO representative requests the local CVMFS repository service administrator to create this repository called \nrepo.domain.name\n.\n\n\n\n\n\n\nThe repository service administrator creates the repository with these command like these, where ownerid is the user id that will have write access:\n\n\nroot@host #\n \necho\n -e \n\\*\\\\t\\\\t-\\\\tnofile\\\\t\\\\t16384\n \n/etc/security/limits.conf \n\nroot@host #\n \nulimit\n -n \n16384\n \n\nroot@host #\n cvmfs_server mkfs -o ownerid repo.domain.name \n\nroot@host #\n \necho\n \nCVMFS_AUTO_TAG=false\n \n/etc/cvmfs/repositories.d/repo.domain.name/server.conf \n\nroot@host #\n \n(\necho\n Order deny,allow\n;\necho\n Deny from all\n;\necho\n Allow from \n127\n.0.0.1\n;\necho\n Allow from ::1\n;\necho\n Allow from \n129\n.79.53.0/24\n;\necho\n Allow from \n2001\n:18e8:2:6::/56\n)\n \n/srv/cvmfs/repo.domain.name/.htaccess\n\n\n\n\n\nIf you might be hosting any hardlinks that span directories (e.g. the git package) and are using aufs (that is, EL6), also do the following:\n\n\nroot@host #\n \necho\n \nCVMFS_IGNORE_XDIR_HARDLINKS=true\n \n/etc/cvmfs/repositories.d/repo.domain.name/server.conf\n\n\n\n\n\nVerify that the repository is readable over http with the following command:\n\n\nroot@host #\n wget -qO- http://localhost:8000/cvmfs/repo.domain.name/.cvmfswhitelist\n|\ncat -v\n\n\n\n\n\nThat should print several lines including some gibberish at the end.\n\n\n\n\n\n\nThe repository service administrator next creates a \nGOC ticket\n using the following format:\n\n\nPlease add a new CVMFS repository to OASIS for VO voname using the URL \n    http://fully.qualified.domain:8000/cvmfs/repo.domain.name \nby doing step #5 at\n    https://opensciencegrid.github.io/docs/data/external-oasis-repos\nThe VO responsible manager will be Vorep Name.\n\n\n\n\n\nreplacing \"voname\" with the VO's name, \"fully.qualified.domain\" with the full name of the repository server, \"repo.domain.name\" with the full name of the repository, and \"Vorep Name\" with the name of the VO representative.\n\n\n\n\n\n\nThe GOC representative next ensures that the repository service administrator is a valid representative of a host site for the VO. This can be done by (a) the GOC representative already having a relationship with the person or (b) the GOC representative contacting the VO manager to find out. The GOC representative makes sure that the repo.domain.name in the URL is derived from the VO name. \n\n\n\n\n\n\nThe GOC representative then adds the URL in OIM under OASIS Repo URLs for the VO. The repository will then be added to the GOC stratum 0 within 15 minutes.  Within an hour after step 8 is completed, the repository should also be automatically added to the GOC and FNAL stratum 1s.\n\n\n\n\n\n\nIf domain.name is a new domain that has not been distributed before, the GOC representative next places a copy of the domain.name.pub public key from domain.name into /srv/etc/keys on both oasis-replica and oasis-replica-itb. If the GOC representative does not have that key, he or she can ask the repository service representative in the ticket how to get it. In addition, in order to support cvmfs client versions 2.2.X (both older and newer clients do not need it), a symbolic link of \ndomain.name\n.conf has to be made in /cvmfs/config-osg.opensciencegrid.org/etc/cvmfs/domain.d pointing to default.conf. This symbolic link has to be created on the oasis-itb machine's copy of the config-osg.opensciencegrid.org repository and then copied to production with the \ncopy_config_osg\n command on the oasis machine.\n\n\n\n\n\n\nIf the repository name matches \n*.opensciencegrid.org\n or \n*.osgstorage.org\n, the GOC representative responds in the ticket to ask that step #8 be done; all other repositories (such as \n*.egi.eu\n) skip this step. The repository service administrator next executes the following commands (replacing repo.opensciencegrid.org with repo.osgstorage.org when needed):\n\n\nroot@host #\n wget -O /srv/cvmfs/repo.opensciencegrid.org/.cvmfswhitelist http://oasis.opensciencegrid.org/cvmfs/repo.opensciencegrid.org/.cvmfswhitelist \n\nroot@host #\n /bin/cp /etc/cvmfs/keys/opensciencegrid.org/opensciencegrid.org.pub /etc/cvmfs/keys/repo.opensciencegrid.org.pub\n\n\n\n\n\nNext the administrator verifies that a publish operation using the owner's privileges succeeds by making sure there's no errors from the following commands replacing \"ownerid\" with the owner's username:\n\n\nroot@host #\n su ownerid -c \ncvmfs_server transaction repo.opensciencegrid.org\n \n\nroot@host #\n su ownerid -c \ncvmfs_server publish repo.opensciencegrid.org\n\n\n\n\n\n\nIf that works then add the wget command to a daily cron:\n\n\nroot@host #\n \necho\n \n5 4 \\* \\* \\* ownerid cd /srv/cvmfs/repo.opensciencegrid.org \n wget -qO .cvmfswhitelist.new http://oasis.opensciencegrid.org/cvmfs/repo.opensciencegrid.org/.cvmfswhitelist \n mv .cvmfswhitelist.new .cvmfswhitelist\n \n/etc/cron.d/fetch-cvmfs-whitelist\n\n\n\n\n\nNote that this eliminates the need for the repository service administrator to periodically use \ncvmfs_server resign\n to update \n.cvmfswhitelist\n. Then the repository service administrator goes back to the open GOC ticket and asks to proceed to step #9.\n\n\n\n\n\n\nThe GOC representative then asks the administrator of the BNL stratum 1 to also add the new repository. He should set up his stratum 1s to read from \nhttp://oasis-replica.opensciencegrid.org:8000/cvmfs/repo.domain.name\n. When he has reported back that the replication is ready, the GOC representative reports in the ticket that the repository is ready on the OSG and closes the ticket.\n\n\n\n\n\n\nThe repository service administrator then gives the VO representative access to the repository under the \"ownerid\" login, informs them of the \nCVMFS documentation on maintaining repositories\n and requests that they adhere to the \nCVMFS documentation on repository content limitations\n. If the domain.name is opensciencegrid.org, the repository service administrator should also inform the VO representative that if they want the repository to be accessed outside of the U.S. they should open a ggus ticket following the EGI's [PROC20](https://wiki.egi.eu/wiki/PROC2). (Note: EGI does not have a concept of sub-VOs like OSG does; to EGI, all sub-VOs under fermilab are the fermilab VO).\n\n\n\n\n\n\nEmergency procedure to blank an externally-hosted repository to OASIS\n\n\n\n\nIf there is an emergency request from OSG security to shut down the distribution of a repository, the GOC representative just needs to run \nblank_osg_repository\n on oasis-replica and give it the full name of the repository. This will rename the repository directories to a name with the current timestamp and replace it with a blank repository. It includes a step to run on the oasis machine, and attempts to do it with ssh, but if that fails it prints instructions on how to finish by logging in to the oasis machine manually.\n\n\nWhen it is time to put the repository back into production, the GOC representative runs \nunblank_osg_repository\n on oasis-replica and gives it the full name of the repository again. This will find the directory with the old timestamp and put it back into service. This step also attempts to ssh to the oasis machine.\n\n\n\n\nProcedure to change the URL of an external repository\n\n\n\n\nThe repository service administrator opens a GOC ticket with the value of the new URL.\n\n\nThe GOC representative then changes the registered value in OIM for the VO in OASIS Repo URLs. The GOC stratum 1 will then be updated within an hour.\n\n\n\n\nProcedure to shut down and remove an external repository\n\n\n\n\nFirst, if the repository has been replicated outside of the U.S., the repository service administrator should open a GGUS ticket asking that the replication be removed from EGI stratum 1s. Wait until they say they are finished before going to the next step. \n\n\nNext, the repository service administrator opens a GOC ticket asking to shut down the repository, giving the repository name, for example \nrepo.domain.name\n, and the VO it belongs to. \n\n\nAfter validating that the ticket submitter is authorized by a registered OASIS manager, the GOC representative next deletes the registered value for \nrepo.domain.name\n in OIM for the VO in OASIS Repo URLs. \n\n\nNext, the GOC representative adds the FNAL and BNL stratum 1 administrators to the ticket to asks them to remove the repository. \n\n\nWhen the FNAL and BNL stratum 1 administrators say they have finished, the GOC representative then runs \ncvmfs_server rmfs -f repo.domain.name\n and \nrm -r /oasissrv/cvmfs/repo.domain.name\n on oasis-replica-itb and oasis-replica. \n\n\nFinally, the GOC representative does \nrm -r /srv/cvmfs/repo.domain.name\n on oasis-itb and oasis.", 
            "title": "External OASIS repositories"
        }, 
        {
            "location": "/data/external-oasis-repos/#procedures-for-handling-oasis-externally-hosted-cvmfs-repositories", 
            "text": "Technical information contained here is believed to be reliable but administrative procedures, particularly as concerns organizational entities outside the OSG, should be considered unapproved proposals.", 
            "title": "Procedures for handling OASIS externally-hosted CVMFS repositories"
        }, 
        {
            "location": "/data/external-oasis-repos/#about-oasis", 
            "text": "OASIS is the OSG Application Software Installation Service. It is the recommended method to install software on the Open Science Grid. It is implemented using CernVM FileSystem (CVMFS) technology.", 
            "title": "About OASIS"
        }, 
        {
            "location": "/data/external-oasis-repos/#about-this-document", 
            "text": "This document describes the detailed procedures for dealing with CVMFS repositories that are part of the OASIS system but are not hosted at the OSG Global Operations Center (GOC). Rather, they are hosted at the home institution of a Virtual Organization (VO). The procedures include the steps performed by both the GOC and by the repository service administrator. If you are only interested in using an OASIS repository that is hosted at the GOC, see  this document  instead.", 
            "title": "About this Document"
        }, 
        {
            "location": "/data/external-oasis-repos/#policies", 
            "text": "These are the policies regarding OSG CVMFS repositories:   The GOC will not host VO-specific CVMFS repositories. This means it will not use its machines to publish files to a repository that is dedicated to a VO. The GOC hosts one and only one repository and that is oasis.opensciencegrid.org.  The GOC will replicate an external repository of a VO or provide access to use oasis.opensciencegrid.org per the existing mechanisms but not both for a given VO.  Quotas on size of repos may be implemented at the discretion of the GOC. A minimum of 100 GB per VO/repo is guaranteed, larger limits may be requested.  The fully qualified repository names of CVMFS repositories distributed to the OSG may be in any domain, but only those in the opensciencegrid.org domain may be requested to be exported to the European grid EGI at this time. There must be one secured master key for all the repositories in a domain. (Although note that the OSG OASIS servers replace that key with their own when distributing non-opensciencegrid.org repositories to OSG sites.)", 
            "title": "Policies"
        }, 
        {
            "location": "/data/external-oasis-repos/#requirements-setting-up-a-cvmfs-repository-server", 
            "text": "This document doesn't cover requirements for the GOC computers since those are already set up. The requirements for a CVMFS repository server are that it have installed cvmfs-server and cvmfs client, both version 2.2.2 or greater; version 2.3.3 or greater is recommended on EL6 and required on EL7. This requires using aufs or OverlayFS: aufs requires a modified kernel from CERN for Redhat EL6-based systems, and OverlayFS requires at least EL7.3; the latter uses a standard kernel so it is highly recommended. Also an apache httpd server needs to be enabled. NOTE: multiple cvmfs repositories can be hosted on the same machine.  /srv/cvmfs  will hold all the published data, so make sure it is large enough to hold all the repositories hosted on the machine.  /var/spool/cvmfs  will hold files during publish, so it should be large enough to hold the most amount of data that might be attempted to be published at once. In addition, on EL7 and cvmfs-server-2.4.1,  /var/spool/cvmfs  needs to be of filesystem type 'ext3' or 'ext4', or the  cvmfs_server  command will not allow it. There is a variable to override the filesystem type check, but other filesystem types are not guaranteed to work (xfs created with  ftype=1  should work with  CVMFS_DONT_CHECK_OVERLAYFS_VERSION=yes ).  As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system  Obtain root access to the host  Prepare the  required Yum repositories   This is the procedure for installing on a Redhat EL6-based system:  root@host #  rpm -i https://cvmrepo.web.cern.ch/cvmrepo/yum/cvmfs-release-latest.noarch.rpm  root@host #  yum install --enablerepo = cernvm-kernel --disablerepo = cernvm kernel aufs2-util cvmfs-server.x86_64 cvmfs.x86_64 cvmfs-config-osg  root@host #   echo   cvmfs_server mount -a   /etc/rc.local  root@host #  reboot  This is the procedure for installing on a Redhat EL7-based system:  root@host #  yum install cvmfs-server osg-oasis  root@host #   echo   cvmfs_server mount -a   /etc/rc.local root@host #  chmod +x /etc/rc.local  In addition, apache should listen on port 8000, have KeepAlive enabled, and be started. Use commands like these:  root@host #   echo  Listen  8000   /etc/httpd/conf.d/cvmfs.conf  root@host #   echo  KeepAlive on  /etc/httpd/conf.d/cvmfs.conf  root@host #  chkconfig httpd on  root@host #  service httpd start  Make sure that port 8000 is available to the internet through any firewalls.", 
            "title": "Requirements: setting up a CVMFS repository server"
        }, 
        {
            "location": "/data/external-oasis-repos/#procedure-to-add-an-externally-hosted-repository-to-oasis", 
            "text": "A VO representative who will have responsibility for the contents of the repository chooses a name for the repository. This name should be the name of the VO or be derived from it (or a project in the VO for the case of the OSG VO), and in a domain that has a secured master key. The recommended domain name for a new repository that originates at an OSG site is opensciencegrid.org. The full name used as an example in this document is  repo.domain.name . Note: the VO representative's name will be registered in OIM as an OASIS manager for the VO, and names can be added or changed later with GOC tickets. If there is more than one repository for the VO, all the OASIS managers are assumed to be contacts for all the repositories.    Using whatever mechanism is appropriate at their institution, the VO representative requests the local CVMFS repository service administrator to create this repository called  repo.domain.name .    The repository service administrator creates the repository with these command like these, where ownerid is the user id that will have write access:  root@host #   echo  -e  \\*\\\\t\\\\t-\\\\tnofile\\\\t\\\\t16384   /etc/security/limits.conf  root@host #   ulimit  -n  16384   root@host #  cvmfs_server mkfs -o ownerid repo.domain.name  root@host #   echo   CVMFS_AUTO_TAG=false   /etc/cvmfs/repositories.d/repo.domain.name/server.conf  root@host #   ( echo  Order deny,allow ; echo  Deny from all ; echo  Allow from  127 .0.0.1 ; echo  Allow from ::1 ; echo  Allow from  129 .79.53.0/24 ; echo  Allow from  2001 :18e8:2:6::/56 )   /srv/cvmfs/repo.domain.name/.htaccess  If you might be hosting any hardlinks that span directories (e.g. the git package) and are using aufs (that is, EL6), also do the following:  root@host #   echo   CVMFS_IGNORE_XDIR_HARDLINKS=true   /etc/cvmfs/repositories.d/repo.domain.name/server.conf  Verify that the repository is readable over http with the following command:  root@host #  wget -qO- http://localhost:8000/cvmfs/repo.domain.name/.cvmfswhitelist | cat -v  That should print several lines including some gibberish at the end.    The repository service administrator next creates a  GOC ticket  using the following format:  Please add a new CVMFS repository to OASIS for VO voname using the URL \n    http://fully.qualified.domain:8000/cvmfs/repo.domain.name \nby doing step #5 at\n    https://opensciencegrid.github.io/docs/data/external-oasis-repos\nThe VO responsible manager will be Vorep Name.  replacing \"voname\" with the VO's name, \"fully.qualified.domain\" with the full name of the repository server, \"repo.domain.name\" with the full name of the repository, and \"Vorep Name\" with the name of the VO representative.    The GOC representative next ensures that the repository service administrator is a valid representative of a host site for the VO. This can be done by (a) the GOC representative already having a relationship with the person or (b) the GOC representative contacting the VO manager to find out. The GOC representative makes sure that the repo.domain.name in the URL is derived from the VO name.     The GOC representative then adds the URL in OIM under OASIS Repo URLs for the VO. The repository will then be added to the GOC stratum 0 within 15 minutes.  Within an hour after step 8 is completed, the repository should also be automatically added to the GOC and FNAL stratum 1s.    If domain.name is a new domain that has not been distributed before, the GOC representative next places a copy of the domain.name.pub public key from domain.name into /srv/etc/keys on both oasis-replica and oasis-replica-itb. If the GOC representative does not have that key, he or she can ask the repository service representative in the ticket how to get it. In addition, in order to support cvmfs client versions 2.2.X (both older and newer clients do not need it), a symbolic link of  domain.name .conf has to be made in /cvmfs/config-osg.opensciencegrid.org/etc/cvmfs/domain.d pointing to default.conf. This symbolic link has to be created on the oasis-itb machine's copy of the config-osg.opensciencegrid.org repository and then copied to production with the  copy_config_osg  command on the oasis machine.    If the repository name matches  *.opensciencegrid.org  or  *.osgstorage.org , the GOC representative responds in the ticket to ask that step #8 be done; all other repositories (such as  *.egi.eu ) skip this step. The repository service administrator next executes the following commands (replacing repo.opensciencegrid.org with repo.osgstorage.org when needed):  root@host #  wget -O /srv/cvmfs/repo.opensciencegrid.org/.cvmfswhitelist http://oasis.opensciencegrid.org/cvmfs/repo.opensciencegrid.org/.cvmfswhitelist  root@host #  /bin/cp /etc/cvmfs/keys/opensciencegrid.org/opensciencegrid.org.pub /etc/cvmfs/keys/repo.opensciencegrid.org.pub  Next the administrator verifies that a publish operation using the owner's privileges succeeds by making sure there's no errors from the following commands replacing \"ownerid\" with the owner's username:  root@host #  su ownerid -c  cvmfs_server transaction repo.opensciencegrid.org   root@host #  su ownerid -c  cvmfs_server publish repo.opensciencegrid.org   If that works then add the wget command to a daily cron:  root@host #   echo   5 4 \\* \\* \\* ownerid cd /srv/cvmfs/repo.opensciencegrid.org   wget -qO .cvmfswhitelist.new http://oasis.opensciencegrid.org/cvmfs/repo.opensciencegrid.org/.cvmfswhitelist   mv .cvmfswhitelist.new .cvmfswhitelist   /etc/cron.d/fetch-cvmfs-whitelist  Note that this eliminates the need for the repository service administrator to periodically use  cvmfs_server resign  to update  .cvmfswhitelist . Then the repository service administrator goes back to the open GOC ticket and asks to proceed to step #9.    The GOC representative then asks the administrator of the BNL stratum 1 to also add the new repository. He should set up his stratum 1s to read from  http://oasis-replica.opensciencegrid.org:8000/cvmfs/repo.domain.name . When he has reported back that the replication is ready, the GOC representative reports in the ticket that the repository is ready on the OSG and closes the ticket.    The repository service administrator then gives the VO representative access to the repository under the \"ownerid\" login, informs them of the  CVMFS documentation on maintaining repositories  and requests that they adhere to the  CVMFS documentation on repository content limitations . If the domain.name is opensciencegrid.org, the repository service administrator should also inform the VO representative that if they want the repository to be accessed outside of the U.S. they should open a ggus ticket following the EGI's [PROC20](https://wiki.egi.eu/wiki/PROC2). (Note: EGI does not have a concept of sub-VOs like OSG does; to EGI, all sub-VOs under fermilab are the fermilab VO).", 
            "title": "Procedure to add an externally-hosted repository to OASIS"
        }, 
        {
            "location": "/data/external-oasis-repos/#emergency-procedure-to-blank-an-externally-hosted-repository-to-oasis", 
            "text": "If there is an emergency request from OSG security to shut down the distribution of a repository, the GOC representative just needs to run  blank_osg_repository  on oasis-replica and give it the full name of the repository. This will rename the repository directories to a name with the current timestamp and replace it with a blank repository. It includes a step to run on the oasis machine, and attempts to do it with ssh, but if that fails it prints instructions on how to finish by logging in to the oasis machine manually.  When it is time to put the repository back into production, the GOC representative runs  unblank_osg_repository  on oasis-replica and gives it the full name of the repository again. This will find the directory with the old timestamp and put it back into service. This step also attempts to ssh to the oasis machine.", 
            "title": "Emergency procedure to blank an externally-hosted repository to OASIS"
        }, 
        {
            "location": "/data/external-oasis-repos/#procedure-to-change-the-url-of-an-external-repository", 
            "text": "The repository service administrator opens a GOC ticket with the value of the new URL.  The GOC representative then changes the registered value in OIM for the VO in OASIS Repo URLs. The GOC stratum 1 will then be updated within an hour.", 
            "title": "Procedure to change the URL of an external repository"
        }, 
        {
            "location": "/data/external-oasis-repos/#procedure-to-shut-down-and-remove-an-external-repository", 
            "text": "First, if the repository has been replicated outside of the U.S., the repository service administrator should open a GGUS ticket asking that the replication be removed from EGI stratum 1s. Wait until they say they are finished before going to the next step.   Next, the repository service administrator opens a GOC ticket asking to shut down the repository, giving the repository name, for example  repo.domain.name , and the VO it belongs to.   After validating that the ticket submitter is authorized by a registered OASIS manager, the GOC representative next deletes the registered value for  repo.domain.name  in OIM for the VO in OASIS Repo URLs.   Next, the GOC representative adds the FNAL and BNL stratum 1 administrators to the ticket to asks them to remove the repository.   When the FNAL and BNL stratum 1 administrators say they have finished, the GOC representative then runs  cvmfs_server rmfs -f repo.domain.name  and  rm -r /oasissrv/cvmfs/repo.domain.name  on oasis-replica-itb and oasis-replica.   Finally, the GOC representative does  rm -r /srv/cvmfs/repo.domain.name  on oasis-itb and oasis.", 
            "title": "Procedure to shut down and remove an external repository"
        }, 
        {
            "location": "/data/gridftp/", 
            "text": "Installing and Maintaining a GridFTP Server\n\n\nAbout This Guide\n\n\nThis page explains how to install the stand-alone Globus GridFTP server.\n\n\nThe GridFTP package contains components necessary to set up a stand-alone gsiftp server and tools used to monitor and report its performance. A stand-alone GridFTP server might be used under the following circumstances:\n\n\n\n\nYou are serving VOs that use storage heavily (CMS, ATLAS, CDF, and D0) and your site has more than 250 cores\n\n\nYour site will be managing more than 50 TB of disk space\n\n\nA simple front-end to a filesystem allowing access over WAN - for example NFS.\n\n\n\n\n\n\nNote\n\n\nThis document is for a standalone GridFTP server on top of POSIX storage.  \nSee this page\n for installation and configuration of a GridFTP server on top of the Hadoop Distributed File System.\n\n\n\n\nBefore Starting\n\n\nBefore starting the installation process you will need to fulfill these prerequisites.\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare \nthe required Yum repositories\n\n\nInstall \nCA certificates\n\n\nService certificate: The GridFTP service uses a host certificate at \n/etc/grid-security/hostcert.pem\n and an accompanying key at \n/etc/grid-security/hostkey.pem\n\n\nNetwork ports: GridFTP listens on TCP port 2811 and the list of ports configured by the \nGLOBUS_TCP_SOURCE_RANGE\n environment variable.\n\n\n\n\nInstalling GridFTP\n\n\nFirst, you will need to install the GridFTP meta-package:\n\n\nroot@host #\n yum install osg-gridftp\n\n\n\n\n\nConfiguring GridFTP\n\n\nConfiguring authentication\n\n\nIn OSG 3.3, there are three methods to manage authentication for incoming jobs: the \nLCMAPS VOMS plugin\n, \nedg-mkgridmap\n and \nGUMS\n. Of these, GUMS has the most features and capabilities. The LCMAPS VOMS plugin is the new OSG-preferred authentication, offering the simplicity of edg-mkgridmap and many of GUMS' rich feature set. If you need to support \"\npool accounts\n\", GUMS is the only option with that capability.\n\n\nIn OSG 3.4, the LCMAPS VOMS plugin is the only available authentication solution.\n\n\nAuthentication with the LCMAPS VOMS plugin\n\n\n\n\n\n\nAdd the following line to \n/etc/sysconfig/globus-gridftp-server\n:\n\n\nexport\n \nLLGT_VOMS_ENABLE_CREDENTIAL_CHECK\n=\n1\n\n\n\n\n\n\nThis should \nonly\n be done for OSG 3.3; it is unnecessary for OSG 3.4.\n\n\n\n\n\n\nFollow the instructions in \nthe LCMAPS VOMS plugin installation and configuration document\n to prepare the LCMAPS VOMS plugin.\n\n\n\n\n\n\n\n\nNote\n\n\nThis is the suggested mechanism for all new installs.\n\n\n\n\nAuthentication with edg-mkgridmap\n\n\n\n\nWarning\n\n\nAs of the June 2017 release of OSG 3.4.0, this software is officially deprecated. Support is scheduled to end as of May 2018.\n\n\n\n\nBy default, GridFTP uses a gridmap file, found in \n/etc/grid-security/grid-mapfile\n. This is the file generated by \nedg-mkgridmap\n if you follow the default \ninstall instructions\n.  No further configuration is needed.\n\n\nAuthentication with GUMS\n\n\n\n\nWarning\n\n\nAs of the June 2017 release of OSG 3.4.0, this software is officially deprecated. Support is scheduled to end as of May 2018.\n\n\n\n\nIf you want to use GUMS security (recommended), you will need to enable it using the following steps:\n\n\n\n\n\n\nEdit \n/etc/grid-security/gsi-authz.conf\n and uncomment the globus callout.\n\n\nglobus_mapping liblcas_lcmaps_gt4_mapping.so lcmaps_callout\n\n\n\n\n\nNote that this used to be the full path to the library (\n/usr/lib64\n or \n/usr/lib\n), but now we rely on the linker for proper resolution in this file.\n\n\n\n\n\n\nEdit \n/etc/lcmaps.db\n to include your service endpoint:\n\n\n...\ngumsclient = \nlcmaps_gums_client.mod\n\n             \n-resourcetype ce\n\n             \n-actiontype execute-now\n\n             \n-capath /etc/grid-security/certificates\n\n             \n-cert   /etc/grid-security/hostcert.pem\n\n             \n-key    /etc/grid-security/hostkey.pem\n\n             \n--cert-owner root\n\n# Change this URL to your GUMS server\n             \n--endpoint https://\ngums.example.com:8443\n/gums/services/GUMSXACMLAuthorizationServicePort\n\n\n\n\n\n\n\n\n\n\nEnabling Gratia GridFTP transfer probe\n\n\nThe \nGratia GridFTP probe\n collects the information about the Gridftp transfers and forwards it to central Gratia collector. You need to enable the probe first. To do this, edit \n/etc/gratia/gridftp-transfer/ProbeConfig\n to set:\n\n\nEnableProbe=\n1\n\n\n\n\n\n\nAll other configuration settings should be suitable for most purposes. However, you can edit them if needed. The probe runs every 30 minutes as a cron job.\n\n\nOptional configuration\n\n\nModifying the environment\n\n\nEnvironment variables are stored in \n/etc/sysconfig/globus-gridftp-server\n which is sourced on service startup.  If you want to change LCMAPS log levels, or GridFTP port ranges, you can edit them there.\n\n\n#Uncomment and modify for firewalls\n\n\n#export GLOBUS_TCP_PORT_RANGE=min,max\n\n\n#export GLOBUS_TCP_SOURCE_RANGE=min,max\n\n\n\n\n\n\nNote that the variables \nGLOBUS_TCP_PORT_RANGE\n and \nGLOBUS_TCP_SOURCE_RANGE\n can be set here to allow GridFTP to navigate around firewall rules (these affect the inbound and outbound ports, respectively).\n\n\nTo troubleshoot LCMAPS authorization, you can add the following to \n/etc/sysconfig/globus-gridftp-server\n and choose a higher debug level:\n\n\n# level 0: no messages, 1: errors, 2: also warnings, 3: also notices,\n#  4: also info, 5: maximum debug\nLCMAPS_DEBUG_LEVEL=2\n\n\n\n\n\nOutput goes to \n/var/log/messages\n by default. Do not set logging to 5 on any production systems as that may cause systems to slow down significantly or become unresponsive.\n\n\nConfiguring a multi-homed server\n\n\nThe GridFTP uses control connections, data connections and IPC connections. By default it listens in all interfaces but this can be changed by editing the configuration file \n/etc/gridftp.conf\n.\n\n\nTo use a single interface you can set \nhostname\n to the Hostname or IP address to use:\n\n\nhostname IP-TO-USE\n\n\n\n\n\nYou can also set separately the \ncontrol_interface\n, \ndata_interface\n and \nipc_interface\n.  On systems that have multiple network interfaces, you may want to associate data transfers with the fastest possible NIC available. This can be done in the GridFTP server by setting \ndata_interface\n:\n\n\ncontrol_interface IP-TO-USE\ndata_interface IP-TO-USE\nipc_interface IP-TO-USE\n\n\n\n\n\nFor more options available for the GridFTP server, read the comments in the configuration file (\n/etc/gridftp.conf\n) or see the \nGlobus manual\n.\n\n\nManaging GridFTP\n\n\nIn addition to the GridFTP service itself, there are a number of supporting services in your installation. The specific services are:\n\n\n\n\n\n\n\n\nSoftware\n\n\nService name\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nFetch CRL\n\n\nfetch-crl-boot\n and \nfetch-crl-cron\n\n\nSee \nCA documentation\n for more info\n\n\n\n\n\n\nGratia\n\n\ngratia-probes-cron\n\n\nAccounting software\n\n\n\n\n\n\nGridFTP\n\n\nglobus-gridftp-server\n\n\n\n\n\n\n\n\n\n\nValidating GridFTP\n\n\nThe GridFTP service can be validated by using globus-url-copy. You will need to run \ngrid-proxy-init\n or \nvoms-proxy-init\n in order to get a valid user proxy in order to communicate with the GridFTP server.\n\n\nroot@host #\n globus-url-copy file:///tmp/zero.source gsiftp://yourhost.yourdomain/tmp/zero\n\nroot@host #\n \necho\n \n$?\n\n\n0\n\n\n\n\n\n\nRun the validation as an unprivileged user; when invoked as root, \nglobus-url-copy\n will attempt to use the host certificate instead of your user certificate, with confusing results.\n\n\nGetting Help\n\n\nFor assistance, please use \nthis page\n.\n\n\nReference\n\n\n\n\nGlobus GridFTP administration manual\n\n\nGlobus GridFTP tutorial\n\n\n\n\nConfiguration and Log Files\n\n\n\n\n\n\n\n\nService/Process\n\n\nConfiguration File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGridFTP\n\n\n/etc/sysconfig/globus-gridftp-server\n\n\nEnvironment variables for GridFTP and LCMAPS\n\n\n\n\n\n\n\n\n/usr/share/osg/sysconfig/globus-gridftp-server-plugin\n\n\nWhere environment variables for GridFTP plugin are included\n\n\n\n\n\n\nGratia Probe\n\n\n/etc/gratia/gridftp-transfer/ProbeConfig\n\n\nGridFTP Gratia Probe configuration\n\n\n\n\n\n\nGratia Probe\n\n\n/etc/cron.d/gratia-probe-gridftp-transfer.cron\n\n\nCron tab file\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nService/Process\n\n\nLog File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGridFTP\n\n\n/var/log/gridftp.log\n\n\nGridFTP transfer log\n\n\n\n\n\n\n\n\n/var/log/gridftp-auth.log\n\n\nGridFTP authorization log\n\n\n\n\n\n\nGratia probe\n\n\n/var/logs/gratia\n\n\n\n\n\n\n\n\n\n\nCertificates\n\n\n\n\n\n\n\n\nCertificate\n\n\nUser that owns certificate\n\n\nPath to certificate\n\n\n\n\n\n\n\n\n\n\nHost certificate\n\n\nroot\n\n\n/etc/grid-security/hostcert.pem\n and \n/etc/grid-security/hostkey.pem\n\n\n\n\n\n\n\n\nInstructions\n to request a service certificate.\n\n\nYou will also need a copy of CA certificates.\n\n\nUsers\n\n\nFor this package to function correctly, you will have to create the users needed for grid operation. Any Unix username that can be mapped by LCMAPS VOMS, \nedg-mkgridmap\n, or GUMS should be created on the GridFTP host.\n\n\nFor example, VOs newly-added to the LCMAPS VOMS configuration will not be able to transfer files until the corresponding Unix user account is created.\n\n\nNetworking\n\n\nFor more details on overall firewall configuration, please see our \nfirewall documentation\n.\n\n\n\n\n\n\n\n\nService Name\n\n\nProtocol\n\n\nPort Number\n\n\nInbound\n\n\nOutbound\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nGridFTP data channels\n\n\ntcp\n\n\nGLOBUS_TCP_PORT_RANGE\n\n\nX\n\n\n\n\ncontiguous range of ports is necessary.\n\n\n\n\n\n\nGridFTP data channels\n\n\ntcp\n\n\nGLOBUS_TCP_SOURCE_RANGE\n\n\n\n\nX\n\n\ncontiguous range of ports is necessary.\n\n\n\n\n\n\nGridFTP control channel\n\n\ntcp\n\n\n2811\n\n\nX", 
            "title": "Install GridFTP Server"
        }, 
        {
            "location": "/data/gridftp/#installing-and-maintaining-a-gridftp-server", 
            "text": "", 
            "title": "Installing and Maintaining a GridFTP Server"
        }, 
        {
            "location": "/data/gridftp/#about-this-guide", 
            "text": "This page explains how to install the stand-alone Globus GridFTP server.  The GridFTP package contains components necessary to set up a stand-alone gsiftp server and tools used to monitor and report its performance. A stand-alone GridFTP server might be used under the following circumstances:   You are serving VOs that use storage heavily (CMS, ATLAS, CDF, and D0) and your site has more than 250 cores  Your site will be managing more than 50 TB of disk space  A simple front-end to a filesystem allowing access over WAN - for example NFS.    Note  This document is for a standalone GridFTP server on top of POSIX storage.   See this page  for installation and configuration of a GridFTP server on top of the Hadoop Distributed File System.", 
            "title": "About This Guide"
        }, 
        {
            "location": "/data/gridftp/#before-starting", 
            "text": "Before starting the installation process you will need to fulfill these prerequisites.   Ensure the host has  a supported operating system  Obtain root access to the host  Prepare  the required Yum repositories  Install  CA certificates  Service certificate: The GridFTP service uses a host certificate at  /etc/grid-security/hostcert.pem  and an accompanying key at  /etc/grid-security/hostkey.pem  Network ports: GridFTP listens on TCP port 2811 and the list of ports configured by the  GLOBUS_TCP_SOURCE_RANGE  environment variable.", 
            "title": "Before Starting"
        }, 
        {
            "location": "/data/gridftp/#installing-gridftp", 
            "text": "First, you will need to install the GridFTP meta-package:  root@host #  yum install osg-gridftp", 
            "title": "Installing GridFTP"
        }, 
        {
            "location": "/data/gridftp/#configuring-gridftp", 
            "text": "", 
            "title": "Configuring GridFTP"
        }, 
        {
            "location": "/data/gridftp/#configuring-authentication", 
            "text": "In OSG 3.3, there are three methods to manage authentication for incoming jobs: the  LCMAPS VOMS plugin ,  edg-mkgridmap  and  GUMS . Of these, GUMS has the most features and capabilities. The LCMAPS VOMS plugin is the new OSG-preferred authentication, offering the simplicity of edg-mkgridmap and many of GUMS' rich feature set. If you need to support \" pool accounts \", GUMS is the only option with that capability.  In OSG 3.4, the LCMAPS VOMS plugin is the only available authentication solution.", 
            "title": "Configuring authentication"
        }, 
        {
            "location": "/data/gridftp/#authentication-with-the-lcmaps-voms-plugin", 
            "text": "Add the following line to  /etc/sysconfig/globus-gridftp-server :  export   LLGT_VOMS_ENABLE_CREDENTIAL_CHECK = 1   This should  only  be done for OSG 3.3; it is unnecessary for OSG 3.4.    Follow the instructions in  the LCMAPS VOMS plugin installation and configuration document  to prepare the LCMAPS VOMS plugin.     Note  This is the suggested mechanism for all new installs.", 
            "title": "Authentication with the LCMAPS VOMS plugin"
        }, 
        {
            "location": "/data/gridftp/#authentication-with-edg-mkgridmap", 
            "text": "Warning  As of the June 2017 release of OSG 3.4.0, this software is officially deprecated. Support is scheduled to end as of May 2018.   By default, GridFTP uses a gridmap file, found in  /etc/grid-security/grid-mapfile . This is the file generated by  edg-mkgridmap  if you follow the default  install instructions .  No further configuration is needed.", 
            "title": "Authentication with edg-mkgridmap"
        }, 
        {
            "location": "/data/gridftp/#authentication-with-gums", 
            "text": "Warning  As of the June 2017 release of OSG 3.4.0, this software is officially deprecated. Support is scheduled to end as of May 2018.   If you want to use GUMS security (recommended), you will need to enable it using the following steps:    Edit  /etc/grid-security/gsi-authz.conf  and uncomment the globus callout.  globus_mapping liblcas_lcmaps_gt4_mapping.so lcmaps_callout  Note that this used to be the full path to the library ( /usr/lib64  or  /usr/lib ), but now we rely on the linker for proper resolution in this file.    Edit  /etc/lcmaps.db  to include your service endpoint:  ...\ngumsclient =  lcmaps_gums_client.mod \n              -resourcetype ce \n              -actiontype execute-now \n              -capath /etc/grid-security/certificates \n              -cert   /etc/grid-security/hostcert.pem \n              -key    /etc/grid-security/hostkey.pem \n              --cert-owner root \n# Change this URL to your GUMS server\n              --endpoint https:// gums.example.com:8443 /gums/services/GUMSXACMLAuthorizationServicePort", 
            "title": "Authentication with GUMS"
        }, 
        {
            "location": "/data/gridftp/#enabling-gratia-gridftp-transfer-probe", 
            "text": "The  Gratia GridFTP probe  collects the information about the Gridftp transfers and forwards it to central Gratia collector. You need to enable the probe first. To do this, edit  /etc/gratia/gridftp-transfer/ProbeConfig  to set:  EnableProbe= 1   All other configuration settings should be suitable for most purposes. However, you can edit them if needed. The probe runs every 30 minutes as a cron job.", 
            "title": "Enabling Gratia GridFTP transfer probe"
        }, 
        {
            "location": "/data/gridftp/#optional-configuration", 
            "text": "", 
            "title": "Optional configuration"
        }, 
        {
            "location": "/data/gridftp/#modifying-the-environment", 
            "text": "Environment variables are stored in  /etc/sysconfig/globus-gridftp-server  which is sourced on service startup.  If you want to change LCMAPS log levels, or GridFTP port ranges, you can edit them there.  #Uncomment and modify for firewalls  #export GLOBUS_TCP_PORT_RANGE=min,max  #export GLOBUS_TCP_SOURCE_RANGE=min,max   Note that the variables  GLOBUS_TCP_PORT_RANGE  and  GLOBUS_TCP_SOURCE_RANGE  can be set here to allow GridFTP to navigate around firewall rules (these affect the inbound and outbound ports, respectively).  To troubleshoot LCMAPS authorization, you can add the following to  /etc/sysconfig/globus-gridftp-server  and choose a higher debug level:  # level 0: no messages, 1: errors, 2: also warnings, 3: also notices,\n#  4: also info, 5: maximum debug\nLCMAPS_DEBUG_LEVEL=2  Output goes to  /var/log/messages  by default. Do not set logging to 5 on any production systems as that may cause systems to slow down significantly or become unresponsive.", 
            "title": "Modifying the environment"
        }, 
        {
            "location": "/data/gridftp/#configuring-a-multi-homed-server", 
            "text": "The GridFTP uses control connections, data connections and IPC connections. By default it listens in all interfaces but this can be changed by editing the configuration file  /etc/gridftp.conf .  To use a single interface you can set  hostname  to the Hostname or IP address to use:  hostname IP-TO-USE  You can also set separately the  control_interface ,  data_interface  and  ipc_interface .  On systems that have multiple network interfaces, you may want to associate data transfers with the fastest possible NIC available. This can be done in the GridFTP server by setting  data_interface :  control_interface IP-TO-USE\ndata_interface IP-TO-USE\nipc_interface IP-TO-USE  For more options available for the GridFTP server, read the comments in the configuration file ( /etc/gridftp.conf ) or see the  Globus manual .", 
            "title": "Configuring a multi-homed server"
        }, 
        {
            "location": "/data/gridftp/#managing-gridftp", 
            "text": "In addition to the GridFTP service itself, there are a number of supporting services in your installation. The specific services are:     Software  Service name  Notes      Fetch CRL  fetch-crl-boot  and  fetch-crl-cron  See  CA documentation  for more info    Gratia  gratia-probes-cron  Accounting software    GridFTP  globus-gridftp-server", 
            "title": "Managing GridFTP"
        }, 
        {
            "location": "/data/gridftp/#validating-gridftp", 
            "text": "The GridFTP service can be validated by using globus-url-copy. You will need to run  grid-proxy-init  or  voms-proxy-init  in order to get a valid user proxy in order to communicate with the GridFTP server.  root@host #  globus-url-copy file:///tmp/zero.source gsiftp://yourhost.yourdomain/tmp/zero root@host #   echo   $?  0   Run the validation as an unprivileged user; when invoked as root,  globus-url-copy  will attempt to use the host certificate instead of your user certificate, with confusing results.", 
            "title": "Validating GridFTP"
        }, 
        {
            "location": "/data/gridftp/#getting-help", 
            "text": "For assistance, please use  this page .", 
            "title": "Getting Help"
        }, 
        {
            "location": "/data/gridftp/#reference", 
            "text": "Globus GridFTP administration manual  Globus GridFTP tutorial", 
            "title": "Reference"
        }, 
        {
            "location": "/data/gridftp/#configuration-and-log-files", 
            "text": "Service/Process  Configuration File  Description      GridFTP  /etc/sysconfig/globus-gridftp-server  Environment variables for GridFTP and LCMAPS     /usr/share/osg/sysconfig/globus-gridftp-server-plugin  Where environment variables for GridFTP plugin are included    Gratia Probe  /etc/gratia/gridftp-transfer/ProbeConfig  GridFTP Gratia Probe configuration    Gratia Probe  /etc/cron.d/gratia-probe-gridftp-transfer.cron  Cron tab file        Service/Process  Log File  Description      GridFTP  /var/log/gridftp.log  GridFTP transfer log     /var/log/gridftp-auth.log  GridFTP authorization log    Gratia probe  /var/logs/gratia", 
            "title": "Configuration and Log Files"
        }, 
        {
            "location": "/data/gridftp/#certificates", 
            "text": "Certificate  User that owns certificate  Path to certificate      Host certificate  root  /etc/grid-security/hostcert.pem  and  /etc/grid-security/hostkey.pem     Instructions  to request a service certificate.  You will also need a copy of CA certificates.", 
            "title": "Certificates"
        }, 
        {
            "location": "/data/gridftp/#users", 
            "text": "For this package to function correctly, you will have to create the users needed for grid operation. Any Unix username that can be mapped by LCMAPS VOMS,  edg-mkgridmap , or GUMS should be created on the GridFTP host.  For example, VOs newly-added to the LCMAPS VOMS configuration will not be able to transfer files until the corresponding Unix user account is created.", 
            "title": "Users"
        }, 
        {
            "location": "/data/gridftp/#networking", 
            "text": "For more details on overall firewall configuration, please see our  firewall documentation .     Service Name  Protocol  Port Number  Inbound  Outbound  Comment      GridFTP data channels  tcp  GLOBUS_TCP_PORT_RANGE  X   contiguous range of ports is necessary.    GridFTP data channels  tcp  GLOBUS_TCP_SOURCE_RANGE   X  contiguous range of ports is necessary.    GridFTP control channel  tcp  2811  X", 
            "title": "Networking"
        }, 
        {
            "location": "/data/load-balanced-gridftp/", 
            "text": "Load Balancing GridFTP\n\n\nGridFTP is designed for high throughput data transfers and in many cases can handle all of the transfers for a site. However, in some cases it may be useful to run multiple GridFTP servers to distribute the load. For such sites, we recommend using a \nload balancer\n to distribute requests and present the appearance of a single high-throughput GridFTP server.\n\n\nOne general-purpose technology for implementing a load balancer on Linux is \nLinux Virtual Server\n (LVS). To use it with GridFTP, a single load balancer listens on a virtual IP address, monitors the health of the set of real GridFTP servers, and forwards requests to available ones. Optionally, there can be one or more inactive, backup load balancers that can activate and take over the virtual IP address in case the primary load balancer fails, resulting in a system that is more resilient to failure. LVS is implemented by the \nIP Virtual Server\n kernel module, which can be managed by userspace services on the load balancers such as \nkeepalived\n.\n\n\nThis guide explains how to install, configure, run, test, and troubleshoot the \nkeepalived\n service on a load balancing host for a set of \nGridFTP\n servers.\n\n\n\n\nBefore starting the installation process, consider the following requirements:\n\n\n\n\nYou must have \ngrid administrator privileges\n\n\nThere must be a shared file system for file propagation across GridFTP servers\n\n\nYou must have reserved a virtual IP address and associated virtual hostname\n\n\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to each host\n\n\n\n\nDesigning Your Load-Balanced GridFTP System\n\n\nBefore beginning the installation process, you will need to plan the overall architecture of your load-balanced GridFTP system: the number of GridFTP servers, the type of shared file system to run on the GridFTP servers, whether or not backup load balancers are required, and hardware requirements.\n\n\nGridFTP servers\n\n\nThe number of GridFTP servers that you should run is determined first and foremost by the expected GridFTP transfer load at your site and the speed of the links available to each server. For example, if you expect a 20Gbps peak transfer load and have 10Gb links with 80\u201390% efficiency, you would need a minimum of 4 GridFTP servers: 3 to satisfy your desired throughput + 1 for failover or growth.\n\n\nShared file system\n\n\nThe number of GridFTP servers can also be determined by your hardware needs, which are determined by your choice of shared file system. If you choose a POSIX-based shared file system, plan for machines with more cores, or more GridFTP hosts to distribute the CPU load. If you are running \nGridFTP with Hadoop\n, plan for machines with more memory, or more GridFTP hosts to distribute the memory load.\n\n\n\n\nNote\n\n\nIf you determine that you need only a single GridFTP host, you do not need load balancing. Instead, follow the \nstandalone-GridFTP installation guide\n.\n\n\n\n\nLoad balancer(s)\n\n\nIn the recommended direct routing mode, load balancers simply rewrite initial packets from a given request so the hardware requirements are minimal. When choosing load balancer hosts, aim for stability. If your chosen host is unstable or if you do not want to introduce downtime for operating system or hardware updates, one additional load balancer as a backup should be sufficient.\n\n\nPreparing the GridFTP Servers\n\n\nBefore adding your GridFTP hosts to the load-balanced system, each host requires the GridFTP software, special certificates, and load-balancing configuration.\n\n\nAcquiring service certificate(s)\n\n\nWhen authenticating with a GridFTP server, clients verify that the server's host certificate matches the hostname of the server. In the case of a load-balanced GridFTP system, clients contact the GridFTP server through the virtual hostname, so the GridFTP server will have to present a certificate containing the virtual hostname as well as the GridFTP server's hostname. Use the \nOSG PKI tools\n to request these types of certificates.\n\n\nIf your GridFTP servers are also running XRootD, you will need unique certificates for each GridFTP server. Otherwise, you can request a single certificate that can be shared among the GridFTP servers.\n\n\nWithout XRootD\n\n\nThe single shared certificate must have the hostname associated with the load-balanced GridFTP system as its \ncommon name\n and each GridFTP servers hostname listed as \nsubject alternative names\n.\n\n\n\n\n\n\nRequest and generate the shared certificate:\n\n\nuser@host $\n osg-gridadmin-cert-request -H \nVIRTUAL-HOSTNAME\n -a \nGRIDFTP-SERVER-#1-HOSTNAME\n \n[\nhellip\n;\n]\n\n\n\n\n\n\n\n\n\n\nCopy the resulting certificate-key pair to each GridFTP server\n\n\n\n\n\n\nCreate a directory to contain the shared service certificate:\n\n\nroot@host #\n mkdir /etc/grid-security/gridftp\n\n\n\n\n\n\n\n\n\nPlace the shared service certificate-key pair in the newly created directory:\n\n\nroot@host #\n mv \nPATH-TO-SERVICE-CERT\n /etc/grid-security/gridftp/gridftp-hostcert.pem\n\nroot@host #\n mv \nPATH-TO-SERVICE-KEY\n /etc/grid-security/gridftp/gridftp-hostkey.pem\n\n\n\n\n\n\n\n\n\nEdit \n/etc/sysconfig/globus-gridftp-server\n to identify the shared service certificate-key pair:\n\n\nexport X509_USER_CERT=/etc/grid-security/gridftp/gridftp-hostcert.pem\nexport X509_USER_KEY=/etc/grid-security/gridftp/gridftp-hostkey.pem\n\n\n\n\n\n\n\n\n\nWith XRootD\n\n\nXRootD requires that the certificate's \ncommon name\n refers specifically to the host it resides on. To ensure each GridFTP server can authenticate using the virtual hostname, add it as the \nsubject alternative name\n for each certificate.\n\n\n\n\n\n\nCreate a list of GridFTP server hostnames in \nload-balanced-hosts.txt\n:\n\n\nGRIDFTP-SERVER-#1-HOSTNAME\n \nVIRTUAL-HOSTNAME\n\n\nGRIDFTP-SERVER-#2-HOSTNAME\n \nVIRTUAL-HOSTNAME\n\n[\nhellip;]\n\n\n\n\n\n\n\n\n\nSubmit a batch request for the per-GridFTP server certificates:\n\n\n::: console\n\n\nuser@host $\n osg-gridadmin-cert-request -f load-balanced-hosts.txt\n\n\n\n\n\n\n\n\n\nCopy the resulting certificates and keys to their corresponding GridFTP servers in \n/etc/grid-security/hostcert.pem\n and \n/etc/grid-security/hostkey.pem\n, respectively.\n\n\n\n\n\n\nInstalling GridFTP\n\n\nWhether you are starting from scratch or adding more GridFTP servers to your load-balanced GridFTP system, follow the documentation for \ninstalling a standalone GridFTP server\n for each of your intended GridFTP servers (skip section 2.2, requesting a certificate). For hosts with GridFTP already installed, skip this section.\n\n\nConfiguring your GridFTP servers\n\n\nEach GridFTP server requires changes to its IP configuration and potentially its arptables:\n\n\n\n\nAdding your virtual IP address\n\n\nDisabling ARP\n \u2212 if your GridFTP servers are on the same network segment as the virtual IP\n\n\n\n\nAdding your virtual IP address\n\n\nUse the virtual IP address of your load balancer(s) as the secondary IPs of each of your GridFTP servers.\n\n\n\n\n\n\nAdd the virtual IP using the \nip\n tool:\n\n\nroot@host #\n ip addr add \nVIRTUAL-IP-ADDRESS/\nSUBNET-MASK dev \nNETWORK-INTERFACE\n\n\n\n\n\n\n\n\n\nTo persist the virtual IP changes across reboots, edit \n/etc/rc.d/rc.local\n, and add the same command as used above.\n\n\n\n\nMake sure that \n/etc/rc.d/rc.local\n is executable:\nroot@host #\n chmod u+x /etc/rc.d/rc.local\n\n\n\n\n\n\n\n\n\nDisabling ARP\n\n\nIf your GridFTP servers and load balancer(s) are on the same network segment, you will have to disable ARP on the GridFTP servers to avoid \nARP race conditions\n. Otherwise, skip to \nthe section on preparing keepalived\n.\n\n\n\n\n\n\nSelect the appropriate RPM:\n\n\n\n\n\n\n\n\nIf your operating system version is...\n\n\nThen use the following package(s)...\n\n\n\n\n\n\n\n\n\n\nEnterprise Linux 6\n\n\narptables_jf\n\n\n\n\n\n\nEnterprise Linux 7\n\n\narptables\n\n\n\n\n\n\n\n\n\n\n\n\nInstall the arptables software:\n\n\nroot@host #\n yum install \nPACKAGE\n\n\n\n\n\n\n\n\n\n\nDisable ARP:\n\n\nroot@host #\n arptables -F \n\nroot@host #\n arptables -A IN -d \nVIRTUAL-IP-ADDRESS\n -j DROP \n\nroot@host #\n arptables -A OUT -s \nVIRTUAL-IP-ADDRESS\n -j mangle --mangle-ip-s \nGRIDFTP-REAL-IP-ADDRESS\n\n\n\n\n\n\n\n\n\n\nSave ARP tables to survive reboots:\n\n\nroot@host #\n arptables-save \n /etc/sysconfig/arptables\n\n\n\n\n\n\n\n\n\nPreparing Keepalived Load Balancer(s)\n\n\nInstalling Keepalived\n\n\nWhether you run a single load balancer, or have one active load balancer and some inactive backups, each load balancer host must have the \nkeepalived\n software installed, configured, and running.\n\n\n\n\nNote\n\n\nDo not install \nkeepalived\n on the GridFTP servers themselves.\n\n\n\n\nThe \nkeepalived\n package is available from standard operating system repositories. Install it on each load balancer host using the following commands:\n\n\n\n\n\n\nClean yum cache:\n\n\nroot@host #\n yum clean all --enablerepo\n=\n*\n\n\n\n\n\n\n\n\n\nUpdate software:\n\n\nroot@host #\n yum update\n\n\n\n\n\nThis command will update \nall\n packages\n\n\n\n\n\n\nInstall the \nkeepalived\n package:\n\n\nroot@host # yum install keepalived\n\n\n\n\n\n\n\n\n\nRequired configuration\n\n\nOn the primary load balancer, edit \n/etc/keepalived/keepalived.conf\n:\n\n\nglobal_defs {\n   router_id \nSTRING-LABEL-FOR-YOUR-LOAD-BALANCED-SYSTEM\n\n}\n\nvrrp_instance VI_gridftp {\n    state MASTER\n    interface \nNETWORK-INTERFACE\n\n    virtual_router_id \nINTEGER-BETWEEN-0-AND-255\n\n    priority 100\n    virtual_ipaddress {\n        \nVIRTUAL-IP-ADDRESS\n/\nSUBNET-MASK\n dev \nNETWORK-INTERFACE\n\n    }\n}\n\nvirtual_server \nVIRTUAL-IP-ADDRESS\n 2811 {\n    delay_loop 10\n    lb_algo wlc\n    lb_kind DR\n    protocol tcp\n\n    real_server \nGRIDFTP-SERVER-#1-IP ADDRESS\n {\n        TCP_CHECK {\n            connect_timeout 3\n            connect_port 2811\n        }\n    }\n    real_server \nGRIDFTP-SERVER-#2-IP-ADDRESS\n {\n        TCP_CHECK {\n            connect_timeout 3\n            connect_port 2811\n        }\n    }\n    \n[\nhellip;]\n\n}\n\n\n\n\n\n\n\nNote\n\n\nUse the same \nVIRTUAL-IP-ADDRESS\n throughout the configuration of your load-balanced GridFTP system.\n\n\n\n\n\n\nNote\n\n\nIn the \nvirtual_server\n section, write one \nreal_server\n subsection for each GridFTP server behind the load balancer.\n\n\n\n\nOptional configuration\n\n\nThe following configuration steps are optional and will likely not be required for setting up a small cluster of GridFTP hosts. If you do not need any of the following special configurations, skip to \nthe section on using keepalived\n.\n\n\n\n\nAdding backup load balancers\n\n\nEnabling e-mail notifications\n\n\n\n\nAdding backup load balancers\n\n\nIf you need to add backup load balancers, copy \n/etc/keepalived/keepalived.conf\n from your primary load balancer and change the \nstate\n and \npriority\n attributes under your \nvrrp_instance VI_gridftp\n section:\n\n\n\n\nNote\n\n\nPriority specifies the order of preferred load balancer fallback, where larger values corresponds to a higher preference.\n\n\n\n\nvrrp_instance VI_gridftp {\n    state BACKUP\n    interface \nNETWORK-INTERFACE\n\n    virtual_router_id \nSAME-ID-AS-MASTER-LOAD-BALANCER\n\n    priority \nPRIORITY-INTEGER\n\n    virtual_ipaddress {\n        \nVIRTUAL-IP-ADDRESS\n/\nSUBNET-MASK\n dev \nNETWORK-INTERFACE\n\n    }\n}\n\n\n\n\n\nEnabling e-mail notifications\n\n\nTo receive e-mails when the state of your load-balanced system changes, update the \nglobal_defs\n section of \n/etc/keepalived/keepalived.conf\n for each of your load balancer nodes:\n\n\nnotification_email {\n\nNOTIFY-EMAIL-ADDRESS-#1\n\n\nNOTIFY-EMAIL-ADDRESS-#2\n\n[...]\n}\nnotification_email_from \nFROM-EMAIL-ADDRESS\n\nsmtp_server \nSMTP-SERVER-IP-ADDRESS\n\nsmtp_connect_timeout 60\nrouter_id \nMACHINE-IDENTIFYING-STRING\n\n\n\n\n\n\nUsing Your Load Balanced GridFTP System\n\n\nUsing GridFTP\n\n\nOn the GridFTP real servers, arptables is the only additional service required for running a load-balanced GridFTP system. The name of the arptables service depends on the version of your host OS:\n\n\n\n\n\n\nSelect the appropriate RPM:\n\n\n\n\n\n\n\n\nIf your operating system version is...\n\n\nThen use the following package(s)...\n\n\n\n\n\n\n\n\n\n\nEnterprise Linux 6\n\n\narptables_jf\n\n\n\n\n\n\nEnterprise Linux 7\n\n\narptables\n\n\n\n\n\n\n\n\n\n\n\n\nManage the service with the following commands:\n\n\n\n\n\n\n\n\nTo ...\n\n\nOn EL\u00a06, run the command...\n\n\nOn EL\u00a07, run the command...\n\n\n\n\n\n\n\n\n\n\nStart a service\n\n\nservice SERVICE-NAME start\n\n\nsystemctl start SERVICE-NAME\n\n\n\n\n\n\nStop a service\n\n\nservice SERVICE-NAME stop\n\n\nsystemctl start SERVICE-NAME\n\n\n\n\n\n\nEnable a service to start during boot\n\n\nchkconfig SERVICE-NAME on\n\n\nsystemctl enable SERVICE-NAME\n\n\n\n\n\n\nDisable a service from starting during boot\n\n\nchkconfig SERVICE-NAME off\n\n\nsystemctl disable SERVICE-NAME\n\n\n\n\n\n\n\n\n\n\n\n\nFor information on how to use your individual GridFTP servers, please refer to the \nManaging GridFTP section\n of the GridFTP installation guide.\n\n\nUsing Keepalived\n\n\nOn the load balancer nodes, \nkeepalived\n is the only additional service required for running a load-balanced GridFTP system. As a reminder, here are common service commands (all run as \nroot\n):\n\n\n\n\n\n\n\n\nTo...\n\n\nOn EL\u00a06, run the command...\n\n\nOn EL\u00a07, run the command...\n\n\n\n\n\n\n\n\n\n\nStart a service\n\n\nservice keepalived start\n\n\nsystemctl start keepalived\n\n\n\n\n\n\nStop a service\n\n\nservice keepalived stop\n\n\nsystemctl start keepalived\n\n\n\n\n\n\nEnable a service to start during boot\n\n\nchkconfig keepalived on\n\n\nsystemctl enable keepalived\n\n\n\n\n\n\nDisable a service from starting during boot\n\n\nchkconfig keepalived off\n\n\nsystemctl disable keepalived\n\n\n\n\n\n\n\n\nGetting Help\n\n\nTo get assistance with \nkeepalived\n in front of OSG Software services, please use the \nthis page\n.\n\n\n\n\n\n\nLinux Virtual Server homepage\n\n\nKeepalived homepage\n\n\nRHEL 7 Load Balancer Administration Guide\n\n\nRHEL 6 Load Balancer Administration Guide\n\n\nT2 Nebraska LVS installation notes", 
            "title": "Install Load Balanced GridFTP"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#load-balancing-gridftp", 
            "text": "GridFTP is designed for high throughput data transfers and in many cases can handle all of the transfers for a site. However, in some cases it may be useful to run multiple GridFTP servers to distribute the load. For such sites, we recommend using a  load balancer  to distribute requests and present the appearance of a single high-throughput GridFTP server.  One general-purpose technology for implementing a load balancer on Linux is  Linux Virtual Server  (LVS). To use it with GridFTP, a single load balancer listens on a virtual IP address, monitors the health of the set of real GridFTP servers, and forwards requests to available ones. Optionally, there can be one or more inactive, backup load balancers that can activate and take over the virtual IP address in case the primary load balancer fails, resulting in a system that is more resilient to failure. LVS is implemented by the  IP Virtual Server  kernel module, which can be managed by userspace services on the load balancers such as  keepalived .  This guide explains how to install, configure, run, test, and troubleshoot the  keepalived  service on a load balancing host for a set of  GridFTP  servers.   Before starting the installation process, consider the following requirements:   You must have  grid administrator privileges  There must be a shared file system for file propagation across GridFTP servers  You must have reserved a virtual IP address and associated virtual hostname   As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system  Obtain root access to each host", 
            "title": "Load Balancing GridFTP"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#designing-your-load-balanced-gridftp-system", 
            "text": "Before beginning the installation process, you will need to plan the overall architecture of your load-balanced GridFTP system: the number of GridFTP servers, the type of shared file system to run on the GridFTP servers, whether or not backup load balancers are required, and hardware requirements.", 
            "title": "Designing Your Load-Balanced GridFTP System"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#gridftp-servers", 
            "text": "The number of GridFTP servers that you should run is determined first and foremost by the expected GridFTP transfer load at your site and the speed of the links available to each server. For example, if you expect a 20Gbps peak transfer load and have 10Gb links with 80\u201390% efficiency, you would need a minimum of 4 GridFTP servers: 3 to satisfy your desired throughput + 1 for failover or growth.", 
            "title": "GridFTP servers"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#shared-file-system", 
            "text": "The number of GridFTP servers can also be determined by your hardware needs, which are determined by your choice of shared file system. If you choose a POSIX-based shared file system, plan for machines with more cores, or more GridFTP hosts to distribute the CPU load. If you are running  GridFTP with Hadoop , plan for machines with more memory, or more GridFTP hosts to distribute the memory load.   Note  If you determine that you need only a single GridFTP host, you do not need load balancing. Instead, follow the  standalone-GridFTP installation guide .", 
            "title": "Shared file system"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#load-balancers", 
            "text": "In the recommended direct routing mode, load balancers simply rewrite initial packets from a given request so the hardware requirements are minimal. When choosing load balancer hosts, aim for stability. If your chosen host is unstable or if you do not want to introduce downtime for operating system or hardware updates, one additional load balancer as a backup should be sufficient.", 
            "title": "Load balancer(s)"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#preparing-the-gridftp-servers", 
            "text": "Before adding your GridFTP hosts to the load-balanced system, each host requires the GridFTP software, special certificates, and load-balancing configuration.", 
            "title": "Preparing the GridFTP Servers"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#acquiring-service-certificates", 
            "text": "When authenticating with a GridFTP server, clients verify that the server's host certificate matches the hostname of the server. In the case of a load-balanced GridFTP system, clients contact the GridFTP server through the virtual hostname, so the GridFTP server will have to present a certificate containing the virtual hostname as well as the GridFTP server's hostname. Use the  OSG PKI tools  to request these types of certificates.  If your GridFTP servers are also running XRootD, you will need unique certificates for each GridFTP server. Otherwise, you can request a single certificate that can be shared among the GridFTP servers.", 
            "title": "Acquiring service certificate(s)"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#without-xrootd", 
            "text": "The single shared certificate must have the hostname associated with the load-balanced GridFTP system as its  common name  and each GridFTP servers hostname listed as  subject alternative names .    Request and generate the shared certificate:  user@host $  osg-gridadmin-cert-request -H  VIRTUAL-HOSTNAME  -a  GRIDFTP-SERVER-#1-HOSTNAME   [ hellip ; ]     Copy the resulting certificate-key pair to each GridFTP server    Create a directory to contain the shared service certificate:  root@host #  mkdir /etc/grid-security/gridftp    Place the shared service certificate-key pair in the newly created directory:  root@host #  mv  PATH-TO-SERVICE-CERT  /etc/grid-security/gridftp/gridftp-hostcert.pem root@host #  mv  PATH-TO-SERVICE-KEY  /etc/grid-security/gridftp/gridftp-hostkey.pem    Edit  /etc/sysconfig/globus-gridftp-server  to identify the shared service certificate-key pair:  export X509_USER_CERT=/etc/grid-security/gridftp/gridftp-hostcert.pem\nexport X509_USER_KEY=/etc/grid-security/gridftp/gridftp-hostkey.pem", 
            "title": "Without XRootD"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#with-xrootd", 
            "text": "XRootD requires that the certificate's  common name  refers specifically to the host it resides on. To ensure each GridFTP server can authenticate using the virtual hostname, add it as the  subject alternative name  for each certificate.    Create a list of GridFTP server hostnames in  load-balanced-hosts.txt :  GRIDFTP-SERVER-#1-HOSTNAME   VIRTUAL-HOSTNAME  GRIDFTP-SERVER-#2-HOSTNAME   VIRTUAL-HOSTNAME \n[ hellip;]    Submit a batch request for the per-GridFTP server certificates:  ::: console  user@host $  osg-gridadmin-cert-request -f load-balanced-hosts.txt    Copy the resulting certificates and keys to their corresponding GridFTP servers in  /etc/grid-security/hostcert.pem  and  /etc/grid-security/hostkey.pem , respectively.", 
            "title": "With XRootD"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#installing-gridftp", 
            "text": "Whether you are starting from scratch or adding more GridFTP servers to your load-balanced GridFTP system, follow the documentation for  installing a standalone GridFTP server  for each of your intended GridFTP servers (skip section 2.2, requesting a certificate). For hosts with GridFTP already installed, skip this section.", 
            "title": "Installing GridFTP"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#configuring-your-gridftp-servers", 
            "text": "Each GridFTP server requires changes to its IP configuration and potentially its arptables:   Adding your virtual IP address  Disabling ARP  \u2212 if your GridFTP servers are on the same network segment as the virtual IP", 
            "title": "Configuring your GridFTP servers"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#adding-your-virtual-ip-address", 
            "text": "Use the virtual IP address of your load balancer(s) as the secondary IPs of each of your GridFTP servers.    Add the virtual IP using the  ip  tool:  root@host #  ip addr add  VIRTUAL-IP-ADDRESS/ SUBNET-MASK dev  NETWORK-INTERFACE    To persist the virtual IP changes across reboots, edit  /etc/rc.d/rc.local , and add the same command as used above.   Make sure that  /etc/rc.d/rc.local  is executable: root@host #  chmod u+x /etc/rc.d/rc.local", 
            "title": "Adding your virtual IP address"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#disabling-arp", 
            "text": "If your GridFTP servers and load balancer(s) are on the same network segment, you will have to disable ARP on the GridFTP servers to avoid  ARP race conditions . Otherwise, skip to  the section on preparing keepalived .    Select the appropriate RPM:     If your operating system version is...  Then use the following package(s)...      Enterprise Linux 6  arptables_jf    Enterprise Linux 7  arptables       Install the arptables software:  root@host #  yum install  PACKAGE     Disable ARP:  root@host #  arptables -F  root@host #  arptables -A IN -d  VIRTUAL-IP-ADDRESS  -j DROP  root@host #  arptables -A OUT -s  VIRTUAL-IP-ADDRESS  -j mangle --mangle-ip-s  GRIDFTP-REAL-IP-ADDRESS     Save ARP tables to survive reboots:  root@host #  arptables-save   /etc/sysconfig/arptables", 
            "title": "Disabling ARP"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#preparing-keepalived-load-balancers", 
            "text": "", 
            "title": "Preparing Keepalived Load Balancer(s)"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#installing-keepalived", 
            "text": "Whether you run a single load balancer, or have one active load balancer and some inactive backups, each load balancer host must have the  keepalived  software installed, configured, and running.   Note  Do not install  keepalived  on the GridFTP servers themselves.   The  keepalived  package is available from standard operating system repositories. Install it on each load balancer host using the following commands:    Clean yum cache:  root@host #  yum clean all --enablerepo = *    Update software:  root@host #  yum update  This command will update  all  packages    Install the  keepalived  package:  root@host # yum install keepalived", 
            "title": "Installing Keepalived"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#required-configuration", 
            "text": "On the primary load balancer, edit  /etc/keepalived/keepalived.conf :  global_defs {\n   router_id  STRING-LABEL-FOR-YOUR-LOAD-BALANCED-SYSTEM \n}\n\nvrrp_instance VI_gridftp {\n    state MASTER\n    interface  NETWORK-INTERFACE \n    virtual_router_id  INTEGER-BETWEEN-0-AND-255 \n    priority 100\n    virtual_ipaddress {\n         VIRTUAL-IP-ADDRESS / SUBNET-MASK  dev  NETWORK-INTERFACE \n    }\n}\n\nvirtual_server  VIRTUAL-IP-ADDRESS  2811 {\n    delay_loop 10\n    lb_algo wlc\n    lb_kind DR\n    protocol tcp\n\n    real_server  GRIDFTP-SERVER-#1-IP ADDRESS  {\n        TCP_CHECK {\n            connect_timeout 3\n            connect_port 2811\n        }\n    }\n    real_server  GRIDFTP-SERVER-#2-IP-ADDRESS  {\n        TCP_CHECK {\n            connect_timeout 3\n            connect_port 2811\n        }\n    }\n     [ hellip;] \n}   Note  Use the same  VIRTUAL-IP-ADDRESS  throughout the configuration of your load-balanced GridFTP system.    Note  In the  virtual_server  section, write one  real_server  subsection for each GridFTP server behind the load balancer.", 
            "title": "Required configuration"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#optional-configuration", 
            "text": "The following configuration steps are optional and will likely not be required for setting up a small cluster of GridFTP hosts. If you do not need any of the following special configurations, skip to  the section on using keepalived .   Adding backup load balancers  Enabling e-mail notifications", 
            "title": "Optional configuration"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#adding-backup-load-balancers", 
            "text": "If you need to add backup load balancers, copy  /etc/keepalived/keepalived.conf  from your primary load balancer and change the  state  and  priority  attributes under your  vrrp_instance VI_gridftp  section:   Note  Priority specifies the order of preferred load balancer fallback, where larger values corresponds to a higher preference.   vrrp_instance VI_gridftp {\n    state BACKUP\n    interface  NETWORK-INTERFACE \n    virtual_router_id  SAME-ID-AS-MASTER-LOAD-BALANCER \n    priority  PRIORITY-INTEGER \n    virtual_ipaddress {\n         VIRTUAL-IP-ADDRESS / SUBNET-MASK  dev  NETWORK-INTERFACE \n    }\n}", 
            "title": "Adding backup load balancers"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#enabling-e-mail-notifications", 
            "text": "To receive e-mails when the state of your load-balanced system changes, update the  global_defs  section of  /etc/keepalived/keepalived.conf  for each of your load balancer nodes:  notification_email { NOTIFY-EMAIL-ADDRESS-#1  NOTIFY-EMAIL-ADDRESS-#2 \n[...]\n}\nnotification_email_from  FROM-EMAIL-ADDRESS \nsmtp_server  SMTP-SERVER-IP-ADDRESS \nsmtp_connect_timeout 60\nrouter_id  MACHINE-IDENTIFYING-STRING", 
            "title": "Enabling e-mail notifications"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#using-your-load-balanced-gridftp-system", 
            "text": "", 
            "title": "Using Your Load Balanced GridFTP System"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#using-gridftp", 
            "text": "On the GridFTP real servers, arptables is the only additional service required for running a load-balanced GridFTP system. The name of the arptables service depends on the version of your host OS:    Select the appropriate RPM:     If your operating system version is...  Then use the following package(s)...      Enterprise Linux 6  arptables_jf    Enterprise Linux 7  arptables       Manage the service with the following commands:     To ...  On EL\u00a06, run the command...  On EL\u00a07, run the command...      Start a service  service SERVICE-NAME start  systemctl start SERVICE-NAME    Stop a service  service SERVICE-NAME stop  systemctl start SERVICE-NAME    Enable a service to start during boot  chkconfig SERVICE-NAME on  systemctl enable SERVICE-NAME    Disable a service from starting during boot  chkconfig SERVICE-NAME off  systemctl disable SERVICE-NAME       For information on how to use your individual GridFTP servers, please refer to the  Managing GridFTP section  of the GridFTP installation guide.", 
            "title": "Using GridFTP"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#using-keepalived", 
            "text": "On the load balancer nodes,  keepalived  is the only additional service required for running a load-balanced GridFTP system. As a reminder, here are common service commands (all run as  root ):     To...  On EL\u00a06, run the command...  On EL\u00a07, run the command...      Start a service  service keepalived start  systemctl start keepalived    Stop a service  service keepalived stop  systemctl start keepalived    Enable a service to start during boot  chkconfig keepalived on  systemctl enable keepalived    Disable a service from starting during boot  chkconfig keepalived off  systemctl disable keepalived", 
            "title": "Using Keepalived"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#getting-help", 
            "text": "To get assistance with  keepalived  in front of OSG Software services, please use the  this page .    Linux Virtual Server homepage  Keepalived homepage  RHEL 7 Load Balancer Administration Guide  RHEL 6 Load Balancer Administration Guide  T2 Nebraska LVS installation notes", 
            "title": "Getting Help"
        }, 
        {
            "location": "/data/install-gridftp-xrootd/", 
            "text": "Install Xrootd Grid FTP\n\n\nFor a full storage element, visit \nInstallBestmanXrootdSE\n. However, bigger sites (tier-2, etc) may require multiple GridFTP to be load balanced under a single BeStMan Gateway SRM interface. This page aims to explain how to install such a GridFTP server.\n\n\nThis package could also be used to install a stand-alone GridFTP server on top of XRootD servers, but most installations use this\n\n\nRequirements\n\n\nHost and OS\n\n\n\n\nOS is Red Hat Enterprise Linux 6, 7, and variants (see \ndetails...\n).\n\n\nEPEL\n repos enabled.\n\n\nA working XRootD Server. See \nInstallXrootd\n for details.\n\n\nRoot access\n\n\n\n\nUsers\n\n\nThis installation will create several users unless they are already created.\n\n\n\n\n\n\n\n\nUser\n\n\nComment\n\n\n\n\n\n\n\n\n\n\ndaemon\n\n\nUsed by globus-gridftp-server.\n\n\n\n\n\n\nxrootd\n\n\nUsed by the xrootd client to contact xrootd redirector.\n\n\n\n\n\n\n\n\nCertificates\n\n\n\n\n\n\n\n\nCertificate\n\n\nUser that owns certificate\n\n\nPath to certificate\n\n\n\n\n\n\n\n\n\n\nHost certificate\n\n\nroot\n\n\n/etc/grid-security/hostcert.pem\n, \n/etc/grid-security/hostkey.pem\n\n\n\n\n\n\n\n\nInstructions\n to request a service certificate. You will also need a copy of CA certificates (see below).\n\n\nNetworking\n\n\n\n\n\n\n\n\nService Name\n\n\nProtocol\n\n\nPort Number\n\n\nInbound\n\n\nOutbound\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nGRAM callback\n\n\ntcp\n\n\nGLOBUS_TCP_PORT_RANGE\n\n\nY\n\n\n\n\ncontiguous range of ports\n\n\n\n\n\n\nGRAM callback\n\n\ntcp\n\n\nGLOBUS_TCP_SOURCE_RANGE\n\n\n\n\nY\n\n\ncontiguous range of ports\n\n\n\n\n\n\nGridFTP\n\n\ntcp\n\n\n2811 and \nGLOBUS_TCP_SOURCE_RANGE\n\n\nY\n\n\n\n\ncontiguous range of ports\n\n\n\n\n\n\n\n\nEngineering Considerations\n\n\nThe GridFTP server provides high-performance, secure and reliable data transfer. This guide is primarily intended for installations that require one \nBeStMan\n endpoint but multiple GridFTP servers for scalability. Multiple GridFTP instances on different servers are recommended if:\n\n\n\n\nYou have a BeStMan-gateway/Xrootd SE (Storage Element) serving data to more than 250 cores for VOs that use storage heavily (e.g. CMS, ATLAS, CDF, and D0)\n\n\nYour storage will be managing more than 50 TB of disk space\n\n\nYou have a BeStMan-gateway/Xrootd SE with more than 1Gbps bandwidth: plan on at least one GridFTP server for each 4Gbps of available bandwidth (assuming you have 10Gbps interfaces on the server) if you want to maximize throughput.\n\n\n\n\nInstall Instructions\n\n\nNote that this package is primarily intended for GridFTP acting as an interface for XRootD server, usually part of a bigger storage element installation. If you have not installed an XRootD server yet, follow the instructions in \nInstallXrootd\n.\n\n\nCertificates\n\n\nGridFTP, which is a part of this meta-package, requires a certificate package to run. If you require a specific certificate package, follow the \nInstallCertAuth\n instructions to install it. If you do not install a grid certificate package first, the install procedure will install one for you as part of its dependencies. (usually osg-ca-certs).\n\n\nPackage installation instructions\n\n\nFirst, you will need to install the XRootD GridFTP meta-package.\n\n\nroot@host #\n yum install osg-gridftp-xrootd\n\n\n\n\n\nConfiguring GridFTP authentication support\n\n\nFor information on how to configure authentication for your GridFTP installation, please refer to the \nconfiguring authentication section of the GridFTP guide\n.\n\n\nConfiguring GridFTP XRootD support\n\n\n(Optional) Enabling GridFTP server for a BeStMan SE\n\n\nIf this installation is part of a greater SE deployment, you will probably want to add this server to your existing BeStMan installation.\n\n\nIn \n/etc/bestman2/conf/bestman2.rc\n, you will need to modify the \nsupportedProtocolList\n line, such as\n\n\nsupportedProtocolList\n=\ngsiftp\n://\ngridftp\n.\nserver\n.\ntld\n;\ngsiftp\n://\ngridftp2\n.\nserver\n.\ntld\n;\ngsiftp\n://\ngridftp3\n.\nserver\n.\ntld\n\n\n\n\n\n\nConfiguring xrootdfs\n\n\n(Optional) Configuring secured xrootdfs\n\n\nConfigure Xrootd Gratia Probes\n\n\nNote that you can also enable the GridFTP gratia probe. However, the XRootD probes are likely sufficient. More information on the GridFTP probe can be found \nhere\n.\n\n\nStarting Services\n\n\n\n\n\n\nfetch-crl\n\n\n\n\n\n\nGridFTP\n\n\n\n\n\n\nStarting GridFTP:\n\n\nroot@host #\n service globus-gridftp-server start\n\n\n\n\n\nTo start Gridftp automatically at boot time\n\n\nroot@host #\n chkconfig globus-gridftp-server on\n\n\n\n\n\n\n\nGratia transfer and storage probes\n\n\n\n\nStopping Services\n\n\n\n\nfetch-crl\n\n\n\n\n* (other grid service running on the machine may still use it)\n\n\n* (other grid service running on the machine may still use it)\n\n\n\n\nGridFTP\n\n\n\n\nStopping GridFTP:\n\n\nroot@host #\n service globus-gridftp-server stop\n\n\n\n\n\n\n\nGratia transfer and storage probes\n\n\n\n\nFile Locations\n\n\n\n\n\n\n\n\nService/Process\n\n\nConfiguration File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGridFTP\n\n\n/etc/sysconfig/globus-gridftp-server\n\n\nEnvironment variables for GridFTP and LCMAPS\n\n\n\n\n\n\n\n\n/usr/share/osg/sysconfig/globus-gridftp-server-plugin\n\n\nWhere environment variables for GridFTP plugin are included\n\n\n\n\n\n\nGratia Probe\n\n\n/etc/gratia/xrootd-storage/ProbeConfig\n\n\nGridFTP Xrootd Storage Probe configuration\n\n\n\n\n\n\n\n\n/etc/gratia/xrootd-transfer/ProbeConfig\n\n\nGridFTP Xrootd Transfer Probe configuration\n\n\n\n\n\n\n\n\n| Service/Process | Log File | Description |\n| GridFTP | /var/log/gridftp.log | GridFTP transfer log |\n| | /var/log/gridftp-auth.log | GridFTP authorization log |\n| Gratia probe | /var/logs/gratia | |\n\n\nHow to get Help?\n\n\nFor a full set of help options, see \nHelp Procedure\n.", 
            "title": "Install GridFtp XRootD"
        }, 
        {
            "location": "/data/install-gridftp-xrootd/#install-xrootd-grid-ftp", 
            "text": "For a full storage element, visit  InstallBestmanXrootdSE . However, bigger sites (tier-2, etc) may require multiple GridFTP to be load balanced under a single BeStMan Gateway SRM interface. This page aims to explain how to install such a GridFTP server.  This package could also be used to install a stand-alone GridFTP server on top of XRootD servers, but most installations use this", 
            "title": "Install Xrootd Grid FTP"
        }, 
        {
            "location": "/data/install-gridftp-xrootd/#requirements", 
            "text": "", 
            "title": "Requirements"
        }, 
        {
            "location": "/data/install-gridftp-xrootd/#host-and-os", 
            "text": "OS is Red Hat Enterprise Linux 6, 7, and variants (see  details... ).  EPEL  repos enabled.  A working XRootD Server. See  InstallXrootd  for details.  Root access", 
            "title": "Host and OS"
        }, 
        {
            "location": "/data/install-gridftp-xrootd/#users", 
            "text": "This installation will create several users unless they are already created.     User  Comment      daemon  Used by globus-gridftp-server.    xrootd  Used by the xrootd client to contact xrootd redirector.", 
            "title": "Users"
        }, 
        {
            "location": "/data/install-gridftp-xrootd/#certificates", 
            "text": "Certificate  User that owns certificate  Path to certificate      Host certificate  root  /etc/grid-security/hostcert.pem ,  /etc/grid-security/hostkey.pem     Instructions  to request a service certificate. You will also need a copy of CA certificates (see below).", 
            "title": "Certificates"
        }, 
        {
            "location": "/data/install-gridftp-xrootd/#networking", 
            "text": "Service Name  Protocol  Port Number  Inbound  Outbound  Comment      GRAM callback  tcp  GLOBUS_TCP_PORT_RANGE  Y   contiguous range of ports    GRAM callback  tcp  GLOBUS_TCP_SOURCE_RANGE   Y  contiguous range of ports    GridFTP  tcp  2811 and  GLOBUS_TCP_SOURCE_RANGE  Y   contiguous range of ports", 
            "title": "Networking"
        }, 
        {
            "location": "/data/install-gridftp-xrootd/#engineering-considerations", 
            "text": "The GridFTP server provides high-performance, secure and reliable data transfer. This guide is primarily intended for installations that require one  BeStMan  endpoint but multiple GridFTP servers for scalability. Multiple GridFTP instances on different servers are recommended if:   You have a BeStMan-gateway/Xrootd SE (Storage Element) serving data to more than 250 cores for VOs that use storage heavily (e.g. CMS, ATLAS, CDF, and D0)  Your storage will be managing more than 50 TB of disk space  You have a BeStMan-gateway/Xrootd SE with more than 1Gbps bandwidth: plan on at least one GridFTP server for each 4Gbps of available bandwidth (assuming you have 10Gbps interfaces on the server) if you want to maximize throughput.", 
            "title": "Engineering Considerations"
        }, 
        {
            "location": "/data/install-gridftp-xrootd/#install-instructions", 
            "text": "Note that this package is primarily intended for GridFTP acting as an interface for XRootD server, usually part of a bigger storage element installation. If you have not installed an XRootD server yet, follow the instructions in  InstallXrootd .", 
            "title": "Install Instructions"
        }, 
        {
            "location": "/data/install-gridftp-xrootd/#certificates_1", 
            "text": "GridFTP, which is a part of this meta-package, requires a certificate package to run. If you require a specific certificate package, follow the  InstallCertAuth  instructions to install it. If you do not install a grid certificate package first, the install procedure will install one for you as part of its dependencies. (usually osg-ca-certs).", 
            "title": "Certificates"
        }, 
        {
            "location": "/data/install-gridftp-xrootd/#package-installation-instructions", 
            "text": "First, you will need to install the XRootD GridFTP meta-package.  root@host #  yum install osg-gridftp-xrootd", 
            "title": "Package installation instructions"
        }, 
        {
            "location": "/data/install-gridftp-xrootd/#configuring-gridftp-authentication-support", 
            "text": "For information on how to configure authentication for your GridFTP installation, please refer to the  configuring authentication section of the GridFTP guide .", 
            "title": "Configuring GridFTP authentication support"
        }, 
        {
            "location": "/data/install-gridftp-xrootd/#configuring-gridftp-xrootd-support", 
            "text": "", 
            "title": "Configuring GridFTP XRootD support"
        }, 
        {
            "location": "/data/install-gridftp-xrootd/#optional-enabling-gridftp-server-for-a-bestman-se", 
            "text": "If this installation is part of a greater SE deployment, you will probably want to add this server to your existing BeStMan installation.  In  /etc/bestman2/conf/bestman2.rc , you will need to modify the  supportedProtocolList  line, such as  supportedProtocolList = gsiftp :// gridftp . server . tld ; gsiftp :// gridftp2 . server . tld ; gsiftp :// gridftp3 . server . tld", 
            "title": "(Optional) Enabling GridFTP server for a BeStMan SE"
        }, 
        {
            "location": "/data/install-gridftp-xrootd/#configuring-xrootdfs", 
            "text": "", 
            "title": "Configuring xrootdfs"
        }, 
        {
            "location": "/data/install-gridftp-xrootd/#optional-configuring-secured-xrootdfs", 
            "text": "", 
            "title": "(Optional) Configuring secured xrootdfs"
        }, 
        {
            "location": "/data/install-gridftp-xrootd/#configure-xrootd-gratia-probes", 
            "text": "Note that you can also enable the GridFTP gratia probe. However, the XRootD probes are likely sufficient. More information on the GridFTP probe can be found  here .", 
            "title": "Configure Xrootd Gratia Probes"
        }, 
        {
            "location": "/data/install-gridftp-xrootd/#starting-services", 
            "text": "fetch-crl    GridFTP    Starting GridFTP:  root@host #  service globus-gridftp-server start  To start Gridftp automatically at boot time  root@host #  chkconfig globus-gridftp-server on   Gratia transfer and storage probes", 
            "title": "Starting Services"
        }, 
        {
            "location": "/data/install-gridftp-xrootd/#stopping-services", 
            "text": "fetch-crl   * (other grid service running on the machine may still use it)  * (other grid service running on the machine may still use it)   GridFTP   Stopping GridFTP:  root@host #  service globus-gridftp-server stop   Gratia transfer and storage probes", 
            "title": "Stopping Services"
        }, 
        {
            "location": "/data/install-gridftp-xrootd/#file-locations", 
            "text": "Service/Process  Configuration File  Description      GridFTP  /etc/sysconfig/globus-gridftp-server  Environment variables for GridFTP and LCMAPS     /usr/share/osg/sysconfig/globus-gridftp-server-plugin  Where environment variables for GridFTP plugin are included    Gratia Probe  /etc/gratia/xrootd-storage/ProbeConfig  GridFTP Xrootd Storage Probe configuration     /etc/gratia/xrootd-transfer/ProbeConfig  GridFTP Xrootd Transfer Probe configuration     | Service/Process | Log File | Description |\n| GridFTP | /var/log/gridftp.log | GridFTP transfer log |\n| | /var/log/gridftp-auth.log | GridFTP authorization log |\n| Gratia probe | /var/logs/gratia | |", 
            "title": "File Locations"
        }, 
        {
            "location": "/data/install-gridftp-xrootd/#how-to-get-help", 
            "text": "For a full set of help options, see  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/data/hadoop-overview/", 
            "text": "Hadoop Overview\n\n\nHadoop Introduction\n\n\nHadoop is a data processing framework. It is an open-source Apache Foundation project, and the main contributor is Yahoo! The framework has two main parts - job scheduling and a distributed file system, the Hadoop Distributed File System (HDFS). We currently utilize HDFS as a general-purpose file system. For this document, we'll use the words \"Hadoop\" and \"HDFS\" interchangeably, but it's nice to know the distinction.\n\n\nWe recommend starting with \nHDFS architecture document\n.\n\n\nPlease do read through this. We will assume you have read this, or at least the important architectural portions. The file system is block-oriented; each file is broken up into 64 MB or 128 MB chunks (user configurable). These chunks are stored on data nodes and served up from there; the central namenode keeps track of the block locations, the namespace information, and block placement policies. HDFS provides POSIX-like semantics; it provides fully random-access reads and non-random-access writes. Currently, fsync and appends (after the file has been initially closed) are experimental and not available to OSG-based installs.\n\n\nHadoop SE Components\n\n\nWe broadly break down the server components of the Hadoop SE into three categories: HDFS core, Grid extensions, and HDFS auxiliary. The components in each of these categories are outlined below:\n\n\n\n\nHDFS Core:\n\n\nNamenode: The core metadata server of Hadoop. This is the most critical piece of the system, and there can only be one of these. This stores both the file system image and the file system journal. The namenode keeps all of the filesystem layout information (files, blocks, directories, permissions, etc) and the block locations. The filesystem layout is persisted on disk and the block locations are kept solely in memory. When a client opens a file, the namenode tells the client the locations of all the blocks in the file; the client then no longer needs to communicate with the namenode for data transfer.\n\n\nDatanode: This node stores copies of the blocks in HDFS. They communicate with the namenode to perform \"housekeeping\" such as creating new replicas, transferring blocks between datanodes, and deleting excess blocks. They also communicate with the clients to transfer data. To reach the best scalability, there should be as many datanodes as possible.\n\n\n\n\n\n\nGrid extensions\n\n\nBeStMan SRM: A generic SRM server that can be run on top of any POSIX-like filesystem. This is run in \"gateway\" mode, which limits the amount of the SRM protocol implemented. To date, this has been sufficient to LHC VOs.\n\n\nGlobus GridFTP: The standard GridFTP from Globus. We use a plug-in module (using the Globus Direct Storage Interface) that allows the GridFTP process to use the HDFS C-bindings directly.\n\n\nGratia probe: Gratia is an accounting system that records batch system and transfer records to a database. The records are collected by a client program called a \"probe\" which runs on the GridFTP server. It parses the GridFTP server's log files and creates transfer records.\n\n\nXrootd server plugin: Xrootd is an extremely flexible and powerful data server popular in the high energy physics community. There exists a HDFS plugin for Xrootd; integrating with Xrootd provides a means to export HDFS securely outside the local cluster, as another Xrootd plugin provides GSI-based authentication and authorization.\n\n\n\n\n\n\nHDFS auxiliary:\n\n\n\"Secondary Namenode\": Perhaps more aptly called a \"checkpoint server\". This server downloads the file system image and journal from the namenode, merges the two together, and uploads the new file system image up to the namenode. This is done on a different server in order to reduce the memory footprint of the namenode.\n\n\nHadoop Balancer: This is a script (unlike the others, which are daemons) that runs on the namenode. It requests transfers of random blocks between the datanodes. This works until all datanodes have approximately the same percentage of free space. Well-balanced datanodes are necessary for having a healthy cluster.\n\n\n\n\n\n\n\n\nIn addition to the server components, there are two client components:\n\n\n\n\nFUSE: This allows HDFS to be mounted as a filesystem on the worker nodes. FUSE is a Linux kernel module that allows kernel I/O calls to be translated into a call to a userspace program. In this case, a program called fuse_dfs translates the POSIX calls into HDFS C-binding calls.\n\n\nHadoop Command Line Client: This command line client exposes a lot of the Unix-like calls without mounting FUSE, plus access to the non-POSIX calls (such as setting quotas and file replication levels). For example, \"hadoop fs -ls /\" is equivalent to \"ls /mnt/hadoop\" if /mnt/hadoop is the mount point of HDFS.\n\n\n\n\n\n\n\n\nNamenode: We recommend at least 8GB of RAM (minimum is 2GB RAM), preferably 16GB or more. A rough rule of thumb is 1GB per 100TB of raw disk space; the actual requirements is around 1GB per million objects (files, directories, and blocks). The CPU requirements are any modern multi-core server CPU. Typically, the namenode will only use 2-5% of your CPU.\n\n\nAs this is a single point of failure, the \nmost important\n requirement is reliable hardware rather than high performance hardware. We suggest a node with redundant power supplies and at least 2 hard drives.\n\n\n\n\n\n\nSecondary namenode: This node needs the same amount of RAM as the namenode for merging namespaces. It does not need to be high performance or high reliability.\n\n\nDatanode: Each datanode should plan to dedicate 200-500MB of RAM to HDFS. A general rule of thumb is to dedicate 1 CPU to HDFS per 5TB of disk capacity under heavily load; clusters with moderate load (i.e., mostly sequential workflows) will need less. At idle, HDFS will consume almost no CPU.\n\n\n\n\nMinimal Installation (0-50TB, WAN transfers up to 1Gbps)\n\n\nThe minimal installation would involve 5 nodes:\n\n\n1 hadoop-name: The namenode for the Hadoop system. Must be on private NAT. 1 hadoop-name2: This will run the HDFS secondary namenode. Must also be on private NAT. 1 hadoop-data1, hadoop-data2: Two HDFS datanodes. They will hold data for the system, so they should have sizable hard drives. These must be on the private NAT. As the Hadoop installation grows to many terabytes, this will be the only class of nodes one adds. 1 hadoop-grid: Runs the BeStMan SRM and Globus GridFTP server. Must have a public interface and a private interface.\n\n\nIf desired, hadoop-name and hadoop-name2 may be virtualized. Prior to installation, DNS / host name resolution \nmust\n work. That is, you should be able to resolve all the hadoop servers either through DNS or /etc/hosts. Because of the grid software, hadoop-grid \nmust\n have reverse DNS working.\n\n\nMedium Installation (50-150TB, WAN transfers up to 2 Gbps)\n\n\nFor a medium install, make the following changes over the minimal install: 1 Run BeStMan on a separate machine, hadoop-srm. This host may be virtualized. 1 Run multiple GridFTP servers (2 should be fine); if possible, use 10 Gbps cards for these hosts. 1 Add many more HDFS datanodes. This usually means starting to add 1 or 2 TB hard drives to some worker nodes.\n\n\nLarge Installation (\n150TB, WAN transfers over 2 Gbps)\n\n\nFor a large installation, make the following changes: 1 Run more GridFTP servers; plan for 800 Mbps per host with 1 Gbps card in order to have excess capacity. 1 Purchase machines suitable for HDFS datanodes. It is possible to get a 2U box with 8-12 hard drives; for many projects, this will provide a more suitable CPU to disk ratio than the 1U boxes with 2 hard drives.\n\n\nHadoop Security\n\n\nHDFS has unix-like user/group authorization, but no strict authentication. HDFS should only be exposed to a secure internal network which only non-malicious users are able to access. For users with access to the local cluster, it is not difficult at all to bypass authentication.\n\n\nThe default ports are listed here\n.\n\n\nThere are some ways to improve security of your cluster:\n\n\n\n\nKeep the namenode behind a firewall. One possibility is to run hadoop entirely on the private subnet of a cluster.\n\n\nUse firewalls to protect the HDFS ports (default for the datanode is 50010 and 50075; for the namenode, 50070 and 9000).\n\n\nFor clusters utilizing FUSE, one can block outgoing connections to the HDFS ports except for user root. This means that only root-owned processes (such as FUSE-DFS) will be able to access Hadoop.\n\n\nThis is sufficient for grid environments, but does not protect one in the case where the attacker has physical access to the network switch.\n\n\n\n\n\n\nThere exists another option, currently untested. It is possible to limit all HDFS socket connections to SSL-based sockets. Using this to only allow known hosts to connect to HDFS and only allowing FUSE-DFS to connect on those known hosts, one might be able to satisfy even fairly stringent security folks (but not paranoid ones).\n\n\n\n\nThere are three options to export your data outside your cluster:\n\n\n\n\nGlobus GridFTP / SRM. This is covered in these web pages.\n\n\nXrootd. Documentation is nascent.\n\n\nApache HTTP (authenticated via HTTPS). This is in use at Caltech, but there isn't any documentation available.", 
            "title": "Hadoop Overview"
        }, 
        {
            "location": "/data/hadoop-overview/#hadoop-overview", 
            "text": "", 
            "title": "Hadoop Overview"
        }, 
        {
            "location": "/data/hadoop-overview/#hadoop-introduction", 
            "text": "Hadoop is a data processing framework. It is an open-source Apache Foundation project, and the main contributor is Yahoo! The framework has two main parts - job scheduling and a distributed file system, the Hadoop Distributed File System (HDFS). We currently utilize HDFS as a general-purpose file system. For this document, we'll use the words \"Hadoop\" and \"HDFS\" interchangeably, but it's nice to know the distinction.  We recommend starting with  HDFS architecture document .  Please do read through this. We will assume you have read this, or at least the important architectural portions. The file system is block-oriented; each file is broken up into 64 MB or 128 MB chunks (user configurable). These chunks are stored on data nodes and served up from there; the central namenode keeps track of the block locations, the namespace information, and block placement policies. HDFS provides POSIX-like semantics; it provides fully random-access reads and non-random-access writes. Currently, fsync and appends (after the file has been initially closed) are experimental and not available to OSG-based installs.", 
            "title": "Hadoop Introduction"
        }, 
        {
            "location": "/data/hadoop-overview/#hadoop-se-components", 
            "text": "We broadly break down the server components of the Hadoop SE into three categories: HDFS core, Grid extensions, and HDFS auxiliary. The components in each of these categories are outlined below:   HDFS Core:  Namenode: The core metadata server of Hadoop. This is the most critical piece of the system, and there can only be one of these. This stores both the file system image and the file system journal. The namenode keeps all of the filesystem layout information (files, blocks, directories, permissions, etc) and the block locations. The filesystem layout is persisted on disk and the block locations are kept solely in memory. When a client opens a file, the namenode tells the client the locations of all the blocks in the file; the client then no longer needs to communicate with the namenode for data transfer.  Datanode: This node stores copies of the blocks in HDFS. They communicate with the namenode to perform \"housekeeping\" such as creating new replicas, transferring blocks between datanodes, and deleting excess blocks. They also communicate with the clients to transfer data. To reach the best scalability, there should be as many datanodes as possible.    Grid extensions  BeStMan SRM: A generic SRM server that can be run on top of any POSIX-like filesystem. This is run in \"gateway\" mode, which limits the amount of the SRM protocol implemented. To date, this has been sufficient to LHC VOs.  Globus GridFTP: The standard GridFTP from Globus. We use a plug-in module (using the Globus Direct Storage Interface) that allows the GridFTP process to use the HDFS C-bindings directly.  Gratia probe: Gratia is an accounting system that records batch system and transfer records to a database. The records are collected by a client program called a \"probe\" which runs on the GridFTP server. It parses the GridFTP server's log files and creates transfer records.  Xrootd server plugin: Xrootd is an extremely flexible and powerful data server popular in the high energy physics community. There exists a HDFS plugin for Xrootd; integrating with Xrootd provides a means to export HDFS securely outside the local cluster, as another Xrootd plugin provides GSI-based authentication and authorization.    HDFS auxiliary:  \"Secondary Namenode\": Perhaps more aptly called a \"checkpoint server\". This server downloads the file system image and journal from the namenode, merges the two together, and uploads the new file system image up to the namenode. This is done on a different server in order to reduce the memory footprint of the namenode.  Hadoop Balancer: This is a script (unlike the others, which are daemons) that runs on the namenode. It requests transfers of random blocks between the datanodes. This works until all datanodes have approximately the same percentage of free space. Well-balanced datanodes are necessary for having a healthy cluster.     In addition to the server components, there are two client components:   FUSE: This allows HDFS to be mounted as a filesystem on the worker nodes. FUSE is a Linux kernel module that allows kernel I/O calls to be translated into a call to a userspace program. In this case, a program called fuse_dfs translates the POSIX calls into HDFS C-binding calls.  Hadoop Command Line Client: This command line client exposes a lot of the Unix-like calls without mounting FUSE, plus access to the non-POSIX calls (such as setting quotas and file replication levels). For example, \"hadoop fs -ls /\" is equivalent to \"ls /mnt/hadoop\" if /mnt/hadoop is the mount point of HDFS.     Namenode: We recommend at least 8GB of RAM (minimum is 2GB RAM), preferably 16GB or more. A rough rule of thumb is 1GB per 100TB of raw disk space; the actual requirements is around 1GB per million objects (files, directories, and blocks). The CPU requirements are any modern multi-core server CPU. Typically, the namenode will only use 2-5% of your CPU.  As this is a single point of failure, the  most important  requirement is reliable hardware rather than high performance hardware. We suggest a node with redundant power supplies and at least 2 hard drives.    Secondary namenode: This node needs the same amount of RAM as the namenode for merging namespaces. It does not need to be high performance or high reliability.  Datanode: Each datanode should plan to dedicate 200-500MB of RAM to HDFS. A general rule of thumb is to dedicate 1 CPU to HDFS per 5TB of disk capacity under heavily load; clusters with moderate load (i.e., mostly sequential workflows) will need less. At idle, HDFS will consume almost no CPU.", 
            "title": "Hadoop SE Components"
        }, 
        {
            "location": "/data/hadoop-overview/#minimal-installation-0-50tb-wan-transfers-up-to-1gbps", 
            "text": "The minimal installation would involve 5 nodes:  1 hadoop-name: The namenode for the Hadoop system. Must be on private NAT. 1 hadoop-name2: This will run the HDFS secondary namenode. Must also be on private NAT. 1 hadoop-data1, hadoop-data2: Two HDFS datanodes. They will hold data for the system, so they should have sizable hard drives. These must be on the private NAT. As the Hadoop installation grows to many terabytes, this will be the only class of nodes one adds. 1 hadoop-grid: Runs the BeStMan SRM and Globus GridFTP server. Must have a public interface and a private interface.  If desired, hadoop-name and hadoop-name2 may be virtualized. Prior to installation, DNS / host name resolution  must  work. That is, you should be able to resolve all the hadoop servers either through DNS or /etc/hosts. Because of the grid software, hadoop-grid  must  have reverse DNS working.", 
            "title": "Minimal Installation (0-50TB, WAN transfers up to 1Gbps)"
        }, 
        {
            "location": "/data/hadoop-overview/#medium-installation-50-150tb-wan-transfers-up-to-2-gbps", 
            "text": "For a medium install, make the following changes over the minimal install: 1 Run BeStMan on a separate machine, hadoop-srm. This host may be virtualized. 1 Run multiple GridFTP servers (2 should be fine); if possible, use 10 Gbps cards for these hosts. 1 Add many more HDFS datanodes. This usually means starting to add 1 or 2 TB hard drives to some worker nodes.", 
            "title": "Medium Installation (50-150TB, WAN transfers up to 2 Gbps)"
        }, 
        {
            "location": "/data/hadoop-overview/#large-installation-150tb-wan-transfers-over-2-gbps", 
            "text": "For a large installation, make the following changes: 1 Run more GridFTP servers; plan for 800 Mbps per host with 1 Gbps card in order to have excess capacity. 1 Purchase machines suitable for HDFS datanodes. It is possible to get a 2U box with 8-12 hard drives; for many projects, this will provide a more suitable CPU to disk ratio than the 1U boxes with 2 hard drives.", 
            "title": "Large Installation (&gt;150TB, WAN transfers over 2 Gbps)"
        }, 
        {
            "location": "/data/hadoop-overview/#hadoop-security", 
            "text": "HDFS has unix-like user/group authorization, but no strict authentication. HDFS should only be exposed to a secure internal network which only non-malicious users are able to access. For users with access to the local cluster, it is not difficult at all to bypass authentication.  The default ports are listed here .  There are some ways to improve security of your cluster:   Keep the namenode behind a firewall. One possibility is to run hadoop entirely on the private subnet of a cluster.  Use firewalls to protect the HDFS ports (default for the datanode is 50010 and 50075; for the namenode, 50070 and 9000).  For clusters utilizing FUSE, one can block outgoing connections to the HDFS ports except for user root. This means that only root-owned processes (such as FUSE-DFS) will be able to access Hadoop.  This is sufficient for grid environments, but does not protect one in the case where the attacker has physical access to the network switch.    There exists another option, currently untested. It is possible to limit all HDFS socket connections to SSL-based sockets. Using this to only allow known hosts to connect to HDFS and only allowing FUSE-DFS to connect on those known hosts, one might be able to satisfy even fairly stringent security folks (but not paranoid ones).   There are three options to export your data outside your cluster:   Globus GridFTP / SRM. This is covered in these web pages.  Xrootd. Documentation is nascent.  Apache HTTP (authenticated via HTTPS). This is in use at Caltech, but there isn't any documentation available.", 
            "title": "Hadoop Security"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/", 
            "text": "Hadoop 2.0.0 (CDH4)\n\n\nThe purpose of this document is to provide Hadoop based SE administrators the information on how to prepare, install and validate the SE.\n\n\nIntroduction\n\n\nHadoop Distributed File System\n (HDFS) is a scalable reliable distributed file system developed in the Apache project. It is based on map-reduce framework and design of the Google file system. The VDT distribution of Hadoop includes all components needed to operate a multi-terabyte storage site. Included are:\n\n\n\n\nAn \nSRM interface\n for grid access;\n\n\nGridFTP-HDFS as transport layer; and\n\n\nA \nFUSE interface\n for localized POSIX access.\n\n\nApache Hadoop\n\n\n\n\nThe OSG packaging and distribution of Hadoop is based on YUM. All components are packaged as RPMs and are available from the OSG repositories. It is also recommended that you enable \nEPEL\n repos.\n\n\nRequirements\n\n\nArchitecture\n\n\n\n\nNote\n\n\nThere are several important components to a storage element installation. Throughout this document, it will be stated which node the relevant installation instructions apply to. It can apply to one of the following:\n\n\n\n\n\n\nNamenode\n: You will have at least one namenode. The name node functions as the directory server and coordinator of the hadoop cluster. It houses all the meta-data for the hadoop cluster. \nThe namenode and secondary namenode need to have a directory that they can both access on a shared filesystem so that they can exchange filesystem checkpoints.\n\n\nSecondary Namenode\n: This is a secondary machine that periodically merges updates to the HDFS file system back into the fsimage. This dramatically improves startup and restart times.\n\n\nDatanode\n: You will have many datanodes. Each data node stores large blocks of files to be stored on the hadoop cluster.\n\n\nClient\n: This is a documentation shorthand that refers to any machine with the hadoop client commands and \nFUSE\n mount. Any machine that needs a FUSE mount to access data in a POSIX-like fashion will need this.\n\n\nGridFTP node\n: This is a node with \nGlobus GridFTP\n. The GridFTP server for Hadoop can be very memory-hungry, up to 500MB/transfer in the default configuration. You should plan accordingly to provision enough GridFTP servers to handle the bandwidth that your site can support.\n\n\nSRM node\n: This node will contain the BeStMan SRM frontend for accessing the Hadoop cluster via the SRM protocol. \nBeStMan2 SRM\n\n\n\n\nNote that these components are not necessarily mutually exclusive. For instance, you may consider having your GridFTP server co-located on the SRM node. Alternatively, you can locate a client (or even a GridFTP node) co-located on each data node. That way, each data node also acts as an access point to the hadoop cluster.\n\n\n\n\nNote\n\n\nTotal installation time, on an average, should not exceed 8 to 24 man-hours. If your site needs further assistance to help expedite, please email \n.\n\n\n\n\nHost and OS\n\n\nHadoop will run anywhere that Java is supported (including Solaris). However, these instructions are for RedHat derivants (including Scientific Linux) because of the RPM based installation. The current supported Operating Systems supported by the OSG are Red Hat Enterprise Linux 6, 7, and variants (see \ndetails...\n).\n\n\nThe HDFS prerequisites are:\n\n\n\n\nMinimum of 1 headnode (the namenode)\n\n\nAt least one node which will hold data, preferably at least 2. Most sites will have 20 to 200 datanodes.\n\n\nWorking Yum and RPM installation on every system.\n\n\nfuse\n kernel module and \nfuse-libs\n.\n\n\nJava RPM. If java isn't already installed we supply the Oracle jdk 1.6.0 rpm and it will come in as a dependency. Oracle jdk is currently the only jdk supported by OSG so we highly recommend you use the version supplied.\n\n\n\n\n\n\nNote\n\n\nVersions of OpenAFS less than 1.4.7 and greater than 1.4.1 create nameless groups on Linux; these groups confuse Hadoop and prevent its components from starting up successfully. If you plan to install Hadoop on a Linux OpenAFS client, make sure you're running at least OpenAFS 1.4.7.\n\n\n\n\nUsers\n\n\nThis installation will create following users unless they are already created.\n\n\n\n\n\n\n\n\nUser\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nbestman\n\n\nUsed by Bestman SRM server (needs sudo access).\n\n\n\n\n\n\nhdfs\n\n\nUsed by Hadoop to store data blocks and meta-data\n\n\n\n\n\n\n\n\nFor this package to function correctly, you will have to create the users needed for grid operation. Any user that can be authenticated should be created.\n\n\nFor grid-mapfile users, each line of the grid-mapfile is a certificate/user pair. Each user in this file should be created on the server.\n\n\nFor gums users, this means that each user that can be authenticated by gums should be created on the server.\n\n\nNote that these users must be kept in sync with the authentication method. For instance, if new users or rules are added in gums, then new users should also be added here.\n\n\nCertificates\n\n\n\n\n\n\n\n\nCertificate\n\n\nUser that owns certificate\n\n\nPath to certificate\n\n\n\n\n\n\n\n\n\n\nHost certificate\n\n\nroot\n\n\n/etc/grid-security/hostcert.pem\n \n \n/etc/grid-security/hostkey.pem\n\n\n\n\n\n\nBestman service certificate\n\n\nbestman\n\n\n/etc/grid-security/bestman/bestmancert.pem\n \n \n/etc/grid-security/bestman/bestmankey.pem\n\n\n\n\n\n\n\n\nInstructions\n to request a service certificate.\n\n\nYou will also need a copy of CA certificates (see below). Note that the \nosg-se-hadoop-srm\n and \nosg-se-hadoop-gridftp\n package will automatically install a certificate package but will not necessarily pick the cert package you expect. For instance, certain installs will prefer the \nosg-ca-scripts\n package to fulfill this requirement, which installs a set of scripts to automatically update the certificates, but does not initialize the CA certs by default (you have to run it first). For this reason, you may want to specifically install the cert package of your choice first, before installing Hadoop.\n\n\nNetworking\n\n\nInitializing Certificate Authority\n\n\nThis is needed by GridFTP and SRM nodes, but it is recommended for all nodes in the cluster. Enable \nfetch-crl\n\n\nInstallation\n\n\nInstallation depends on the node you are installing:\n\n\nNamenode Installation\n\n\nroot@host #\n yum install osg-se-hadoop-namenode\n\n\n\n\n\nSecondary Namenode Installation\n\n\nroot@host #\n yum install osg-se-hadoop-secondarynamenode\n\n\n\n\n\nDatanode Installation\n\n\nroot@host #\n yum install osg-se-hadoop-datanode\n\n\n\n\n\nClient/FUSE Installation\n\n\nroot@host #\n yum install osg-se-hadoop-client\n\n\n\n\n\nStandalone Gridftp Node Installation\n\n\nroot@host #\n yum install osg-se-hadoop-gridftp\n\n\n\n\n\nIf you are using GUMS authorization, the follow rpms need to be installed as well:\n\n\nroot@host #\n yum install lcmaps-plugins-gums-client\n\nroot@host #\n yum install lcmaps-plugins-basic\n\n\n\n\n\nSRM Node Installation\n\n\nroot@host #\n yum install osg-se-hadoop-srm\n\n\n\n\n\n\n\nNote\n\n\nIf you are using a single system to host the SRM software and the gridftp node, you'll also need to install the \nosg-se-hadoop-gridftp\n rpm as well.\n\n\n\n\nConfiguration\n\n\nHadoop Configuration\n\n\n\n\nNote\n\n\nNeeded by: Hadoop namenode, Hadoop datanodes, Hadoop client, GridFTP, SRM\n\n\n\n\nHadoop configuration is needed by every node in the hadoop cluster. However, in most cases, you can do the configuration once and copy it to all nodes in the cluster (possibly using your favorite configuration management tool). Special configuration for various special components is given in the below sections.\n\n\nHadoop configuration is stored in \n/etc/hadoop/conf\n. However, by default, these files are mostly blank. OSG provides a sample configuration in \n/etc/hadoop/conf.osg\n with most common values filled in. You will need to copy these into \n/etc/hadoop/conf\n before they become active. Please let us know if there are any common values that should be added/changed across the whole grid. You will likely need to modify \nhdfs-site.xml\n and \ncore-site.xml\n. Review all the settings in these files, but listed below are common settings to modify:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFile\n\n\nSetting\n\n\nExample\n\n\nComments\n\n\n\n\n\n\ncore-site.xml\n\n\nfs.default.name\n\n\nhdfs://namenode.domain.tld.:9000\n\n\nThis is the address of the namenode\n\n\n\n\n\n\ncore-site.xml\n\n\nhadoop.tmp.dir\n\n\n/data/scratch\n\n\nScratch temp directory used by Hadoop\n\n\n\n\n\n\ncore-site.xml\n\n\nhadoop.log.dir\n\n\n/var/log/hadoop-hdfs\n\n\nLog directory used by Hadoop\n\n\n\n\n\n\ncore-site.xml\n\n\ndfs.umaskmode\n\n\n002\n\n\numask for permissions used by default\n\n\n\n\n\n\nhdfs-site.xml\n\n\ndfs.block.size\n\n\n134217728\n\n\nBlock size: 128MB by default\n\n\n\n\n\n\nhdfs-site.xml\n\n\ndfs.replication\n\n\n2\n\n\nDefault replication factor. Generally the same as dfs.replication.min/max\n\n\n\n\n\n\nhdfs-site.xml\n\n\ndfs.datanode.du.reserved\n\n\n100000000\n\n\nHow much free space hadoop will reserve for non-Hadoop usage\n\n\n\n\n\n\nhdfs-site.xml\n\n\ndfs.datanode.handler.count\n\n\n20\n\n\nNumber of server threads for datanodes. Increase if you have many more client connections\n\n\n\n\n\n\nhdfs-site.xml\n\n\ndfs.namenode.handler.count\n\n\n40\n\n\nNumber of server threads for namenodes. Increase if you need more connections\n\n\n\n\n\n\nhdfs-site.xml\n\n\ndfs.http.address\n\n\nnamenode.domain.tld.:50070\n\n\nWeb address for dfs health monitoring page\n\n\n\n\n\n\n\n\nSee \nhttp://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml\n for more parameters to configure.\n\n\n\n\nNote\n\n\nNamenodes must have a \n/etc/hosts_exclude\n present\n\n\n\n\nSpecial namenode instructions for brand new installs\n\n\nIf this is a new installation (\nand only if this is a brand new installation\n), you should run the following command as the \nhdfs\n user. (Otherwise, be sure to \nchown\n your storage directory to hdfs after running):\n\n\nhadoop namenode -format\n\n\n\n\n\n\nThis will initialize the storage directory on your namenode\n\n\nFUSE Client Configuration\n\n\n\n\nNote\n\n\nNeeded by: Hadoop client and SRM node. Recommended but not neccessary for GridFTP nodes.\n\n\n\n\nA FUSE mount is required on any node that you would like to use standard POSIX-like commands on the Hadoop filesystem. FUSE (or \"file system in user space\") is a way to access Hadoop using typical UNIX directory commands (ie POSIX-like access). Note that not all advanced functions of a full POSIX-compliant file system are necessarily available.\n\n\nFUSE is typically installed as part of this installation, but, if you are running a customized or non-standard system, make sure that the fuse kernel module is installed and loaded with \nmodprobe fuse\n.\n\n\nYou can add the FUSE to be mounted at boot time by adding the following line to \n/etc/fstab\n:\n\n\nhadoop-fuse-dfs# \n/mnt/hadoop\n fuse server=\nnamenode.host\n,port=9000,rdbuffer=131072,allow_other 0 0\n\n\n\n\n\nBe sure to change the \n/mnt/hadoop\n mount point and \nnamenode.host\n to match your local configuration. To match the help documents, we recommend using \n/mnt/hadoop\n as your mountpoint.\n\n\nOnce your \n/etc/fstab\n is updated, to mount FUSE run:\n\n\nroot@host #\n mkdir /mnt/hadoop\n\nroot@host #\n mount /mnt/hadoop\n\n\n\n\n\nWhen mounting the HDFS FUSE mount, you will see the following harmless warnings printed to the screen:\n\n\n#\n mount /mnt/hadoop\n\nINFO fuse_options.c:162 Adding FUSE arg /mnt/hadoop\n\n\nINFO fuse_options.c:110 Ignoring option allow_other\n\n\n\n\n\n\nIf you have troubles mounting FUSE refer to \nRunning FUSE in Debug Mode\n in the Troubleshooting section.\n\n\nCreating VO and User Areas\n\n\n\n\nNote\n\n\nGrid Users are needed by GridFTP and SRM nodes. VO areas are common to all nodes.\n\n\n\n\nFor this package to function correctly, you will have to create the users needed for grid operation. Any user that can be authenticated should be created.\n\n\nFor grid-mapfile users, each line of the grid-mapfile is a certificate/user pair. Each user in this file should be created on the server.\n\n\nFor gums users, this means that each user that can be authenticated by gums should be created on the server.\n\n\nNote that these users must be kept in sync with the authentication method. For instance, if new users or rules are added in gums, then new users should also be added here.\n\n\nPrior to starting basic day-to-day operations, it is important to create dedicated areas for each VO and/or user. This is similar to user management in simple UNIX filesystems. Create (and maintain) usernames and groups with UIDs and GIDs on \nall nodes\n. These are maintained in basic system files such as \n/etc/passwd\n and \n/etc/group\n.\n\n\n\n\nNote\n\n\nIn the examples below It is assumed a FUSE mount is set to \n/mnt/hadoop\n. As an alternative \nhadoop fs\n commands could have been used.\n\n\n\n\nFor clean HDFS operations and filesystem management:\n\n\n(a) Create top-level VO subdirectories under \n/mnt/hadoop\n.\n\n\nExample:\n\n\nroot@host #\n mkdir /mnt/hadoop/cms\n\nroot@host #\n mkdir /mnt/hadoop/dzero\n\nroot@host #\n mkdir /mnt/hadoop/sbgrid\n\nroot@host #\n mkdir /mnt/hadoop/fermigrid\n\nroot@host #\n mkdir /mnt/hadoop/cmstest\n\nroot@host #\n mkdir /mnt/hadoop/osg\n\n\n\n\n\n(b) Create individual top-level user areas, under each VO area, as needed.\n\n\nroot@host #\n mkdir -p /mnt/hadoop/cms/store/user/tanyalevshina\n\nroot@host #\n mkdir -p /mnt/hadoop/cms/store/user/michaelthomas\n\nroot@host #\n mkdir -p /mnt/hadoop/cms/store/user/brianbockelman\n\nroot@host #\n mkdir -p /mnt/hadoop/cms/store/user/douglasstrain\n\nroot@host #\n mkdir -p /mnt/hadoop/cms/store/user/abhisheksinghrana\n\n\n\n\n\n(c) Adjust username:group ownership of each area.\n\n\nroot@host #\n chown -R cms:cms /mnt/hadoop/cms\n\nroot@host #\n chown -R sam:sam /mnt/hadoop/dzero\n\n\nroot@host #\n chown -R michaelthomas:cms /mnt/hadoop/cms/store/user/michaelthomas\n\n\n\n\n\nGridFTP Configuration\n\n\ngridftp-hdfs reads the Hadoop configuration file to learn how to talk to Hadoop. By now, you should have followed the instruction for installing hadoop as detailed in the previous section as well as created the proper users/directories.\n\n\nThe default settings in \n/etc/gridftp.conf\n along with \n/etc/gridftp.d/gridftp-hdfs.conf\n are used by the init.d script and should be ok for most installations. The file \n/etc/gridftp-hdfs/gridftp-debug.conf\n is used by \n/usr/bin/gridftp-hdfs-standalone\n for starting up the GridFTP server in a testing mode. Any additional config files under \n/etc/gridftp.d\n will be used for both the init.d and standalone GridFTP server. \n/etc/sysconfig/gridftp-hdfs\n contains additional site-specific environment variables that are used by the gridftp-hdfs dsi module in both the init.d and standalone GridFTP server. Some of the environment variables that can be used in \n/etc/sysconfig/gridftp-hdfs\n include:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOption Name\n\n\nNeeds Editing?\n\n\nSuggested value\n\n\n\n\n\n\nGRIDFTP_HDFS_REPLICA_MAP\n\n\nNo\n\n\nFile containing a list of paths and replica values for setting the default # of replicas for specific file paths\n\n\n\n\n\n\nGRIDFTP_BUFFER_COUNT\n\n\nNo\n\n\nThe number of 1MB memory buffers used to reorder data streams before writing them to Hadoop\n\n\n\n\n\n\nGRIDFTP_FILE_BUFFER_COUNT\n\n\nNo\n\n\nThe number of 1MB file-based buffers used to reorder data streams before writing them to Hadoop\n\n\n\n\n\n\nGRIDFTP_SYSLOG\n\n\nNo\n\n\nSet this to 1 in case if you want to send transfer activity data to syslog (only used for the HadoopViz application)\n\n\n\n\n\n\nGRIDFTP_HDFS_MOUNT_POINT\n\n\nMaybe\n\n\nThe location of the FUSE mount point used during the Hadoop installation. Defaults to /mnt/hadoop. This is needed so that gridftp-hdfs can convert fuse paths on the incoming URL to native Hadoop paths. \nNote:\n this does not imply you need FUSE mounted on GridFTP nodes!\n\n\n\n\n\n\nGRIDFTP_LOAD_LIMIT\n\n\nNo\n\n\nGridFTP will refuse to start new transfers if the load on the GridFTP host is higher than this number; defaults to 20.\n\n\n\n\n\n\nTMPDIR\n\n\nMaybe\n\n\nThe temp directory where the file-based buffers are stored. Defaults to /tmp.\n\n\n\n\n\n\n\n\n/etc/sysconfig/gridftp-hdfs\n is also a good place to increase per-process resource limits. For example, many installations will require more than the default number of open files (\nulimit -n\n).\n\n\nLastly, you will need to configure an authentication mechanism for GridFTP.\n\n\nConfiguring authentication\n\n\nFor information on how to configure authentication for your GridFTP installation, please refer to the \nconfiguring authentication section of the GridFTP guide\n.\n\n\nGridFTP Gratia Transfer Probe Configuration\n\n\n\n\nNote\n\n\nNeeded by GridFTP node only.\n\n\n\n\nThe Gratia probe requires the file \nuser-vo-map\n to exist and be up to date. This file is created and updated by the \ngums-client\n package that comes in as a dependency of \nosg-se-hadoop-gridftp\n or \nosg-gridftp-hdfs\n. Assuming you installed GridFTP using the \nosg-se-hadoop-gridftp\n rpm, the Gratia Transfer Probe will already be installed.\n\n\nHere are the most relevant file and directory locations:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPurpose\n\n\nNeeds Editing?\n\n\nLocation\n\n\n\n\n\n\nProbe Configuration\n\n\nYes\n\n\n/etc/gratia/gridftp-transfer/ProbeConfig\n\n\n\n\n\n\nProbe Executables\n\n\nNo\n\n\n/usr/share/gratia/gridftp-transfer\n\n\n\n\n\n\nLog files\n\n\nNo\n\n\n/var/log/gratia\n\n\n\n\n\n\nTemporary files\n\n\nNo\n\n\n/var/lib/gratia/tmp\n\n\n\n\n\n\nGums configuration\n\n\nYes\n\n\n/etc/gums/gums-client.properties\n\n\n\n\n\n\n\n\nThe RPM installs the Gratia probe into the system crontab, but does not configure it. The configuration of the probe is controlled by the file\n\n\n/etc/gratia/gridftp-transfer/ProbeConfig\n\n\n\n\n\nThis is usually one XML node spread over multiple lines. Note that comments (#) have no effect on this file. You will need to edit the following:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAttribute\n\n\nNeeds Editing\n\n\nValue\n\n\n\n\n\n\nProbeName\n\n\nMaybe\n\n\nThis should be set to \"gridftp-transfer:\n\", where \n is the fully-qualified domain name of your gridftp host.\n\n\n\n\n\n\nCollectorHost\n\n\nMaybe\n\n\nSet to the hostname and port of the central collector. By default it sends to the OSG collector. See below.\n\n\n\n\n\n\nSiteName\n\n\nYes\n\n\nSet to the resource group name of your site as registered in OIM.\n\n\n\n\n\n\nGridftpLogDir\n\n\nYes\n\n\nSet to /var/log, or wherever your current gridftp logs are located\n\n\n\n\n\n\nGrid\n\n\nMaybe\n\n\nSet to \"ITB\" if this is a test resource; otherwise, leave as OSG.\n\n\n\n\n\n\nUserVOMapFile\n\n\nNo\n\n\nThis should be set to /var/lib/osg/user-vo-map; see below for information about this file.\n\n\n\n\n\n\nSuppressUnknownVORecords\n\n\nMaybe\n\n\nSet to 1 to suppress any records that can't be matched to a VO; 0 is strongly recommended.\n\n\n\n\n\n\nSuppressNoDNRecords\n\n\nMaybe\n\n\nSet to 1 to suppress records that can't be matched to a DN; 0 is strongly recommended.\n\n\n\n\n\n\nEnableProbe\n\n\nYes\n\n\nSet to 1 to enable the probe.\n\n\n\n\n\n\n\n\nSelecting a collector host\n\n\nThe collector is the central server which logs the GridFTP transfers into a database. There are usually three options:\n\n\n\n\nOSG Transfer Collector\n: This is the primary collector for transfers in the OSG. Use CollectorHost=\"gratia-osg-prod.opensciencegrid.org:80\".\n\n\nOSG-ITB Transfer Collector\n: This is the test collector for transfers in the OSG. Use CollectorHost=\" gratia-osg-itb.opensciencegrid.org:80\".\n\n\nSite local collector\n: If your site has set up its own collector, then your admin will be able to give you an endpoint to use. Typically, this is along the lines of CollectorHost=\"collector.example.com:8880\".\n\n\n\n\nNote:\n if you are installing on an itb site, use \ngratia-osg-itb.opensciencegrid.org\n instead of \"gratia-osg-transfer.opensciencegrid.org* above.\n\n\nUsing GUMS authorization mode\n\n\nThe \nuser-vo-map\n file is a simple, space-separated format that contains 2 columns; the first is a unix username and the second is the VO which that username correspond to. In order to create it you need to configure the gums client.\n\n\nThe primary configuration file for the gums-client utilities is located in \n/etc/gums/gums-client.properties\n. The two properties that you must change are:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAttribute\n\n\nNeeds Editing\n\n\nValue\n\n\n\n\n\n\ngums.location\n\n\nYes\n\n\nThis should be set to the admin URL for your gums server, usually of the form gums.location=https://GUMS_HOSTNAME:8443/gums/services/GUMSAdmin\n\n\n\n\n\n\ngums.authz\n\n\nYes\n\n\nThis should be set to the authorization interface URL for your gums server, usually of the form gums.authz=https://GUMS_HOSTNAME:8443/gums/services/GUMSXACMLAuthorizationServicePort\n\n\n\n\n\n\n\n\nAfter the gums client is configured to generate the file run the following once by hand:\n\n\nroot@host #\n gums-host-cron\n\n\n\n\n\nuser-vo-map\n should be created in the following location:\n\n\n/var/lib/osg/user-vo-map\n\n\n\n\n\nTo have cron regularly update this file start the following service:\n\n\nroot@host #\n service gums-client-cron start\n\n\n\n\n\nMake sure the \nUserVOMapFile\n field is set to this location in\n\n\n/etc/gratia/gridftp-transfer/ProbeConfig\n\n\n\n\n\nWithout \nuser-vo-map\n , all gridftp transfers will show up as belonging to the VO \"Unknown\".\n\n\nUsing Gridmap based authorization mode\n\n\nNote: If you are using this mode for authorization, make sure the files /etc/grid-security/gsi-authz.conf and /etc/grid-security/prima-authz.conf do not exist.\n\n\nIn order to enable generation of grid-mapfile and osg-user-vo-map.txt by using the edg-mkgridmap cron process to get information form VOMS servers do the following:\n\n\nedg-mkgridmap \n\n\n\n\n\n\nIf you have not installed this package, you will need to run \nyum install edg-mkgridmap\n first.\n\n\nValidation\n\n\nRun the Gratia probe once by hand to check for functionality:\n\n\nroot@host #\n /usr/share/gratia/gridftp-transfer/GridftpTransferProbeDriver\n\n\n\n\n\nLook for any abnormal termination and report it if it is a non-trivial site issue. Look in the log files in \n/var/log/gratia/\ndate\n.log\n and make sure there are no error messages printed.\n\n\nBeStMan Configuration\n\n\nSee the \nBeStMaN documentation\n for details.\n\n\nBeStManHadoop-specific configuration\n\n\nBeStMan2 SRM uses the Hadoop FUSE mount to perform namespace operations, such as mkdir, rm, and ls. As per the Hadoop install instructions, edit \n/etc/sysconfig/hadoop\n and run \nservice hadoop-firstboot start\n. It is \nnot\n necessary (or even recommended) to start any hadoop services with \nservice hadoop start\n.\n\n\nMake sure that you modify \nlocalPathListAllowed\n to use the Hadoop mount in \n/etc/bestman2/conf/bestman2.rc\n.\n\n\nModify /etc/sudoers\n\n\nBeStman requires the \"sudo\" command in order to write information as the proper user. You will need to give the bestman user the proper permissions to run these commands.\n\n\nModify \n/etc/sudoers\n and comment the following line.\n\n\n#Defaults    requiretty\n\n\n\n\n\nThen add the following lines at the end of the \n/etc/sudoers\n file.\n\n\nCmnd_Alias SRM_CMD = /bin/rm, /bin/mkdir, /bin/rmdir, /bin/mv, /bin/cp, /bin/ls\nRunas_Alias SRM_USR = ALL, !root\nbestman   ALL=(SRM_USR) NOPASSWD: SRM_CMD\n\n\n\n\n\nCopy certificates to bestman location\n\n\nBeStMan2 is preconfigured to look for the \nhost\n certificate and key in \n/etc/grid-security/bestman/bestman*.pem\n. Either, these files \nmust\n exist and be \nowned\n by the \nbestman\n user, or you must change the settings in \nbestman2.rc\n. Note that you must use host certificates here or lcg-utils may experience issues.\n\n\nBeStMan requires a certificate pair to function. In order to use lcg-utils, this must be a host certificate (rather than a service certificate). The following shows how to copy your certificates\n\n\ncp /etc/grid-security/hostkey.pem /etc/grid-security/bestman/bestmankey.pem\n\n\ncp /etc/grid-security/hostcert.pem /etc/grid-security/bestman/bestmancert.pem\n\n\nchown -R bestman:bestman /etc/grid-security/bestman/\n\n\n\n\n\n\nThen modify \nCertFileName\n, \nKeyFileName\n in \n/etc/bestman2/conf/bestman2.rc\n.\n\n\nHadoop Storage Probe Configuration\n\n\n\n\nNote\n\n\nThis is only needed by the Hadoop Namenode\n\n\n\n\nHere are the most relevant file and directory locations:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPurpose\n\n\nNeeds Editing?\n\n\nLocation\n\n\n\n\n\n\nProbe Configuration\n\n\nYes\n\n\n/etc/gratia/hadoop-storage/ProbeConfig\n\n\n\n\n\n\nProbe Executable\n\n\nNo\n\n\n/usr/share/gratia/hadoop-storage/hadoop_storage_probe\n\n\n\n\n\n\nLog files\n\n\nNo\n\n\n/var/log/gratia\n\n\n\n\n\n\nTemporary files\n\n\nNo\n\n\n/var/lib/gratia/tmp\n\n\n\n\n\n\n\n\nThe RPM installs the Gratia probe into the system crontab, but does not configure it. The configuration of the probe is controlled by two files\n\n\n/etc/gratia/hadoop-storage/ProbeConfig\n/etc/gratia/hadoop-storage/storage.cfg\n\n\n\n\n\nProbeConfig\n\n\nThis is usually one XML node spread over multiple lines. Note that comments (#) have no effect on this file. You will need to edit the following:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAttribute\n\n\nNeeds Editing\n\n\nValue\n\n\n\n\n\n\nCollectorHost\n\n\nMaybe\n\n\nSet to the hostname and port of the central collector. By default it sends to the OSG collector. You probably do not want to change it.\n\n\n\n\n\n\nSiteName\n\n\nYes\n\n\nSet to the resource group name of your SE as registered in OIM.\n\n\n\n\n\n\nGrid\n\n\nMaybe\n\n\nSet to \"ITB\" if this is a test resource; otherwise, leave as OSG.\n\n\n\n\n\n\nEnableProbe\n\n\nYes\n\n\nSet to 1 to enable the probe.\n\n\n\n\n\n\n\n\nstorage.cfg\n\n\nThis file controls which paths in HDFS should be monitored. This is in the Windows INI format.\n\n\nNote: for the current version of the storage.cfg, there is an error, and you may need to delete the \"probe/\" subdirectory for the ProbeConfig location\n\n\nProbeConfig = /etc/gratia/\nprobe/\nhadoop-storage/ProbeConfig\n\n\n\n\n\nFor each logical \"area\" (arbitrarily defined by you), specify both a given name and a list of paths that belong to that area. Unix globs are accepted.\n\n\nTo configure an area named \"CMS /store\" that monitors the space usage in the paths /user/cms/store/*, one would add the following to the storage.cfg file.\n\n\n[Area CMS /store]\n\n\nName\n \n=\n \nCMS /store\n\n\nPath\n \n=\n \n/user/cms/store/*\n\n\nTrim\n \n=\n \n/user/cms\n\n\n\n\n\n\nFor each such area, add a section to your configuration file.\n\n\nExample file\n\n\nBelow is a configuration file that includes three distinct areas. Note that you shouldn't have to touch the [Gratia] section if you edited the ProbeConfig above:\n\n\n[Gratia]\n\n\ngratia_location\n \n=\n \n/opt/vdt/gratia\n\n\nProbeConfig\n \n=\n \n%(gratia_location)s/probe/hadoop-storage/ProbeConfig\n\n\n\n[Area /store]\n\n\nName\n \n=\n \nCMS /store\n\n\nPath\n \n=\n \n/store/*\n\n\n\n[Area /store/user]\n\n\nName\n \n=\n \nCMS /store/user\n\n\nPath\n \n=\n \n/store/user/*\n\n\n\n[Area /user]\n\n\nName\n \n=\n \nHadoop /user\n\n\nPath\n \n=\n \n/user/*\n\n\n\n\n\n\n*\nNOTE These lines in the [gratia] section are wrong and need to be changed to the following by hand for now until the rpm is updated:\n\n\ngratia_location = /etc/gratia\nProbeConfig = %(gratia_location)s/hadoop-storage/ProbeConfig\n\n\n\n\n\nRunning Services\n\n\nNamenode:\n\n\n#\nStarting namenode\n\nservice hadoop-hdfs-namenode start\n\n\n#\nStopping namenode\n\nservice hadoop-hdfs-namenode stop\n\n\n\n\n\n\nSecondary Namenode:\n\n\n#\nStarting secondary namenode\n\nservice hadoop-hdfs-secondarynamenode start\n\n\n#\nStopping secondary namenode\n\nservice hadoop-hdfs-secondarynamenode stop\n\n\n\n\n\n\nDatanode:\n\n\n#\nStarting namenode\n\nservice hadoop-hdfs-datanode start\n\n\n#\nStopping namenode\n\nservice hadoop-hdfs-datanode stop\n\n\n\n\n\n\nGridFTP:\n\n\nroot@host #\n service globus-gridftp-server start\n\n\n\n\n\nTo start Gridftp automatically at boot time\n\n\nroot@host #\n chkconfig globus-gridftp-server on\n\n\n\n\n\nStopping GridFTP:\n\n\nroot@host #\n service globus-gridftp-server stop\n\n\n\n\n\nroot@host #\n service bestman2 start\n\n\n\n\n\nTo start Bestman automatically at boot time\n\n\nroot@host #\n chkconfig bestman2 on\n\n\n\n\n\nValidation\n\n\nThe first thing you may want to do after installing and starting your \nNamenode\n is to verify that the web interface works. In your web browser go to:\n\n\nhttp://\nnamenode.hostname\n:50070/dfshealth.jsp\n\n\n\n\n\nGet familiar with Hadoop commands. Run hadoop with no arguments to see the list of commands.\n\n\n\n  \nShow detailed ouput\n\n   \n\n\nuser$ hadoop\n\n\nUsage: hadoop [--config confdir] COMMAND\n\n\nwhere COMMAND is one of:\n\n\n  namenode -format     format the DFS filesystem\n\n\n  secondarynamenode    run the DFS secondary namenode\n\n\n  namenode             run the DFS namenode\n\n\n  datanode             run a DFS datanode\n\n\n  dfsadmin             run a DFS admin client\n\n\n  mradmin              run a Map-Reduce admin client\n\n\n  fsck                 run a DFS filesystem checking utility\n\n\n  fs                   run a generic filesystem user client\n\n\n  balancer             run a cluster balancing utility\n\n\n  fetchdt              fetch a delegation token from the NameNode\n\n\n  jobtracker           run the MapReduce job Tracker node\n\n\n  pipes                run a Pipes job\n\n\n  tasktracker          run a MapReduce task Tracker node\n\n\n  job                  manipulate MapReduce jobs\n\n\n  queue                get information regarding JobQueues\n\n\n  version              print the version\n\n\n  jar \njar\n            run a jar file\n\n\n  distcp \nsrcurl\n \ndesturl\n copy file or directories recursively\n\n\n  archive -archiveName NAME -p \nparent path\n \nsrc\n* \ndest\n create a hadoop archive\n\n\n  oiv                  apply the offline fsimage viewer to an fsimage\n\n\n  classpath            prints the class path needed to get the\n\n\n                       Hadoop jar and the required libraries\n\n\n  daemonlog            get/set the log level for each daemon\n\n\n or\n\n\n  CLASSNAME            run the class named CLASSNAME\n\n\nMost commands print help when invoked w/o parameters.\n\n\n\n\n\n\n\n\n\n\nFor a list of supported filesystem commands:\n\n\n\n  \nShow 'hadoop fs' detailed ouput\n\n   \n\n\nuser$ hadoop fs\n\n\nUsage: java FsShell\n\n\n           [-ls \npath\n]\n\n\n           [-lsr \npath\n]\n\n\n           [-df [\npath\n]]\n\n\n           [-du \npath\n]\n\n\n           [-dus \npath\n]\n\n\n           [-count[-q] \npath\n]\n\n\n           [-mv \nsrc\n \ndst\n]\n\n\n           [-cp \nsrc\n \ndst\n]\n\n\n           [-rm [-skipTrash] \npath\n]\n\n\n           [-rmr [-skipTrash] \npath\n]\n\n\n           [-expunge]\n\n\n           [-put \nlocalsrc\n ... \ndst\n]\n\n\n           [-copyFromLocal \nlocalsrc\n ... \ndst\n]\n\n\n           [-moveFromLocal \nlocalsrc\n ... \ndst\n]\n\n\n           [-get [-ignoreCrc] [-crc] \nsrc\n \nlocaldst\n]\n\n\n           [-getmerge \nsrc\n \nlocaldst\n [addnl]]\n\n\n           [-cat \nsrc\n]\n\n\n           [-text \nsrc\n]\n\n\n           [-copyToLocal [-ignoreCrc] [-crc] \nsrc\n \nlocaldst\n]\n\n\n           [-moveToLocal [-crc] \nsrc\n \nlocaldst\n]\n\n\n           [-mkdir \npath\n]\n\n\n           [-setrep [-R] [-w] \nrep\n \npath/file\n]\n\n\n           [-touchz \npath\n]\n\n\n           [-test -[ezd] \npath\n]\n\n\n           [-stat [format] \npath\n]\n\n\n           [-tail [-f] \nfile\n]\n\n\n           [-chmod [-R] \nMODE[,MODE]... | OCTALMODE\n PATH...]\n\n\n           [-chown [-R] [OWNER][:[GROUP]] PATH...]\n\n\n           [-chgrp [-R] GROUP PATH...]\n\n\n           [-help [cmd]]\n\n\n\nGeneric options supported are\n\n\n-conf \nconfiguration file\n     specify an application configuration file\n\n\n-D \nproperty=value\n            use value for given property\n\n\n-fs \nlocal|namenode:port\n      specify a namenode\n\n\n-jt \nlocal|jobtracker:port\n    specify a job tracker\n\n\n-files \ncomma separated list of files\n    specify comma separated files to be copied to the map reduce cluster\n\n\n-libjars \ncomma separated list of jars\n    specify comma separated jar files to include in the classpath.\n\n\n-archives \ncomma separated list of archives\n    specify comma separated archives to be unarchived on the compute machines.\n\n\n\nThe general command line syntax is\n\n\nbin/hadoop command [genericOptions] [commandOptions]\n\n\n\n\n\n\n\n\n\n\nAn online guide is also available at \nApache Hadoop commands manual\n. You can use Hadoop commands to perform filesystem operations with more consistency.\n\n\nExample, to look into the internal hadoop namespace:\n\n\nuser$ hadoop fs -ls /\n\n\nFound 1 items\n\n\ndrwxrwxr-x   - engage engage          0 2011-07-25 06:32 /engage\n\n\n\n\n\n\nExample, to adjust ownership of filesystem areas (there is usually no need to specify the mount itself \n/mnt/hadoop\n in Hadoop commands):\n\n\nroot@host #\n hadoop fs -chown -R engage:engage /engage\n\n\n\n\n\nExample, compare \nhadoop fs\n command vs. using FUSE mount:\n\n\nuser$ hadoop fs -ls /engage\n\n\nFound 3 items\n\n\n-rw-rw-r--   2 engage engage  733669376 2011-06-15 16:55 /engage/CentOS-5.6-x86_64-LiveCD.iso\n\n\n-rw-rw-r--   2 engage engage  215387183 2011-06-15 16:28 /engage/condor-7.6.1-x86_rhap_5-stripped.tar.gz\n\n\n-rw-rw-r--   2 engage engage    9259360 2011-06-15 16:32 /engage/glideinWMS_v2_5_1.tgz\n\n\n\nuser$ ls -l /mnt/hadoop/engage\n\n\ntotal 935855\n\n\n-rw-rw-r-- 1 engage engage 733669376 Jun 15 16:55 CentOS-5.6-x86_64-LiveCD.iso\n\n\n-rw-rw-r-- 1 engage engage 215387183 Jun 15 16:28 condor-7.6.1-x86_rhap_5-stripped.tar.gz\n\n\n-rw-rw-r-- 1 engage engage   9259360 Jun 15 16:32 glideinWMS_v2_5_1.tgz\n\n\n\n\n\n\nGridFTP Validation\n\n\n\n\nNote\n\n\nThe commands used to verify GridFTP below assume you have access to a node where you can first generate a valid proxy using \nvoms-proxy-init\n or \ngrid-proxy-init\n. Obtaining grid credentials is beyond the scope of this document.\n\n\n\n\nuser$ globus-url-copy file:///home/users/jdost/test.txt gsiftp://devg-7.t2.ucsd.edu:2811/mnt/hadoop/engage/test.txt\n\n\n\n\n\n\nIf you are having troubles running GridFTP refer to \nStarting GridFTP in Standalone Mode\n in the Troubleshooting section.\n\n\nBeStMan Validation\n\n\nThere are three ways of validating BeStMan: * SrmTester: BeStMan testing application * InstallRSV: RSV monitoring tools * BestMan client tools\n\n\nSee the relevant pages for the first two options. This section will detail some basic client commands to validate. You will need grid credentials in order to test using client tools.\n\n\nsrm-ping srm://BeStMan_host:secured_http_port/srm/v2/server\n\n\nsrm-copy file:////tmp/test1  srm://BeStMan_host:secured_http_port/srm/v2/server\\?SFN=/mnt/hadoop/VONAME/test_1\n\n\n\n\n\n\nThe \nsrm-ping\n tool should return a valid mapping \ngumsIDMapped\n that is not null\n\n\nInstalling Hadoop Storage Reports (Optional)\n\n\n*\nNOTE the GratiaReporting rpm has not yet been migrated to the new osg repos and this section is subject to change. Please skip this section until this warning goes away or request Nebraska to host your reports\n\n\n\n\nNote\n\n\nThe Hadoop Storage Reports may be installed on any node that has access to a local Gratia Collector\n\n\n\n\nThe Hadoop storage reports provides a daily report on the status and usage of your SE. This serves as a handy tool for both site administrators and site executives. An example report is copied at the end of this guide.\n\n\n\n  \nShow Hadoop Storage Reports information\n\n\nPrerequisites\n\n\n\n\nA working HDFS installation\n\n\nA local \nGratia Collector\n installed\n\n\nA Hadoop Storage Probe installed and configured to point to the local Gratia Collector\n\n\n\n\nInstallation\n\n\n[root@client ~]$\n yum install GratiaReporting\n\n\n\n\n\nUpdates can be installed with:\n\n\n[root@client ~]$\n yum upgrade GratiaReporting\n\n\n\n\n\nConfiguration\n\n\nThis RPM uses Linux-standard file locations. Here are the most relevant file and directory locations:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPurpose\n\n\nNeeds Editing?\n\n\nLocation\n\n\n\n\n\n\nReport Configuration\n\n\nYes\n\n\n/etc/gratia_reporting\n\n\n\n\n\n\nCron template\n\n\nYes\n\n\n/etc/gratia_reporting/gratia_reporting/gratia_reporting.cron (move to /etc/cron.d)\n\n\n\n\n\n\nLogging Configuration\n\n\nNo\n\n\n/etc/gratia_reporting/logging.cfg\n\n\n\n\n\n\nLog files\n\n\nNo\n\n\n/var/log/gratia_reporting.log\n\n\n\n\n\n\n\n\nConfiguration file\n\n\nCopy the file \n/etc/gratia_reporting/reporting.cfg\n to a new filename in \n/etc/gratia_reporting\n (for example, \n/etc/gratia_reporting/reporting_cms.cfg\n). You will do this once for every report you want to send out.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAttribute\n\n\nNeeds Editing\n\n\nValue\n\n\n\n\n\n\nSiteName\n\n\nYes\n\n\nSet to the resource group name of your SE as registered in OIM.\n\n\n\n\n\n\ndatabase\n\n\nMaybe\n\n\nSet to the database section containing the login details for your Gratia Collector (a few, non-functioning examples sections are included). Installing a Gratia Collector is \ncovered here\n, but ask around on osg-hadoop: Nebraska will usually run these reports for you if requested.\n\n\n\n\n\n\ntoNames\n\n\nYes\n\n\nPython list for the \"to names\" for the report email.\n\n\n\n\n\n\ntoEmails\n\n\nYes\n\n\nPython list for the \"to emails\" for the report email.\n\n\n\n\n\n\nsmtphost\n\n\nMaybe\n\n\nHostname of a SMTP server that accepts email from this host.\n\n\n\n\n\n\nfromName\n\n\nMaybe\n\n\nSet to the \"from name\" for the report email.\n\n\n\n\n\n\nfromEmail\n\n\nMaybe\n\n\nSet to the \"from email\" for the report email.\n\n\n\n\n\n\n\n\nCron\n\n\nCopy the file \n/etc/gratia_reporting/gratia_reporting.cron\n to \n/etc/cron.d\n. There is one line per report; comment out all except the hadoop report. It is the line containing \n-n hadoop\n. Update the line to point at your new configuration file.\n\n\n\n\n\n  \nExpand sample report from the Nebraska HDFS instance\n\n    \n\n\n============================================================\n  The Hadoop Chronicle | 85 % | 2009-09-25\n============================================================\n\n--------------------\n| Global Storage   |\n-----------------------------------------------------\n|                  |  Today  | Yesterday | One Week |\n-----------------------------------------------------\n| Total Space (GB) | 311,470 |   357,818 |  368,711 |\n| Free Space (GB)  |  47,304 |    93,719 |  128,391 |\n| Used Space (GB)  | 264,166 |   264,100 |  240,320 |\n| Used Percentage  |     85% |       74% |      65% |\n-----------------------------------------------------\n--------------\n| CMS /store |\n-------------------------------------------------------------------------------------------------------------------------------------\n|           Path           | Size(GB) | 1 Day Change | 7 Day Change | Remaining | # Files | 1 Day Change | 7 Day Change | Remaining |\n-------------------------------------------------------------------------------------------------------------------------------------\n| /store/user              |      771 |            0 | UNKNOWN      | NO QUOTA  |   4,859 |            0 | UNKNOWN      | NO QUOTA  |\n| /store/mc                |   95,865 |         -353 | UNKNOWN      | NO QUOTA  |  86,830 |         -171 | UNKNOWN      | NO QUOTA  |\n| /store/test              |        0 |            0 | UNKNOWN      | NO QUOTA  |     569 |           25 | UNKNOWN      | NO QUOTA  |\n| /store/results           |      237 |            0 | UNKNOWN      | NO QUOTA  |     198 |            0 | UNKNOWN      | NO QUOTA  |\n| /store/phedex_monarctest |      729 |            0 | UNKNOWN      | NO QUOTA  |     257 |            0 | UNKNOWN      | NO QUOTA  |\n| /store/unmerged          |    3,681 |            3 | UNKNOWN      | NO QUOTA  |  35,687 |           23 | UNKNOWN      | NO QUOTA  |\n| /store/CSA07             |        0 |            0 | UNKNOWN      | NO QUOTA  |       0 |            0 | UNKNOWN      | NO QUOTA  |\n| /store/data              |        0 |            0 | UNKNOWN      | NO QUOTA  |       0 |            0 | UNKNOWN      | NO QUOTA  |\n| /store/PhEDEx_LoadTest07 |        0 |          -21 | UNKNOWN      | NO QUOTA  |       1 |          -22 | UNKNOWN      | NO QUOTA  |\n-------------------------------------------------------------------------------------------------------------------------------------\n\n-------------------\n| CMS /store/user |\n----------------------------------------------------------------------------------------------------------------------------------\n|          Path         | Size(GB) | 1 Day Change | 7 Day Change | Remaining | # Files | 1 Day Change | 7 Day Change | Remaining |\n----------------------------------------------------------------------------------------------------------------------------------\n| /store/user/hpi       |        0 |            0 | UNKNOWN      |     1,099 |      15 |            0 | UNKNOWN      |     9,985 |\n| /store/user/gattebury |        0 |            0 | UNKNOWN      |     1,100 |       1 |            0 | UNKNOWN      |     9,999 |\n| /store/user/mkirn     |        0 |            0 | UNKNOWN      |     1,100 |       3 |            0 | UNKNOWN      |     9,997 |\n| /store/user/spadhi    |       12 |            0 | UNKNOWN      |     1,062 |   1,114 |            0 | UNKNOWN      |     8,886 |\n| /store/user/creed     |        0 |            0 | UNKNOWN      |     1,100 |       0 |            0 | UNKNOWN      |    10,000 |\n| /store/user/rossman   |        0 |            0 | UNKNOWN      |     1,099 |       5 |            0 | UNKNOWN      |     9,995 |\n| /store/user/eluiggi   |        0 |            0 | UNKNOWN      |     1,099 |       6 |            0 | UNKNOWN      |     9,994 |\n| /store/user/ewv       |        7 |            0 | UNKNOWN      |     1,081 |     284 |            0 | UNKNOWN      |     9,716 |\n| /store/user/test      |        0 |            0 | UNKNOWN      | NO QUOTA  |     167 |            0 | UNKNOWN      |     9,833 |\n| /store/user/schiefer  |      751 |            0 | UNKNOWN      |     1,044 |   3,264 |            0 | UNKNOWN      |     6,736 |\n----------------------------------------------------------------------------------------------------------------------------------\n\n----------------\n| Hadoop /user |\n----------------------------------------------------------------------------------------------------------------------------------\n|       Path      | Size(GB) | 1 Day Change | 7 Day Change | Remaining | # Files | 1 Day Change | 7 Day Change |    Remaining    |\n----------------------------------------------------------------------------------------------------------------------------------\n| /user/djbender  |        0 |            0 | UNKNOWN      | NO QUOTA  |       1 |            0 | UNKNOWN      | NO QUOTA        |\n| /user/lhcb      |        0 |            0 | UNKNOWN      |        54 |       0 |            0 | UNKNOWN      | NO QUOTA        |\n| /user/dzero     |      897 |            0 | UNKNOWN      |       347 |  89,376 |            0 | UNKNOWN      |         410,624 |\n| /user/bloom     |      454 |            0 | UNKNOWN      | NO QUOTA  |   1,410 |            0 | UNKNOWN      | NO QUOTA        |\n| /user/uscms01   |  101,384 |         -362 | UNKNOWN      | NO QUOTA  | 129,739 |         -141 | UNKNOWN      | NO QUOTA        |\n| /user/cdf       |        0 |            0 | UNKNOWN      | NO QUOTA  |       6 |            0 | UNKNOWN      | 536,870,911,994 |\n| /user/osg       |        1 |            0 | UNKNOWN      | NO QUOTA  |       3 |            0 | UNKNOWN      |   5,368,709,117 |\n| /user/dweitzel  |       20 |            0 | UNKNOWN      | NO QUOTA  |   2,282 |            0 | UNKNOWN      | NO QUOTA        |\n| /user/gattebury |        5 |            0 | UNKNOWN      | NO QUOTA  |  10,002 |            0 | UNKNOWN      | NO QUOTA        |\n| /user/brian     |       72 |            0 | UNKNOWN      | NO QUOTA  |   2,697 |            0 | UNKNOWN      | NO QUOTA        |\n| /user/usatlas   |        0 |            0 | UNKNOWN      | NO QUOTA  |       0 |            0 | UNKNOWN      | NO QUOTA        |\n| /user/powers    |        1 |            1 | UNKNOWN      | NO QUOTA  |     211 |          211 | UNKNOWN      | NO QUOTA        |\n| /user/ifisk     |        0 |            0 | UNKNOWN      | NO QUOTA  |       1 |            0 | UNKNOWN      | NO QUOTA        |\n| /user/gpn       |      261 |           -5 | UNKNOWN      |     1,360 |   3,805 |            1 | UNKNOWN      |         996,195 |\n| /user/engage    |      461 |          367 | UNKNOWN      | NO QUOTA  |      16 |           13 | UNKNOWN      |         999,984 |\n| /user/clundst   |        0 |            0 | UNKNOWN      | NO QUOTA  |       6 |            0 | UNKNOWN      | NO QUOTA        |\n| /user/che       |        0 |            0 | UNKNOWN      | NO QUOTA  |      13 |            0 | UNKNOWN      | NO QUOTA        |\n| /user/store     |        0 |            0 | UNKNOWN      | NO QUOTA  |       0 |            0 | UNKNOWN      | NO QUOTA        |\n| /user/dteam     |        0 |            0 | UNKNOWN      |        53 |      18 |            0 | UNKNOWN      | NO QUOTA        |\n| /user/root      |        0 |            0 | UNKNOWN      | NO QUOTA  |       1 |            0 | UNKNOWN      | NO QUOTA        |\n----------------------------------------------------------------------------------------------------------------------------------\n\n-------------\n| FSCK Data |\n-------------\n Total size:    114592906796932 B (Total open files size: 38923141120 B)\n Total dirs:    41293\n Total files:   295431 (Files currently being written: 38)\n Total blocks (validated):  1356788 (avg. block size 84458962 B) (Total open file blocks (not validated): 297)\n Minimally replicated blocks:   1356788 (100.0 %)\n Over-replicated blocks:    1 (7.370348E-5 %)\n Under-replicated blocks:   0 (0.0 %)\n Mis-replicated blocks:     0 (0.0 %)\n Default replication factor:    3\n Average block replication: 2.2943976\n Corrupt blocks:        0\n Missing replicas:      0 (0.0 %)\n Number of data-nodes:      101\n Number of racks:       1\nThe filesystem under path \n/\n is HEALTHY\n\n\n\n\n\n\n\n\n\nTroubleshooting\n\n\nHadoop\n\n\nTo view all of the currently configured settings of Hadoop from the web interface, enter the following url in your browser:\n\n\nhttp://\nnamenode.hostname\n:50070/conf\n\n\n\n\n\nYou will see the entire configuration in XML format, for example:\n\n\n\n  \nExpand XML configuration\n\n    \n\n\n?xml version=\n1.0\n encoding=\nUTF-8\n standalone=\nno\n?\nconfiguration\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.s3n.impl\n/name\nvalue\norg.apache.hadoop.fs.s3native.NativeS3FileSystem\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.task.cache.levels\n/name\nvalue\n2\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmap.sort.class\n/name\nvalue\norg.apache.hadoop.util.QuickSort\n/value\n/property\n\n\nproperty\n!--Loaded from core-site.xml--\nname\nhadoop.tmp.dir\n/name\nvalue\n/data1/hadoop//scratch\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nhadoop.native.lib\n/name\nvalue\ntrue\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.namenode.decommission.nodes.per.interval\n/name\nvalue\n5\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.https.need.client.auth\n/name\nvalue\nfalse\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nipc.client.idlethreshold\n/name\nvalue\n4000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.system.dir\n/name\nvalue\n${\nhadoop\n.\ntmp\n.\ndir\n}\n/mapred/system\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.datanode.data.dir.perm\n/name\nvalue\n755\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.job.tracker.persist.jobstatus.hours\n/name\nvalue\n0\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-site.xml--\nname\ndfs.namenode.logging.level\n/name\nvalue\nall\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.datanode.address\n/name\nvalue\n0.0.0.0:50010\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nio.skip.checksum.errors\n/name\nvalue\nfalse\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.block.access.token.enable\n/name\nvalue\nfalse\n/value\n/property\n\n\nproperty\n!--Loaded from Unknown--\nname\nfs.default.name\n/name\nvalue\nhdfs://nagios.t2.ucsd.edu:9000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.child.tmp\n/name\nvalue\n./tmp\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.har.impl.disable.cache\n/name\nvalue\ntrue\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.skip.reduce.max.skip.groups\n/name\nvalue\n0\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.safemode.threshold.pct\n/name\nvalue\n0.999f\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.heartbeats.in.second\n/name\nvalue\n100\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-site.xml--\nname\ndfs.namenode.handler.count\n/name\nvalue\n40\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.blockreport.initialDelay\n/name\nvalue\n0\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.jobtracker.instrumentation\n/name\nvalue\norg.apache.hadoop.mapred.JobTrackerMetricsInst\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.tasktracker.dns.nameserver\n/name\nvalue\ndefault\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nio.sort.factor\n/name\nvalue\n10\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.task.timeout\n/name\nvalue\n600000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.max.tracker.failures\n/name\nvalue\n4\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nhadoop.rpc.socket.factory.class.default\n/name\nvalue\norg.apache.hadoop.net.StandardSocketFactory\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.job.tracker.jobhistory.lru.cache.size\n/name\nvalue\n5\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.hdfs.impl\n/name\nvalue\norg.apache.hadoop.hdfs.DistributedFileSystem\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.skip.map.auto.incr.proc.count\n/name\nvalue\ntrue\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.block.access.key.update.interval\n/name\nvalue\n600\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapreduce.job.complete.cancel.delegation.tokens\n/name\nvalue\ntrue\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nio.mapfile.bloom.size\n/name\nvalue\n1048576\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapreduce.reduce.shuffle.connect.timeout\n/name\nvalue\n180000\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.safemode.extension\n/name\nvalue\n30000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-site.xml--\nname\ntasktracker.http.threads\n/name\nvalue\n50\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.job.shuffle.merge.percent\n/name\nvalue\n0.66\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.ftp.impl\n/name\nvalue\norg.apache.hadoop.fs.ftp.FTPFileSystem\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.output.compress\n/name\nvalue\nfalse\n/value\n/property\n\n\nproperty\n!--Loaded from core-site.xml--\nname\nio.bytes.per.checksum\n/name\nvalue\n4096\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.healthChecker.script.timeout\n/name\nvalue\n600000\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\ntopology.node.switch.mapping.impl\n/name\nvalue\norg.apache.hadoop.net.ScriptBasedMapping\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.https.server.keystore.resource\n/name\nvalue\nssl-server.xml\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.reduce.slowstart.completed.maps\n/name\nvalue\n0.05\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.reduce.max.attempts\n/name\nvalue\n4\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.ramfs.impl\n/name\nvalue\norg.apache.hadoop.fs.InMemoryFileSystem\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.block.access.token.lifetime\n/name\nvalue\n600\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.skip.map.max.skip.records\n/name\nvalue\n0\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.name.edits.dir\n/name\nvalue\n${\ndfs\n.\nname\n.\ndir\n}\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nhadoop.security.group.mapping\n/name\nvalue\norg.apache.hadoop.security.ShellBasedUnixGroupsMapping\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.job.tracker.persist.jobstatus.dir\n/name\nvalue\n/jobtracker/jobsInfo\n/value\n/property\n\n\nproperty\n!--Loaded from core-site.xml--\nname\nhadoop.log.dir\n/name\nvalue\n/var/log/hadoop\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.s3.buffer.dir\n/name\nvalue\n${\nhadoop\n.\ntmp\n.\ndir\n}\n/s3\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-site.xml--\nname\ndfs.block.size\n/name\nvalue\n134217728\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\njob.end.retry.attempts\n/name\nvalue\n0\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.file.impl\n/name\nvalue\norg.apache.hadoop.fs.LocalFileSystem\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.output.compression.type\n/name\nvalue\nRECORD\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.local.dir.minspacestart\n/name\nvalue\n0\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.datanode.ipc.address\n/name\nvalue\n0.0.0.0:50020\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.permissions\n/name\nvalue\ntrue\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\ntopology.script.number.args\n/name\nvalue\n100\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nio.mapfile.bloom.error.rate\n/name\nvalue\n0.005\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.max.tracker.blacklists\n/name\nvalue\n4\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.task.profile.maps\n/name\nvalue\n0-2\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.datanode.https.address\n/name\nvalue\n0.0.0.0:50475\n/value\n/property\n\n\nproperty\n!--Loaded from core-site.xml--\nname\ndfs.umaskmode\n/name\nvalue\n002\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.userlog.retain.hours\n/name\nvalue\n24\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-site.xml--\nname\ndfs.secondary.http.address\n/name\nvalue\ngratia-1:50090\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-site.xml--\nname\ndfs.replication.max\n/name\nvalue\n32\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.job.tracker.persist.jobstatus.active\n/name\nvalue\nfalse\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nhadoop.security.authorization\n/name\nvalue\nfalse\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nlocal.cache.size\n/name\nvalue\n10737418240\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.min.split.size\n/name\nvalue\n0\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.namenode.delegation.token.renew-interval\n/name\nvalue\n86400000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-site.xml--\nname\nmapred.map.tasks\n/name\nvalue\n7919\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.child.java.opts\n/name\nvalue\n-Xmx200m\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.https.client.keystore.resource\n/name\nvalue\nssl-client.xml\n/value\n/property\n\n\nproperty\n!--Loaded from Unknown--\nname\ndfs.namenode.startup\n/name\nvalue\nREGULAR\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.job.queue.name\n/name\nvalue\ndefault\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.job.tracker.retiredjobs.cache.size\n/name\nvalue\n1000\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.https.address\n/name\nvalue\n0.0.0.0:50470\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-site.xml--\nname\ndfs.balance.bandwidthPerSec\n/name\nvalue\n2000000000\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nipc.server.listen.queue.size\n/name\nvalue\n128\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\njob.end.retry.interval\n/name\nvalue\n30000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.inmem.merge.threshold\n/name\nvalue\n1000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.skip.attempts.to.start.skipping\n/name\nvalue\n2\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-site.xml--\nname\nfs.checkpoint.dir\n/name\nvalue\n/var/hadoop/checkpoint-a\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-site.xml--\nname\nmapred.reduce.tasks\n/name\nvalue\n1543\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.merge.recordsBeforeProgress\n/name\nvalue\n10000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.userlog.limit.kb\n/name\nvalue\n0\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nwebinterface.private.actions\n/name\nvalue\nfalse\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.max.objects\n/name\nvalue\n0\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.job.shuffle.input.buffer.percent\n/name\nvalue\n0.70\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nio.sort.spill.percent\n/name\nvalue\n0.80\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.map.tasks.speculative.execution\n/name\nvalue\ntrue\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nhadoop.util.hash.type\n/name\nvalue\nmurmur\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.datanode.dns.nameserver\n/name\nvalue\ndefault\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.blockreport.intervalMsec\n/name\nvalue\n3600000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.map.max.attempts\n/name\nvalue\n4\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapreduce.job.acl-view-job\n/name\nvalue\n \n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.job.tracker.handler.count\n/name\nvalue\n10\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.client.block.write.retries\n/name\nvalue\n3\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.max.reduces.per.node\n/name\nvalue\n-1\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapreduce.reduce.shuffle.read.timeout\n/name\nvalue\n180000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.tasktracker.expiry.interval\n/name\nvalue\n600000\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.https.enable\n/name\nvalue\nfalse\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.jobtracker.maxtasks.per.job\n/name\nvalue\n-1\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.jobtracker.job.history.block.size\n/name\nvalue\n3145728\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nkeep.failed.task.files\n/name\nvalue\nfalse\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.datanode.failed.volumes.tolerated\n/name\nvalue\n0\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.task.profile.reduces\n/name\nvalue\n0-2\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nipc.client.tcpnodelay\n/name\nvalue\nfalse\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.output.compression.codec\n/name\nvalue\norg.apache.hadoop.io.compress.DefaultCodec\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nio.map.index.skip\n/name\nvalue\n0\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nipc.server.tcpnodelay\n/name\nvalue\nfalse\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.namenode.delegation.key.update-interval\n/name\nvalue\n86400000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.running.map.limit\n/name\nvalue\n-1\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\njobclient.progress.monitor.poll.interval\n/name\nvalue\n1000\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.default.chunk.view.size\n/name\nvalue\n32768\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nhadoop.logfile.size\n/name\nvalue\n10000000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.reduce.tasks.speculative.execution\n/name\nvalue\ntrue\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapreduce.tasktracker.outofband.heartbeat\n/name\nvalue\nfalse\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.s3n.block.size\n/name\nvalue\n67108864\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-site.xml--\nname\ndfs.datanode.du.reserved\n/name\nvalue\n10000000000\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nhadoop.security.authentication\n/name\nvalue\nsimple\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-site.xml--\nname\nfs.checkpoint.period\n/name\nvalue\n3600\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.running.reduce.limit\n/name\nvalue\n-1\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.job.reuse.jvm.num.tasks\n/name\nvalue\n1\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.web.ugi\n/name\nvalue\nwebuser,webgroup\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.jobtracker.completeuserjobs.maximum\n/name\nvalue\n100\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.df.interval\n/name\nvalue\n60000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.task.tracker.task-controller\n/name\nvalue\norg.apache.hadoop.mapred.DefaultTaskController\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-site.xml--\nname\ndfs.data.dir\n/name\nvalue\n/data1/hadoop//data\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.s3.maxRetries\n/name\nvalue\n4\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.datanode.dns.interface\n/name\nvalue\ndefault\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.support.append\n/name\nvalue\ntrue\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapreduce.job.acl-modify-job\n/name\nvalue\n \n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.local.dir\n/name\nvalue\n${\nhadoop\n.\ntmp\n.\ndir\n}\n/mapred/local\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.hftp.impl\n/name\nvalue\norg.apache.hadoop.hdfs.HftpFileSystem\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-site.xml--\nname\ndfs.permissions.supergroup\n/name\nvalue\nroot\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.trash.interval\n/name\nvalue\n0\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.s3.sleepTimeSeconds\n/name\nvalue\n10\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.submit.replication\n/name\nvalue\n10\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-site.xml--\nname\ndfs.replication.min\n/name\nvalue\n1\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.har.impl\n/name\nvalue\norg.apache.hadoop.fs.HarFileSystem\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.map.output.compression.codec\n/name\nvalue\norg.apache.hadoop.io.compress.DefaultCodec\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.tasktracker.dns.interface\n/name\nvalue\ndefault\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.namenode.decommission.interval\n/name\nvalue\n30\n/value\n/property\n\n\nproperty\n!--Loaded from Unknown--\nname\ndfs.http.address\n/name\nvalue\nnagios:50070\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-site.xml--\nname\nmapred.job.tracker\n/name\nvalue\nnagios:9000\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.heartbeat.interval\n/name\nvalue\n3\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nio.seqfile.sorter.recordlimit\n/name\nvalue\n1000000\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.name.dir\n/name\nvalue\n${\nhadoop\n.\ntmp\n.\ndir\n}\n/dfs/name\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.line.input.format.linespermap\n/name\nvalue\n1\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.jobtracker.taskScheduler\n/name\nvalue\norg.apache.hadoop.mapred.JobQueueTaskScheduler\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.tasktracker.instrumentation\n/name\nvalue\norg.apache.hadoop.mapred.TaskTrackerMetricsInst\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.datanode.http.address\n/name\nvalue\n0.0.0.0:50075\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\njobclient.completion.poll.interval\n/name\nvalue\n5000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.max.maps.per.node\n/name\nvalue\n-1\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.local.dir.minspacekill\n/name\nvalue\n0\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.replication.interval\n/name\nvalue\n3\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nio.sort.record.percent\n/name\nvalue\n0.05\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.kfs.impl\n/name\nvalue\norg.apache.hadoop.fs.kfs.KosmosFileSystem\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.temp.dir\n/name\nvalue\n${\nhadoop\n.\ntmp\n.\ndir\n}\n/mapred/temp\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-site.xml--\nname\nmapred.tasktracker.reduce.tasks.maximum\n/name\nvalue\n4\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-site.xml--\nname\ndfs.replication\n/name\nvalue\n2\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.checkpoint.edits.dir\n/name\nvalue\n${\nfs\n.\ncheckpoint\n.\ndir\n}\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.tasktracker.tasks.sleeptime-before-sigkill\n/name\nvalue\n5000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.job.reduce.input.buffer.percent\n/name\nvalue\n0.0\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.tasktracker.indexcache.mb\n/name\nvalue\n10\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapreduce.job.split.metainfo.maxsize\n/name\nvalue\n10000000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.skip.reduce.auto.incr.proc.count\n/name\nvalue\ntrue\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nhadoop.logfile.count\n/name\nvalue\n10\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.automatic.close\n/name\nvalue\ntrue\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nio.seqfile.compress.blocksize\n/name\nvalue\n1000000\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-site.xml--\nname\ndfs.hosts.exclude\n/name\nvalue\n/etc/hadoop-0.20/conf/hosts_exclude\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.s3.block.size\n/name\nvalue\n67108864\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.tasktracker.taskmemorymanager.monitoring-interval\n/name\nvalue\n5000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.acls.enabled\n/name\nvalue\nfalse\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapreduce.jobtracker.staging.root.dir\n/name\nvalue\n${\nhadoop\n.\ntmp\n.\ndir\n}\n/mapred/staging\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.queue.names\n/name\nvalue\ndefault\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.access.time.precision\n/name\nvalue\n3600000\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.hsftp.impl\n/name\nvalue\norg.apache.hadoop.hdfs.HsftpFileSystem\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.task.tracker.http.address\n/name\nvalue\n0.0.0.0:50060\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.reduce.parallel.copies\n/name\nvalue\n5\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nio.seqfile.lazydecompress\n/name\nvalue\ntrue\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.safemode.min.datanodes\n/name\nvalue\n0\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nio.sort.mb\n/name\nvalue\n100\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nipc.client.connection.maxidletime\n/name\nvalue\n10000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.compress.map.output\n/name\nvalue\nfalse\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.task.tracker.report.address\n/name\nvalue\n127.0.0.1:0\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.healthChecker.interval\n/name\nvalue\n60000\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nipc.client.kill.max\n/name\nvalue\n10\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nipc.client.connect.max.retries\n/name\nvalue\n10\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.s3.impl\n/name\nvalue\norg.apache.hadoop.fs.s3.S3FileSystem\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.job.tracker.http.address\n/name\nvalue\n0.0.0.0:50030\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nio.file.buffer.size\n/name\nvalue\n4096\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.jobtracker.restart.recover\n/name\nvalue\nfalse\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nio.serializations\n/name\nvalue\norg.apache.hadoop.io.serializer.WritableSerialization\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.task.profile\n/name\nvalue\nfalse\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-site.xml--\nname\ndfs.datanode.handler.count\n/name\nvalue\n10\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.reduce.copy.backoff\n/name\nvalue\n300\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.replication.considerLoad\n/name\nvalue\ntrue\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\njobclient.output.filter\n/name\nvalue\nFAILED\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.namenode.delegation.token.max-lifetime\n/name\nvalue\n604800000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-site.xml--\nname\nmapred.tasktracker.map.tasks.maximum\n/name\nvalue\n4\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nio.compression.codecs\n/name\nvalue\norg.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.checkpoint.size\n/name\nvalue\n67108864\n/value\n/property\n\n\n/configuration\n\n\n\n\n\n\n\n\n\n\nPlease refer to \nOSG Hadoop debug webpage\n and \nApache Hadoop FAQ webpage\n for answers to common questions/concerns\n\n\nFUSE\n\n\nNotes on Building a FUSE Module\n\n\nIf you are running a custom kernel, then be sure to enable the \nfuse\n module with \nCONFIG_FUSE_FS=m\n in your kernel config. Building and installing a \nfuse\n kernel module for your custom kernel is beyond the scope of this document.\n\n\nRunning FUSE in Debug Mode\n\n\nTo start the FUSE mount in debug mode, you can run the FUSE mount command by hand:\n\n\nroot@host #\n  /usr/bin/hadoop-fuse-dfs  /mnt/hadoop -o rw,server\n=\nnamenode.host\n,port\n=\n9000\n,rdbuffer\n=\n131072\n,allow_other -d\n\n\n\n\n\nDebug output will be printed to stderr, which you will probably want to redirect to a file. Most FUSE-related problems can be tackled by reading through the stderr and looking for error messages.\n\n\nGridFTP\n\n\n#GridFTPStand\n\n\nStarting GridFTP in Standalone Mode\n\n\nIf you would like to test the gridftp-hdfs server in a debug standalone mode, you can run the command:\n\n\nroot@host #\n gridftp-hdfs-standalone\n\n\n\n\n\nThe standalone server runs on port 5002, handles a single GridFTP request, and will log output to stdout/stderr.\n\n\nFile Locations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComponent\n\n\nFile Type\n\n\nLocation\n\n\nNeeds editing?\n\n\n\n\n\n\nHadoop\n\n\nLog files\n\n\n/var/log/hadoop/*\n\n\nNo\n\n\n\n\n\n\nHadoop\n\n\nPID files\n\n\n/var/run/hadoop/*.pid\n\n\nNo\n\n\n\n\n\n\nHadoop\n\n\ninit scripts\n\n\n/etc/init.d/hadoop\n\n\nNo\n\n\n\n\n\n\nHadoop\n\n\ninit script config file\n\n\n/etc/sysconfig/hadoop\n\n\nYes\n\n\n\n\n\n\nHadoop\n\n\nruntime config files\n\n\n/etc/hadoop/conf/*\n\n\nMaybe\n\n\n\n\n\n\nHadoop\n\n\nSystem binaries\n\n\n/usr/bin/hadoop\n\n\nNo\n\n\n\n\n\n\nHadoop\n\n\nJARs\n\n\n/usr/lib/hadoop/*\n\n\nNo\n\n\n\n\n\n\nHadoop\n\n\nruntime config files\n\n\n/etc/hosts_exclude\n\n\nYes, must be present on namenodes\n\n\n\n\n\n\nGridFTP\n\n\nLog files\n\n\n/var/log/gridftp-auth.log\n, \n/var/log/gridftp.log\n\n\nNo\n\n\n\n\n\n\nGridFTP\n\n\ninit.d script\n\n\n/etc/init.d/globus-gridftp-server\n\n\nNo\n\n\n\n\n\n\nGridFTP\n\n\nruntime config files\n\n\n/etc/gridftp-hdfs/*\n, \n/etc/sysconfig/gridftp-hdfs\n\n\nMaybe\n\n\n\n\n\n\nGridFTP\n\n\nSystem binaries\n\n\n/usr/bin/gridftp-hdfs-standalone\n, \n/usr/sbin/globus-gridftp-server\n\n\nNo\n\n\n\n\n\n\nGridFTP\n\n\nSystem libraries\n\n\n/usr/lib64/libglobus_gridftp_server_hdfs.so*\n\n\nNo\n\n\n\n\n\n\nGridFTP\n\n\nGUMS client (called by LCMAPS) configuration\n\n\n/etc/lcmaps.db\n\n\nYes\n\n\n\n\n\n\nGridFTP\n\n\nCA certificates\n\n\n/etc/grid-security/certificates/*\n\n\nNo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nService/Process\n\n\nConfiguration File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nBeStMan2\n\n\n/etc/bestman2/conf/bestman2.rc\n\n\nMain Configuration file\n\n\n\n\n\n\n\n\n/etc/sysconfig/bestman2\n\n\nEnvironment variables used by BeStMan2\n\n\n\n\n\n\n\n\n/etc/sysconfig/bestman2lib\n\n\nEnvironment variables that store values of various client and server libraries used by BeStMan2\n\n\n\n\n\n\n\n\n/etc/bestman2/conf/*\n\n\nOther runtime configuration files\n\n\n\n\n\n\n\n\n/etc/init.d/bestman2\n\n\ninit.d startup script\n\n\n\n\n\n\n\n\n/etc/gridftp.conf\n\n\nStartup parameters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nService/Process\n\n\nLog File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nBeStMan2\n\n\n/var/log/bestman2/bestman2.log\n\n\nBeStMan2 server log and errors\n\n\n\n\n\n\n\n\n/var/log/bestman2/event.srm.log\n\n\nRecords all SRM transactions\n\n\n\n\n\n\nGridFTP\n\n\n/var/log/gridftp.log\n\n\nTransfer log\n\n\n\n\n\n\n\n\n/var/log/gridftp-auth.log\n\n\nAuthentication log\n\n\n\n\n\n\n\n\n/var/log/messages\n\n\nMain system log (look here for LCMAPS errors)\n\n\n\n\n\n\n\n\nKnown Issues\n\n\nReplicas\n\n\nYou may need to change the following line in \n/usr/share/gridftp-hdfs/gridftp-hdfs-environment\n:\n\n\nexport GRIDFTP_HDFS_REPLICAS=2\n\n\n\n\n\ncopyFromLocal java IOException\n\n\nWhen trying to copy a local file into Hadoop you may come across the following java exception:\n\n\n\n  \nShow detailed java exception\n\n    \n\n\n11/06/24 11:10:50 WARN hdfs.DFSClient: Error Recovery for block null bad datanode[0]\n\n\nnodes == null\n\n\n11/06/24 11:10:50 WARN hdfs.DFSClient: Could not get block locations. Source file\n\n\n/osg/ddd\n - Aborting...\n\n\ncopyFromLocal: java.io.IOException: File /osg/ddd could only be replicated to 0\n\n\nnodes, instead of 1\n\n\n11/06/24 11:10:50 ERROR hdfs.DFSClient: Exception closing file /osg/ddd :\n\n\norg.apache.hadoop.ipc.RemoteException: java.io.IOException: File /osg/ddd could only\n\n\nbe replicated to 0 nodes, instead of 1\n\n\n        at\n\n\norg.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1415)\n\n\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:588)\n\n\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\n\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\n\n        at\n\n\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\n\n        at java.lang.reflect.Method.invoke(Method.java:597)\n\n\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:528)\n\n\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1319)\n\n\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1315)\n\n\n        at java.security.AccessController.doPrivileged(Native Method)\n\n\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n\n\n        at\n\n\norg.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1063)\n\n\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1313)\n\n\n\n\n\n\n\n\n\n\nThis can occur if you try to install a Datanode on a machine with less than 10GB of disk space available. This can be changed by lowering the value of the following property in \n/usr/lib/hadoop-0.20/conf/hdfs-site.xml\n:\n\n\nproperty\n\n  \nname\ndfs.datanode.du.reserved\n/name\n\n  \nvalue\n10000000000\n/value\n\n\n/property\n\n\n\n\n\n\nHadoop always requires this amount of disk space to be available for non-hdfs usage on the machine.\n\n\nHow to get Help?\n\n\nIf you cannot resolve the problem, there are several ways to receive help:\n\n\n\n\nFor bug support and issues, submit a ticket to the \nGrid Operations Center\n.\n\n\nFor community support and best-effort software team support contact \n.\n\n\nFor additional community support, contact \n. Note, this is only best-effort help from OSG Software team.\n\n\n\n\nFor a full set of help options, see \nHelp Procedure\n.\n\n\nReferences\n\n\n\n\nInstructions for Upgrading from Hadoop 0.19 to Hadoop 0.20\n\n\n*\nNOTE these instructions are subject to change and the upgrade doc linked is intended to upgrade from the caltech hosted 0.19 rpms in the caltech hosted 0.20 rpms, NOT the new rpms hosted in the new OSG repos.\n\n\n\n\n\n\n\n\nBenchmarking\n\n\n\n\nUsing Hadoop as a Grid Storage Element\n, \nJournal of Physics Conference Series, 2009\n.\n\n\nHadoop Distributed File System for the Grid\n, \nIEEE Nuclear Science Symposium, 2009\n.", 
            "title": "Hadoop"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#hadoop-200-cdh4", 
            "text": "The purpose of this document is to provide Hadoop based SE administrators the information on how to prepare, install and validate the SE.", 
            "title": "Hadoop 2.0.0 (CDH4)"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#introduction", 
            "text": "Hadoop Distributed File System  (HDFS) is a scalable reliable distributed file system developed in the Apache project. It is based on map-reduce framework and design of the Google file system. The VDT distribution of Hadoop includes all components needed to operate a multi-terabyte storage site. Included are:   An  SRM interface  for grid access;  GridFTP-HDFS as transport layer; and  A  FUSE interface  for localized POSIX access.  Apache Hadoop   The OSG packaging and distribution of Hadoop is based on YUM. All components are packaged as RPMs and are available from the OSG repositories. It is also recommended that you enable  EPEL  repos.", 
            "title": "Introduction"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#requirements", 
            "text": "", 
            "title": "Requirements"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#architecture", 
            "text": "Note  There are several important components to a storage element installation. Throughout this document, it will be stated which node the relevant installation instructions apply to. It can apply to one of the following:    Namenode : You will have at least one namenode. The name node functions as the directory server and coordinator of the hadoop cluster. It houses all the meta-data for the hadoop cluster.  The namenode and secondary namenode need to have a directory that they can both access on a shared filesystem so that they can exchange filesystem checkpoints.  Secondary Namenode : This is a secondary machine that periodically merges updates to the HDFS file system back into the fsimage. This dramatically improves startup and restart times.  Datanode : You will have many datanodes. Each data node stores large blocks of files to be stored on the hadoop cluster.  Client : This is a documentation shorthand that refers to any machine with the hadoop client commands and  FUSE  mount. Any machine that needs a FUSE mount to access data in a POSIX-like fashion will need this.  GridFTP node : This is a node with  Globus GridFTP . The GridFTP server for Hadoop can be very memory-hungry, up to 500MB/transfer in the default configuration. You should plan accordingly to provision enough GridFTP servers to handle the bandwidth that your site can support.  SRM node : This node will contain the BeStMan SRM frontend for accessing the Hadoop cluster via the SRM protocol.  BeStMan2 SRM   Note that these components are not necessarily mutually exclusive. For instance, you may consider having your GridFTP server co-located on the SRM node. Alternatively, you can locate a client (or even a GridFTP node) co-located on each data node. That way, each data node also acts as an access point to the hadoop cluster.   Note  Total installation time, on an average, should not exceed 8 to 24 man-hours. If your site needs further assistance to help expedite, please email  .", 
            "title": "Architecture"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#host-and-os", 
            "text": "Hadoop will run anywhere that Java is supported (including Solaris). However, these instructions are for RedHat derivants (including Scientific Linux) because of the RPM based installation. The current supported Operating Systems supported by the OSG are Red Hat Enterprise Linux 6, 7, and variants (see  details... ).  The HDFS prerequisites are:   Minimum of 1 headnode (the namenode)  At least one node which will hold data, preferably at least 2. Most sites will have 20 to 200 datanodes.  Working Yum and RPM installation on every system.  fuse  kernel module and  fuse-libs .  Java RPM. If java isn't already installed we supply the Oracle jdk 1.6.0 rpm and it will come in as a dependency. Oracle jdk is currently the only jdk supported by OSG so we highly recommend you use the version supplied.    Note  Versions of OpenAFS less than 1.4.7 and greater than 1.4.1 create nameless groups on Linux; these groups confuse Hadoop and prevent its components from starting up successfully. If you plan to install Hadoop on a Linux OpenAFS client, make sure you're running at least OpenAFS 1.4.7.", 
            "title": "Host and OS"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#users", 
            "text": "This installation will create following users unless they are already created.     User  Comment      bestman  Used by Bestman SRM server (needs sudo access).    hdfs  Used by Hadoop to store data blocks and meta-data     For this package to function correctly, you will have to create the users needed for grid operation. Any user that can be authenticated should be created.  For grid-mapfile users, each line of the grid-mapfile is a certificate/user pair. Each user in this file should be created on the server.  For gums users, this means that each user that can be authenticated by gums should be created on the server.  Note that these users must be kept in sync with the authentication method. For instance, if new users or rules are added in gums, then new users should also be added here.", 
            "title": "Users"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#certificates", 
            "text": "Certificate  User that owns certificate  Path to certificate      Host certificate  root  /etc/grid-security/hostcert.pem     /etc/grid-security/hostkey.pem    Bestman service certificate  bestman  /etc/grid-security/bestman/bestmancert.pem     /etc/grid-security/bestman/bestmankey.pem     Instructions  to request a service certificate.  You will also need a copy of CA certificates (see below). Note that the  osg-se-hadoop-srm  and  osg-se-hadoop-gridftp  package will automatically install a certificate package but will not necessarily pick the cert package you expect. For instance, certain installs will prefer the  osg-ca-scripts  package to fulfill this requirement, which installs a set of scripts to automatically update the certificates, but does not initialize the CA certs by default (you have to run it first). For this reason, you may want to specifically install the cert package of your choice first, before installing Hadoop.", 
            "title": "Certificates"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#networking", 
            "text": "", 
            "title": "Networking"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#initializing-certificate-authority", 
            "text": "This is needed by GridFTP and SRM nodes, but it is recommended for all nodes in the cluster. Enable  fetch-crl", 
            "title": "Initializing Certificate Authority"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#installation", 
            "text": "Installation depends on the node you are installing:", 
            "title": "Installation"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#namenode-installation", 
            "text": "root@host #  yum install osg-se-hadoop-namenode", 
            "title": "Namenode Installation"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#secondary-namenode-installation", 
            "text": "root@host #  yum install osg-se-hadoop-secondarynamenode", 
            "title": "Secondary Namenode Installation"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#datanode-installation", 
            "text": "root@host #  yum install osg-se-hadoop-datanode", 
            "title": "Datanode Installation"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#clientfuse-installation", 
            "text": "root@host #  yum install osg-se-hadoop-client", 
            "title": "Client/FUSE Installation"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#standalone-gridftp-node-installation", 
            "text": "root@host #  yum install osg-se-hadoop-gridftp  If you are using GUMS authorization, the follow rpms need to be installed as well:  root@host #  yum install lcmaps-plugins-gums-client root@host #  yum install lcmaps-plugins-basic", 
            "title": "Standalone Gridftp Node Installation"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#srm-node-installation", 
            "text": "root@host #  yum install osg-se-hadoop-srm   Note  If you are using a single system to host the SRM software and the gridftp node, you'll also need to install the  osg-se-hadoop-gridftp  rpm as well.", 
            "title": "SRM Node Installation"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#configuration", 
            "text": "", 
            "title": "Configuration"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#hadoop-configuration", 
            "text": "Note  Needed by: Hadoop namenode, Hadoop datanodes, Hadoop client, GridFTP, SRM   Hadoop configuration is needed by every node in the hadoop cluster. However, in most cases, you can do the configuration once and copy it to all nodes in the cluster (possibly using your favorite configuration management tool). Special configuration for various special components is given in the below sections.  Hadoop configuration is stored in  /etc/hadoop/conf . However, by default, these files are mostly blank. OSG provides a sample configuration in  /etc/hadoop/conf.osg  with most common values filled in. You will need to copy these into  /etc/hadoop/conf  before they become active. Please let us know if there are any common values that should be added/changed across the whole grid. You will likely need to modify  hdfs-site.xml  and  core-site.xml . Review all the settings in these files, but listed below are common settings to modify:             File  Setting  Example  Comments    core-site.xml  fs.default.name  hdfs://namenode.domain.tld.:9000  This is the address of the namenode    core-site.xml  hadoop.tmp.dir  /data/scratch  Scratch temp directory used by Hadoop    core-site.xml  hadoop.log.dir  /var/log/hadoop-hdfs  Log directory used by Hadoop    core-site.xml  dfs.umaskmode  002  umask for permissions used by default    hdfs-site.xml  dfs.block.size  134217728  Block size: 128MB by default    hdfs-site.xml  dfs.replication  2  Default replication factor. Generally the same as dfs.replication.min/max    hdfs-site.xml  dfs.datanode.du.reserved  100000000  How much free space hadoop will reserve for non-Hadoop usage    hdfs-site.xml  dfs.datanode.handler.count  20  Number of server threads for datanodes. Increase if you have many more client connections    hdfs-site.xml  dfs.namenode.handler.count  40  Number of server threads for namenodes. Increase if you need more connections    hdfs-site.xml  dfs.http.address  namenode.domain.tld.:50070  Web address for dfs health monitoring page     See  http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml  for more parameters to configure.   Note  Namenodes must have a  /etc/hosts_exclude  present", 
            "title": "Hadoop Configuration"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#special-namenode-instructions-for-brand-new-installs", 
            "text": "If this is a new installation ( and only if this is a brand new installation ), you should run the following command as the  hdfs  user. (Otherwise, be sure to  chown  your storage directory to hdfs after running):  hadoop namenode -format   This will initialize the storage directory on your namenode", 
            "title": "Special namenode instructions for brand new installs"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#fuse-client-configuration", 
            "text": "Note  Needed by: Hadoop client and SRM node. Recommended but not neccessary for GridFTP nodes.   A FUSE mount is required on any node that you would like to use standard POSIX-like commands on the Hadoop filesystem. FUSE (or \"file system in user space\") is a way to access Hadoop using typical UNIX directory commands (ie POSIX-like access). Note that not all advanced functions of a full POSIX-compliant file system are necessarily available.  FUSE is typically installed as part of this installation, but, if you are running a customized or non-standard system, make sure that the fuse kernel module is installed and loaded with  modprobe fuse .  You can add the FUSE to be mounted at boot time by adding the following line to  /etc/fstab :  hadoop-fuse-dfs#  /mnt/hadoop  fuse server= namenode.host ,port=9000,rdbuffer=131072,allow_other 0 0  Be sure to change the  /mnt/hadoop  mount point and  namenode.host  to match your local configuration. To match the help documents, we recommend using  /mnt/hadoop  as your mountpoint.  Once your  /etc/fstab  is updated, to mount FUSE run:  root@host #  mkdir /mnt/hadoop root@host #  mount /mnt/hadoop  When mounting the HDFS FUSE mount, you will see the following harmless warnings printed to the screen:  #  mount /mnt/hadoop INFO fuse_options.c:162 Adding FUSE arg /mnt/hadoop  INFO fuse_options.c:110 Ignoring option allow_other   If you have troubles mounting FUSE refer to  Running FUSE in Debug Mode  in the Troubleshooting section.", 
            "title": "FUSE Client Configuration"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#creating-vo-and-user-areas", 
            "text": "Note  Grid Users are needed by GridFTP and SRM nodes. VO areas are common to all nodes.   For this package to function correctly, you will have to create the users needed for grid operation. Any user that can be authenticated should be created.  For grid-mapfile users, each line of the grid-mapfile is a certificate/user pair. Each user in this file should be created on the server.  For gums users, this means that each user that can be authenticated by gums should be created on the server.  Note that these users must be kept in sync with the authentication method. For instance, if new users or rules are added in gums, then new users should also be added here.  Prior to starting basic day-to-day operations, it is important to create dedicated areas for each VO and/or user. This is similar to user management in simple UNIX filesystems. Create (and maintain) usernames and groups with UIDs and GIDs on  all nodes . These are maintained in basic system files such as  /etc/passwd  and  /etc/group .   Note  In the examples below It is assumed a FUSE mount is set to  /mnt/hadoop . As an alternative  hadoop fs  commands could have been used.   For clean HDFS operations and filesystem management:  (a) Create top-level VO subdirectories under  /mnt/hadoop .  Example:  root@host #  mkdir /mnt/hadoop/cms root@host #  mkdir /mnt/hadoop/dzero root@host #  mkdir /mnt/hadoop/sbgrid root@host #  mkdir /mnt/hadoop/fermigrid root@host #  mkdir /mnt/hadoop/cmstest root@host #  mkdir /mnt/hadoop/osg  (b) Create individual top-level user areas, under each VO area, as needed.  root@host #  mkdir -p /mnt/hadoop/cms/store/user/tanyalevshina root@host #  mkdir -p /mnt/hadoop/cms/store/user/michaelthomas root@host #  mkdir -p /mnt/hadoop/cms/store/user/brianbockelman root@host #  mkdir -p /mnt/hadoop/cms/store/user/douglasstrain root@host #  mkdir -p /mnt/hadoop/cms/store/user/abhisheksinghrana  (c) Adjust username:group ownership of each area.  root@host #  chown -R cms:cms /mnt/hadoop/cms root@host #  chown -R sam:sam /mnt/hadoop/dzero root@host #  chown -R michaelthomas:cms /mnt/hadoop/cms/store/user/michaelthomas", 
            "title": "Creating VO and User Areas"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#gridftp-configuration", 
            "text": "gridftp-hdfs reads the Hadoop configuration file to learn how to talk to Hadoop. By now, you should have followed the instruction for installing hadoop as detailed in the previous section as well as created the proper users/directories.  The default settings in  /etc/gridftp.conf  along with  /etc/gridftp.d/gridftp-hdfs.conf  are used by the init.d script and should be ok for most installations. The file  /etc/gridftp-hdfs/gridftp-debug.conf  is used by  /usr/bin/gridftp-hdfs-standalone  for starting up the GridFTP server in a testing mode. Any additional config files under  /etc/gridftp.d  will be used for both the init.d and standalone GridFTP server.  /etc/sysconfig/gridftp-hdfs  contains additional site-specific environment variables that are used by the gridftp-hdfs dsi module in both the init.d and standalone GridFTP server. Some of the environment variables that can be used in  /etc/sysconfig/gridftp-hdfs  include:            Option Name  Needs Editing?  Suggested value    GRIDFTP_HDFS_REPLICA_MAP  No  File containing a list of paths and replica values for setting the default # of replicas for specific file paths    GRIDFTP_BUFFER_COUNT  No  The number of 1MB memory buffers used to reorder data streams before writing them to Hadoop    GRIDFTP_FILE_BUFFER_COUNT  No  The number of 1MB file-based buffers used to reorder data streams before writing them to Hadoop    GRIDFTP_SYSLOG  No  Set this to 1 in case if you want to send transfer activity data to syslog (only used for the HadoopViz application)    GRIDFTP_HDFS_MOUNT_POINT  Maybe  The location of the FUSE mount point used during the Hadoop installation. Defaults to /mnt/hadoop. This is needed so that gridftp-hdfs can convert fuse paths on the incoming URL to native Hadoop paths.  Note:  this does not imply you need FUSE mounted on GridFTP nodes!    GRIDFTP_LOAD_LIMIT  No  GridFTP will refuse to start new transfers if the load on the GridFTP host is higher than this number; defaults to 20.    TMPDIR  Maybe  The temp directory where the file-based buffers are stored. Defaults to /tmp.     /etc/sysconfig/gridftp-hdfs  is also a good place to increase per-process resource limits. For example, many installations will require more than the default number of open files ( ulimit -n ).  Lastly, you will need to configure an authentication mechanism for GridFTP.", 
            "title": "GridFTP Configuration"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#configuring-authentication", 
            "text": "For information on how to configure authentication for your GridFTP installation, please refer to the  configuring authentication section of the GridFTP guide .", 
            "title": "Configuring authentication"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#gridftp-gratia-transfer-probe-configuration", 
            "text": "Note  Needed by GridFTP node only.   The Gratia probe requires the file  user-vo-map  to exist and be up to date. This file is created and updated by the  gums-client  package that comes in as a dependency of  osg-se-hadoop-gridftp  or  osg-gridftp-hdfs . Assuming you installed GridFTP using the  osg-se-hadoop-gridftp  rpm, the Gratia Transfer Probe will already be installed.  Here are the most relevant file and directory locations:            Purpose  Needs Editing?  Location    Probe Configuration  Yes  /etc/gratia/gridftp-transfer/ProbeConfig    Probe Executables  No  /usr/share/gratia/gridftp-transfer    Log files  No  /var/log/gratia    Temporary files  No  /var/lib/gratia/tmp    Gums configuration  Yes  /etc/gums/gums-client.properties     The RPM installs the Gratia probe into the system crontab, but does not configure it. The configuration of the probe is controlled by the file  /etc/gratia/gridftp-transfer/ProbeConfig  This is usually one XML node spread over multiple lines. Note that comments (#) have no effect on this file. You will need to edit the following:            Attribute  Needs Editing  Value    ProbeName  Maybe  This should be set to \"gridftp-transfer: \", where   is the fully-qualified domain name of your gridftp host.    CollectorHost  Maybe  Set to the hostname and port of the central collector. By default it sends to the OSG collector. See below.    SiteName  Yes  Set to the resource group name of your site as registered in OIM.    GridftpLogDir  Yes  Set to /var/log, or wherever your current gridftp logs are located    Grid  Maybe  Set to \"ITB\" if this is a test resource; otherwise, leave as OSG.    UserVOMapFile  No  This should be set to /var/lib/osg/user-vo-map; see below for information about this file.    SuppressUnknownVORecords  Maybe  Set to 1 to suppress any records that can't be matched to a VO; 0 is strongly recommended.    SuppressNoDNRecords  Maybe  Set to 1 to suppress records that can't be matched to a DN; 0 is strongly recommended.    EnableProbe  Yes  Set to 1 to enable the probe.", 
            "title": "GridFTP Gratia Transfer Probe Configuration"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#selecting-a-collector-host", 
            "text": "The collector is the central server which logs the GridFTP transfers into a database. There are usually three options:   OSG Transfer Collector : This is the primary collector for transfers in the OSG. Use CollectorHost=\"gratia-osg-prod.opensciencegrid.org:80\".  OSG-ITB Transfer Collector : This is the test collector for transfers in the OSG. Use CollectorHost=\" gratia-osg-itb.opensciencegrid.org:80\".  Site local collector : If your site has set up its own collector, then your admin will be able to give you an endpoint to use. Typically, this is along the lines of CollectorHost=\"collector.example.com:8880\".   Note:  if you are installing on an itb site, use  gratia-osg-itb.opensciencegrid.org  instead of \"gratia-osg-transfer.opensciencegrid.org* above.", 
            "title": "Selecting a collector host"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#using-gums-authorization-mode", 
            "text": "The  user-vo-map  file is a simple, space-separated format that contains 2 columns; the first is a unix username and the second is the VO which that username correspond to. In order to create it you need to configure the gums client.  The primary configuration file for the gums-client utilities is located in  /etc/gums/gums-client.properties . The two properties that you must change are:            Attribute  Needs Editing  Value    gums.location  Yes  This should be set to the admin URL for your gums server, usually of the form gums.location=https://GUMS_HOSTNAME:8443/gums/services/GUMSAdmin    gums.authz  Yes  This should be set to the authorization interface URL for your gums server, usually of the form gums.authz=https://GUMS_HOSTNAME:8443/gums/services/GUMSXACMLAuthorizationServicePort     After the gums client is configured to generate the file run the following once by hand:  root@host #  gums-host-cron  user-vo-map  should be created in the following location:  /var/lib/osg/user-vo-map  To have cron regularly update this file start the following service:  root@host #  service gums-client-cron start  Make sure the  UserVOMapFile  field is set to this location in  /etc/gratia/gridftp-transfer/ProbeConfig  Without  user-vo-map  , all gridftp transfers will show up as belonging to the VO \"Unknown\".", 
            "title": "Using GUMS authorization mode"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#using-gridmap-based-authorization-mode", 
            "text": "Note: If you are using this mode for authorization, make sure the files /etc/grid-security/gsi-authz.conf and /etc/grid-security/prima-authz.conf do not exist.  In order to enable generation of grid-mapfile and osg-user-vo-map.txt by using the edg-mkgridmap cron process to get information form VOMS servers do the following:  edg-mkgridmap    If you have not installed this package, you will need to run  yum install edg-mkgridmap  first.", 
            "title": "Using Gridmap based authorization mode"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#validation", 
            "text": "Run the Gratia probe once by hand to check for functionality:  root@host #  /usr/share/gratia/gridftp-transfer/GridftpTransferProbeDriver  Look for any abnormal termination and report it if it is a non-trivial site issue. Look in the log files in  /var/log/gratia/ date .log  and make sure there are no error messages printed.", 
            "title": "Validation"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#bestman-configuration", 
            "text": "See the  BeStMaN documentation  for details.", 
            "title": "BeStMan Configuration"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#bestmanhadoop-specific-configuration", 
            "text": "BeStMan2 SRM uses the Hadoop FUSE mount to perform namespace operations, such as mkdir, rm, and ls. As per the Hadoop install instructions, edit  /etc/sysconfig/hadoop  and run  service hadoop-firstboot start . It is  not  necessary (or even recommended) to start any hadoop services with  service hadoop start .  Make sure that you modify  localPathListAllowed  to use the Hadoop mount in  /etc/bestman2/conf/bestman2.rc .", 
            "title": "BeStManHadoop-specific configuration"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#modify-etcsudoers", 
            "text": "BeStman requires the \"sudo\" command in order to write information as the proper user. You will need to give the bestman user the proper permissions to run these commands.  Modify  /etc/sudoers  and comment the following line.  #Defaults    requiretty  Then add the following lines at the end of the  /etc/sudoers  file.  Cmnd_Alias SRM_CMD = /bin/rm, /bin/mkdir, /bin/rmdir, /bin/mv, /bin/cp, /bin/ls\nRunas_Alias SRM_USR = ALL, !root\nbestman   ALL=(SRM_USR) NOPASSWD: SRM_CMD", 
            "title": "Modify /etc/sudoers"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#copy-certificates-to-bestman-location", 
            "text": "BeStMan2 is preconfigured to look for the  host  certificate and key in  /etc/grid-security/bestman/bestman*.pem . Either, these files  must  exist and be  owned  by the  bestman  user, or you must change the settings in  bestman2.rc . Note that you must use host certificates here or lcg-utils may experience issues.  BeStMan requires a certificate pair to function. In order to use lcg-utils, this must be a host certificate (rather than a service certificate). The following shows how to copy your certificates  cp /etc/grid-security/hostkey.pem /etc/grid-security/bestman/bestmankey.pem  cp /etc/grid-security/hostcert.pem /etc/grid-security/bestman/bestmancert.pem  chown -R bestman:bestman /etc/grid-security/bestman/   Then modify  CertFileName ,  KeyFileName  in  /etc/bestman2/conf/bestman2.rc .", 
            "title": "Copy certificates to bestman location"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#hadoop-storage-probe-configuration", 
            "text": "Note  This is only needed by the Hadoop Namenode   Here are the most relevant file and directory locations:            Purpose  Needs Editing?  Location    Probe Configuration  Yes  /etc/gratia/hadoop-storage/ProbeConfig    Probe Executable  No  /usr/share/gratia/hadoop-storage/hadoop_storage_probe    Log files  No  /var/log/gratia    Temporary files  No  /var/lib/gratia/tmp     The RPM installs the Gratia probe into the system crontab, but does not configure it. The configuration of the probe is controlled by two files  /etc/gratia/hadoop-storage/ProbeConfig\n/etc/gratia/hadoop-storage/storage.cfg", 
            "title": "Hadoop Storage Probe Configuration"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#probeconfig", 
            "text": "This is usually one XML node spread over multiple lines. Note that comments (#) have no effect on this file. You will need to edit the following:            Attribute  Needs Editing  Value    CollectorHost  Maybe  Set to the hostname and port of the central collector. By default it sends to the OSG collector. You probably do not want to change it.    SiteName  Yes  Set to the resource group name of your SE as registered in OIM.    Grid  Maybe  Set to \"ITB\" if this is a test resource; otherwise, leave as OSG.    EnableProbe  Yes  Set to 1 to enable the probe.", 
            "title": "ProbeConfig"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#storagecfg", 
            "text": "This file controls which paths in HDFS should be monitored. This is in the Windows INI format.  Note: for the current version of the storage.cfg, there is an error, and you may need to delete the \"probe/\" subdirectory for the ProbeConfig location  ProbeConfig = /etc/gratia/ probe/ hadoop-storage/ProbeConfig  For each logical \"area\" (arbitrarily defined by you), specify both a given name and a list of paths that belong to that area. Unix globs are accepted.  To configure an area named \"CMS /store\" that monitors the space usage in the paths /user/cms/store/*, one would add the following to the storage.cfg file.  [Area CMS /store]  Name   =   CMS /store  Path   =   /user/cms/store/*  Trim   =   /user/cms   For each such area, add a section to your configuration file.", 
            "title": "storage.cfg"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#example-file", 
            "text": "Below is a configuration file that includes three distinct areas. Note that you shouldn't have to touch the [Gratia] section if you edited the ProbeConfig above:  [Gratia]  gratia_location   =   /opt/vdt/gratia  ProbeConfig   =   %(gratia_location)s/probe/hadoop-storage/ProbeConfig  [Area /store]  Name   =   CMS /store  Path   =   /store/*  [Area /store/user]  Name   =   CMS /store/user  Path   =   /store/user/*  [Area /user]  Name   =   Hadoop /user  Path   =   /user/*   * NOTE These lines in the [gratia] section are wrong and need to be changed to the following by hand for now until the rpm is updated:  gratia_location = /etc/gratia\nProbeConfig = %(gratia_location)s/hadoop-storage/ProbeConfig", 
            "title": "Example file"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#running-services", 
            "text": "Namenode:  # Starting namenode service hadoop-hdfs-namenode start  # Stopping namenode service hadoop-hdfs-namenode stop   Secondary Namenode:  # Starting secondary namenode service hadoop-hdfs-secondarynamenode start  # Stopping secondary namenode service hadoop-hdfs-secondarynamenode stop   Datanode:  # Starting namenode service hadoop-hdfs-datanode start  # Stopping namenode service hadoop-hdfs-datanode stop   GridFTP:  root@host #  service globus-gridftp-server start  To start Gridftp automatically at boot time  root@host #  chkconfig globus-gridftp-server on  Stopping GridFTP:  root@host #  service globus-gridftp-server stop  root@host #  service bestman2 start  To start Bestman automatically at boot time  root@host #  chkconfig bestman2 on", 
            "title": "Running Services"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#validation_1", 
            "text": "The first thing you may want to do after installing and starting your  Namenode  is to verify that the web interface works. In your web browser go to:  http:// namenode.hostname :50070/dfshealth.jsp  Get familiar with Hadoop commands. Run hadoop with no arguments to see the list of commands.  \n   Show detailed ouput \n     user$ hadoop  Usage: hadoop [--config confdir] COMMAND  where COMMAND is one of:    namenode -format     format the DFS filesystem    secondarynamenode    run the DFS secondary namenode    namenode             run the DFS namenode    datanode             run a DFS datanode    dfsadmin             run a DFS admin client    mradmin              run a Map-Reduce admin client    fsck                 run a DFS filesystem checking utility    fs                   run a generic filesystem user client    balancer             run a cluster balancing utility    fetchdt              fetch a delegation token from the NameNode    jobtracker           run the MapReduce job Tracker node    pipes                run a Pipes job    tasktracker          run a MapReduce task Tracker node    job                  manipulate MapReduce jobs    queue                get information regarding JobQueues    version              print the version    jar  jar             run a jar file    distcp  srcurl   desturl  copy file or directories recursively    archive -archiveName NAME -p  parent path   src *  dest  create a hadoop archive    oiv                  apply the offline fsimage viewer to an fsimage    classpath            prints the class path needed to get the                         Hadoop jar and the required libraries    daemonlog            get/set the log level for each daemon   or    CLASSNAME            run the class named CLASSNAME  Most commands print help when invoked w/o parameters.     For a list of supported filesystem commands:  \n   Show 'hadoop fs' detailed ouput \n     user$ hadoop fs  Usage: java FsShell             [-ls  path ]             [-lsr  path ]             [-df [ path ]]             [-du  path ]             [-dus  path ]             [-count[-q]  path ]             [-mv  src   dst ]             [-cp  src   dst ]             [-rm [-skipTrash]  path ]             [-rmr [-skipTrash]  path ]             [-expunge]             [-put  localsrc  ...  dst ]             [-copyFromLocal  localsrc  ...  dst ]             [-moveFromLocal  localsrc  ...  dst ]             [-get [-ignoreCrc] [-crc]  src   localdst ]             [-getmerge  src   localdst  [addnl]]             [-cat  src ]             [-text  src ]             [-copyToLocal [-ignoreCrc] [-crc]  src   localdst ]             [-moveToLocal [-crc]  src   localdst ]             [-mkdir  path ]             [-setrep [-R] [-w]  rep   path/file ]             [-touchz  path ]             [-test -[ezd]  path ]             [-stat [format]  path ]             [-tail [-f]  file ]             [-chmod [-R]  MODE[,MODE]... | OCTALMODE  PATH...]             [-chown [-R] [OWNER][:[GROUP]] PATH...]             [-chgrp [-R] GROUP PATH...]             [-help [cmd]]  Generic options supported are  -conf  configuration file      specify an application configuration file  -D  property=value             use value for given property  -fs  local|namenode:port       specify a namenode  -jt  local|jobtracker:port     specify a job tracker  -files  comma separated list of files     specify comma separated files to be copied to the map reduce cluster  -libjars  comma separated list of jars     specify comma separated jar files to include in the classpath.  -archives  comma separated list of archives     specify comma separated archives to be unarchived on the compute machines.  The general command line syntax is  bin/hadoop command [genericOptions] [commandOptions]     An online guide is also available at  Apache Hadoop commands manual . You can use Hadoop commands to perform filesystem operations with more consistency.  Example, to look into the internal hadoop namespace:  user$ hadoop fs -ls /  Found 1 items  drwxrwxr-x   - engage engage          0 2011-07-25 06:32 /engage   Example, to adjust ownership of filesystem areas (there is usually no need to specify the mount itself  /mnt/hadoop  in Hadoop commands):  root@host #  hadoop fs -chown -R engage:engage /engage  Example, compare  hadoop fs  command vs. using FUSE mount:  user$ hadoop fs -ls /engage  Found 3 items  -rw-rw-r--   2 engage engage  733669376 2011-06-15 16:55 /engage/CentOS-5.6-x86_64-LiveCD.iso  -rw-rw-r--   2 engage engage  215387183 2011-06-15 16:28 /engage/condor-7.6.1-x86_rhap_5-stripped.tar.gz  -rw-rw-r--   2 engage engage    9259360 2011-06-15 16:32 /engage/glideinWMS_v2_5_1.tgz  user$ ls -l /mnt/hadoop/engage  total 935855  -rw-rw-r-- 1 engage engage 733669376 Jun 15 16:55 CentOS-5.6-x86_64-LiveCD.iso  -rw-rw-r-- 1 engage engage 215387183 Jun 15 16:28 condor-7.6.1-x86_rhap_5-stripped.tar.gz  -rw-rw-r-- 1 engage engage   9259360 Jun 15 16:32 glideinWMS_v2_5_1.tgz", 
            "title": "Validation"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#gridftp-validation", 
            "text": "Note  The commands used to verify GridFTP below assume you have access to a node where you can first generate a valid proxy using  voms-proxy-init  or  grid-proxy-init . Obtaining grid credentials is beyond the scope of this document.   user$ globus-url-copy file:///home/users/jdost/test.txt gsiftp://devg-7.t2.ucsd.edu:2811/mnt/hadoop/engage/test.txt   If you are having troubles running GridFTP refer to  Starting GridFTP in Standalone Mode  in the Troubleshooting section.", 
            "title": "GridFTP Validation"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#bestman-validation", 
            "text": "There are three ways of validating BeStMan: * SrmTester: BeStMan testing application * InstallRSV: RSV monitoring tools * BestMan client tools  See the relevant pages for the first two options. This section will detail some basic client commands to validate. You will need grid credentials in order to test using client tools.  srm-ping srm://BeStMan_host:secured_http_port/srm/v2/server  srm-copy file:////tmp/test1  srm://BeStMan_host:secured_http_port/srm/v2/server\\?SFN=/mnt/hadoop/VONAME/test_1   The  srm-ping  tool should return a valid mapping  gumsIDMapped  that is not null", 
            "title": "BeStMan Validation"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#installing-hadoop-storage-reports-optional", 
            "text": "* NOTE the GratiaReporting rpm has not yet been migrated to the new osg repos and this section is subject to change. Please skip this section until this warning goes away or request Nebraska to host your reports   Note  The Hadoop Storage Reports may be installed on any node that has access to a local Gratia Collector   The Hadoop storage reports provides a daily report on the status and usage of your SE. This serves as a handy tool for both site administrators and site executives. An example report is copied at the end of this guide.  \n   Show Hadoop Storage Reports information", 
            "title": "Installing Hadoop Storage Reports (Optional)"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#prerequisites", 
            "text": "A working HDFS installation  A local  Gratia Collector  installed  A Hadoop Storage Probe installed and configured to point to the local Gratia Collector", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#installation_1", 
            "text": "[root@client ~]$  yum install GratiaReporting  Updates can be installed with:  [root@client ~]$  yum upgrade GratiaReporting", 
            "title": "Installation"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#configuration_1", 
            "text": "This RPM uses Linux-standard file locations. Here are the most relevant file and directory locations:            Purpose  Needs Editing?  Location    Report Configuration  Yes  /etc/gratia_reporting    Cron template  Yes  /etc/gratia_reporting/gratia_reporting/gratia_reporting.cron (move to /etc/cron.d)    Logging Configuration  No  /etc/gratia_reporting/logging.cfg    Log files  No  /var/log/gratia_reporting.log", 
            "title": "Configuration"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#configuration-file", 
            "text": "Copy the file  /etc/gratia_reporting/reporting.cfg  to a new filename in  /etc/gratia_reporting  (for example,  /etc/gratia_reporting/reporting_cms.cfg ). You will do this once for every report you want to send out.            Attribute  Needs Editing  Value    SiteName  Yes  Set to the resource group name of your SE as registered in OIM.    database  Maybe  Set to the database section containing the login details for your Gratia Collector (a few, non-functioning examples sections are included). Installing a Gratia Collector is  covered here , but ask around on osg-hadoop: Nebraska will usually run these reports for you if requested.    toNames  Yes  Python list for the \"to names\" for the report email.    toEmails  Yes  Python list for the \"to emails\" for the report email.    smtphost  Maybe  Hostname of a SMTP server that accepts email from this host.    fromName  Maybe  Set to the \"from name\" for the report email.    fromEmail  Maybe  Set to the \"from email\" for the report email.", 
            "title": "Configuration file"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#cron", 
            "text": "Copy the file  /etc/gratia_reporting/gratia_reporting.cron  to  /etc/cron.d . There is one line per report; comment out all except the hadoop report. It is the line containing  -n hadoop . Update the line to point at your new configuration file.   \n   Expand sample report from the Nebraska HDFS instance \n      ============================================================\n  The Hadoop Chronicle | 85 % | 2009-09-25\n============================================================\n\n--------------------\n| Global Storage   |\n-----------------------------------------------------\n|                  |  Today  | Yesterday | One Week |\n-----------------------------------------------------\n| Total Space (GB) | 311,470 |   357,818 |  368,711 |\n| Free Space (GB)  |  47,304 |    93,719 |  128,391 |\n| Used Space (GB)  | 264,166 |   264,100 |  240,320 |\n| Used Percentage  |     85% |       74% |      65% |\n-----------------------------------------------------\n--------------\n| CMS /store |\n-------------------------------------------------------------------------------------------------------------------------------------\n|           Path           | Size(GB) | 1 Day Change | 7 Day Change | Remaining | # Files | 1 Day Change | 7 Day Change | Remaining |\n-------------------------------------------------------------------------------------------------------------------------------------\n| /store/user              |      771 |            0 | UNKNOWN      | NO QUOTA  |   4,859 |            0 | UNKNOWN      | NO QUOTA  |\n| /store/mc                |   95,865 |         -353 | UNKNOWN      | NO QUOTA  |  86,830 |         -171 | UNKNOWN      | NO QUOTA  |\n| /store/test              |        0 |            0 | UNKNOWN      | NO QUOTA  |     569 |           25 | UNKNOWN      | NO QUOTA  |\n| /store/results           |      237 |            0 | UNKNOWN      | NO QUOTA  |     198 |            0 | UNKNOWN      | NO QUOTA  |\n| /store/phedex_monarctest |      729 |            0 | UNKNOWN      | NO QUOTA  |     257 |            0 | UNKNOWN      | NO QUOTA  |\n| /store/unmerged          |    3,681 |            3 | UNKNOWN      | NO QUOTA  |  35,687 |           23 | UNKNOWN      | NO QUOTA  |\n| /store/CSA07             |        0 |            0 | UNKNOWN      | NO QUOTA  |       0 |            0 | UNKNOWN      | NO QUOTA  |\n| /store/data              |        0 |            0 | UNKNOWN      | NO QUOTA  |       0 |            0 | UNKNOWN      | NO QUOTA  |\n| /store/PhEDEx_LoadTest07 |        0 |          -21 | UNKNOWN      | NO QUOTA  |       1 |          -22 | UNKNOWN      | NO QUOTA  |\n-------------------------------------------------------------------------------------------------------------------------------------\n\n-------------------\n| CMS /store/user |\n----------------------------------------------------------------------------------------------------------------------------------\n|          Path         | Size(GB) | 1 Day Change | 7 Day Change | Remaining | # Files | 1 Day Change | 7 Day Change | Remaining |\n----------------------------------------------------------------------------------------------------------------------------------\n| /store/user/hpi       |        0 |            0 | UNKNOWN      |     1,099 |      15 |            0 | UNKNOWN      |     9,985 |\n| /store/user/gattebury |        0 |            0 | UNKNOWN      |     1,100 |       1 |            0 | UNKNOWN      |     9,999 |\n| /store/user/mkirn     |        0 |            0 | UNKNOWN      |     1,100 |       3 |            0 | UNKNOWN      |     9,997 |\n| /store/user/spadhi    |       12 |            0 | UNKNOWN      |     1,062 |   1,114 |            0 | UNKNOWN      |     8,886 |\n| /store/user/creed     |        0 |            0 | UNKNOWN      |     1,100 |       0 |            0 | UNKNOWN      |    10,000 |\n| /store/user/rossman   |        0 |            0 | UNKNOWN      |     1,099 |       5 |            0 | UNKNOWN      |     9,995 |\n| /store/user/eluiggi   |        0 |            0 | UNKNOWN      |     1,099 |       6 |            0 | UNKNOWN      |     9,994 |\n| /store/user/ewv       |        7 |            0 | UNKNOWN      |     1,081 |     284 |            0 | UNKNOWN      |     9,716 |\n| /store/user/test      |        0 |            0 | UNKNOWN      | NO QUOTA  |     167 |            0 | UNKNOWN      |     9,833 |\n| /store/user/schiefer  |      751 |            0 | UNKNOWN      |     1,044 |   3,264 |            0 | UNKNOWN      |     6,736 |\n----------------------------------------------------------------------------------------------------------------------------------\n\n----------------\n| Hadoop /user |\n----------------------------------------------------------------------------------------------------------------------------------\n|       Path      | Size(GB) | 1 Day Change | 7 Day Change | Remaining | # Files | 1 Day Change | 7 Day Change |    Remaining    |\n----------------------------------------------------------------------------------------------------------------------------------\n| /user/djbender  |        0 |            0 | UNKNOWN      | NO QUOTA  |       1 |            0 | UNKNOWN      | NO QUOTA        |\n| /user/lhcb      |        0 |            0 | UNKNOWN      |        54 |       0 |            0 | UNKNOWN      | NO QUOTA        |\n| /user/dzero     |      897 |            0 | UNKNOWN      |       347 |  89,376 |            0 | UNKNOWN      |         410,624 |\n| /user/bloom     |      454 |            0 | UNKNOWN      | NO QUOTA  |   1,410 |            0 | UNKNOWN      | NO QUOTA        |\n| /user/uscms01   |  101,384 |         -362 | UNKNOWN      | NO QUOTA  | 129,739 |         -141 | UNKNOWN      | NO QUOTA        |\n| /user/cdf       |        0 |            0 | UNKNOWN      | NO QUOTA  |       6 |            0 | UNKNOWN      | 536,870,911,994 |\n| /user/osg       |        1 |            0 | UNKNOWN      | NO QUOTA  |       3 |            0 | UNKNOWN      |   5,368,709,117 |\n| /user/dweitzel  |       20 |            0 | UNKNOWN      | NO QUOTA  |   2,282 |            0 | UNKNOWN      | NO QUOTA        |\n| /user/gattebury |        5 |            0 | UNKNOWN      | NO QUOTA  |  10,002 |            0 | UNKNOWN      | NO QUOTA        |\n| /user/brian     |       72 |            0 | UNKNOWN      | NO QUOTA  |   2,697 |            0 | UNKNOWN      | NO QUOTA        |\n| /user/usatlas   |        0 |            0 | UNKNOWN      | NO QUOTA  |       0 |            0 | UNKNOWN      | NO QUOTA        |\n| /user/powers    |        1 |            1 | UNKNOWN      | NO QUOTA  |     211 |          211 | UNKNOWN      | NO QUOTA        |\n| /user/ifisk     |        0 |            0 | UNKNOWN      | NO QUOTA  |       1 |            0 | UNKNOWN      | NO QUOTA        |\n| /user/gpn       |      261 |           -5 | UNKNOWN      |     1,360 |   3,805 |            1 | UNKNOWN      |         996,195 |\n| /user/engage    |      461 |          367 | UNKNOWN      | NO QUOTA  |      16 |           13 | UNKNOWN      |         999,984 |\n| /user/clundst   |        0 |            0 | UNKNOWN      | NO QUOTA  |       6 |            0 | UNKNOWN      | NO QUOTA        |\n| /user/che       |        0 |            0 | UNKNOWN      | NO QUOTA  |      13 |            0 | UNKNOWN      | NO QUOTA        |\n| /user/store     |        0 |            0 | UNKNOWN      | NO QUOTA  |       0 |            0 | UNKNOWN      | NO QUOTA        |\n| /user/dteam     |        0 |            0 | UNKNOWN      |        53 |      18 |            0 | UNKNOWN      | NO QUOTA        |\n| /user/root      |        0 |            0 | UNKNOWN      | NO QUOTA  |       1 |            0 | UNKNOWN      | NO QUOTA        |\n----------------------------------------------------------------------------------------------------------------------------------\n\n-------------\n| FSCK Data |\n-------------\n Total size:    114592906796932 B (Total open files size: 38923141120 B)\n Total dirs:    41293\n Total files:   295431 (Files currently being written: 38)\n Total blocks (validated):  1356788 (avg. block size 84458962 B) (Total open file blocks (not validated): 297)\n Minimally replicated blocks:   1356788 (100.0 %)\n Over-replicated blocks:    1 (7.370348E-5 %)\n Under-replicated blocks:   0 (0.0 %)\n Mis-replicated blocks:     0 (0.0 %)\n Default replication factor:    3\n Average block replication: 2.2943976\n Corrupt blocks:        0\n Missing replicas:      0 (0.0 %)\n Number of data-nodes:      101\n Number of racks:       1\nThe filesystem under path  /  is HEALTHY", 
            "title": "Cron"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#hadoop", 
            "text": "To view all of the currently configured settings of Hadoop from the web interface, enter the following url in your browser:  http:// namenode.hostname :50070/conf  You will see the entire configuration in XML format, for example:  \n   Expand XML configuration \n      ?xml version= 1.0  encoding= UTF-8  standalone= no ? configuration  property !--Loaded from core-default.xml-- name fs.s3n.impl /name value org.apache.hadoop.fs.s3native.NativeS3FileSystem /value /property  property !--Loaded from mapred-default.xml-- name mapred.task.cache.levels /name value 2 /value /property  property !--Loaded from mapred-default.xml-- name map.sort.class /name value org.apache.hadoop.util.QuickSort /value /property  property !--Loaded from core-site.xml-- name hadoop.tmp.dir /name value /data1/hadoop//scratch /value /property  property !--Loaded from core-default.xml-- name hadoop.native.lib /name value true /value /property  property !--Loaded from hdfs-default.xml-- name dfs.namenode.decommission.nodes.per.interval /name value 5 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.https.need.client.auth /name value false /value /property  property !--Loaded from core-default.xml-- name ipc.client.idlethreshold /name value 4000 /value /property  property !--Loaded from mapred-default.xml-- name mapred.system.dir /name value ${ hadoop . tmp . dir } /mapred/system /value /property  property !--Loaded from hdfs-default.xml-- name dfs.datanode.data.dir.perm /name value 755 /value /property  property !--Loaded from mapred-default.xml-- name mapred.job.tracker.persist.jobstatus.hours /name value 0 /value /property  property !--Loaded from hdfs-site.xml-- name dfs.namenode.logging.level /name value all /value /property  property !--Loaded from hdfs-default.xml-- name dfs.datanode.address /name value 0.0.0.0:50010 /value /property  property !--Loaded from core-default.xml-- name io.skip.checksum.errors /name value false /value /property  property !--Loaded from hdfs-default.xml-- name dfs.block.access.token.enable /name value false /value /property  property !--Loaded from Unknown-- name fs.default.name /name value hdfs://nagios.t2.ucsd.edu:9000 /value /property  property !--Loaded from mapred-default.xml-- name mapred.child.tmp /name value ./tmp /value /property  property !--Loaded from core-default.xml-- name fs.har.impl.disable.cache /name value true /value /property  property !--Loaded from mapred-default.xml-- name mapred.skip.reduce.max.skip.groups /name value 0 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.safemode.threshold.pct /name value 0.999f /value /property  property !--Loaded from mapred-default.xml-- name mapred.heartbeats.in.second /name value 100 /value /property  property !--Loaded from hdfs-site.xml-- name dfs.namenode.handler.count /name value 40 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.blockreport.initialDelay /name value 0 /value /property  property !--Loaded from mapred-default.xml-- name mapred.jobtracker.instrumentation /name value org.apache.hadoop.mapred.JobTrackerMetricsInst /value /property  property !--Loaded from mapred-default.xml-- name mapred.tasktracker.dns.nameserver /name value default /value /property  property !--Loaded from mapred-default.xml-- name io.sort.factor /name value 10 /value /property  property !--Loaded from mapred-default.xml-- name mapred.task.timeout /name value 600000 /value /property  property !--Loaded from mapred-default.xml-- name mapred.max.tracker.failures /name value 4 /value /property  property !--Loaded from core-default.xml-- name hadoop.rpc.socket.factory.class.default /name value org.apache.hadoop.net.StandardSocketFactory /value /property  property !--Loaded from mapred-default.xml-- name mapred.job.tracker.jobhistory.lru.cache.size /name value 5 /value /property  property !--Loaded from core-default.xml-- name fs.hdfs.impl /name value org.apache.hadoop.hdfs.DistributedFileSystem /value /property  property !--Loaded from mapred-default.xml-- name mapred.skip.map.auto.incr.proc.count /name value true /value /property  property !--Loaded from hdfs-default.xml-- name dfs.block.access.key.update.interval /name value 600 /value /property  property !--Loaded from mapred-default.xml-- name mapreduce.job.complete.cancel.delegation.tokens /name value true /value /property  property !--Loaded from core-default.xml-- name io.mapfile.bloom.size /name value 1048576 /value /property  property !--Loaded from mapred-default.xml-- name mapreduce.reduce.shuffle.connect.timeout /name value 180000 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.safemode.extension /name value 30000 /value /property  property !--Loaded from mapred-site.xml-- name tasktracker.http.threads /name value 50 /value /property  property !--Loaded from mapred-default.xml-- name mapred.job.shuffle.merge.percent /name value 0.66 /value /property  property !--Loaded from core-default.xml-- name fs.ftp.impl /name value org.apache.hadoop.fs.ftp.FTPFileSystem /value /property  property !--Loaded from mapred-default.xml-- name mapred.output.compress /name value false /value /property  property !--Loaded from core-site.xml-- name io.bytes.per.checksum /name value 4096 /value /property  property !--Loaded from mapred-default.xml-- name mapred.healthChecker.script.timeout /name value 600000 /value /property  property !--Loaded from core-default.xml-- name topology.node.switch.mapping.impl /name value org.apache.hadoop.net.ScriptBasedMapping /value /property  property !--Loaded from hdfs-default.xml-- name dfs.https.server.keystore.resource /name value ssl-server.xml /value /property  property !--Loaded from mapred-default.xml-- name mapred.reduce.slowstart.completed.maps /name value 0.05 /value /property  property !--Loaded from mapred-default.xml-- name mapred.reduce.max.attempts /name value 4 /value /property  property !--Loaded from core-default.xml-- name fs.ramfs.impl /name value org.apache.hadoop.fs.InMemoryFileSystem /value /property  property !--Loaded from hdfs-default.xml-- name dfs.block.access.token.lifetime /name value 600 /value /property  property !--Loaded from mapred-default.xml-- name mapred.skip.map.max.skip.records /name value 0 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.name.edits.dir /name value ${ dfs . name . dir } /value /property  property !--Loaded from core-default.xml-- name hadoop.security.group.mapping /name value org.apache.hadoop.security.ShellBasedUnixGroupsMapping /value /property  property !--Loaded from mapred-default.xml-- name mapred.job.tracker.persist.jobstatus.dir /name value /jobtracker/jobsInfo /value /property  property !--Loaded from core-site.xml-- name hadoop.log.dir /name value /var/log/hadoop /value /property  property !--Loaded from core-default.xml-- name fs.s3.buffer.dir /name value ${ hadoop . tmp . dir } /s3 /value /property  property !--Loaded from hdfs-site.xml-- name dfs.block.size /name value 134217728 /value /property  property !--Loaded from mapred-default.xml-- name job.end.retry.attempts /name value 0 /value /property  property !--Loaded from core-default.xml-- name fs.file.impl /name value org.apache.hadoop.fs.LocalFileSystem /value /property  property !--Loaded from mapred-default.xml-- name mapred.output.compression.type /name value RECORD /value /property  property !--Loaded from mapred-default.xml-- name mapred.local.dir.minspacestart /name value 0 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.datanode.ipc.address /name value 0.0.0.0:50020 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.permissions /name value true /value /property  property !--Loaded from core-default.xml-- name topology.script.number.args /name value 100 /value /property  property !--Loaded from core-default.xml-- name io.mapfile.bloom.error.rate /name value 0.005 /value /property  property !--Loaded from mapred-default.xml-- name mapred.max.tracker.blacklists /name value 4 /value /property  property !--Loaded from mapred-default.xml-- name mapred.task.profile.maps /name value 0-2 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.datanode.https.address /name value 0.0.0.0:50475 /value /property  property !--Loaded from core-site.xml-- name dfs.umaskmode /name value 002 /value /property  property !--Loaded from mapred-default.xml-- name mapred.userlog.retain.hours /name value 24 /value /property  property !--Loaded from hdfs-site.xml-- name dfs.secondary.http.address /name value gratia-1:50090 /value /property  property !--Loaded from hdfs-site.xml-- name dfs.replication.max /name value 32 /value /property  property !--Loaded from mapred-default.xml-- name mapred.job.tracker.persist.jobstatus.active /name value false /value /property  property !--Loaded from core-default.xml-- name hadoop.security.authorization /name value false /value /property  property !--Loaded from core-default.xml-- name local.cache.size /name value 10737418240 /value /property  property !--Loaded from mapred-default.xml-- name mapred.min.split.size /name value 0 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.namenode.delegation.token.renew-interval /name value 86400000 /value /property  property !--Loaded from mapred-site.xml-- name mapred.map.tasks /name value 7919 /value /property  property !--Loaded from mapred-default.xml-- name mapred.child.java.opts /name value -Xmx200m /value /property  property !--Loaded from hdfs-default.xml-- name dfs.https.client.keystore.resource /name value ssl-client.xml /value /property  property !--Loaded from Unknown-- name dfs.namenode.startup /name value REGULAR /value /property  property !--Loaded from mapred-default.xml-- name mapred.job.queue.name /name value default /value /property  property !--Loaded from mapred-default.xml-- name mapred.job.tracker.retiredjobs.cache.size /name value 1000 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.https.address /name value 0.0.0.0:50470 /value /property  property !--Loaded from hdfs-site.xml-- name dfs.balance.bandwidthPerSec /name value 2000000000 /value /property  property !--Loaded from core-default.xml-- name ipc.server.listen.queue.size /name value 128 /value /property  property !--Loaded from mapred-default.xml-- name job.end.retry.interval /name value 30000 /value /property  property !--Loaded from mapred-default.xml-- name mapred.inmem.merge.threshold /name value 1000 /value /property  property !--Loaded from mapred-default.xml-- name mapred.skip.attempts.to.start.skipping /name value 2 /value /property  property !--Loaded from hdfs-site.xml-- name fs.checkpoint.dir /name value /var/hadoop/checkpoint-a /value /property  property !--Loaded from mapred-site.xml-- name mapred.reduce.tasks /name value 1543 /value /property  property !--Loaded from mapred-default.xml-- name mapred.merge.recordsBeforeProgress /name value 10000 /value /property  property !--Loaded from mapred-default.xml-- name mapred.userlog.limit.kb /name value 0 /value /property  property !--Loaded from core-default.xml-- name webinterface.private.actions /name value false /value /property  property !--Loaded from hdfs-default.xml-- name dfs.max.objects /name value 0 /value /property  property !--Loaded from mapred-default.xml-- name mapred.job.shuffle.input.buffer.percent /name value 0.70 /value /property  property !--Loaded from mapred-default.xml-- name io.sort.spill.percent /name value 0.80 /value /property  property !--Loaded from mapred-default.xml-- name mapred.map.tasks.speculative.execution /name value true /value /property  property !--Loaded from core-default.xml-- name hadoop.util.hash.type /name value murmur /value /property  property !--Loaded from hdfs-default.xml-- name dfs.datanode.dns.nameserver /name value default /value /property  property !--Loaded from hdfs-default.xml-- name dfs.blockreport.intervalMsec /name value 3600000 /value /property  property !--Loaded from mapred-default.xml-- name mapred.map.max.attempts /name value 4 /value /property  property !--Loaded from mapred-default.xml-- name mapreduce.job.acl-view-job /name value   /value /property  property !--Loaded from mapred-default.xml-- name mapred.job.tracker.handler.count /name value 10 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.client.block.write.retries /name value 3 /value /property  property !--Loaded from mapred-default.xml-- name mapred.max.reduces.per.node /name value -1 /value /property  property !--Loaded from mapred-default.xml-- name mapreduce.reduce.shuffle.read.timeout /name value 180000 /value /property  property !--Loaded from mapred-default.xml-- name mapred.tasktracker.expiry.interval /name value 600000 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.https.enable /name value false /value /property  property !--Loaded from mapred-default.xml-- name mapred.jobtracker.maxtasks.per.job /name value -1 /value /property  property !--Loaded from mapred-default.xml-- name mapred.jobtracker.job.history.block.size /name value 3145728 /value /property  property !--Loaded from mapred-default.xml-- name keep.failed.task.files /name value false /value /property  property !--Loaded from hdfs-default.xml-- name dfs.datanode.failed.volumes.tolerated /name value 0 /value /property  property !--Loaded from mapred-default.xml-- name mapred.task.profile.reduces /name value 0-2 /value /property  property !--Loaded from core-default.xml-- name ipc.client.tcpnodelay /name value false /value /property  property !--Loaded from mapred-default.xml-- name mapred.output.compression.codec /name value org.apache.hadoop.io.compress.DefaultCodec /value /property  property !--Loaded from mapred-default.xml-- name io.map.index.skip /name value 0 /value /property  property !--Loaded from core-default.xml-- name ipc.server.tcpnodelay /name value false /value /property  property !--Loaded from hdfs-default.xml-- name dfs.namenode.delegation.key.update-interval /name value 86400000 /value /property  property !--Loaded from mapred-default.xml-- name mapred.running.map.limit /name value -1 /value /property  property !--Loaded from mapred-default.xml-- name jobclient.progress.monitor.poll.interval /name value 1000 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.default.chunk.view.size /name value 32768 /value /property  property !--Loaded from core-default.xml-- name hadoop.logfile.size /name value 10000000 /value /property  property !--Loaded from mapred-default.xml-- name mapred.reduce.tasks.speculative.execution /name value true /value /property  property !--Loaded from mapred-default.xml-- name mapreduce.tasktracker.outofband.heartbeat /name value false /value /property  property !--Loaded from core-default.xml-- name fs.s3n.block.size /name value 67108864 /value /property  property !--Loaded from hdfs-site.xml-- name dfs.datanode.du.reserved /name value 10000000000 /value /property  property !--Loaded from core-default.xml-- name hadoop.security.authentication /name value simple /value /property  property !--Loaded from hdfs-site.xml-- name fs.checkpoint.period /name value 3600 /value /property  property !--Loaded from mapred-default.xml-- name mapred.running.reduce.limit /name value -1 /value /property  property !--Loaded from mapred-default.xml-- name mapred.job.reuse.jvm.num.tasks /name value 1 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.web.ugi /name value webuser,webgroup /value /property  property !--Loaded from mapred-default.xml-- name mapred.jobtracker.completeuserjobs.maximum /name value 100 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.df.interval /name value 60000 /value /property  property !--Loaded from mapred-default.xml-- name mapred.task.tracker.task-controller /name value org.apache.hadoop.mapred.DefaultTaskController /value /property  property !--Loaded from hdfs-site.xml-- name dfs.data.dir /name value /data1/hadoop//data /value /property  property !--Loaded from core-default.xml-- name fs.s3.maxRetries /name value 4 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.datanode.dns.interface /name value default /value /property  property !--Loaded from hdfs-default.xml-- name dfs.support.append /name value true /value /property  property !--Loaded from mapred-default.xml-- name mapreduce.job.acl-modify-job /name value   /value /property  property !--Loaded from mapred-default.xml-- name mapred.local.dir /name value ${ hadoop . tmp . dir } /mapred/local /value /property  property !--Loaded from core-default.xml-- name fs.hftp.impl /name value org.apache.hadoop.hdfs.HftpFileSystem /value /property  property !--Loaded from hdfs-site.xml-- name dfs.permissions.supergroup /name value root /value /property  property !--Loaded from core-default.xml-- name fs.trash.interval /name value 0 /value /property  property !--Loaded from core-default.xml-- name fs.s3.sleepTimeSeconds /name value 10 /value /property  property !--Loaded from mapred-default.xml-- name mapred.submit.replication /name value 10 /value /property  property !--Loaded from hdfs-site.xml-- name dfs.replication.min /name value 1 /value /property  property !--Loaded from core-default.xml-- name fs.har.impl /name value org.apache.hadoop.fs.HarFileSystem /value /property  property !--Loaded from mapred-default.xml-- name mapred.map.output.compression.codec /name value org.apache.hadoop.io.compress.DefaultCodec /value /property  property !--Loaded from mapred-default.xml-- name mapred.tasktracker.dns.interface /name value default /value /property  property !--Loaded from hdfs-default.xml-- name dfs.namenode.decommission.interval /name value 30 /value /property  property !--Loaded from Unknown-- name dfs.http.address /name value nagios:50070 /value /property  property !--Loaded from mapred-site.xml-- name mapred.job.tracker /name value nagios:9000 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.heartbeat.interval /name value 3 /value /property  property !--Loaded from core-default.xml-- name io.seqfile.sorter.recordlimit /name value 1000000 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.name.dir /name value ${ hadoop . tmp . dir } /dfs/name /value /property  property !--Loaded from mapred-default.xml-- name mapred.line.input.format.linespermap /name value 1 /value /property  property !--Loaded from mapred-default.xml-- name mapred.jobtracker.taskScheduler /name value org.apache.hadoop.mapred.JobQueueTaskScheduler /value /property  property !--Loaded from mapred-default.xml-- name mapred.tasktracker.instrumentation /name value org.apache.hadoop.mapred.TaskTrackerMetricsInst /value /property  property !--Loaded from hdfs-default.xml-- name dfs.datanode.http.address /name value 0.0.0.0:50075 /value /property  property !--Loaded from mapred-default.xml-- name jobclient.completion.poll.interval /name value 5000 /value /property  property !--Loaded from mapred-default.xml-- name mapred.max.maps.per.node /name value -1 /value /property  property !--Loaded from mapred-default.xml-- name mapred.local.dir.minspacekill /name value 0 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.replication.interval /name value 3 /value /property  property !--Loaded from mapred-default.xml-- name io.sort.record.percent /name value 0.05 /value /property  property !--Loaded from core-default.xml-- name fs.kfs.impl /name value org.apache.hadoop.fs.kfs.KosmosFileSystem /value /property  property !--Loaded from mapred-default.xml-- name mapred.temp.dir /name value ${ hadoop . tmp . dir } /mapred/temp /value /property  property !--Loaded from mapred-site.xml-- name mapred.tasktracker.reduce.tasks.maximum /name value 4 /value /property  property !--Loaded from hdfs-site.xml-- name dfs.replication /name value 2 /value /property  property !--Loaded from core-default.xml-- name fs.checkpoint.edits.dir /name value ${ fs . checkpoint . dir } /value /property  property !--Loaded from mapred-default.xml-- name mapred.tasktracker.tasks.sleeptime-before-sigkill /name value 5000 /value /property  property !--Loaded from mapred-default.xml-- name mapred.job.reduce.input.buffer.percent /name value 0.0 /value /property  property !--Loaded from mapred-default.xml-- name mapred.tasktracker.indexcache.mb /name value 10 /value /property  property !--Loaded from mapred-default.xml-- name mapreduce.job.split.metainfo.maxsize /name value 10000000 /value /property  property !--Loaded from mapred-default.xml-- name mapred.skip.reduce.auto.incr.proc.count /name value true /value /property  property !--Loaded from core-default.xml-- name hadoop.logfile.count /name value 10 /value /property  property !--Loaded from core-default.xml-- name fs.automatic.close /name value true /value /property  property !--Loaded from core-default.xml-- name io.seqfile.compress.blocksize /name value 1000000 /value /property  property !--Loaded from hdfs-site.xml-- name dfs.hosts.exclude /name value /etc/hadoop-0.20/conf/hosts_exclude /value /property  property !--Loaded from core-default.xml-- name fs.s3.block.size /name value 67108864 /value /property  property !--Loaded from mapred-default.xml-- name mapred.tasktracker.taskmemorymanager.monitoring-interval /name value 5000 /value /property  property !--Loaded from mapred-default.xml-- name mapred.acls.enabled /name value false /value /property  property !--Loaded from mapred-default.xml-- name mapreduce.jobtracker.staging.root.dir /name value ${ hadoop . tmp . dir } /mapred/staging /value /property  property !--Loaded from mapred-default.xml-- name mapred.queue.names /name value default /value /property  property !--Loaded from hdfs-default.xml-- name dfs.access.time.precision /name value 3600000 /value /property  property !--Loaded from core-default.xml-- name fs.hsftp.impl /name value org.apache.hadoop.hdfs.HsftpFileSystem /value /property  property !--Loaded from mapred-default.xml-- name mapred.task.tracker.http.address /name value 0.0.0.0:50060 /value /property  property !--Loaded from mapred-default.xml-- name mapred.reduce.parallel.copies /name value 5 /value /property  property !--Loaded from core-default.xml-- name io.seqfile.lazydecompress /name value true /value /property  property !--Loaded from hdfs-default.xml-- name dfs.safemode.min.datanodes /name value 0 /value /property  property !--Loaded from mapred-default.xml-- name io.sort.mb /name value 100 /value /property  property !--Loaded from core-default.xml-- name ipc.client.connection.maxidletime /name value 10000 /value /property  property !--Loaded from mapred-default.xml-- name mapred.compress.map.output /name value false /value /property  property !--Loaded from mapred-default.xml-- name mapred.task.tracker.report.address /name value 127.0.0.1:0 /value /property  property !--Loaded from mapred-default.xml-- name mapred.healthChecker.interval /name value 60000 /value /property  property !--Loaded from core-default.xml-- name ipc.client.kill.max /name value 10 /value /property  property !--Loaded from core-default.xml-- name ipc.client.connect.max.retries /name value 10 /value /property  property !--Loaded from core-default.xml-- name fs.s3.impl /name value org.apache.hadoop.fs.s3.S3FileSystem /value /property  property !--Loaded from mapred-default.xml-- name mapred.job.tracker.http.address /name value 0.0.0.0:50030 /value /property  property !--Loaded from core-default.xml-- name io.file.buffer.size /name value 4096 /value /property  property !--Loaded from mapred-default.xml-- name mapred.jobtracker.restart.recover /name value false /value /property  property !--Loaded from core-default.xml-- name io.serializations /name value org.apache.hadoop.io.serializer.WritableSerialization /value /property  property !--Loaded from mapred-default.xml-- name mapred.task.profile /name value false /value /property  property !--Loaded from hdfs-site.xml-- name dfs.datanode.handler.count /name value 10 /value /property  property !--Loaded from mapred-default.xml-- name mapred.reduce.copy.backoff /name value 300 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.replication.considerLoad /name value true /value /property  property !--Loaded from mapred-default.xml-- name jobclient.output.filter /name value FAILED /value /property  property !--Loaded from hdfs-default.xml-- name dfs.namenode.delegation.token.max-lifetime /name value 604800000 /value /property  property !--Loaded from mapred-site.xml-- name mapred.tasktracker.map.tasks.maximum /name value 4 /value /property  property !--Loaded from core-default.xml-- name io.compression.codecs /name value org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec /value /property  property !--Loaded from core-default.xml-- name fs.checkpoint.size /name value 67108864 /value /property  /configuration     Please refer to  OSG Hadoop debug webpage  and  Apache Hadoop FAQ webpage  for answers to common questions/concerns", 
            "title": "Hadoop"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#fuse", 
            "text": "", 
            "title": "FUSE"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#notes-on-building-a-fuse-module", 
            "text": "If you are running a custom kernel, then be sure to enable the  fuse  module with  CONFIG_FUSE_FS=m  in your kernel config. Building and installing a  fuse  kernel module for your custom kernel is beyond the scope of this document.", 
            "title": "Notes on Building a FUSE Module"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#running-fuse-in-debug-mode", 
            "text": "To start the FUSE mount in debug mode, you can run the FUSE mount command by hand:  root@host #   /usr/bin/hadoop-fuse-dfs  /mnt/hadoop -o rw,server = namenode.host ,port = 9000 ,rdbuffer = 131072 ,allow_other -d  Debug output will be printed to stderr, which you will probably want to redirect to a file. Most FUSE-related problems can be tackled by reading through the stderr and looking for error messages.", 
            "title": "Running FUSE in Debug Mode"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#gridftp", 
            "text": "#GridFTPStand", 
            "title": "GridFTP"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#starting-gridftp-in-standalone-mode", 
            "text": "If you would like to test the gridftp-hdfs server in a debug standalone mode, you can run the command:  root@host #  gridftp-hdfs-standalone  The standalone server runs on port 5002, handles a single GridFTP request, and will log output to stdout/stderr.", 
            "title": "Starting GridFTP in Standalone Mode"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#file-locations", 
            "text": "Component  File Type  Location  Needs editing?    Hadoop  Log files  /var/log/hadoop/*  No    Hadoop  PID files  /var/run/hadoop/*.pid  No    Hadoop  init scripts  /etc/init.d/hadoop  No    Hadoop  init script config file  /etc/sysconfig/hadoop  Yes    Hadoop  runtime config files  /etc/hadoop/conf/*  Maybe    Hadoop  System binaries  /usr/bin/hadoop  No    Hadoop  JARs  /usr/lib/hadoop/*  No    Hadoop  runtime config files  /etc/hosts_exclude  Yes, must be present on namenodes    GridFTP  Log files  /var/log/gridftp-auth.log ,  /var/log/gridftp.log  No    GridFTP  init.d script  /etc/init.d/globus-gridftp-server  No    GridFTP  runtime config files  /etc/gridftp-hdfs/* ,  /etc/sysconfig/gridftp-hdfs  Maybe    GridFTP  System binaries  /usr/bin/gridftp-hdfs-standalone ,  /usr/sbin/globus-gridftp-server  No    GridFTP  System libraries  /usr/lib64/libglobus_gridftp_server_hdfs.so*  No    GridFTP  GUMS client (called by LCMAPS) configuration  /etc/lcmaps.db  Yes    GridFTP  CA certificates  /etc/grid-security/certificates/*  No        Service/Process  Configuration File  Description      BeStMan2  /etc/bestman2/conf/bestman2.rc  Main Configuration file     /etc/sysconfig/bestman2  Environment variables used by BeStMan2     /etc/sysconfig/bestman2lib  Environment variables that store values of various client and server libraries used by BeStMan2     /etc/bestman2/conf/*  Other runtime configuration files     /etc/init.d/bestman2  init.d startup script     /etc/gridftp.conf  Startup parameters        Service/Process  Log File  Description      BeStMan2  /var/log/bestman2/bestman2.log  BeStMan2 server log and errors     /var/log/bestman2/event.srm.log  Records all SRM transactions    GridFTP  /var/log/gridftp.log  Transfer log     /var/log/gridftp-auth.log  Authentication log     /var/log/messages  Main system log (look here for LCMAPS errors)", 
            "title": "File Locations"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#known-issues", 
            "text": "", 
            "title": "Known Issues"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#replicas", 
            "text": "You may need to change the following line in  /usr/share/gridftp-hdfs/gridftp-hdfs-environment :  export GRIDFTP_HDFS_REPLICAS=2", 
            "title": "Replicas"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#copyfromlocal-java-ioexception", 
            "text": "When trying to copy a local file into Hadoop you may come across the following java exception:  \n   Show detailed java exception \n      11/06/24 11:10:50 WARN hdfs.DFSClient: Error Recovery for block null bad datanode[0]  nodes == null  11/06/24 11:10:50 WARN hdfs.DFSClient: Could not get block locations. Source file  /osg/ddd  - Aborting...  copyFromLocal: java.io.IOException: File /osg/ddd could only be replicated to 0  nodes, instead of 1  11/06/24 11:10:50 ERROR hdfs.DFSClient: Exception closing file /osg/ddd :  org.apache.hadoop.ipc.RemoteException: java.io.IOException: File /osg/ddd could only  be replicated to 0 nodes, instead of 1          at  org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1415)          at org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:588)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)          at java.lang.reflect.Method.invoke(Method.java:597)          at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:528)          at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1319)          at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1315)          at java.security.AccessController.doPrivileged(Native Method)          at javax.security.auth.Subject.doAs(Subject.java:396)          at  org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1063)          at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1313)     This can occur if you try to install a Datanode on a machine with less than 10GB of disk space available. This can be changed by lowering the value of the following property in  /usr/lib/hadoop-0.20/conf/hdfs-site.xml :  property \n   name dfs.datanode.du.reserved /name \n   value 10000000000 /value  /property   Hadoop always requires this amount of disk space to be available for non-hdfs usage on the machine.", 
            "title": "copyFromLocal java IOException"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#how-to-get-help", 
            "text": "If you cannot resolve the problem, there are several ways to receive help:   For bug support and issues, submit a ticket to the  Grid Operations Center .  For community support and best-effort software team support contact  .  For additional community support, contact  . Note, this is only best-effort help from OSG Software team.   For a full set of help options, see  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/data/install-hadoop-2-0-0/#references", 
            "text": "Instructions for Upgrading from Hadoop 0.19 to Hadoop 0.20  * NOTE these instructions are subject to change and the upgrade doc linked is intended to upgrade from the caltech hosted 0.19 rpms in the caltech hosted 0.20 rpms, NOT the new rpms hosted in the new OSG repos.     Benchmarking   Using Hadoop as a Grid Storage Element ,  Journal of Physics Conference Series, 2009 .  Hadoop Distributed File System for the Grid ,  IEEE Nuclear Science Symposium, 2009 .", 
            "title": "References"
        }, 
        {
            "location": "/data/xrootd-overview/", 
            "text": "XRootD Overview\n\n\nThe XRootD project aims at giving high performance, scalable fault tolerant access to data repositories of many kinds. The typical usage is to give access to file-based ones. It is based on a scalable architecture, a communication protocol, and a set of plugins and tools based on those. The freedom to configure it and to make it scale (for size and performance) allows the deployment of data access clusters of virtually any size, which can include sophisticated features, like authentication/authorization, integrations with other systems, WAN data distribution, etc.\n\n\nXRootD software framework is a fully generic suite for fast, low latency and scalable data access, which can serve natively any kind of data, organized as a hierarchical filesystem-like namespace, based on the concept of directory. As a general rule, particular emphasis has been put in the quality of the core software parts.\n\n\nPlanning\n\n\n\n\nXrootd Homepage\n\n\n\n\nInstallation\n\n\n\n\nInstall Xrootd Server\n: This page explains how to install an XRootD redirector and data nodes\n\n\nInstall Bestman Gateway Xrootd\n: Installing a SRM frontend wit GridFTP for XRootD redirector\n\n\nInstall GridFTP on Xrootd\n: For sites with multiple GridFTP servers, this page explains how to install a GridFTP server.\n\n\n\n\nOperations\n\n\n\n\nInstall XRootD client\n: Installing and using XRootD clients\n\n\nXrootd bug reporting\n\n\nSource code", 
            "title": "XRootD Overview"
        }, 
        {
            "location": "/data/xrootd-overview/#xrootd-overview", 
            "text": "The XRootD project aims at giving high performance, scalable fault tolerant access to data repositories of many kinds. The typical usage is to give access to file-based ones. It is based on a scalable architecture, a communication protocol, and a set of plugins and tools based on those. The freedom to configure it and to make it scale (for size and performance) allows the deployment of data access clusters of virtually any size, which can include sophisticated features, like authentication/authorization, integrations with other systems, WAN data distribution, etc.  XRootD software framework is a fully generic suite for fast, low latency and scalable data access, which can serve natively any kind of data, organized as a hierarchical filesystem-like namespace, based on the concept of directory. As a general rule, particular emphasis has been put in the quality of the core software parts.", 
            "title": "XRootD Overview"
        }, 
        {
            "location": "/data/xrootd-overview/#planning", 
            "text": "Xrootd Homepage", 
            "title": "Planning"
        }, 
        {
            "location": "/data/xrootd-overview/#installation", 
            "text": "Install Xrootd Server : This page explains how to install an XRootD redirector and data nodes  Install Bestman Gateway Xrootd : Installing a SRM frontend wit GridFTP for XRootD redirector  Install GridFTP on Xrootd : For sites with multiple GridFTP servers, this page explains how to install a GridFTP server.", 
            "title": "Installation"
        }, 
        {
            "location": "/data/xrootd-overview/#operations", 
            "text": "Install XRootD client : Installing and using XRootD clients  Xrootd bug reporting  Source code", 
            "title": "Operations"
        }, 
        {
            "location": "/data/install-xroot-client/", 
            "text": "Install XRootd Client\n\n\nXRootD is a high performance network storage system widely used in high energy physics experiments such as ATLAS and ALICE. The underlying XRootD data transfer protocol provides highly efficient access to ROOT based data files.\n\n\nThis page provides instructions for using a XRootD client to connect to a XRootD storage system.\n\n\nInstalling a XRootD Client\n\n\nAs a user you have three different ways to interact with XRootd: a file system mounted with XRootDFS, the application \nxrdcp\n, LD_PRELOAD.\n\n\nTo use a XRootD file system mounted with XRootDFS you don't need any special setup, you will need the XRootD FUSE module.\n\n\nTo use \nxrdcp\n you need to have it in your path (e.g. \nsource $INSTALL_DIR/setup.sh\n, where \n$INSTALL_DIR\n is the XRootD installation directory)\n\n\nThe use of LD_PRELOAD is not recommended if you have any alternative available.\n\n\nInstall Repositories\n\n\nIf you have not done so, you will need to install the \nproper yum repositories\n:\n\n\nInstalling XRootD Client\n\n\nThe XRootD client is needed to access XRootD via \nxrdcp\n or via POSIX preload.\n\n\nroot@host #\n yum install xrootd-client\n\n\n\n\n\nInstalling XRootD Fuse\n\n\nThe XRootD fuse package is needed to access via XRootDFS.\n\n\nroot@host #\n yum install xrootd-fuse\n\n\n\n\n\nUsing the XRootD Client Mechanisms\n\n\nUsing the xrdcp client\n\n\nFrom the redirector node, you can run this:\n\n\nroot@host #\n \necho\n \nThis is a test\n \n/tmp/test \n\nroot@host #\n xrdcp /tmp/test xroot://redirector.yourdomain.org:1094//storage/path/test \n\nroot@host #\n xrdcp xroot://redirector.yourdomain.org:1094//storage/path/test /tmp/test1 \n\nroot@host #\n diff /tmp/test1 /tmp/test \n\n\n\n\n\nUsers can write in their own space. E.g. a user with Unix account name \nmyuser\n (the account must exist, eg. in \n/etc/passwd\n, etc, on the redirector and on all the data servers) could save a file in XRootD:\n\n\nroot@host #\n \necho\n \nThis is a user test\n \n/tmp/test \n\nroot@host #\n xrdcp /tmp/test xroot://redirector.yourdomain.org:1094//storage/path/myuser/test \n\n\n\n\n\nNote that \nxrdcp\n creates missing directories.\n\n\nTests using the libXrdPosixPreload library\n\n\nUsing XRootD with the POSIX preload library (replace \n/usr/lib64\n with \n/usr/lib\n on 32-bit systems)\n\n\nroot@host #\n \nexport\n \nLD_PRELOAD\n=\n/usr/lib64/libXrdPosixPreload.so \n\nroot@host #\n \necho\n \nThis is a new test\n \n/tmp/test \n\nroot@host #\n mkdir xroot://redirector.yourdomain.org:1094//storage/path/subdir\n\nroot@host #\n cp /tmp/test xroot://redirector.yourdomain.org:1094//storage/path/subdir/test \n\nroot@host #\n cp xroot://redirector.yourdomain.org:1094//storage/path/subdir/test /tmp/test1 \n\nroot@host #\n diff /tmp/test1 /tmp/test \n\nroot@host #\n rm xroot://redirector.yourdomain.org:1094//storage/path/subdir/test \n\nroot@host #\n rmdir xroot://redirector.yourdomain.org:1094//storage/path/subdir\n\n\n\n\n\nTests using XRootDFS\n\n\nThe directory mounted using XRootDFS can be used as any other directory mounted on your file system. All the normal Unix commands should work. You don't need any special setup or library. Try using \ncp\n, \nrm\n, \nmv\n, \nmkdir\n, \nrmdir\n.\n\n\nAssuming your mount is \n/mnt/xrootd\n:\n\n\nroot@host #\n \necho\n \nThis is a new test\n \n/tmp/test \n\nroot@host #\n mkdir -p /mnt/xrootd/subdir/sub2\n\nroot@host #\n cp /tmp/test /mnt/xrootd/subdir/sub2/test \n\nroot@host #\n cp /mnt/xrootd/subdir/sub2/test /mnt/xrootd/subdir/sub2/test1 \n\nroot@host #\n cp /mnt/xrootd/subdir/sub2/test1 /tmp/test1 \n\nroot@host #\n diff /tmp/test1 /tmp/test \n\nroot@host #\n rm -r /mnt/xrootd/subdir\n\n\n\n\n\nHow to get Help?\n\n\nIf you cannot resolve the problem, the best way to get help is by contacting \n.\nFor a full set of help options, see \nHelp Procedure\n.", 
            "title": "Install XRootD Client"
        }, 
        {
            "location": "/data/install-xroot-client/#install-xrootd-client", 
            "text": "XRootD is a high performance network storage system widely used in high energy physics experiments such as ATLAS and ALICE. The underlying XRootD data transfer protocol provides highly efficient access to ROOT based data files.  This page provides instructions for using a XRootD client to connect to a XRootD storage system.", 
            "title": "Install XRootd Client"
        }, 
        {
            "location": "/data/install-xroot-client/#installing-a-xrootd-client", 
            "text": "As a user you have three different ways to interact with XRootd: a file system mounted with XRootDFS, the application  xrdcp , LD_PRELOAD.  To use a XRootD file system mounted with XRootDFS you don't need any special setup, you will need the XRootD FUSE module.  To use  xrdcp  you need to have it in your path (e.g.  source $INSTALL_DIR/setup.sh , where  $INSTALL_DIR  is the XRootD installation directory)  The use of LD_PRELOAD is not recommended if you have any alternative available.", 
            "title": "Installing a XRootD Client"
        }, 
        {
            "location": "/data/install-xroot-client/#install-repositories", 
            "text": "If you have not done so, you will need to install the  proper yum repositories :", 
            "title": "Install Repositories"
        }, 
        {
            "location": "/data/install-xroot-client/#installing-xrootd-client", 
            "text": "The XRootD client is needed to access XRootD via  xrdcp  or via POSIX preload.  root@host #  yum install xrootd-client", 
            "title": "Installing XRootD Client"
        }, 
        {
            "location": "/data/install-xroot-client/#installing-xrootd-fuse", 
            "text": "The XRootD fuse package is needed to access via XRootDFS.  root@host #  yum install xrootd-fuse", 
            "title": "Installing XRootD Fuse"
        }, 
        {
            "location": "/data/install-xroot-client/#using-the-xrootd-client-mechanisms", 
            "text": "", 
            "title": "Using the XRootD Client Mechanisms"
        }, 
        {
            "location": "/data/install-xroot-client/#using-the-xrdcp-client", 
            "text": "From the redirector node, you can run this:  root@host #   echo   This is a test   /tmp/test  root@host #  xrdcp /tmp/test xroot://redirector.yourdomain.org:1094//storage/path/test  root@host #  xrdcp xroot://redirector.yourdomain.org:1094//storage/path/test /tmp/test1  root@host #  diff /tmp/test1 /tmp/test   Users can write in their own space. E.g. a user with Unix account name  myuser  (the account must exist, eg. in  /etc/passwd , etc, on the redirector and on all the data servers) could save a file in XRootD:  root@host #   echo   This is a user test   /tmp/test  root@host #  xrdcp /tmp/test xroot://redirector.yourdomain.org:1094//storage/path/myuser/test   Note that  xrdcp  creates missing directories.", 
            "title": "Using the xrdcp client"
        }, 
        {
            "location": "/data/install-xroot-client/#tests-using-the-libxrdposixpreload-library", 
            "text": "Using XRootD with the POSIX preload library (replace  /usr/lib64  with  /usr/lib  on 32-bit systems)  root@host #   export   LD_PRELOAD = /usr/lib64/libXrdPosixPreload.so  root@host #   echo   This is a new test   /tmp/test  root@host #  mkdir xroot://redirector.yourdomain.org:1094//storage/path/subdir root@host #  cp /tmp/test xroot://redirector.yourdomain.org:1094//storage/path/subdir/test  root@host #  cp xroot://redirector.yourdomain.org:1094//storage/path/subdir/test /tmp/test1  root@host #  diff /tmp/test1 /tmp/test  root@host #  rm xroot://redirector.yourdomain.org:1094//storage/path/subdir/test  root@host #  rmdir xroot://redirector.yourdomain.org:1094//storage/path/subdir", 
            "title": "Tests using the libXrdPosixPreload library"
        }, 
        {
            "location": "/data/install-xroot-client/#tests-using-xrootdfs", 
            "text": "The directory mounted using XRootDFS can be used as any other directory mounted on your file system. All the normal Unix commands should work. You don't need any special setup or library. Try using  cp ,  rm ,  mv ,  mkdir ,  rmdir .  Assuming your mount is  /mnt/xrootd :  root@host #   echo   This is a new test   /tmp/test  root@host #  mkdir -p /mnt/xrootd/subdir/sub2 root@host #  cp /tmp/test /mnt/xrootd/subdir/sub2/test  root@host #  cp /mnt/xrootd/subdir/sub2/test /mnt/xrootd/subdir/sub2/test1  root@host #  cp /mnt/xrootd/subdir/sub2/test1 /tmp/test1  root@host #  diff /tmp/test1 /tmp/test  root@host #  rm -r /mnt/xrootd/subdir", 
            "title": "Tests using XRootDFS"
        }, 
        {
            "location": "/data/install-xroot-client/#how-to-get-help", 
            "text": "If you cannot resolve the problem, the best way to get help is by contacting  .\nFor a full set of help options, see  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/data/install-xrootd/", 
            "text": "Installing and Maintaining XRootD\n\n\nXRootD\n is a hierarchical storage system that can be used\nin a variety of ways to access data, typically distributed among actual storage\nresources. One way to use XRootD is to have it refer to many data resources at a\nsingle site, and another way to use it is to refer to many storage systems, most\nlikely distributed among sites. An XRootD system includes a \nredirector\n, which\naccepts requests for data and finds a storage repository\u00a0\u2014 locally or\notherwise\u00a0\u2014 that can provide the data to the requestor.\n\n\nUse this page to learn how to install, configure, and use an XRootD redirector\nas part of a Storage Element (SE) or as part of a global namespace.\n\n\nBefore Starting\n\n\nBefore starting the installation process, consider the following points:\n\n\n\n\nUser IDs:\n If it does not exist already, the installation will create the Linux user ID \nxrootd\n\n\nService certificate:\n The XRootD service uses a host certificate at \n/etc/grid-security/host*.pem\n\n\nNetworking:\n The XRootD service uses port 1094 by default\n\n\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare \nthe required Yum repositories\n\n\nInstall \nCA certificates\n\n\n\n\nInstalling an XRootD Server\n\n\nAn installation of the XRootD server consists of the server itself and its\ndependencies. Install these with Yum:\n\n\nroot@host #\n yum install xrootd\n\n\n\n\n\nConfiguring an XRootD Server\n\n\nMinimal configuration\n\n\nA new installation of XRootD is already configured to run a standalone server\nthat serves files from \n/tmp\n on the local file system. This configuration is\nuseful to verify basic connectivity between your clients and your server. To do\nthis, start the \nxrootd\n service with standalone config as described in the\n\nmanaging services section\n.\n\n\nYou should be able now to copy a file such as \n/bin/sh\n using \nxrdcp\n command\ninto \n/tmp\n. To test, do:\n\n\nroot@host #\n yum install xrootd-client\n\nroot@host #\n xrdcp /bin/sh root://localhost:1094//tmp/first_test\n\n[xrootd] Total 0.76 MB  |====================| 100.00 % [inf MB/s]\n\n\nroot@host #\n ls -l /tmp/first_test\n\n-rw-r--r-- 1 xrootd xrootd 801512 Apr 11 10:48 /tmp/first_test\n\n\n\n\n\n\nOther than for testing, a standalone server is useful when you want to serve\nfiles off of a single host with lots of large disks. If your storage capacity is\nspread out over multiple hosts, you will need to set up an XRootD cluster.\n\n\nAdvanced configuration\n\n\nAn advanced XRootD setup has multiple components; it is important to validate\nthat each additional component that you set up is working before moving on to\nthe next component. We have included validation instructions after each\ncomponent below.\n\n\nCreating an XRootD cluster\n\n\n\n\nIf your storage is spread out over multiple hosts, you will need to set up an\nXRootD \ncluster\n. The cluster uses one \"redirector\" node as a frontend for user\naccesses, and multiple data nodes that have the data that users request. Two\ndaemons will run on each node:\n\n\nxrootd\n\nThe eXtended Root Daemon controls file access and storage.\n\n\ncmsd\n\nThe Cluster Management Services Daemon controls communication between nodes.\n\n\nNote that for large virtual organizations, a site-level redirector may actually\nalso communicate upwards to a regional or global redirector that handles access\nto a multi-level hierarchy. This section will only cover handling one level of\nXRootD hierarchy.\n\n\nIn the instructions below, \nRDRNODE\n will refer to the redirector host\nand \nDATANODE\n will refer to the data node host. These should be\nreplaced with the fully-qualified domain name of the host in question.\n\n\nModify /etc/xrootd/xrootd-clustered.cfg\n\n\nYou will need to modify the \nxrootd-clustered.cfg\n on the redirector node and\neach data node. The following example should serve as a base configuration for\nclustering. Further customizations are detailed below.\n\n\nall.export \n/tmp\n stage\nset xrdr = \nRDRNODE\n\nall.manager $(xrdr):3121\n\nif $(xrdr)\n  # Lines in this block are only executed on the redirector node\n  all.role manager\nelse\n  # Lines in this block are executed on all nodes but the redirector node\n  all.role server\n  cms.space min \n2g 5g\n\nfi\n\n\n\n\n\nYou will need to customize the following lines:\n\n\n\n\n\n\n\n\nConfiguration Line\n\n\nChanges Needed\n\n\n\n\n\n\n\n\n\n\nall.export \n/tmp\n stage\n\n\nChange \n/tmp\n to the directory to allow XRootD access to\n\n\n\n\n\n\nset xrdr=\nRDRNODE\n\n\nChange to the hostname of the redirector\n\n\n\n\n\n\ncms.space min \n2g 5g\n\n\nReserve this amount of free space on the node. For this example, if space falls below 2GB, xrootd will not store further files on this node until space climbs above 5GB. You can use \nk\n, \nm\n, \ng\n, or \nt\n to indicate kilobyte, megabytes, gigabytes, or terabytes, respectively.\n\n\n\n\n\n\n\n\nFurther information can be found at \nhttp://xrootd.slac.stanford.edu/doc\n\n\nVerifying the clustered config\n\n\nStart both \nxrootd\n and \ncmsd\n on all nodes according to the instructions in the\n\nmanaging services section\n.\n\n\nVerify that you can copy a file such as \n/bin/sh\n to \n/tmp\n on the server data\nvia the redirector:\n\n\nroot@host #\n xrdcp /bin/sh  root://\nRDRNODE\n:1094///tmp/second_test\n\n[xrootd] Total 0.76 MB  |====================| 100.00 % [inf MB/s]\n\n\n\n\n\n\nCheck that the \n/tmp/second_test\n is located on data server \nDATANODE\n.\n\n\n(Optional) Adding Simple Server Inventory to your cluster\n\n\nThe Simple Server Inventory (SSI) provide means to have an inventory for each\ndata server. SSI requires:\n\n\n\n\nA second instance of the \nxrootd\n daemon on the redirector\n\n\nA \"composite name space daemon\" (\nXrdCnsd\n) on each data server; this daemon handles the inventory\n\n\n\n\nAs an example, we will set up a two-node XRootD cluster with SSI.\n\n\nHost A is a redirector node that is running the following daemons:\n\n\n\n\nxrootd redirector\n\n\ncmsd\n\n\nxrootd - second instance that required for SSI\n\n\n\n\nHost B is a data server that is running the following daemons:\n\n\n\n\nxrootd data server\n\n\ncmsd\n\n\nXrdCnsd - started automatically by xrootd\n\n\n\n\nWe will need to create a directory on the redirector node for Inventory files.\n\n\nroot@host #\n mkdir -p /data/inventory\n\nroot@host #\n chown xrootd:xrootd /data/inventory\n\n\n\n\n\nOn the data server (host B) let's create the storage cache that will be different from \n/tmp\n.\n\n\nroot@host #\n mkdir -p  /local/xrootd\n\nroot@host #\n chown xrootd:xrootd /local/xrootd\n\n\n\n\n\nWe will be running two instances of XRootD on \nhostA\n. Modify\n\n/etc/xrootd/xrootd-clustered.cfg\n to give the two instances different behavior,\nas such:\n\n\nall.export /data/xrootdfs\nset xrdr=\nhostA\n\nall.manager $(xrdr):3121\nif $(xrdr) \namp;\namp; named cns\n      all.export /data/inventory\n      xrd.port 1095\nelse if $(xrdr)\n      all.role manager\n      xrd.port 1094\nelse\n      all.role server\n      oss.localroot /local/xrootd\n      ofs.notify closew create mkdir mv rm rmdir trunc | /usr/bin/XrdCnsd -d -D 2 -i 90 -b $(xrdr):1095:/data/inventory\n      #add cms.space if you have less the 11GB\n      # cms.space options http://xrootd.slac.stanford.edu/doc/dev/cms_config.htm\n      cms.space min 2g 5g\nfi\n\n\n\n\n\nStarting a second instance of XRootD on EL 6\n\n\nThe procedure for starting a second instance differs between EL 6 and EL 7. This\nsection is the procedure for EL 6.\n\n\nNow, we have to change \n/etc/sysconfig/xrootd\n on the redirector node (\nhostA\n) to run multiple instances of xrootd. The second instance of xrootd will be named \"cns\" and will be used for SSI.\n\n\nXROOTD\\_USER=xrootd \nXROOTD\\_GROUP=xrootd \nXROOTD\\_DEFAULT\\_OPTIONS=\n-k 7\n -l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-clustered.cfg\n\n\nXROOTD\\_CNS\\_OPTIONS=\n-k 7 -l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-clustered.cfg\n \nCMSD\\_DEFAULT\\_OPTIONS=\n-k 7\n -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg\n \nFRMD\\_DEFAULT\\_OPTIONS=\n-k 7\n -l /var/log/xrootd/frmd.log -c /etc/xrootd/xrootd-clustered.cfg\n \n\nXROOTD\\_INSTANCES=\ndefault cns\n \nCMSD\\_INSTANCES=\ndefault\n \nFRMD\\_INSTANCES=\ndefault\n \n\n\n\n\n\nNow, we can start xrootd cluster executing the following commands. On redirector you will see:\n\n\nroot@host #\n service xrootd start \n\nStarting xrootd (xrootd, default): \n\\[ OK \\]\n \n\n\nStarting xrootd (xrootd, cns): \n\\[ OK \\]\n \n\n\nroot@host #\n service cmsd start \n\nStarting xrootd (cmsd, default): \n\\[ OK \\]\n \n\n\n\n\n\n\nOn redirector node you should see two instances of xrootd running:\n\n\nroot@host #\n ps auxww\n|\ngrep xrootd \n\nxrootd 29036 0.0 0.0 44008 3172 ? Sl Apr11 0:00 /usr/bin/xrootd -k 7 -l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-clustered.cfg -b -s /var/run/xrootd/xrootd-default.pid -n default \n\n\nxrootd 29108 0.0 0.0 43868 3016 ? Sl Apr11 0:00 /usr/bin/xrootd -k 7 -l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-clustered.cfg -b -s /var/run/xrootd/xrootd-cns.pid -n cns \n\n\nxrootd 29196 0.0 0.0 51420 3692 ? Sl Apr11 0:00 /usr/bin/cmsd -k 7 -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg -b -s /var/run/xrootd/cmsd-default.pid -n default \n\n\n\n\n\n\nwarning\n the log file for second named instance of xrootd with be placed in \n/var/log/xrootd/cns/xrootd.log\n\n\nOn data server node you should that XrdCnsd process has been started:\n\n\nroot@host #\n ps auxww\n|\ngrep xrootd \n\nxrootd 19156 0.0 0.0 48096 3256 ? Sl 07:37 0:00 /usr/bin/cmsd -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg -b -s /var/run/xrootd/cmsd-default.pid -n default \n\n\nxrootd 19880 0.0 0.0 46124 2916 ? Sl 08:33 0:00 /usr/bin/xrootd -l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-clustered.cfg -b -s /var/run/xrootd/xrootd-default.pid -n default \n\n\nxrootd 19894 0.0 0.1 71164 6960 ? Sl 08:33 0:00 /usr/bin/XrdCnsd -d -D 2 -i 90 -b fermicloud053.fnal.gov:1095:/data/inventory \n\n\n\n\n\n\nStarting a second instance of XRootD on EL 7\n\n\nThe procedure for starting a second instance differs between EL 6 and EL 7. This section is the procedure for EL 7.\n\n\n\n\nCreate a symlink pointing to \n/etc/xrootd/xrootd-clustered.cfg\n at \n/etc/xrootd/xrootd-cns.cfg\n:\n\n\n\n\nroot@host #\n ln -s /etc/xrootd/xrootd-clustered.cfg /etc/xrootd/xrootd-cns.cfg \n\n\n\n\n\n\n\nStart an instance of the \nxrootd\n service named \ncns\n using the syntax in the \nmanaging services section\n:\n\n\n\n\nroot@host #\n systemctl start \nxrootd@cns\n\n\n\n\n\n\nTesting an XRootD cluster with SSI\n\n\n\n\nCopy file to redirector node specifying storage path (/data/xrootdfs instead of /tmp): \n\n\n\n\nroot@host #\n xrdcp /bin/sh root://localhost:1094//data/xrootdfs/test1 \n\n\\[xrootd\\] Total 0.00 MB |**`================`**| 100.00 % \\[inf MB/s\\] \n\n\n\n\n\n\n\n\nTo verify that SSI is working execute cns_ssi command on the redirector node: \n\n\n\n\nroot@host #\n cns\n\\_\nssi list /data/inventory \n\nfermicloud054.fnal.gov incomplete inventory as of Mon Apr 11 17:28:11 2011 \n\n\nroot@host #\n cns\n\\_\nssi updt /data/inventory \n\ncns\\_ssi: fermicloud054.fnal.gov inventory with 1 directory and 1 file updated with 0 errors. \n\n\nroot@host #\n cns\n\\_\nssi list /data/inventory \n\nfermicloud054.fnal.gov complete inventory as of Tue Apr 12 07:38:29 2011 /data/xrootdfs/test1 \n\n\n\n\n\n\nNote\n: In this example, \nfermicloud53.fnal.gov\n is a redirector node and \nfermicloud054.fnal.gov\n is a data node.\n\n\n(Optional) Authorization\n\n\nThere are several authorization options in XRootD available through the security\nplugins. In this document, we will cover two options for security:\n\n\n\n\nSimple Unix Security: Based on user accounts that the client is logged in as.\n\n\nxrootd-lcmaps: Using the lcmaps callout to use GUMS authorization\n\n\n\n\nNote: On the data nodes, the files will actually be owned by unix user \nxrootd\n\n(or other daemon user), not as the user authenticated to, under most\ncircumstances. xrootd will verify the permissions and authorization based on the\nuser that the security plugin authenticates you to (for instance, your unix user\nfor option 1 or your gums id for option 2), but, internally, the data node files\nwill be owned by the \nxrootd\n user.\n\n\nAuthorization file\n\n\nIn order to add security to your cluster you will need to add \"auth_file\" on\nthe your data server node. Create \n/etc/xrootd/auth_file\n :\n\n\n\\# This means that all the users have read access to the datasets \nu \\* \n/data/xrootdfs\n lr\n\n\\# This means that all the users have full access to their private dirs \nu = \n/data/xrootdfs/\n@=/ a\n\n\\# This means that this privileged user can do everything \n\\# You need at least one user like that, in order to create the \n\\# private dir for each user willing to store his data in the facility \nu xrootd \n/data/xrootdfs\n a \n\n\n\n\n\nHere we assume that your storage path is \n/data/xrootdfs\n (same as in the\nprevious example).\n\n\nChange file ownership (if you have created file as root):\n\n\nroot@host #\n chown xrootd:xrootd /etc/xrootd/auth\n\\_\nfile\n\n\n\n\n\nThis file is a flat file of the following form:\n\n\nidtype id path privs\n\n\n\n\n\nSome examples of each option. For more details or examples on how to use\ntemplated user options, see \nXRootd Authorization Database\nFile\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nidtype\n\n\nType of id - u for username, g for group, etc\n\n\n\n\n\n\nid\n\n\nUsername (or groupname). Use \n*\n for all users or \n=\n for user-specific capabilities, like home directories\n\n\n\n\n\n\npath\n\n\nThe path prefix to be used for matching purposes.\n\n\n\n\n\n\nprivs\n\n\nLetter list of privileges: \na - all ; l - lookup ; d - delete ; n - rename ; i - insert ; r - read ; k - lock (not used) ; w - write\n\n\n\n\n\n\n\n\nSecurity option 1: adding simple (Unix) security\n\n\nThe first step in adding simple Unix security to validate based on username is\nto create the \nauth_file\n as in the previous section.\n\n\nThe next step is to modify \n/etc/xrootd/xrootd-clustered.cfg\n on both nodes:\n\n\nall.export /data/xrootdfs \nset xrdr=\nhostA\n \nall.manager $(xrdr):3121 \nif $(xrdr) \n named cns \n    all.export /data/inventory \n    xrd.port 1095 \nelse if $(xrdr) \n    all.role manager \n    xrd.port 1094 \nelse \n    all.role server \n    oss.localroot /local/xrootd \n    ofs.notify closew create mkdir mv rm rmdir trunc | /usr/bin/XrdCnsd -d -D 2 -i 90 -b $(xrdr):1095:/data/inventory \n    cms.space min 2g 5g \n\n    \n \\# ENABLE\\_SECURITY\\_BEGIN \n    xrootd.seclib /usr/lib64/libXrdSec.so \n    \\# this specify that we use the \nunix\n authentication module, additional one can be specified. \n    sec.protocol /usr/lib64 unix \n    \\# this is the authorization file \n    acc.authdb /etc/xrootd/auth\\_file \n    ofs.authorize \n    \\# ENABLE\\_SECURITY\\_END \n \nfi \n\n\n\n\n\nNote that, to access users directories, you will have to create them in\noss.localroot (For instance, \n/local/xrootd/data/xrootdfs/username\n) and make\nsure they are writable by \nxrootd\n user (or the daemon user, if you have changed\nit). Files in localroot on the data nodes are normally owned by \nxrootd\n not by\nthe authenticated username.\n\n\nAfter making all the changes, please, restart xrootd and cmsd daemons on all nodes.\n\n\nTesting an XRootD cluster with simple security enabled\n\n\n\n\nLogin on redirector node as root\n\n\nCheck that user \"root\" still can read files: \n\n\n\n\nroot@host #\n xrdcp root://localhost:1094//data/xrootdfs/test1 /tmp/b \n\n\\[xrootd\\] Total 0.00 MB |**`================`**| 100.00 % \\[inf MB/s\\]\n\n\n\n\n\n\n\n\nCheck that user \"root\" can not write files under /data/xrootdfs:\n\n\n\n\nroot@host #\n xrdcp /tmp/b root://localhost:1094//data/xrootdfs/test2 \n\nLast server error 3010 (\nUnable to create /data/xrootdfs/test2; Permission denied\n) \n\n\nError accessing path/file for root://localhost:1094//data/xrootdfs/test3 \n\n\n\n\n\n\nor you may get this error:\n\n\nroot@host #\n xrdcp /tmp/b root://localhost:1094//data/xrootdfs/test2 \n\nLast server error 3011 (\nNo servers are available to write the file.\n) \n\n\nError accessing path/file for root://localhost:1094//data/xrootdfs/test2\n\n\n\n\n\n\n\n\nCheck that user can copy/retrieve files to/from /data/xrootdfs/~/...\n\n\n\n\nroot@host #\n su - \nuser\n \n\n-bash-3.2$ xrdcp /tmp/a root://localhost:1094//data/xrootdfs/\nuser\n/test1\n\n\n\\[xrootd\\] Total 0.00 MB |**`================`**| 100.00 % \\[inf MB/s\\] \n\n\n-bash-3.2$ xrdcp root://localhost:1094//data/xrootdfs/\nuser\n/test1 /tmp/c \n\n\n\\[xrootd\\] Total 0.00 MB |**`================`**| 100.00 % \\[inf MB/s\\]\n\n\n\n\n\n\nSecurity option 2: xrootd-lcmaps authorization\n\n\nThe xrootd-lcmaps security plugin uses the \nlcmaps\n package to access the \nGUMS\n\nserver to authenticate and authorize users based on X509 certificates.\n\n\nCertificate Installation\n\n\nIn order to use lcmaps, you will need CA certificates and certificate revocation\nlists. See the following documents for instructions:\n\n\n\n\nInstall CA certificates\n\n\nManaging Certificate Revocation Lists\n\n\n\n\nInstall xrootd-lcmaps\n\n\nyum install xrootd-lcmaps\n\n\n\n\n\n\nNote that the xrootd-lcmaps is usually coupled to a specific version of xrootd,\nso make sure that you install xrootd-lcmaps from the same repository that you\ninstall xrootd from. Otherwise, you may have dependency issues due to differing\nversions of shared libraries.\n\n\nCreate host certificates\n\n\nYou will need to have a X509 certificate to talk to GUMS. If you already have a\nhost certificate, you can use a copy of that:\n\n\n mkdir /etc/grid-security/xrd\n\n\n cp /etc/grid-security/hostkey.pem /etc/grid-security/xrd/xrdkey.pem\n\n\n cp /etc/grid-security/hostcert.pem /etc/grid-security/xrd/xrdcert.pem\n\n\n chown -R xrootd:xrootd /etc/grid-security/xrd/\n\n\n chmod 400 /etc/grid-security/xrd/xrdkey.pem\n\n\n\n\n\n\nThis certificate should be owned by xrootd and located in \n/etc/grid-security/xrd\n.\n\n\nAuthorization File\n\n\nNext, create \n/etc/xrootd/auth_file\n using the example in the above section.\n\n\nModify /etc/xrootd/lcmaps.cfg\n\n\nEdit \n/etc/xrootd/lcmaps.cfg\n to point to your authorization server (replace the\nhostname in red):\n\n\nscasclient = \nlcmaps_scas_client.mod\n\n             \n-resourcetype wn\n\n             \n-actiontype execute-now\n\n             \n-capath /etc/grid-security/certificates\n\n             \n-cert   /etc/grid-security/xrd/xrdcert.pem\n\n             \n-key    /etc/grid-security/xrd/xrdkey.pem\n\n             \n--endpoint https://\ngums.fnal.gov\n:8443/gums/services/GUMSXACMLAuthorizationServicePort\n\n\n\n\n\n\nModify /etc/xrootd/xrootd-clustered.cfg\n\n\nYou will need to add the security plugins to \n/etc/xrootd/xrootd-clustered.cfg\n. Add the following lines to the \n/etc/xrootd/xrootd-clustered.cfg\n. This will need to be added to all data nodes in the section relevant to the data node server. (For instance, in the above example(s), this should be placed in the last \"else\" clause after \"all.role server\". See the section above on Unix security for an example of where the security commands should go).\n\n\nxrootd\n.\nseclib\n \n/\nusr\n/\nlib64\n/\nlibXrdSec\n.\nso\n\n\n\nset\n \nCERTDIR\n=-\ncertdir\n:\n/\netc\n/\ngrid\n-\nsecurity\n/\ncertificates\n\n\nset\n \nCERT\n=-\ncert\n:\n/\netc\n/\ngrid\n-\nsecurity\n/\nxrd\n/\nxrdcert\n.\npem\n\n\nset\n \nKEY\n=-\nkey\n:\n/\netc\n/\ngrid\n-\nsecurity\n/\nxrd\n/\nxrdkey\n.\npem\n\n\nset\n \nAUTHZFUN\n=-\nauthzfun\n:\nlibXrdLcmaps\n.\nso\n\n\nset\n \nAUTHZFUNPARMS\n=-\nauthzfunparms\n:\n--osg\n,\n--lcmapscfg\n,\n/\netc\n/\nxrootd\n/\nlcmaps\n.\ncfg\n,\n--loglevel\n,\n0\n|\nuseglobals\n\n\n\nsec\n.\nprotocol\n \n/\nusr\n/\nlib64\n \ngsi\n \n$\nCERTDIR\n \n$\nCERT\n \n$\nKEY\n \n-\ncrl\n:\n3\n \n$\nAUTHZFUN\n \n$\nAUTHZFUNPARMS\n \n--\ngmapopt\n:\n2\n \n--\ngmapto\n:\n0\n\n\nacc\n.\nauthdb\n \n/\netc\n/\nxrootd\n/\nauth_file\n\n\nofs\n.\nauthorize\n\n\n\n\n\n\nRestart xrootd and cmsd\n\n\nroot@host #\n service xrootd restart\n\nroot@host #\n service cmsd restart\n\n\n\n\n\nlcmaps notes\n\n\nFor recent versions of lcmaps (1.5+), VOMS signature verification is enabled by default. You may need to either disable it or install the \nvo-client\n.\n\n\nInstalling vo-client:\n\n\nroot@host #\n yum install vo-client\n\n\n\n\n\nDisabling vo verification can be done by adding a line to \n/etc/sysconfig/xrootd\n:\n\n\nexport LLGT_VOMS_DISABLE_CREDENTIAL_CHECK=1\n\n\n\n\n\nThis feature is evolving, so stay tuned for updates.\n\n\nTesting an XRootd Cluster with LCMAPS security enabled\n\n\nFrom any machine with the \nxrootd-client\n installed, you can test with xrdcp. With a user that has no grid certificate installed, you should get an error:\n\n\nuser@host $\n xrdcp /bin/bash root://HOSTNAME/tmp/lcmaps_test\n\n120327 14:31:52 10509 secgsi_InitProxy: cannot access private key file: /home/dstrain/.globus/userkey.pem\n\n\nXrdSec: No authentication protocols are available.\n\n\nLast server error 3010 (\ncannot obtain credentials for protocol: Secgsi: ErrParseBuffer: error getting user proxies: kXGS_init: unable to get protocol object.\n)\n\n\nError accessing path/file for root://fermicloud121.fnal.gov/tmp/test2\n\n\n\n\n\n\nAfter running \nvoms-proxy-init\n or \ngrid-proxy-init\n to initialize your x509 certificate (usually found in \n/tmp/x509up_uUID\n), the \nxrdcp\n command should execute cleanly. For instance, the following shows an example of a user creating a voms certificate and copying to the xrootd client and then re-executing the xrdcp command.\n\n\nuser@host $\n voms-proxy-init -voms Engage -valid \n999\n:0\n\nYour identity: /DC=org/DC=doegrids/OU=People/CN=Doug Strain 834323\n\n\nCreating temporary proxy ..................................................... Done\n\n\nContacting  osg-engage.renci.org:15001 [/DC=org/DC=doegrids/OU=Services/CN=osg-engage.renci.org] \nEngage\n Done\n\n\nCreating proxy .......................... Done\n\n\n\nYour proxy is valid until Tue May  8 05:34:40 2012\n\n\nuser@host $\n scp /tmp/x509up_u44678 CLIENT_HOSTNAME:/tmp/x509up_u44678\n\n\n\n\n\nOn the \nxrootd-client\n node,\n\n\nuser@host $\n xrdcp /bin/bash root://fermicloud121.fnal.gov//tmp/lcmaps_test\n\n[xrootd] Total 0.73 MB  |====================| 100.00 % [inf MB/s]\n\n\n\n\n\n\nIn the above examples, make sure to change \"/tmp\" to a directory allowed by the\n\n/etc/xrootd/auth_file\n created in a previous section.\n\n\n(Optional) Adding CMS TFC support to XRootD (CMS sites only)\n\n\nFor CMS users, there is a package available to integrate rule-based name lookup\nusing a \nstorage.xml\n file. If you are not setting up a CMS site, you can skip\nthis section.\n\n\nyum install --enablerepo=osg-contrib xrootd-cmstfc\n\n\n\n\n\n\nYou will need to add your \nstorage.xml\n to \n/etc/xrootd/storage.xml\n and then\nadd the following line to your xrootd configuration:\n\n\n# Integrate with CMS TFC, placed in /etc/xrootd/storage.xml\noss.namelib /usr/lib64/libXrdCmsTfc.so file:/etc/xrootd/storage.xml%ORANGE%?protocol=hadoop\n\n\n\n\n\n\nAdd the orange text only if you are running hadoop (see below).\n\n\nSee the CMS TWiki for more information:\n\n\n\n\nhttps://twiki.cern.ch/twiki/bin/view/Main/XrootdTfcChanges\n\n\nhttps://twiki.cern.ch/twiki/bin/view/Main/HdfsXrootdInstall\n\n\n\n\n(Optional) Adding Hadoop support to XRootD\n\n\n\n\nNote\n\n\n3.3 only\n\n\n\n\nUsers with Hadoop-based storage under their XRootD installation have extra steps\nto configure more efficient access to Hadoop Stroage\n\n\nroot@host #\n yum install xrootd-hdfs\n\n\n\n\n\nYou will then need to add the following lines to your\n\n/etc/xrootd/xrootd-clustered.cfg\n:\n\n\nxrootd.fslib /usr/lib64/libXrdOfs.so\nofs.osslib /usr/lib64/libXrdHdfs.so\n\n\n\n\n\nFor more information, see \nStorage.HadoopXrootd\n.\n\n\n(Optional) Adding File Residency Manager (FRM) to an XRootd cluster\n\n\nIf you have a multi-tiered storage system (e.g. some data is stored on SSDs and\nsome on disks or tapes), then install the File Residency Manager (FRM), so you\ncan move data between tiers more easily. If you do not have a multi-tiered\nstorage system, then you do not need FRM and you can skip this section.\n\n\nThe FRM deals with two major mechanisms:\n\n\n\n\nlocal disk\n\n\nremote servers\n\n\n\n\nThe description of fully functional multiple xrootd clusters is beyond the scope\nof this document. In order to have this fully functional system you will need a\nglobal redirector and at least one remote xrootd cluster from where files could\nbe moved to the local cluster.\n\n\nBelow are the modifications you should make in order to enable FRM on your local\ncluster:\n\n\n\n\nMake sure that FRM is enabled in \n/etc/sysconfig/xrootd\n on your data sever:\n\n\n\n\nROOTD\\_USER=xrootd \nXROOTD\\_GROUP=xrootd \nXROOTD\\_DEFAULT\\_OPTIONS=\n-l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-clustered.cfg\n \nCMSD\\_DEFAULT\\_OPTIONS=\n-l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg\n \nFRMD\\_DEFAULT\\_OPTIONS=\n-l /var/log/xrootd/frmd.log -c /etc/xrootd/xrootd-clustered.cfg\n \nXROOTD\\_INSTANCES=\ndefault\n \nCMSD\\_INSTANCES=\ndefault\n \nFRMD\\_INSTANCES=\ndefault\n\n\n\n\n\n\n\n\nModify \n/etc/xrootd/xrootd-clustered.cfg\n on both nodes to specify options for \nfrm_xfrd\n (File Transfer Daemon) and \nfrm_purged\n (File Purging Daemon). For more information, you can visit the \nFRM Documentation\n\n\nStart frm daemons on data server: \n\n\n\n\nroot@host #\n service frm\n\\_\nxfrd start\n\nroot@host #\n service frm\n\\_\npurged start\n\n\n\n\n\nUsing XRootD\n\n\nManaging XRootD services\n\n\nStart services on the redirector node before starting any services on the data\nnodes. If you installed only XRootD itself, you will only need to start the\n\nxrootd\n service. However, if you installed cluster management services, you\nwill need to start \ncmsd\n as well.\n\n\nThe instructions for starting and stopping an XRootD service depend on whether\nthe service is installed on an EL 6 or EL 7 machine, and whether you are using a\nstandalone or clustered configuration.\n\n\nOn EL 6, which config to use is set in the file \n/etc/sysconfig/xrootd\n. For\nexample, to have \nxrootd\n use the clustered config, you would have a line such\nas this:\n\n\nXROOTD_DEFAULT_OPTIONS=\n-l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-\nclustered\n.cfg -k fifo\n\n\n\n\n\n\nTo use the standalone config instead, you would use:\n\n\nXROOTD_DEFAULT_OPTIONS=\n-l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-\nstandalone\n.cfg -k fifo\n\n\n\n\n\n\nOn EL 7, which config to use is determined by the service name given to\n\nsystemctl\n. For example, to have \nxrootd\n use the clustered config, you would\nstart up \nxrootd\n with this line:\n\n\nroot@host #\n systemctl start xrootd@\nclustered\n\n\n\n\n\n\nTo use the standalone config instead, you would use:\n\n\nroot@host #\n systemctl start xrootd@\nstandalone\n\n\n\n\n\n\nThe services are:\n\n\n\n\n\n\n\n\nService\n\n\nEL 6 service name\n\n\nEL 7 service name\n\n\n\n\n\n\n\n\n\n\nXRootD (standalone config)\n\n\nxrootd\n\n\nxrootd@standalone\n\n\n\n\n\n\nXRootD (clustered config)\n\n\nxrootd\n\n\nxrootd@clustered\n\n\n\n\n\n\nCMSD (clustered config)\n\n\ncmsd\n\n\ncmsd@clustered\n\n\n\n\n\n\n\n\nAs a reminder, here are common service commands (all run as \nroot\n):\n\n\n\n\n\n\n\n\nTo \u2026\n\n\nOn EL\u00a06, run the command\u2026\n\n\nOn EL\u00a07, run the command\u2026\n\n\n\n\n\n\n\n\n\n\nStart a service\n\n\nservice \nem\nSERVICE-NAME\n/em\n start\n\n\nsystemctl start \nem\nSERVICE-NAME\n/em\n\n\n\n\n\n\nStop a service\n\n\nservice \nem\nSERVICE-NAME\n/em\n stop\n\n\nsystemctl start \nem\nSERVICE-NAME\n/em\n\n\n\n\n\n\nEnable a service to start during boot\n\n\nchkconfig \nem\nSERVICE-NAME\n/em\n on\n\n\nsystemctl enable \nem\nSERVICE-NAME\n/em\n\n\n\n\n\n\nDisable a service from starting during boot\n\n\nchkconfig \nem\nSERVICE-NAME\n/em\n off\n\n\nsystemctl disable \nem\nSERVICE-NAME\n/em\n\n\n\n\n\n\n\n\nGetting Help\n\n\nTo get assistance. please use the \nHelp Procedure\n page.\n\n\nReference\n\n\nFile locations\n\n\n\n\n\n\n\n\nService/Process\n\n\nConfiguration File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nxrootd\n\n\n/etc/xrootd/xrootd-clustered.cfg\n\n\nMain clustered mode XRootD configuration\n\n\n\n\n\n\n^\n\n\n/etc/xrootd/auth_file\n\n\nAuthorized users file\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nService/Process\n\n\nLog File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nxrootd\n\n\n/var/log/xrootd/xrootd.log\n\n\nXRootD server daemon log\n\n\n\n\n\n\ncmsd\n\n\n/var/log/xrootd/cmsd.log\n\n\nCluster management log\n\n\n\n\n\n\ncns\n\n\n/var/log/xrootd/cns/xrootd.log\n\n\nServer inventory (composite name space) log\n\n\n\n\n\n\nfrm_xfrd\n, \nfrm_purged\n\n\n/var/log/xrootd/frmd.log\n\n\nFile Residency Manager log\n\n\n\n\n\n\n\n\nLinks\n\n\n\n\nXRootD documentation", 
            "title": "Install XRootD"
        }, 
        {
            "location": "/data/install-xrootd/#installing-and-maintaining-xrootd", 
            "text": "XRootD  is a hierarchical storage system that can be used\nin a variety of ways to access data, typically distributed among actual storage\nresources. One way to use XRootD is to have it refer to many data resources at a\nsingle site, and another way to use it is to refer to many storage systems, most\nlikely distributed among sites. An XRootD system includes a  redirector , which\naccepts requests for data and finds a storage repository\u00a0\u2014 locally or\notherwise\u00a0\u2014 that can provide the data to the requestor.  Use this page to learn how to install, configure, and use an XRootD redirector\nas part of a Storage Element (SE) or as part of a global namespace.", 
            "title": "Installing and Maintaining XRootD"
        }, 
        {
            "location": "/data/install-xrootd/#before-starting", 
            "text": "Before starting the installation process, consider the following points:   User IDs:  If it does not exist already, the installation will create the Linux user ID  xrootd  Service certificate:  The XRootD service uses a host certificate at  /etc/grid-security/host*.pem  Networking:  The XRootD service uses port 1094 by default   As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system  Obtain root access to the host  Prepare  the required Yum repositories  Install  CA certificates", 
            "title": "Before Starting"
        }, 
        {
            "location": "/data/install-xrootd/#installing-an-xrootd-server", 
            "text": "An installation of the XRootD server consists of the server itself and its\ndependencies. Install these with Yum:  root@host #  yum install xrootd", 
            "title": "Installing an XRootD Server"
        }, 
        {
            "location": "/data/install-xrootd/#configuring-an-xrootd-server", 
            "text": "", 
            "title": "Configuring an XRootD Server"
        }, 
        {
            "location": "/data/install-xrootd/#minimal-configuration", 
            "text": "A new installation of XRootD is already configured to run a standalone server\nthat serves files from  /tmp  on the local file system. This configuration is\nuseful to verify basic connectivity between your clients and your server. To do\nthis, start the  xrootd  service with standalone config as described in the managing services section .  You should be able now to copy a file such as  /bin/sh  using  xrdcp  command\ninto  /tmp . To test, do:  root@host #  yum install xrootd-client root@host #  xrdcp /bin/sh root://localhost:1094//tmp/first_test [xrootd] Total 0.76 MB  |====================| 100.00 % [inf MB/s]  root@host #  ls -l /tmp/first_test -rw-r--r-- 1 xrootd xrootd 801512 Apr 11 10:48 /tmp/first_test   Other than for testing, a standalone server is useful when you want to serve\nfiles off of a single host with lots of large disks. If your storage capacity is\nspread out over multiple hosts, you will need to set up an XRootD cluster.", 
            "title": "Minimal configuration"
        }, 
        {
            "location": "/data/install-xrootd/#advanced-configuration", 
            "text": "An advanced XRootD setup has multiple components; it is important to validate\nthat each additional component that you set up is working before moving on to\nthe next component. We have included validation instructions after each\ncomponent below.", 
            "title": "Advanced configuration"
        }, 
        {
            "location": "/data/install-xrootd/#creating-an-xrootd-cluster", 
            "text": "If your storage is spread out over multiple hosts, you will need to set up an\nXRootD  cluster . The cluster uses one \"redirector\" node as a frontend for user\naccesses, and multiple data nodes that have the data that users request. Two\ndaemons will run on each node:  xrootd \nThe eXtended Root Daemon controls file access and storage.  cmsd \nThe Cluster Management Services Daemon controls communication between nodes.  Note that for large virtual organizations, a site-level redirector may actually\nalso communicate upwards to a regional or global redirector that handles access\nto a multi-level hierarchy. This section will only cover handling one level of\nXRootD hierarchy.  In the instructions below,  RDRNODE  will refer to the redirector host\nand  DATANODE  will refer to the data node host. These should be\nreplaced with the fully-qualified domain name of the host in question.", 
            "title": "Creating an XRootD cluster"
        }, 
        {
            "location": "/data/install-xrootd/#modify-etcxrootdxrootd-clusteredcfg", 
            "text": "You will need to modify the  xrootd-clustered.cfg  on the redirector node and\neach data node. The following example should serve as a base configuration for\nclustering. Further customizations are detailed below.  all.export  /tmp  stage\nset xrdr =  RDRNODE \nall.manager $(xrdr):3121\n\nif $(xrdr)\n  # Lines in this block are only executed on the redirector node\n  all.role manager\nelse\n  # Lines in this block are executed on all nodes but the redirector node\n  all.role server\n  cms.space min  2g 5g \nfi  You will need to customize the following lines:     Configuration Line  Changes Needed      all.export  /tmp  stage  Change  /tmp  to the directory to allow XRootD access to    set xrdr= RDRNODE  Change to the hostname of the redirector    cms.space min  2g 5g  Reserve this amount of free space on the node. For this example, if space falls below 2GB, xrootd will not store further files on this node until space climbs above 5GB. You can use  k ,  m ,  g , or  t  to indicate kilobyte, megabytes, gigabytes, or terabytes, respectively.     Further information can be found at  http://xrootd.slac.stanford.edu/doc", 
            "title": "Modify /etc/xrootd/xrootd-clustered.cfg"
        }, 
        {
            "location": "/data/install-xrootd/#verifying-the-clustered-config", 
            "text": "Start both  xrootd  and  cmsd  on all nodes according to the instructions in the managing services section .  Verify that you can copy a file such as  /bin/sh  to  /tmp  on the server data\nvia the redirector:  root@host #  xrdcp /bin/sh  root:// RDRNODE :1094///tmp/second_test [xrootd] Total 0.76 MB  |====================| 100.00 % [inf MB/s]   Check that the  /tmp/second_test  is located on data server  DATANODE .", 
            "title": "Verifying the clustered config"
        }, 
        {
            "location": "/data/install-xrootd/#optional-adding-simple-server-inventory-to-your-cluster", 
            "text": "The Simple Server Inventory (SSI) provide means to have an inventory for each\ndata server. SSI requires:   A second instance of the  xrootd  daemon on the redirector  A \"composite name space daemon\" ( XrdCnsd ) on each data server; this daemon handles the inventory   As an example, we will set up a two-node XRootD cluster with SSI.  Host A is a redirector node that is running the following daemons:   xrootd redirector  cmsd  xrootd - second instance that required for SSI   Host B is a data server that is running the following daemons:   xrootd data server  cmsd  XrdCnsd - started automatically by xrootd   We will need to create a directory on the redirector node for Inventory files.  root@host #  mkdir -p /data/inventory root@host #  chown xrootd:xrootd /data/inventory  On the data server (host B) let's create the storage cache that will be different from  /tmp .  root@host #  mkdir -p  /local/xrootd root@host #  chown xrootd:xrootd /local/xrootd  We will be running two instances of XRootD on  hostA . Modify /etc/xrootd/xrootd-clustered.cfg  to give the two instances different behavior,\nas such:  all.export /data/xrootdfs\nset xrdr= hostA \nall.manager $(xrdr):3121\nif $(xrdr)  amp; amp; named cns\n      all.export /data/inventory\n      xrd.port 1095\nelse if $(xrdr)\n      all.role manager\n      xrd.port 1094\nelse\n      all.role server\n      oss.localroot /local/xrootd\n      ofs.notify closew create mkdir mv rm rmdir trunc | /usr/bin/XrdCnsd -d -D 2 -i 90 -b $(xrdr):1095:/data/inventory\n      #add cms.space if you have less the 11GB\n      # cms.space options http://xrootd.slac.stanford.edu/doc/dev/cms_config.htm\n      cms.space min 2g 5g\nfi", 
            "title": "(Optional) Adding Simple Server Inventory to your cluster"
        }, 
        {
            "location": "/data/install-xrootd/#starting-a-second-instance-of-xrootd-on-el-6", 
            "text": "The procedure for starting a second instance differs between EL 6 and EL 7. This\nsection is the procedure for EL 6.  Now, we have to change  /etc/sysconfig/xrootd  on the redirector node ( hostA ) to run multiple instances of xrootd. The second instance of xrootd will be named \"cns\" and will be used for SSI.  XROOTD\\_USER=xrootd \nXROOTD\\_GROUP=xrootd \nXROOTD\\_DEFAULT\\_OPTIONS= -k 7  -l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-clustered.cfg  XROOTD\\_CNS\\_OPTIONS= -k 7 -l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-clustered.cfg  \nCMSD\\_DEFAULT\\_OPTIONS= -k 7  -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg  \nFRMD\\_DEFAULT\\_OPTIONS= -k 7  -l /var/log/xrootd/frmd.log -c /etc/xrootd/xrootd-clustered.cfg   XROOTD\\_INSTANCES= default cns  \nCMSD\\_INSTANCES= default  \nFRMD\\_INSTANCES= default    Now, we can start xrootd cluster executing the following commands. On redirector you will see:  root@host #  service xrootd start  Starting xrootd (xrootd, default):  \\[ OK \\]    Starting xrootd (xrootd, cns):  \\[ OK \\]    root@host #  service cmsd start  Starting xrootd (cmsd, default):  \\[ OK \\]     On redirector node you should see two instances of xrootd running:  root@host #  ps auxww | grep xrootd  xrootd 29036 0.0 0.0 44008 3172 ? Sl Apr11 0:00 /usr/bin/xrootd -k 7 -l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-clustered.cfg -b -s /var/run/xrootd/xrootd-default.pid -n default   xrootd 29108 0.0 0.0 43868 3016 ? Sl Apr11 0:00 /usr/bin/xrootd -k 7 -l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-clustered.cfg -b -s /var/run/xrootd/xrootd-cns.pid -n cns   xrootd 29196 0.0 0.0 51420 3692 ? Sl Apr11 0:00 /usr/bin/cmsd -k 7 -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg -b -s /var/run/xrootd/cmsd-default.pid -n default    warning  the log file for second named instance of xrootd with be placed in  /var/log/xrootd/cns/xrootd.log  On data server node you should that XrdCnsd process has been started:  root@host #  ps auxww | grep xrootd  xrootd 19156 0.0 0.0 48096 3256 ? Sl 07:37 0:00 /usr/bin/cmsd -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg -b -s /var/run/xrootd/cmsd-default.pid -n default   xrootd 19880 0.0 0.0 46124 2916 ? Sl 08:33 0:00 /usr/bin/xrootd -l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-clustered.cfg -b -s /var/run/xrootd/xrootd-default.pid -n default   xrootd 19894 0.0 0.1 71164 6960 ? Sl 08:33 0:00 /usr/bin/XrdCnsd -d -D 2 -i 90 -b fermicloud053.fnal.gov:1095:/data/inventory", 
            "title": "Starting a second instance of XRootD on EL 6"
        }, 
        {
            "location": "/data/install-xrootd/#starting-a-second-instance-of-xrootd-on-el-7", 
            "text": "The procedure for starting a second instance differs between EL 6 and EL 7. This section is the procedure for EL 7.   Create a symlink pointing to  /etc/xrootd/xrootd-clustered.cfg  at  /etc/xrootd/xrootd-cns.cfg :   root@host #  ln -s /etc/xrootd/xrootd-clustered.cfg /etc/xrootd/xrootd-cns.cfg    Start an instance of the  xrootd  service named  cns  using the syntax in the  managing services section :   root@host #  systemctl start  xrootd@cns", 
            "title": "Starting a second instance of XRootD on EL 7"
        }, 
        {
            "location": "/data/install-xrootd/#testing-an-xrootd-cluster-with-ssi", 
            "text": "Copy file to redirector node specifying storage path (/data/xrootdfs instead of /tmp):    root@host #  xrdcp /bin/sh root://localhost:1094//data/xrootdfs/test1  \\[xrootd\\] Total 0.00 MB |**`================`**| 100.00 % \\[inf MB/s\\]     To verify that SSI is working execute cns_ssi command on the redirector node:    root@host #  cns \\_ ssi list /data/inventory  fermicloud054.fnal.gov incomplete inventory as of Mon Apr 11 17:28:11 2011   root@host #  cns \\_ ssi updt /data/inventory  cns\\_ssi: fermicloud054.fnal.gov inventory with 1 directory and 1 file updated with 0 errors.   root@host #  cns \\_ ssi list /data/inventory  fermicloud054.fnal.gov complete inventory as of Tue Apr 12 07:38:29 2011 /data/xrootdfs/test1    Note : In this example,  fermicloud53.fnal.gov  is a redirector node and  fermicloud054.fnal.gov  is a data node.", 
            "title": "Testing an XRootD cluster with SSI"
        }, 
        {
            "location": "/data/install-xrootd/#optional-authorization", 
            "text": "There are several authorization options in XRootD available through the security\nplugins. In this document, we will cover two options for security:   Simple Unix Security: Based on user accounts that the client is logged in as.  xrootd-lcmaps: Using the lcmaps callout to use GUMS authorization   Note: On the data nodes, the files will actually be owned by unix user  xrootd \n(or other daemon user), not as the user authenticated to, under most\ncircumstances. xrootd will verify the permissions and authorization based on the\nuser that the security plugin authenticates you to (for instance, your unix user\nfor option 1 or your gums id for option 2), but, internally, the data node files\nwill be owned by the  xrootd  user.", 
            "title": "(Optional) Authorization"
        }, 
        {
            "location": "/data/install-xrootd/#authorization-file", 
            "text": "In order to add security to your cluster you will need to add \"auth_file\" on\nthe your data server node. Create  /etc/xrootd/auth_file  :  \\# This means that all the users have read access to the datasets \nu \\*  /data/xrootdfs  lr\n\n\\# This means that all the users have full access to their private dirs \nu =  /data/xrootdfs/ @=/ a\n\n\\# This means that this privileged user can do everything \n\\# You need at least one user like that, in order to create the \n\\# private dir for each user willing to store his data in the facility \nu xrootd  /data/xrootdfs  a   Here we assume that your storage path is  /data/xrootdfs  (same as in the\nprevious example).  Change file ownership (if you have created file as root):  root@host #  chown xrootd:xrootd /etc/xrootd/auth \\_ file  This file is a flat file of the following form:  idtype id path privs  Some examples of each option. For more details or examples on how to use\ntemplated user options, see  XRootd Authorization Database\nFile .           idtype  Type of id - u for username, g for group, etc    id  Username (or groupname). Use  *  for all users or  =  for user-specific capabilities, like home directories    path  The path prefix to be used for matching purposes.    privs  Letter list of privileges:  a - all ; l - lookup ; d - delete ; n - rename ; i - insert ; r - read ; k - lock (not used) ; w - write", 
            "title": "Authorization file"
        }, 
        {
            "location": "/data/install-xrootd/#security-option-1-adding-simple-unix-security", 
            "text": "The first step in adding simple Unix security to validate based on username is\nto create the  auth_file  as in the previous section.  The next step is to modify  /etc/xrootd/xrootd-clustered.cfg  on both nodes:  all.export /data/xrootdfs \nset xrdr= hostA  \nall.manager $(xrdr):3121 \nif $(xrdr)   named cns \n    all.export /data/inventory \n    xrd.port 1095 \nelse if $(xrdr) \n    all.role manager \n    xrd.port 1094 \nelse \n    all.role server \n    oss.localroot /local/xrootd \n    ofs.notify closew create mkdir mv rm rmdir trunc | /usr/bin/XrdCnsd -d -D 2 -i 90 -b $(xrdr):1095:/data/inventory \n    cms.space min 2g 5g \n\n      \\# ENABLE\\_SECURITY\\_BEGIN \n    xrootd.seclib /usr/lib64/libXrdSec.so \n    \\# this specify that we use the  unix  authentication module, additional one can be specified. \n    sec.protocol /usr/lib64 unix \n    \\# this is the authorization file \n    acc.authdb /etc/xrootd/auth\\_file \n    ofs.authorize \n    \\# ENABLE\\_SECURITY\\_END   \nfi   Note that, to access users directories, you will have to create them in\noss.localroot (For instance,  /local/xrootd/data/xrootdfs/username ) and make\nsure they are writable by  xrootd  user (or the daemon user, if you have changed\nit). Files in localroot on the data nodes are normally owned by  xrootd  not by\nthe authenticated username.  After making all the changes, please, restart xrootd and cmsd daemons on all nodes.", 
            "title": "Security option 1: adding simple (Unix) security"
        }, 
        {
            "location": "/data/install-xrootd/#testing-an-xrootd-cluster-with-simple-security-enabled", 
            "text": "Login on redirector node as root  Check that user \"root\" still can read files:    root@host #  xrdcp root://localhost:1094//data/xrootdfs/test1 /tmp/b  \\[xrootd\\] Total 0.00 MB |**`================`**| 100.00 % \\[inf MB/s\\]    Check that user \"root\" can not write files under /data/xrootdfs:   root@host #  xrdcp /tmp/b root://localhost:1094//data/xrootdfs/test2  Last server error 3010 ( Unable to create /data/xrootdfs/test2; Permission denied )   Error accessing path/file for root://localhost:1094//data/xrootdfs/test3    or you may get this error:  root@host #  xrdcp /tmp/b root://localhost:1094//data/xrootdfs/test2  Last server error 3011 ( No servers are available to write the file. )   Error accessing path/file for root://localhost:1094//data/xrootdfs/test2    Check that user can copy/retrieve files to/from /data/xrootdfs/~/...   root@host #  su -  user   -bash-3.2$ xrdcp /tmp/a root://localhost:1094//data/xrootdfs/ user /test1  \\[xrootd\\] Total 0.00 MB |**`================`**| 100.00 % \\[inf MB/s\\]   -bash-3.2$ xrdcp root://localhost:1094//data/xrootdfs/ user /test1 /tmp/c   \\[xrootd\\] Total 0.00 MB |**`================`**| 100.00 % \\[inf MB/s\\]", 
            "title": "Testing an XRootD cluster with simple security enabled"
        }, 
        {
            "location": "/data/install-xrootd/#security-option-2-xrootd-lcmaps-authorization", 
            "text": "The xrootd-lcmaps security plugin uses the  lcmaps  package to access the  GUMS \nserver to authenticate and authorize users based on X509 certificates.  Certificate Installation  In order to use lcmaps, you will need CA certificates and certificate revocation\nlists. See the following documents for instructions:   Install CA certificates  Managing Certificate Revocation Lists   Install xrootd-lcmaps  yum install xrootd-lcmaps   Note that the xrootd-lcmaps is usually coupled to a specific version of xrootd,\nso make sure that you install xrootd-lcmaps from the same repository that you\ninstall xrootd from. Otherwise, you may have dependency issues due to differing\nversions of shared libraries.  Create host certificates  You will need to have a X509 certificate to talk to GUMS. If you already have a\nhost certificate, you can use a copy of that:   mkdir /etc/grid-security/xrd   cp /etc/grid-security/hostkey.pem /etc/grid-security/xrd/xrdkey.pem   cp /etc/grid-security/hostcert.pem /etc/grid-security/xrd/xrdcert.pem   chown -R xrootd:xrootd /etc/grid-security/xrd/   chmod 400 /etc/grid-security/xrd/xrdkey.pem   This certificate should be owned by xrootd and located in  /etc/grid-security/xrd .  Authorization File  Next, create  /etc/xrootd/auth_file  using the example in the above section.  Modify /etc/xrootd/lcmaps.cfg  Edit  /etc/xrootd/lcmaps.cfg  to point to your authorization server (replace the\nhostname in red):  scasclient =  lcmaps_scas_client.mod \n              -resourcetype wn \n              -actiontype execute-now \n              -capath /etc/grid-security/certificates \n              -cert   /etc/grid-security/xrd/xrdcert.pem \n              -key    /etc/grid-security/xrd/xrdkey.pem \n              --endpoint https:// gums.fnal.gov :8443/gums/services/GUMSXACMLAuthorizationServicePort   Modify /etc/xrootd/xrootd-clustered.cfg  You will need to add the security plugins to  /etc/xrootd/xrootd-clustered.cfg . Add the following lines to the  /etc/xrootd/xrootd-clustered.cfg . This will need to be added to all data nodes in the section relevant to the data node server. (For instance, in the above example(s), this should be placed in the last \"else\" clause after \"all.role server\". See the section above on Unix security for an example of where the security commands should go).  xrootd . seclib   / usr / lib64 / libXrdSec . so  set   CERTDIR =- certdir : / etc / grid - security / certificates  set   CERT =- cert : / etc / grid - security / xrd / xrdcert . pem  set   KEY =- key : / etc / grid - security / xrd / xrdkey . pem  set   AUTHZFUN =- authzfun : libXrdLcmaps . so  set   AUTHZFUNPARMS =- authzfunparms : --osg , --lcmapscfg , / etc / xrootd / lcmaps . cfg , --loglevel , 0 | useglobals  sec . protocol   / usr / lib64   gsi   $ CERTDIR   $ CERT   $ KEY   - crl : 3   $ AUTHZFUN   $ AUTHZFUNPARMS   -- gmapopt : 2   -- gmapto : 0  acc . authdb   / etc / xrootd / auth_file  ofs . authorize   Restart xrootd and cmsd  root@host #  service xrootd restart root@host #  service cmsd restart  lcmaps notes  For recent versions of lcmaps (1.5+), VOMS signature verification is enabled by default. You may need to either disable it or install the  vo-client .  Installing vo-client:  root@host #  yum install vo-client  Disabling vo verification can be done by adding a line to  /etc/sysconfig/xrootd :  export LLGT_VOMS_DISABLE_CREDENTIAL_CHECK=1  This feature is evolving, so stay tuned for updates.", 
            "title": "Security option 2: xrootd-lcmaps authorization"
        }, 
        {
            "location": "/data/install-xrootd/#testing-an-xrootd-cluster-with-lcmaps-security-enabled", 
            "text": "From any machine with the  xrootd-client  installed, you can test with xrdcp. With a user that has no grid certificate installed, you should get an error:  user@host $  xrdcp /bin/bash root://HOSTNAME/tmp/lcmaps_test 120327 14:31:52 10509 secgsi_InitProxy: cannot access private key file: /home/dstrain/.globus/userkey.pem  XrdSec: No authentication protocols are available.  Last server error 3010 ( cannot obtain credentials for protocol: Secgsi: ErrParseBuffer: error getting user proxies: kXGS_init: unable to get protocol object. )  Error accessing path/file for root://fermicloud121.fnal.gov/tmp/test2   After running  voms-proxy-init  or  grid-proxy-init  to initialize your x509 certificate (usually found in  /tmp/x509up_uUID ), the  xrdcp  command should execute cleanly. For instance, the following shows an example of a user creating a voms certificate and copying to the xrootd client and then re-executing the xrdcp command.  user@host $  voms-proxy-init -voms Engage -valid  999 :0 Your identity: /DC=org/DC=doegrids/OU=People/CN=Doug Strain 834323  Creating temporary proxy ..................................................... Done  Contacting  osg-engage.renci.org:15001 [/DC=org/DC=doegrids/OU=Services/CN=osg-engage.renci.org]  Engage  Done  Creating proxy .......................... Done  Your proxy is valid until Tue May  8 05:34:40 2012  user@host $  scp /tmp/x509up_u44678 CLIENT_HOSTNAME:/tmp/x509up_u44678  On the  xrootd-client  node,  user@host $  xrdcp /bin/bash root://fermicloud121.fnal.gov//tmp/lcmaps_test [xrootd] Total 0.73 MB  |====================| 100.00 % [inf MB/s]   In the above examples, make sure to change \"/tmp\" to a directory allowed by the /etc/xrootd/auth_file  created in a previous section.", 
            "title": "Testing an XRootd Cluster with LCMAPS security enabled"
        }, 
        {
            "location": "/data/install-xrootd/#optional-adding-cms-tfc-support-to-xrootd-cms-sites-only", 
            "text": "For CMS users, there is a package available to integrate rule-based name lookup\nusing a  storage.xml  file. If you are not setting up a CMS site, you can skip\nthis section.  yum install --enablerepo=osg-contrib xrootd-cmstfc   You will need to add your  storage.xml  to  /etc/xrootd/storage.xml  and then\nadd the following line to your xrootd configuration:  # Integrate with CMS TFC, placed in /etc/xrootd/storage.xml\noss.namelib /usr/lib64/libXrdCmsTfc.so file:/etc/xrootd/storage.xml%ORANGE%?protocol=hadoop   Add the orange text only if you are running hadoop (see below).  See the CMS TWiki for more information:   https://twiki.cern.ch/twiki/bin/view/Main/XrootdTfcChanges  https://twiki.cern.ch/twiki/bin/view/Main/HdfsXrootdInstall", 
            "title": "(Optional) Adding CMS TFC support to XRootD (CMS sites only)"
        }, 
        {
            "location": "/data/install-xrootd/#optional-adding-hadoop-support-to-xrootd", 
            "text": "Note  3.3 only   Users with Hadoop-based storage under their XRootD installation have extra steps\nto configure more efficient access to Hadoop Stroage  root@host #  yum install xrootd-hdfs  You will then need to add the following lines to your /etc/xrootd/xrootd-clustered.cfg :  xrootd.fslib /usr/lib64/libXrdOfs.so\nofs.osslib /usr/lib64/libXrdHdfs.so  For more information, see  Storage.HadoopXrootd .", 
            "title": "(Optional) Adding Hadoop support to XRootD"
        }, 
        {
            "location": "/data/install-xrootd/#optional-adding-file-residency-manager-frm-to-an-xrootd-cluster", 
            "text": "If you have a multi-tiered storage system (e.g. some data is stored on SSDs and\nsome on disks or tapes), then install the File Residency Manager (FRM), so you\ncan move data between tiers more easily. If you do not have a multi-tiered\nstorage system, then you do not need FRM and you can skip this section.  The FRM deals with two major mechanisms:   local disk  remote servers   The description of fully functional multiple xrootd clusters is beyond the scope\nof this document. In order to have this fully functional system you will need a\nglobal redirector and at least one remote xrootd cluster from where files could\nbe moved to the local cluster.  Below are the modifications you should make in order to enable FRM on your local\ncluster:   Make sure that FRM is enabled in  /etc/sysconfig/xrootd  on your data sever:   ROOTD\\_USER=xrootd \nXROOTD\\_GROUP=xrootd \nXROOTD\\_DEFAULT\\_OPTIONS= -l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-clustered.cfg  \nCMSD\\_DEFAULT\\_OPTIONS= -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg  \nFRMD\\_DEFAULT\\_OPTIONS= -l /var/log/xrootd/frmd.log -c /etc/xrootd/xrootd-clustered.cfg  \nXROOTD\\_INSTANCES= default  \nCMSD\\_INSTANCES= default  \nFRMD\\_INSTANCES= default    Modify  /etc/xrootd/xrootd-clustered.cfg  on both nodes to specify options for  frm_xfrd  (File Transfer Daemon) and  frm_purged  (File Purging Daemon). For more information, you can visit the  FRM Documentation  Start frm daemons on data server:    root@host #  service frm \\_ xfrd start root@host #  service frm \\_ purged start", 
            "title": "(Optional) Adding File Residency Manager (FRM) to an XRootd cluster"
        }, 
        {
            "location": "/data/install-xrootd/#using-xrootd", 
            "text": "", 
            "title": "Using XRootD"
        }, 
        {
            "location": "/data/install-xrootd/#managing-xrootd-services", 
            "text": "Start services on the redirector node before starting any services on the data\nnodes. If you installed only XRootD itself, you will only need to start the xrootd  service. However, if you installed cluster management services, you\nwill need to start  cmsd  as well.  The instructions for starting and stopping an XRootD service depend on whether\nthe service is installed on an EL 6 or EL 7 machine, and whether you are using a\nstandalone or clustered configuration.  On EL 6, which config to use is set in the file  /etc/sysconfig/xrootd . For\nexample, to have  xrootd  use the clustered config, you would have a line such\nas this:  XROOTD_DEFAULT_OPTIONS= -l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd- clustered .cfg -k fifo   To use the standalone config instead, you would use:  XROOTD_DEFAULT_OPTIONS= -l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd- standalone .cfg -k fifo   On EL 7, which config to use is determined by the service name given to systemctl . For example, to have  xrootd  use the clustered config, you would\nstart up  xrootd  with this line:  root@host #  systemctl start xrootd@ clustered   To use the standalone config instead, you would use:  root@host #  systemctl start xrootd@ standalone   The services are:     Service  EL 6 service name  EL 7 service name      XRootD (standalone config)  xrootd  xrootd@standalone    XRootD (clustered config)  xrootd  xrootd@clustered    CMSD (clustered config)  cmsd  cmsd@clustered     As a reminder, here are common service commands (all run as  root ):     To \u2026  On EL\u00a06, run the command\u2026  On EL\u00a07, run the command\u2026      Start a service  service  em SERVICE-NAME /em  start  systemctl start  em SERVICE-NAME /em    Stop a service  service  em SERVICE-NAME /em  stop  systemctl start  em SERVICE-NAME /em    Enable a service to start during boot  chkconfig  em SERVICE-NAME /em  on  systemctl enable  em SERVICE-NAME /em    Disable a service from starting during boot  chkconfig  em SERVICE-NAME /em  off  systemctl disable  em SERVICE-NAME /em", 
            "title": "Managing XRootD services"
        }, 
        {
            "location": "/data/install-xrootd/#getting-help", 
            "text": "To get assistance. please use the  Help Procedure  page.", 
            "title": "Getting Help"
        }, 
        {
            "location": "/data/install-xrootd/#reference", 
            "text": "", 
            "title": "Reference"
        }, 
        {
            "location": "/data/install-xrootd/#file-locations", 
            "text": "Service/Process  Configuration File  Description      xrootd  /etc/xrootd/xrootd-clustered.cfg  Main clustered mode XRootD configuration    ^  /etc/xrootd/auth_file  Authorized users file        Service/Process  Log File  Description      xrootd  /var/log/xrootd/xrootd.log  XRootD server daemon log    cmsd  /var/log/xrootd/cmsd.log  Cluster management log    cns  /var/log/xrootd/cns/xrootd.log  Server inventory (composite name space) log    frm_xfrd ,  frm_purged  /var/log/xrootd/frmd.log  File Residency Manager log", 
            "title": "File locations"
        }, 
        {
            "location": "/data/install-xrootd/#links", 
            "text": "XRootD documentation", 
            "title": "Links"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/", 
            "text": "Installing and Maintaining the LCMAPS VOMS Plugin\n\n\nLCMAPS is a software library used on \nHTCondor-CE\n, \nGridFTP\n, or \nXRootD\n hosts for mapping grid certificates of incoming connections to specific Unix accounts. The LCMAPS VOMS plugin enables LCMAPS to make mapping decisions based on the VOMS attributes of grid certificates, e.g., \n/cms/Role=production/Capability=NULL\n. Starting in OSG 3.4, the LCMAPS VOMS plugin will replace GUMS and edg-mkgridmap as the authentication method at OSG sites.\n\n\nThe OSG provides a default set of mappings from VOMS attributes to Unix accounts. By configuring LCMAPS, you can override these mappings, including changing the Unix account that a VO is mapped to, banning based on VOMS attributes, banning a specific user, or adding a VO, VO group, VO role, and/or user that is not in the OSG's set of mappings.\n\n\nUse this page to learn how to install and configure the LCMAPS VOMS plugin to authenticate users to access your resources on a per-VO basis.\n\n\nInstalling the LCMAPS VOMS Plugin\n\n\nTo install the LCMAPS VOMS plugin, make sure that your host is up to date before installing the required packages:\n\n\n\n\n\n\nClean yum cache:\n\n\nroot@host #\n yum clean all --enablerepo\n=\n*\n\n\n\n\n\n\n\n\n\nUpdate software:\n\n\nroot@host #\n yum update\n\n\n\n\n\nThis command will update \nall\n packages\n\n\n\n\n\n\nInstall \nlcmaps\n, the default mapfile, and the configuration tools:\n\n\n[root@server]#\n yum install lcmaps vo-client-lcmaps-voms osg-configure-misc\n\n\n\n\n\n\n\n\n\nConfiguring the LCMAPS VOMS Plugin\n\n\nThe following section describes the steps required to configure the LCMAPS VOMS plugin for authentication. If you are using OSG 3.3 packages, there are software-specific instructions that must be followed for \nHTCondor-CE\n, \nGridFTP\n, and \nXRootD\n. To check if you are running OSG 3.3, run the following command:\n\n\n[root@server]#\n rpm -q --queryformat\n=\n%{VERSION}\\n\n osg-release\n\n\n\n\n\nAdditionally, there is \noptional configuration\n if you need to make changes to the default mappings.\n\n\nEnabling the LCMAPS VOMS plugin\n\n\nTo configure your host to use LCMAPS VOMS plugin authentication, edit \n/etc/osg/config.d/10-misc.ini\n and set the following options:\n\n\nedit_lcmaps_db\n \n=\n \nTrue\n\n\nauthorization_method\n \n=\n \nvomsmap\n\n\n\n\n\n\nIf the \nglexec_location\n option is present, you must comment it out or set it to \nUNAVAILABLE\n.\nThe LCMAPS VOMS plugin does not work with gLExec.\n\n\nSupporting mapped VOs and users\n\n\nUnix accounts must exist for each VO, VO role, VO group, or user you choose to support in the \nmapfiles\n:\n\n\n\n\n\n\nConsult the default VO mappings in \n/usr/share/osg/voms-mapfile-default\n to determine the mapped Unix account names. Each of the mapfiles has the following format:\n\n\nVO, VO role, VO group or user\n \nUnix account\n\n\n\n\n\n\n\n\n\n\nCreate Unix accounts for each VO, VO role, VO group, and user that you wish to support\n\n\n\n\nEdit \n/etc/osg/config.d/30-gip.ini\n and specify the supported VOs per \nSubcluster or ResourceEntry section\n:\n\n\n\n\nallowed_vos\n=\nVO1,VO2...\n\n\n\n\n\n\nApplying configuration settings\n\n\nMaking changes to the OSG configuration files in the \n/etc/osg/config.d\n directory does not apply those settings to software automatically. For the OSG settings, use the \nosg-configure\n tool to validate (to a limited extent) and apply the settings to the relevant software components. If instead you wish to manage the LCMAPS VOMS plugin configuration yourself, skip to the \nmanual configuration section\n.\n\n\n\n\n\n\nMake all changes to \n.ini\n files in the \n/etc/osg/config.d\n directory.\n\n\n\n\nNote\n\n\nThis document only describes the critical settings for the LCMAPS VOMS plugin and related software. You may need to configure other software that is installed on your host, too.\n\n\n\n\n\n\n\n\nValidate the configuration settings:\n\n\n[root@server]#\n osg-configure -v\n\n\n\n\n\n\n\n\n\nOnce the validation command succeeds without errors, apply the configuration settings:\n\n\n[root@server]#\n osg-configure -c\n\n\n\n\n\n\n\n\n\nOptional configuration\n\n\nThe following subsections contain information on migration from \nedg-mkgridmap\n, mapping or banning users by their certificates' Distinguished Names (DNs) or by their proxies' VOMS attributes. Any optional configuration is to be performed after the installation and configuration sections above.\n\n\nFor a table of the configuration files and their order of evaluation, consult the \nreference section\n.\n\n\n\n\nMigrating from edg-mkgridmap\n\n\nMapping VOs\n\n\nMapping users\n\n\nBanning VOs\n\n\nBanning users\n\n\nMapping using all FQANs\n\n\n\n\nMigrating from edg-mkgridmap\n\n\nThe program edg-mkgridmap (found in the package \nedg-mkgridmap\n), used for authentication on HTCondor-CE, GridFTP, and XRootD hosts, is no longer available starting in OSG 3.4. The LCMAPS VOMS plugin (package \nlcmaps-plugins-voms\n) now provides the same functionality. To migrate from edg-mkgridmap to the LCMAPS VOMS plugin, perform the following procedure:\n\n\n\n\n\n\nConfigure user DN mappings:\n\n\n\n\n\n\nIf you have a local grid mapfile (see \nthe EDG-mkgridmap docs\n), replace the contents of \n/etc/grid-security/grid-mapfile\n with the contents of the local grid mapfile.\n\n\n\n\n\n\nIf you do not have a local grid mapfile, remove \n/etc/grid-security/grid-mapfile\n.\n\n\n\n\n\n\n\n\n\n\nIf you are remaining on OSG 3.3, ensure that the you have set \nexport LLGT_VOMS_ENABLE_CREDENTIAL_CHECK=1\n in the appropriate file and restart the service. If you have updated your host to OSG 3.4, skip to the next step.\n\n\n\n\n\n\n\n\nIf your host is a(n)...\n\n\nAdd the aforementioned line to...\n\n\nAnd restart this service...\n\n\n\n\n\n\n\n\n\n\nHTCondor-CE\n\n\n/etc/sysconfig/condor-ce\n\n\ncondor-ce\n\n\n\n\n\n\nGridFTP server\n\n\n/etc/sysconfig/globus-gridftp-server\n\n\nglobus-gridftp-server\n\n\n\n\n\n\n\n\n\n\n\n\nIf you are converting an HTCondor-CE host, remove the HTCondor-CE \nGRIDMAP\n configuration. Otherwise, skip to the next step.\n\n\n\n\n\n\nFind where \nGRIDMAP\n is set:\n\n\n[root@ce]#\n condor_ce_config_val -v GRIDMAP\n\n\n\n\n\n\n\n\n\nIf the above command returns \nNot defined: GRIDMAP\n, skip to step 4. Otherwise, delete the line that sets the \nGRIDMAP\n configuration variable\n\n\n\n\nReconfigure HTCondor-CE:\n[root@ce]#\n condor_ce_reconfig\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemove edg-mkgridmap and related packages:\n\n\n[root@ce]#\n yum erase edg-mkgridmap\n\n\n\n\n\n\n\nWarning\n\n\nIn the output from this command, yum should \nnot\n list other packages than the one. If it lists other packages, cancel the erase operation, make sure the other packages are updated to their OSG 3.3 (or 3.4) versions (they should have \".osg33\" or \".osg34\" in their versions), and try again.\n\n\n\n\n\n\n\n\nMapping VOs\n\n\n/etc/grid-security/voms-mapfile\n is used to map VOs, VO roles, or VO groups to Unix accounts based on their VOMS attributes. An example of the format of a \nvoms-mapfile\n follows:\n\n\n# map GLOW jobs in the chtc group to the \nglow1\n Unix account.\n\n/GLOW/chtc/*\n glow1\n# map GLOW jobs with the htpc role to the \nglow2\n Unix account.\n\n/GLOW/Role=htpc/*\n glow2\n# map other GLOW jobs to the \nglow\n Unix account.\n\n/GLOW/*\n glow\n\n\n\n\n\nEach non-commented line is a shell-style pattern which is compared against the user's VOMS attributes, and a Unix account that the user will be mapped to if the pattern matches.\nThe patterns are compared in the order they are listed in. Therefore, more general patterns should be placed later in the file.\n\n\n\n\nNote\n\n\nThe Unix account must exist for the user to be mapped. If a VO's Unix account is missing, that VO will not be able to access your resources.\n\n\nAdditionally, if you map VOMS attributes to a non-existent user in \n/etc/grid-security/voms-mapfile\n, \n/usr/share/osg/voms-mapfile-default\n will be considered next to find a mapping. The best way to ban a VO is edit \n/etc/grid-security/ban-voms-mapfile\n as described in \nBanning VOs\n below. Do not edit \nvoms-mapfile-default\n as your changes will be overwritten upon updates.\n\n\n\n\nMapping users\n\n\n/etc/grid-security/grid-mapfile\n is used to map specific users to Unix accounts based on their certificates' DNs. An example of the format of a \ngrid-mapfile\n follows:\n\n\n# map Matyas\ns FNAL DN to the \nmatyas\n Unix account\n\n/DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Matyas Selmeci/CN=UID:matyas\n matyas\n\n\n\n\n\n\n\nNote\n\n\nThe Unix account must exist for the user to be mapped. If a user's Unix account is missing, that user will not be able to access your resources.\n\n\n\n\nBanning VOs\n\n\n/etc/grid-security/ban-voms-mapfile\n is used to ban an entire VO or a role withing a VO from accessing resources on your machine. An example of the format of a \nban-voms-mapfile\n follows:\n\n\n# ban CMS production jobs\n\n/cms/Role=production/*\n\n\n\n\n\n\nEach non-commented line is a shell-style pattern which is compared against a user's VOMS attributes. If the pattern matches, that user will be unable to access your resources.\n\n\n\n\nWarning\n\n\n/etc/grid-security/ban-voms-mapfile\n \nmust\n exist, even if you are not banning any VOs. In that case, the file should be blank. If the file does not exist, LCMAPS will ban every user.\n\n\n\n\nBanning users\n\n\n/etc/grid-security/ban-mapfile\n is used to ban specific users from accessing your resources based on their certificates' DNs. An example of the format of a \nban-mapfile\n follows:\n\n\n# ban Matyas\ns FNAL DN\n\n/DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Matyas Selmeci/CN=UID:matyas\n\n\n\n\n\n\n\n\nWarning\n\n\n/etc/grid-security/ban-mapfile\n \nmust\n exist, even if you are not banning any users. In that case, the file should be blank. If the file does not exist, LCMAPS will ban every user.\n\n\n\n\nMapping using all FQANs\n\n\nBy default, the LCMAPS VOMS plugin only considers the first FQAN of a VOMS proxy for mapping. This matches the behavior of GUMS. If you want to consider all FQANs, you must set the appropriate option.\n\n\n\n\n\n\nIf you are using osg-configure, set \nall_fqans = True\n in \n10-misc.ini\n, then run \nosg-configure -c\n\n\n\n\nNote\n\n\nIf you are using OSG 3.3, osg-configure should be at least version 1.10.2.  If you are using OSG 3.4, osg-configure should be at least version 2.2.2.\n\n\n\n\n\n\n\n\nIf you are configuring \nlcmaps.db\n manually (see \nmanual configuration\n below), add \n\"-all-fqans\"\n to the module definitions for \nvomsmapfile\n and \ndefaultmapfile\n\n\n\n\n\n\nValidating the LCMAPS VOMS plugin VO mappings\n\n\nTo validate the LCMAPS VOMS plugin by itself, use the following procedure to test mapping your own cert to a user:\n\n\n\n\nVerify your DN is \nnot\n in \n/etc/grid-security/grid-mapfile\n, or else it will generate a false positive\n\n\nVerify your DN is \nnot\n in \n/etc/grid-security/ban-mapfile\n, or else it will generate a false negative\n\n\n\n\nInstall the \nllrun\n and \nvoms-clients\n packages:\n\n\n[root@host]#\n yum install llrun voms-clients\n\n\n\n\n\n\n\n\n\nAs an unprivileged user, create a VOMS proxy (filling in \nYOUR_VO\n with a VO you are a member of):\n\n\n[you@client]$\n voms-proxy-init -voms \nYOUR_VO\n\n\n\n\n\n\n\n\n\n\nVerify that your credentials are mapped as expected:\n\n\n[you@client]$\n llrun -s -l \nmode\n=\npem,policy\n=\nauthorize_only,db\n=\n/etc/lcmaps.db \n\\\n\n    -p/tmp/x509up_u\n`\nid -u\n`\n\n\n\n\n\n\n\n\n\n\nIf you did not get correctly mapped, check your proxy's FQAN by running:\n\n\n[you@client]$\n voms-proxy-info -fqan\n\n\n\n\n\nand make sure it matches one of the patterns in \n/etc/grid-security/voms-mapfile\n or \n/usr/share/osg/voms-mapfile-default\n, and does not match any patterns in \n/etc/grid-security/ban-voms-mapfile\n.\n\n\nTroubleshooting the LCMAPS VOMS plugin\n\n\nLCMAPS logs to \njournalctl\n (EL7) or \n/var/log/messages\n (EL6) and the verbosity of the logging can be increased by setting the \nLCMAPS_DEBUG_LEVEL\n environment variable. You can also change the destination of the logging by setting the \nLCMAPS_LOG_FILE\n environment variable.\n\n\n\n\n\n\nUse the table below to choose the appropriate file to edit:\n\n\n\n\n\n\n\n\nIf your host is a(n)...\n\n\nEdit this file...\n\n\n\n\n\n\n\n\n\n\nHTCondor-CE\n\n\n/etc/sysconfig/condor-ce\n\n\n\n\n\n\nGridFTP server\n\n\n/etc/sysconfig/globus-gridftp-server\n\n\n\n\n\n\n\n\nAdd the following to the file chosen in the previous step:\n\n\nexport\n \nLCMAPS_DEBUG_LEVEL\n=\n5\n\n\n# optional (uncomment the following line to output log messages to a file):\n\n\n# export LCMAPS_LOG_FILE=/tmp/lcmaps.log\n\n\n\n\n\n\n\n\n\n\nUse the table below to choose the appropriate service to restart:\n\n\n\n\n\n\n\n\nIf your host is a(n)...\n\n\nRestart the following service...\n\n\n\n\n\n\n\n\n\n\nHTCondor-CE\n\n\ncondor-ce\n\n\n\n\n\n\nGridFTP server\n\n\nglobus-gridftp-server\n\n\n\n\n\n\n\n\n\n\n\n\nTroubleshooting mapping with HTCondor-CE\n\n\nHTCondor-CE caches auth lookups for 30 minutes by default. If you are testing changes to your various mapfiles with HTCondor-CE, you will need to disable this caching.\n\n\nTo do this, create a file in \n/etc/condor-ce/config.d\n called e.g. \n99-disablegsicache.conf\n with the following line:\n\n\nGSS_ASSIST_GRIDMAP_CACHE_EXPIRATION=0\n\n\n\n\n\nand then restart \ncondor-ce\n.\n\n\nOnce you are satisfied that your mappings are working, you may remove this file and restart \ncondor-ce\n in order to reduce the load on your CE caused by authentication.\n\n\nGetting Help\n\n\nTo get assistance, please use the \nthis page\n.\n\n\nReference\n\n\nConfiguration Files\n\n\nThe files are evaluated in the following order, with earlier files taking precedence over later ones:\n\n\n\n\n\n\n\n\nFile\n\n\nProvider\n\n\nPurpose\n\n\n\n\n\n\n\n\n\n\n/etc/grid-security/ban-mapfile\n\n\nAdmin\n\n\nBan DNs\n\n\n\n\n\n\n/etc/grid-security/ban-voms-mapfile\n\n\nAdmin\n\n\nBan VOs\n\n\n\n\n\n\n/etc/grid-security/grid-mapfile\n\n\nAdmin\n\n\nMap DNs\n\n\n\n\n\n\n/etc/grid-security/voms-mapfile\n\n\nAdmin\n\n\nMap VOs\n\n\n\n\n\n\n/usr/share/osg/voms-mapfile-default\n\n\nOSG\n\n\nMap VOs (default)\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n/usr/share/osg/voms-mapfile-default\n is not meant to be edited and will be overwritten on upgrades.\nAll VO mappings can be overridden by editing the above files in \n/etc/grid-security\n.\n\n\n\n\nManual Configuration\n\n\nThis section is intended for use as reference if you choose to forego configuring the LCMAPS VOMS plugin via osg-configure (i.e., if you prefer a configuration management system like \nAnsible\n or \nPuppet\n). Therefore, the following instructions serve as a replacement for \nthis section\n above.\n\n\nLCMAPS is configured in \n/etc/lcmaps.db\n. and since the VOMS plugin is a newer component, configuration for it may not be present in your existing \n/etc/lcmaps.db\n file.\n\n\n\n\n\n\nEnsure the following lines are present in the \"Module definitions\" section (the top section, before \nauthorize_only\n) of \n/etc/lcmaps.db\n:\n\n\ngridmapfile = \nlcmaps_localaccount.mod\n\n              \n-gridmap /etc/grid-security/grid-mapfile\n\nbanfile = \nlcmaps_ban_dn.mod\n\n          \n-banmapfile /etc/grid-security/ban-mapfile\n\nbanvomsfile = \nlcmaps_ban_fqan.mod\n\n              \n-banmapfile /etc/grid-security/ban-voms-mapfile\n\nvomsmapfile = \nlcmaps_voms_localaccount.mod\n\n              \n-gridmap /etc/grid-security/voms-mapfile\n\ndefaultmapfile = \nlcmaps_voms_localaccount2.mod\n\n                 \n-gridmap /usr/share/osg/voms-mapfile-default\n\n\nverifyproxynokey = \nlcmaps_verify_proxy2.mod\n\n          \n--allow-limited-proxy\n\n          \n--discard_private_key_absence\n\n          \n -certdir /etc/grid-security/certificates\n\n\n\n\n\n\n\n\n\n\nEdit the \nauthorize_only\n section so that it contains only the following uncommented lines:\n\n\nverifyproxynokey -\n banfile\nbanfile -\n banvomsfile | bad\nbanvomsfile -\n gridmapfile | bad\ngridmapfile -\n good | vomsmapfile\nvomsmapfile -\n good | defaultmapfile\ndefaultmapfile -\n good | bad\n\n\n\n\n\n\n\n\n\nEdit \n/etc/grid-security/gsi-authz.conf\n and ensure that it contains the following line with a newline at the end:\n\n\nglobus_mapping liblcas_lcmaps_gt4_mapping.so lcmaps_callout", 
            "title": "LCMAPS-VOMS authentication"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#installing-and-maintaining-the-lcmaps-voms-plugin", 
            "text": "LCMAPS is a software library used on  HTCondor-CE ,  GridFTP , or  XRootD  hosts for mapping grid certificates of incoming connections to specific Unix accounts. The LCMAPS VOMS plugin enables LCMAPS to make mapping decisions based on the VOMS attributes of grid certificates, e.g.,  /cms/Role=production/Capability=NULL . Starting in OSG 3.4, the LCMAPS VOMS plugin will replace GUMS and edg-mkgridmap as the authentication method at OSG sites.  The OSG provides a default set of mappings from VOMS attributes to Unix accounts. By configuring LCMAPS, you can override these mappings, including changing the Unix account that a VO is mapped to, banning based on VOMS attributes, banning a specific user, or adding a VO, VO group, VO role, and/or user that is not in the OSG's set of mappings.  Use this page to learn how to install and configure the LCMAPS VOMS plugin to authenticate users to access your resources on a per-VO basis.", 
            "title": "Installing and Maintaining the LCMAPS VOMS Plugin"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#installing-the-lcmaps-voms-plugin", 
            "text": "To install the LCMAPS VOMS plugin, make sure that your host is up to date before installing the required packages:    Clean yum cache:  root@host #  yum clean all --enablerepo = *    Update software:  root@host #  yum update  This command will update  all  packages    Install  lcmaps , the default mapfile, and the configuration tools:  [root@server]#  yum install lcmaps vo-client-lcmaps-voms osg-configure-misc", 
            "title": "Installing the LCMAPS VOMS Plugin"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#configuring-the-lcmaps-voms-plugin", 
            "text": "The following section describes the steps required to configure the LCMAPS VOMS plugin for authentication. If you are using OSG 3.3 packages, there are software-specific instructions that must be followed for  HTCondor-CE ,  GridFTP , and  XRootD . To check if you are running OSG 3.3, run the following command:  [root@server]#  rpm -q --queryformat = %{VERSION}\\n  osg-release  Additionally, there is  optional configuration  if you need to make changes to the default mappings.", 
            "title": "Configuring the LCMAPS VOMS Plugin"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#enabling-the-lcmaps-voms-plugin", 
            "text": "To configure your host to use LCMAPS VOMS plugin authentication, edit  /etc/osg/config.d/10-misc.ini  and set the following options:  edit_lcmaps_db   =   True  authorization_method   =   vomsmap   If the  glexec_location  option is present, you must comment it out or set it to  UNAVAILABLE .\nThe LCMAPS VOMS plugin does not work with gLExec.", 
            "title": "Enabling the LCMAPS VOMS plugin"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#supporting-mapped-vos-and-users", 
            "text": "Unix accounts must exist for each VO, VO role, VO group, or user you choose to support in the  mapfiles :    Consult the default VO mappings in  /usr/share/osg/voms-mapfile-default  to determine the mapped Unix account names. Each of the mapfiles has the following format:  VO, VO role, VO group or user   Unix account     Create Unix accounts for each VO, VO role, VO group, and user that you wish to support   Edit  /etc/osg/config.d/30-gip.ini  and specify the supported VOs per  Subcluster or ResourceEntry section :   allowed_vos = VO1,VO2...", 
            "title": "Supporting mapped VOs and users"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#applying-configuration-settings", 
            "text": "Making changes to the OSG configuration files in the  /etc/osg/config.d  directory does not apply those settings to software automatically. For the OSG settings, use the  osg-configure  tool to validate (to a limited extent) and apply the settings to the relevant software components. If instead you wish to manage the LCMAPS VOMS plugin configuration yourself, skip to the  manual configuration section .    Make all changes to  .ini  files in the  /etc/osg/config.d  directory.   Note  This document only describes the critical settings for the LCMAPS VOMS plugin and related software. You may need to configure other software that is installed on your host, too.     Validate the configuration settings:  [root@server]#  osg-configure -v    Once the validation command succeeds without errors, apply the configuration settings:  [root@server]#  osg-configure -c", 
            "title": "Applying configuration settings"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#optional-configuration", 
            "text": "The following subsections contain information on migration from  edg-mkgridmap , mapping or banning users by their certificates' Distinguished Names (DNs) or by their proxies' VOMS attributes. Any optional configuration is to be performed after the installation and configuration sections above.  For a table of the configuration files and their order of evaluation, consult the  reference section .   Migrating from edg-mkgridmap  Mapping VOs  Mapping users  Banning VOs  Banning users  Mapping using all FQANs", 
            "title": "Optional configuration"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#migrating-from-edg-mkgridmap", 
            "text": "The program edg-mkgridmap (found in the package  edg-mkgridmap ), used for authentication on HTCondor-CE, GridFTP, and XRootD hosts, is no longer available starting in OSG 3.4. The LCMAPS VOMS plugin (package  lcmaps-plugins-voms ) now provides the same functionality. To migrate from edg-mkgridmap to the LCMAPS VOMS plugin, perform the following procedure:    Configure user DN mappings:    If you have a local grid mapfile (see  the EDG-mkgridmap docs ), replace the contents of  /etc/grid-security/grid-mapfile  with the contents of the local grid mapfile.    If you do not have a local grid mapfile, remove  /etc/grid-security/grid-mapfile .      If you are remaining on OSG 3.3, ensure that the you have set  export LLGT_VOMS_ENABLE_CREDENTIAL_CHECK=1  in the appropriate file and restart the service. If you have updated your host to OSG 3.4, skip to the next step.     If your host is a(n)...  Add the aforementioned line to...  And restart this service...      HTCondor-CE  /etc/sysconfig/condor-ce  condor-ce    GridFTP server  /etc/sysconfig/globus-gridftp-server  globus-gridftp-server       If you are converting an HTCondor-CE host, remove the HTCondor-CE  GRIDMAP  configuration. Otherwise, skip to the next step.    Find where  GRIDMAP  is set:  [root@ce]#  condor_ce_config_val -v GRIDMAP    If the above command returns  Not defined: GRIDMAP , skip to step 4. Otherwise, delete the line that sets the  GRIDMAP  configuration variable   Reconfigure HTCondor-CE: [root@ce]#  condor_ce_reconfig      Remove edg-mkgridmap and related packages:  [root@ce]#  yum erase edg-mkgridmap   Warning  In the output from this command, yum should  not  list other packages than the one. If it lists other packages, cancel the erase operation, make sure the other packages are updated to their OSG 3.3 (or 3.4) versions (they should have \".osg33\" or \".osg34\" in their versions), and try again.", 
            "title": "Migrating from edg-mkgridmap"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#mapping-vos", 
            "text": "/etc/grid-security/voms-mapfile  is used to map VOs, VO roles, or VO groups to Unix accounts based on their VOMS attributes. An example of the format of a  voms-mapfile  follows:  # map GLOW jobs in the chtc group to the  glow1  Unix account. /GLOW/chtc/*  glow1\n# map GLOW jobs with the htpc role to the  glow2  Unix account. /GLOW/Role=htpc/*  glow2\n# map other GLOW jobs to the  glow  Unix account. /GLOW/*  glow  Each non-commented line is a shell-style pattern which is compared against the user's VOMS attributes, and a Unix account that the user will be mapped to if the pattern matches.\nThe patterns are compared in the order they are listed in. Therefore, more general patterns should be placed later in the file.   Note  The Unix account must exist for the user to be mapped. If a VO's Unix account is missing, that VO will not be able to access your resources.  Additionally, if you map VOMS attributes to a non-existent user in  /etc/grid-security/voms-mapfile ,  /usr/share/osg/voms-mapfile-default  will be considered next to find a mapping. The best way to ban a VO is edit  /etc/grid-security/ban-voms-mapfile  as described in  Banning VOs  below. Do not edit  voms-mapfile-default  as your changes will be overwritten upon updates.", 
            "title": "Mapping VOs"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#mapping-users", 
            "text": "/etc/grid-security/grid-mapfile  is used to map specific users to Unix accounts based on their certificates' DNs. An example of the format of a  grid-mapfile  follows:  # map Matyas s FNAL DN to the  matyas  Unix account /DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Matyas Selmeci/CN=UID:matyas  matyas   Note  The Unix account must exist for the user to be mapped. If a user's Unix account is missing, that user will not be able to access your resources.", 
            "title": "Mapping users"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#banning-vos", 
            "text": "/etc/grid-security/ban-voms-mapfile  is used to ban an entire VO or a role withing a VO from accessing resources on your machine. An example of the format of a  ban-voms-mapfile  follows:  # ban CMS production jobs /cms/Role=production/*   Each non-commented line is a shell-style pattern which is compared against a user's VOMS attributes. If the pattern matches, that user will be unable to access your resources.   Warning  /etc/grid-security/ban-voms-mapfile   must  exist, even if you are not banning any VOs. In that case, the file should be blank. If the file does not exist, LCMAPS will ban every user.", 
            "title": "Banning VOs"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#banning-users", 
            "text": "/etc/grid-security/ban-mapfile  is used to ban specific users from accessing your resources based on their certificates' DNs. An example of the format of a  ban-mapfile  follows:  # ban Matyas s FNAL DN /DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Matyas Selmeci/CN=UID:matyas    Warning  /etc/grid-security/ban-mapfile   must  exist, even if you are not banning any users. In that case, the file should be blank. If the file does not exist, LCMAPS will ban every user.", 
            "title": "Banning users"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#mapping-using-all-fqans", 
            "text": "By default, the LCMAPS VOMS plugin only considers the first FQAN of a VOMS proxy for mapping. This matches the behavior of GUMS. If you want to consider all FQANs, you must set the appropriate option.    If you are using osg-configure, set  all_fqans = True  in  10-misc.ini , then run  osg-configure -c   Note  If you are using OSG 3.3, osg-configure should be at least version 1.10.2.  If you are using OSG 3.4, osg-configure should be at least version 2.2.2.     If you are configuring  lcmaps.db  manually (see  manual configuration  below), add  \"-all-fqans\"  to the module definitions for  vomsmapfile  and  defaultmapfile", 
            "title": "Mapping using all FQANs"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#validating-the-lcmaps-voms-plugin-vo-mappings", 
            "text": "To validate the LCMAPS VOMS plugin by itself, use the following procedure to test mapping your own cert to a user:   Verify your DN is  not  in  /etc/grid-security/grid-mapfile , or else it will generate a false positive  Verify your DN is  not  in  /etc/grid-security/ban-mapfile , or else it will generate a false negative   Install the  llrun  and  voms-clients  packages:  [root@host]#  yum install llrun voms-clients    As an unprivileged user, create a VOMS proxy (filling in  YOUR_VO  with a VO you are a member of):  [you@client]$  voms-proxy-init -voms  YOUR_VO     Verify that your credentials are mapped as expected:  [you@client]$  llrun -s -l  mode = pem,policy = authorize_only,db = /etc/lcmaps.db  \\ \n    -p/tmp/x509up_u ` id -u `     If you did not get correctly mapped, check your proxy's FQAN by running:  [you@client]$  voms-proxy-info -fqan  and make sure it matches one of the patterns in  /etc/grid-security/voms-mapfile  or  /usr/share/osg/voms-mapfile-default , and does not match any patterns in  /etc/grid-security/ban-voms-mapfile .", 
            "title": "Validating the LCMAPS VOMS plugin VO mappings"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#troubleshooting-the-lcmaps-voms-plugin", 
            "text": "LCMAPS logs to  journalctl  (EL7) or  /var/log/messages  (EL6) and the verbosity of the logging can be increased by setting the  LCMAPS_DEBUG_LEVEL  environment variable. You can also change the destination of the logging by setting the  LCMAPS_LOG_FILE  environment variable.    Use the table below to choose the appropriate file to edit:     If your host is a(n)...  Edit this file...      HTCondor-CE  /etc/sysconfig/condor-ce    GridFTP server  /etc/sysconfig/globus-gridftp-server     Add the following to the file chosen in the previous step:  export   LCMAPS_DEBUG_LEVEL = 5  # optional (uncomment the following line to output log messages to a file):  # export LCMAPS_LOG_FILE=/tmp/lcmaps.log     Use the table below to choose the appropriate service to restart:     If your host is a(n)...  Restart the following service...      HTCondor-CE  condor-ce    GridFTP server  globus-gridftp-server", 
            "title": "Troubleshooting the LCMAPS VOMS plugin"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#troubleshooting-mapping-with-htcondor-ce", 
            "text": "HTCondor-CE caches auth lookups for 30 minutes by default. If you are testing changes to your various mapfiles with HTCondor-CE, you will need to disable this caching.  To do this, create a file in  /etc/condor-ce/config.d  called e.g.  99-disablegsicache.conf  with the following line:  GSS_ASSIST_GRIDMAP_CACHE_EXPIRATION=0  and then restart  condor-ce .  Once you are satisfied that your mappings are working, you may remove this file and restart  condor-ce  in order to reduce the load on your CE caused by authentication.", 
            "title": "Troubleshooting mapping with HTCondor-CE"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#getting-help", 
            "text": "To get assistance, please use the  this page .", 
            "title": "Getting Help"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#reference", 
            "text": "", 
            "title": "Reference"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#configuration-files", 
            "text": "The files are evaluated in the following order, with earlier files taking precedence over later ones:     File  Provider  Purpose      /etc/grid-security/ban-mapfile  Admin  Ban DNs    /etc/grid-security/ban-voms-mapfile  Admin  Ban VOs    /etc/grid-security/grid-mapfile  Admin  Map DNs    /etc/grid-security/voms-mapfile  Admin  Map VOs    /usr/share/osg/voms-mapfile-default  OSG  Map VOs (default)      Warning  /usr/share/osg/voms-mapfile-default  is not meant to be edited and will be overwritten on upgrades.\nAll VO mappings can be overridden by editing the above files in  /etc/grid-security .", 
            "title": "Configuration Files"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#manual-configuration", 
            "text": "This section is intended for use as reference if you choose to forego configuring the LCMAPS VOMS plugin via osg-configure (i.e., if you prefer a configuration management system like  Ansible  or  Puppet ). Therefore, the following instructions serve as a replacement for  this section  above.  LCMAPS is configured in  /etc/lcmaps.db . and since the VOMS plugin is a newer component, configuration for it may not be present in your existing  /etc/lcmaps.db  file.    Ensure the following lines are present in the \"Module definitions\" section (the top section, before  authorize_only ) of  /etc/lcmaps.db :  gridmapfile =  lcmaps_localaccount.mod \n               -gridmap /etc/grid-security/grid-mapfile \nbanfile =  lcmaps_ban_dn.mod \n           -banmapfile /etc/grid-security/ban-mapfile \nbanvomsfile =  lcmaps_ban_fqan.mod \n               -banmapfile /etc/grid-security/ban-voms-mapfile \nvomsmapfile =  lcmaps_voms_localaccount.mod \n               -gridmap /etc/grid-security/voms-mapfile \ndefaultmapfile =  lcmaps_voms_localaccount2.mod \n                  -gridmap /usr/share/osg/voms-mapfile-default \n\nverifyproxynokey =  lcmaps_verify_proxy2.mod \n           --allow-limited-proxy \n           --discard_private_key_absence \n            -certdir /etc/grid-security/certificates     Edit the  authorize_only  section so that it contains only the following uncommented lines:  verifyproxynokey -  banfile\nbanfile -  banvomsfile | bad\nbanvomsfile -  gridmapfile | bad\ngridmapfile -  good | vomsmapfile\nvomsmapfile -  good | defaultmapfile\ndefaultmapfile -  good | bad    Edit  /etc/grid-security/gsi-authz.conf  and ensure that it contains the following line with a newline at the end:  globus_mapping liblcas_lcmaps_gt4_mapping.so lcmaps_callout", 
            "title": "Manual Configuration"
        }, 
        {
            "location": "/security/host-certs/", 
            "text": "How to Get Host and Service Certificates\n\n\nThis document is for system administrators. After reading this document you should be able to apply for and install a grid certificate on a grid resource. This document does not explain how to apply for a grid user certificate. To apply for a grid user certificate click \nhere\n instead!\n\n\nRequirements\n\n\nBefore requesting a new host or service certificate, you should check if a certificate is not installed already using \nopenssl\n. In this case you may safely skip this document:\n\n\n[user@host ~]$\n openssl x509 -in /etc/grid-security/hostcert.pem -subject -issuer -dates -noout\n\nsubject= /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=host.opensciencegrid.org\n\n\nissuer=/DC=org/DC=cilogon/C=US/O=CILogon/CN=CILogon OSG CA 1\n\n\nnotBefore=Jan  4 21:08:09 2010 GMT\n\n\nnotAfter=Jan  4 21:08:09 2011 GMT\n\n\n\n\n\n\nThe OSG PKI Command Line Clients are tested to work on Python version 2.4+. They have not been tested on Python version 3. In order to proceed you will also need:\n\n\n\n\nthe \nfully qualified domain name\n of the host you need a grid certificate for\n\n\nthe \npurpose\n of the certificate that explains your request to the Certificate Authority\n\n\nthe full \nname of the administrator\n responsible for the host\n\n\nthe \ne-mail address of the administrator\n\n\nthe \ntelephone number of the administrator\n\n\nthe name of the \nCertificate Authority\n your project is affiliated with\n\n\nthe name of the \nVirtual Organization\n affiliated with the Certificate Authority\n\n\n\n\nRequesting host/service certificate using OIM\n\n\nThe OSG PKI Certificate Request \n Management System can be found at: \nhttps://oim.opensciencegrid.org/oim/certificate\n.\n\n\nFor instructions on how to request a host or service certificate using the Web interface please see the \nuser guide maintained by GOC\n.\n\n\nInstallation\n\n\nInstall the OSG PKI Command Line Clients\n\n\n[root@host /]$\n yum install osg-pki-tools\n\n\n\n\n\nConfiguration file pki-clients.ini\n\n\nThis configuration is correctly set by default and you will not need to change them. The information is provided in case you need it for debugging purposes.\n\n\nThe client checks for pki-clients.ini file at three location in order:\n\n\n\n\n$HOME/.osg-pki/OSG_PKI.ini\n\n\n./pki-clients.ini\n\n\n/etc/osg/pki-clients.ini (default location)\n\n\n\n\nThe INI file contains the following information:\n\n\n\n\nRequest URL\n\n\nApprove URL\n\n\nRetrieve URL\n\n\nHost URL\n\n\n\n\nValidate authentication\n\n\nMake sure you can create a valid grid proxy. To do so, please follow instructions:\n\n\n\n\nDouble-check your configuration to make sure that you are allowed (specifically, your credentials are allowed) to access your CE. You can be either member of a VO allowed to run at the site or you can add your personal certificate locally. This will either be in your GUMS server or in your \nedg-mkgridmap\n configuration.\n\n\n\n\nCreate a proxy with \nvoms-proxy-init\n or \ngrid-proxy-init\n. For example:\n\n\n``` console\n[user@host /]$ voms-proxy-init -voms GLOW\nEnter GRID pass phrase for this identity:\nYour identity: /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services=People/CN=Alain Roy 424511\nCreating temporary proxy ................................................................................. Done\nContacting  glow-voms.cs.wisc.edu:15001 [/DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=glow-voms.cs.wisc.edu] \"GLOW\" Done\nCreating proxy ..................................................................... Done\n\n\nYour proxy is valid until Fri Dec  2 01:32:47 2011\n```\n\n\nYou'll substitute your own VO, of course.\n\n\n\n\n\n\nRequest a Host Certificate\n\n\nEvery resource or service contributing to the grid needs a certificate issued by one of the trusted \nCertificate Authorities\n. To proceed you will need following information at hand:\n\n\n\n\nthe \nfully qualified domain\n name of the host you need a grid certificate for\n\n\nthe \npurpose\n of the certificate that explains your request to the Certificate Authority\n\n\nthe full \nname of the administrator\n responsible for the host\n\n\nthe \ne-mail address of the administrator\n\n\nthe \ntelephone number of the administrator\n\n\nthe name of the \nCertificate Authority\n your project is affiliated with\n\n\nthe name of the \nVirtual Organization\n affiliated with the Certificate Authority\n\n\n\n\nSend the request\n\n\nThis command line client generates a private key and submits a request for a certificate to the OSG PKI. The request will be approved by an appropriate Grid Admin. You will receive an email when this approval has been completed containing directions on how to run 'osg-cert-retreive' to retrieve the certificate. It works in two modes:\n\n\n\n\nCSR is provided by the user: Here the csr provided is just trimmed for begin and end certificate lines and the request is sent to the OIM\n\n\nCSR is not provided by the user: Here the script generates a private key for the user. Writes it to default key file name or the one specified by -o. Then generates a new csr and sends it to OIM.\n\n\n\n\nThe second mode will commonly be used by most OSG site admins\n\n\nUse \nosg-cert-request\n to generate a request which will be sent to the Certificate Authority you specified. Change \"host.opensciencegrid.org\" to be the host name for the computer for which you need a certificate and provide your contact details for the grid admin to approve your request.\n\n\n[user@host /]$\n osg-cert-request -H host.opensciencegrid.org -e emailaddress@domain.com -n \nYour Name\n -p \n9999999999\n \n(\nPh No\n)\n -v \nYour VO\n -y \nxyz@domain.com,abc@domain.com\n \n(\nCC list\n)\n -m \nThis is my comment\n -o hostkey.pem\n\n\n\n\n\nExample:\n\n\n[user@host /]$\n osg-cert-request -H sectest.cigi.illinois.edu -e apadmana@domain.edu -n \nAnand\n -p \n9999999999\n -m \nTesting for developing security documentation\n -o hostkey.pem\n\n\nWriting key to hostkey.pem\n\n\n\nConnecting to server...\n\n\nSuccesfully submitted\n\n\nRequest Id#: 570\n\n\n\n\n\n\nAt this point \nosg-cert-request\n has created some files in the directory you specified and an e-mail has been sent to the Certificate Authority containing your request. The files will be needed again once you receive a reply from the Certificate Authority asking you to retrieve the certificate. Please note down the Request Id. You will need it for retrieving the signed certificate.\n\n\nDetailed description of the osg-cert-request usage\n\n\nThis script:\n-   Generates a new host private key and CSR\n-   Only important part of CSR is CN= component\n-   Saves the host private key to disk (as specified by the user)\n-   Authenticates to OIM and posts the CSR as a request to OIM\n-   Returns the request ID to the user\n-   If the user provides the CSR, then this script would just send the same CSR to OIM\n\n\nInputs:\n\n-   fully-qualified hostname\n-   filename to store private key [Optional, default is ./hostkey.pem]\n-   path to user's certificate [Optional, default is path specified by $X509_USER_CERT environment variable, ~/.globus/usercert.pem]\n-   path to user's private key [Optional, default is path specified by $X509_USER_KEY environment variable, ~/.globus/userkey.pem]\n-   Passphrase for user's private key via non-echoing prompt.\n-   User needs to provide VO name if the requested hostname has multiple VO's assigned\n\n\nOutputs:\n\n-   Private key, to filename specified by '-o' or ./hostkey.pem by default\n-   Request Id, to stdout\n\n\nUsage\n: osg-cert-request [options]\n    \nOptions\n:\n\n\n  -h, --help            show this help message and exit\n  -c CSR, --csr=CSR     Specify CSR name (default = gennew.csr)\n  -o Output Keyfile, --outkeyfile=Output Keyfile\n                        Specify the output filename for the retrieved user\n                        certificate.  Default is ./hostkey.pem\n  -y CC List, --cc=CC List\n                        Specify the CC list(the email id\ns to be CCed). Separate values by \n,\n\n  -m Comment, --comment=Comment\n                        The comment to be added to the request\n  -H CN, --hostname=CN  Specify hostname for CSR (FQDN)\n  -e EMAIL, --email=EMAIL\n                        Email address to receive certificate\n  -n NAME, --name=NAME  Name of user receiving certificate\n  -p PHONE, --phone=PHONE\n                        Phone number of user receiving certificate\n  -v VO, --vo=VO\n                        VO name of requested hostname\n  -T, --test            Run in test mode\n  -q, --quiet           don\nt print status messages to stdout\n  -V, --version         Print the script version number and exit.\n\n\n\n\n\nRetrieve and Install the Host Certificate\n\n\nOnce the certificate has been approved by the Certificate Authority you will receive an e-mail for the GOC. Then to retrieve the host certificate we will execute \nosg-cert-retrieve\n: and use the request ID we recorded earlier. Since certificates are public, no authentication of the user is required to retrieve them.\n\n\n[user@host /]$\n  osg-cert-retrieve \n570\n -o hostcert.pem\n\nUsing timeout of 5 minutes\n\n\nRunning in test mode\n\n\nConnecting server to retrieve certificate...\n\n\nCertificate written to hostcert.pem\n\n\n\n\n\n\nDetailed description of the osg-cert-retrive usage\n\n\nThis osg-cert-retrive script:\n-   Accepts a request Id from the user\n-   Connects to OIM and requests the certificate identified by the request ID\n-   Writes the certificate to disk (as specified by the user)\n\n\nInputs:\n\n-   Request ID\n-   Filename to store certificate [Optional, default is ./hostcert.pem]\n\n\nOutputs:\n\n-   Host certificate as PEM, to filename specified or ./hostcert.pem\n\n\nUsage\n:osg-cert-retrieve -h/--help [for detailed explanations of options]\n\n\nOptions:\n  -h, --help            show this help message and exit\n  -i ID, --id=ID        Specify ID# of certificate to retrieve[Required]\n  -o ID, --certfile=ID  Specify the output filename for the retrieved user\n                        certificate . Default is ./hostcert.pem\n  -T, --test            Run in test mode\n  -q, --quiet           don\nt print status messages to stdout\n  -V, --version         Print the script version number and exit.\n\n\n\n\n\nThe certificate consists of two files (default hostcert.pem and hostkey.pem) which have been placed into the current directory. Note, If you did not use the \n--o\n option the filenames will be what you provided.\n\n\n\n\nWarning\n\n\nPlease note that these files represent a public and a private and should be treated accordingly!\n\n\n\n\nPlease take a moment to verify that the \ncertificate matches the hostname\n of the resource where you intend to install it before you proceed:\n\n\n[user@host /]$\n grid-cert-info -file ./hostcert.pem -subject\n\n/DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=host.opensciencegrid.org\n\n\n\n[user@host /]$\n hostname -f\n\nhost.opensciencegrid.org\n\n\n\n\n\n\nFinally, install the certificate in the default location \n/etc/grid-security/\n:\n\n\n[root@host /]$\n cp ./host.opensciencegrid.orgcert.pem /etc/grid-security/hostcert.pem\n\n[root@host /]$\n chmod \n444\n /etc/grid-security/hostcert.pem\n\n[root@host /]$\n cp ./host.opensciencegrid.orgkey.pem /etc/grid-security/hostkey.pem\n\n[root@host /]$\n chmod \n400\n /etc/grid-security/hostkey.pem\n\n\n\n\n\nRequest a Service Certificate\n\n\nYou can use the same \nosg-cert-request\n and \nosg-cert-retrieve\n commands to request and service certificates just like any host certificate. Just use service/hostname for the -t parameter.\n\n\n[user@host /]$\n osg-cert-request -H http/host.opensciencegrid.org -e emailaddress@domain.com -n \nYour Name\n -p \n9999999999\n \n(\nPh No\n)\n -v \nYour VO\n -y \nxyz@domain.com,abc@domain.com\n \n(\nCC list\n)\n -m \nThis is my comment\n -o hostkey.pem\n\nWriting key to ./hostkey.pem\n\n\n\nConnecting to server...\n\n\nSuccesfully submitted\n\n\nRequest Id#: 571\n\n\n\n\n\n\nPlease note down the request ID.\n\n\nOnce the certificate has been approved by the Certificate Authority you will receive an e-mail for the GOC. Then to retrieve the host certificate we will execute \nosg-cert-retrieve\n: and use the request ID we recorded earlier. Since certificates are public, no authentication of the user is required to retrieve them.\n\n\n[user@host /]$\n  osg-cert-retrieve \n571\n -o hostcert.pem\n\nUsing timeout of 5 minutes\n\n\nRunning in test mode\n\n\nConnecting server to retrieve certificate...\n\n\nCertificate written to hostcert.pem\n\n\n\n\n\n\n\n\nWarning\n\n\nPlease note that these files represent a public and a private and should be treated accordingly!\n\n\n\n\nInstall the Service Certificate\n\n\nThe \nService Certificate\n should be installed under a subdirectory in \n/etc/grid-security\n indicating the name of the service. The next step will install the service certificate in the default location \n/etc/grid-security/http\n:\n\n\n[root@host /]$\n cp ./host.opensciencegrid.org-httpcert.pem /etc/grid-security/http/httpcert.pem\n\n[root@host /]$\n chmod \n444\n /etc/grid-security/http/httpcert.pem\n\n[root@host /]$\n cp ./host.opensciencegrid.org-httpkey.pem /etc/grid-security/http/httpkey.pem\n\n[root@host /]$\n chmod \n400\n /etc/grid-security/http/httpkey.pem\n\n\n\n\n\n\n\nWarning\n\n\nPlease note that the service certificate must also be owned by the unix user who runs the service. For \nApache/Tomcat\n this is the tomcat user:\n\n\n\n\n[root@host /]$\n chown tomcat.tomcat /etc/grid-security/http/httpcert.pem\n\n[root@host /]$\n chown tomcat.tomcat /etc/grid-security/http/httpkey.pem\n\n\n\n\n\nPlease refer to \nOperations/OSGPKICommandlineClients\n for full documentation of the Client package\n\n\nInformation for Grid Admins\n\n\nIf you are a grid admin then you can use a single command to request and retrieve a certificate immediately. For getting grid admin privileges, request enrollment \nhere\n after obtaining your \nuser certificate\n.\n\n\nRequest and retrieve multliple host certificates from OIM. Authenticates to OIM and is only for use by Grid Admins for certificates they are authorized to approve. This script is only supported with all hosts being in the same domain (so we ensure they go to the same Grid Admin). The certificates are stored with the format of 'hostname-requestid.pem' (i.e. the id generated from the request for the certificate). The key is stored as 'hostname-serial-key.pem'.\n\n\nExamples:\n\n\n[user@host /]$\n osg-gridadmin-cert-request -H host.opensciencegrid.org\n\n\n\n\n\nIf you want to request more then one certificate you can list them in a file (one host per line) and use the following command\n\n\n[user@host /]$\n osg-gridadmin-cert-request -f hostfile\n\n\n\n\n\nDetailed description of the osg-gridadmin-cert-request usage\n\n\nThis osg-gridadmin-cert-request script does the following in the process of acquiring certificates for the hostnames specified:\n\n\nReads a list of fully-qualified hostnames from a file specified by the user. For reach hostname: Generates a new private key and CSR Only important part of CSR is CN= component Writes the private key to a file with filename: /-key.pem Prompts the user for their private key pass phrase Pass phrase is cached so user is not re-prompted Authenticates to OIM and posts the CSRs as a single request to OIM Request id is returned and subsequently used Authenticates to OIM and approves the request Waits one minute for request to be processed by OIM Connects to OIM and attempts to retrieve certificates Writes out any certificates it retrieves with filename of /-\n.pem if all certificates have been retrieved, exits loop Wait 5 seconds and repeat.\n\n\nInputs:\n\n\nfilename of list of hostnames prefix path in which to write private keys and certificares [default: .] path to user's certificate [Optional, default is path specified by $X509_USER_CERT environment variable, ~/.globus/usercert.pem] path to user's private key [Optional, default is path specified by $X509_USER_KEY environment variable, ~/.globus/userkey.pem] Passphrase for user's private key via non-echoing prompt.\n\n\nOutputs:\n\n\nN host certificates in PEM format N private keys in PEM format\n\n\nUsage:osg-gridadmin-cert-request -h/--help [for detailed explanations of options]\n\n\nOptions\n: -h, --help show this help message and exit -k PKEY, --pkey=PKEY Specify Requestor's private key (PEM Format). If not specified will take the value of X509_USER_KEY or $HOME/.globus/userkey.pem -c CERT, --cert=CERT Specify Requestor's certificate (PEM Format). If not specified will take the value of X509_USER_CERT or $HOME/.globus/usercert.pem -T, --test Run in test mode -q, --quiet don't print status messages to stdout -V, --version Print the script version number and exit.\n\n\nHostname Options\n: Use either of these options. Specify hostname as a single hostname using -H/--hostname or specify from a file using -f/--hostfile.\n\n\n-H HOSTNAME, --hostname=HOSTNAME Specify the hostname or service/hostname for which you want to request the certificate for. If specified -f/--hostfile will be ignored -f HOSTFILE, --hostfile=HOSTFILE Filename with one hostname or service/hostname per line\n\n\nFrequently Asked Questions\n\n\nCan I use any host to request a certificate for a different host?\n\n\nYES, you can use any host to create a certificate request as long as the hostname for the certificate is a fully qualified domain name.\n\n\nMay I reuse my host certificate as a service certificate?\n\n\nNO! For security reasons, please do not use clones of your host certificate for additional certificates even though it's technically possible.\n\n\nHow do I renew a host/service certificate?\n\n\nThere is no separate procedure. Simply ask for a new certificate the same way you asked for it the previous time.\n\n\nI get a \"GSS authentication failure\" when users try to authenticate with my site?\n\n\nYou likely used an \nalias\n for the host instead of the fully qualified domain name when you generated the certificate request. This can cause the GSS authentication failures similar to the following when a user tries to authenticate to the host after your certificate is installed:\n\n\nGSS authentication failure \n\n\nGSS Major Status: General failure \n\n\nGSS Minor Status Error Chain: \n\n\naccept_sec_context.c:gss_accept_sec_context:403: \n\n\nError during delegation: Delegation protocol violation \n\n\nFailure: GSS failed Major:000d0000 Minor:00000001 Token:00000000 \n\n\n\n\n\n\nHow can I check if I have a host certificate installed already?\n\n\nBy default the host certificate key pair will be installed in \n/etc/grid-security/hostcert.pem\n and \n/etc/grid-security/hostkey.pem\n. You can use \nopenssl\n to access basic information about the certificate:\n\n\n[root@osg-se robert]#\n openssl x509 -in /etc/grid-security/hostcert.pem -subject -issuer -dates -noout\n\nsubject= /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=host.opensciencegrid.org\n\n\nissuer= /DC=org/DC=cilogon/C=US/O=CILogon/CN=CILogon OSG CA 1\n\n\nnotBefore=Apr  8 00:00:00 2013 GMT\n\n\nnotAfter=May 17 12:00:00 2014 GMT\n\n\n\n\n\n\nHow can I check the expiration time of my installed host certificate?\n\n\nIf you installed the Certificates Script Package you can use \ngrid-cert-info\n to retrieve information about the certificate:\n\n\n[root@osg-se robert]#\n grid-cert-info -file /etc/grid-security/hostcert.pem -startdate -enddate\n\nJan  4 21:08:41 2010 GMT\n\n\nJan  4 21:08:41 2011 GMT\n\n\n\n\n\n\nAlternatively you can use \nopenssl\n:\n\n\n[root@osg-se robert]#\n openssl x509 -in /etc/grid-security/hostcert.pem -dates -noout\n\nnotBefore=Jan  4 21:08:41 2010 GMT\n\n\nnotAfter=Jan  4 21:08:41 2011 GMT\n\n\n\n\n\n\nReferences\n\n\n\n\nUseful OpenSSL commands (from NCSA)\n - e.g. how to convert the format of your certificate.", 
            "title": "Host Certificates"
        }, 
        {
            "location": "/security/host-certs/#how-to-get-host-and-service-certificates", 
            "text": "This document is for system administrators. After reading this document you should be able to apply for and install a grid certificate on a grid resource. This document does not explain how to apply for a grid user certificate. To apply for a grid user certificate click  here  instead!", 
            "title": "How to Get Host and Service Certificates"
        }, 
        {
            "location": "/security/host-certs/#requirements", 
            "text": "Before requesting a new host or service certificate, you should check if a certificate is not installed already using  openssl . In this case you may safely skip this document:  [user@host ~]$  openssl x509 -in /etc/grid-security/hostcert.pem -subject -issuer -dates -noout subject= /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=host.opensciencegrid.org  issuer=/DC=org/DC=cilogon/C=US/O=CILogon/CN=CILogon OSG CA 1  notBefore=Jan  4 21:08:09 2010 GMT  notAfter=Jan  4 21:08:09 2011 GMT   The OSG PKI Command Line Clients are tested to work on Python version 2.4+. They have not been tested on Python version 3. In order to proceed you will also need:   the  fully qualified domain name  of the host you need a grid certificate for  the  purpose  of the certificate that explains your request to the Certificate Authority  the full  name of the administrator  responsible for the host  the  e-mail address of the administrator  the  telephone number of the administrator  the name of the  Certificate Authority  your project is affiliated with  the name of the  Virtual Organization  affiliated with the Certificate Authority", 
            "title": "Requirements"
        }, 
        {
            "location": "/security/host-certs/#requesting-hostservice-certificate-using-oim", 
            "text": "The OSG PKI Certificate Request   Management System can be found at:  https://oim.opensciencegrid.org/oim/certificate .  For instructions on how to request a host or service certificate using the Web interface please see the  user guide maintained by GOC .", 
            "title": "Requesting host/service certificate using OIM"
        }, 
        {
            "location": "/security/host-certs/#installation", 
            "text": "", 
            "title": "Installation"
        }, 
        {
            "location": "/security/host-certs/#install-the-osg-pki-command-line-clients", 
            "text": "[root@host /]$  yum install osg-pki-tools", 
            "title": "Install the OSG PKI Command Line Clients"
        }, 
        {
            "location": "/security/host-certs/#configuration-file-pki-clientsini", 
            "text": "This configuration is correctly set by default and you will not need to change them. The information is provided in case you need it for debugging purposes.  The client checks for pki-clients.ini file at three location in order:   $HOME/.osg-pki/OSG_PKI.ini  ./pki-clients.ini  /etc/osg/pki-clients.ini (default location)   The INI file contains the following information:   Request URL  Approve URL  Retrieve URL  Host URL", 
            "title": "Configuration file pki-clients.ini"
        }, 
        {
            "location": "/security/host-certs/#validate-authentication", 
            "text": "Make sure you can create a valid grid proxy. To do so, please follow instructions:   Double-check your configuration to make sure that you are allowed (specifically, your credentials are allowed) to access your CE. You can be either member of a VO allowed to run at the site or you can add your personal certificate locally. This will either be in your GUMS server or in your  edg-mkgridmap  configuration.   Create a proxy with  voms-proxy-init  or  grid-proxy-init . For example:  ``` console\n[user@host /]$ voms-proxy-init -voms GLOW\nEnter GRID pass phrase for this identity:\nYour identity: /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services=People/CN=Alain Roy 424511\nCreating temporary proxy ................................................................................. Done\nContacting  glow-voms.cs.wisc.edu:15001 [/DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=glow-voms.cs.wisc.edu] \"GLOW\" Done\nCreating proxy ..................................................................... Done  Your proxy is valid until Fri Dec  2 01:32:47 2011\n```  You'll substitute your own VO, of course.", 
            "title": "Validate authentication"
        }, 
        {
            "location": "/security/host-certs/#request-a-host-certificate", 
            "text": "Every resource or service contributing to the grid needs a certificate issued by one of the trusted  Certificate Authorities . To proceed you will need following information at hand:   the  fully qualified domain  name of the host you need a grid certificate for  the  purpose  of the certificate that explains your request to the Certificate Authority  the full  name of the administrator  responsible for the host  the  e-mail address of the administrator  the  telephone number of the administrator  the name of the  Certificate Authority  your project is affiliated with  the name of the  Virtual Organization  affiliated with the Certificate Authority", 
            "title": "Request a Host Certificate"
        }, 
        {
            "location": "/security/host-certs/#send-the-request", 
            "text": "This command line client generates a private key and submits a request for a certificate to the OSG PKI. The request will be approved by an appropriate Grid Admin. You will receive an email when this approval has been completed containing directions on how to run 'osg-cert-retreive' to retrieve the certificate. It works in two modes:   CSR is provided by the user: Here the csr provided is just trimmed for begin and end certificate lines and the request is sent to the OIM  CSR is not provided by the user: Here the script generates a private key for the user. Writes it to default key file name or the one specified by -o. Then generates a new csr and sends it to OIM.   The second mode will commonly be used by most OSG site admins  Use  osg-cert-request  to generate a request which will be sent to the Certificate Authority you specified. Change \"host.opensciencegrid.org\" to be the host name for the computer for which you need a certificate and provide your contact details for the grid admin to approve your request.  [user@host /]$  osg-cert-request -H host.opensciencegrid.org -e emailaddress@domain.com -n  Your Name  -p  9999999999   ( Ph No )  -v  Your VO  -y  xyz@domain.com,abc@domain.com   ( CC list )  -m  This is my comment  -o hostkey.pem  Example:  [user@host /]$  osg-cert-request -H sectest.cigi.illinois.edu -e apadmana@domain.edu -n  Anand  -p  9999999999  -m  Testing for developing security documentation  -o hostkey.pem Writing key to hostkey.pem  Connecting to server...  Succesfully submitted  Request Id#: 570   At this point  osg-cert-request  has created some files in the directory you specified and an e-mail has been sent to the Certificate Authority containing your request. The files will be needed again once you receive a reply from the Certificate Authority asking you to retrieve the certificate. Please note down the Request Id. You will need it for retrieving the signed certificate.", 
            "title": "Send the request"
        }, 
        {
            "location": "/security/host-certs/#detailed-description-of-the-osg-cert-request-usage", 
            "text": "This script:\n-   Generates a new host private key and CSR\n-   Only important part of CSR is CN= component\n-   Saves the host private key to disk (as specified by the user)\n-   Authenticates to OIM and posts the CSR as a request to OIM\n-   Returns the request ID to the user\n-   If the user provides the CSR, then this script would just send the same CSR to OIM  Inputs: \n-   fully-qualified hostname\n-   filename to store private key [Optional, default is ./hostkey.pem]\n-   path to user's certificate [Optional, default is path specified by $X509_USER_CERT environment variable, ~/.globus/usercert.pem]\n-   path to user's private key [Optional, default is path specified by $X509_USER_KEY environment variable, ~/.globus/userkey.pem]\n-   Passphrase for user's private key via non-echoing prompt.\n-   User needs to provide VO name if the requested hostname has multiple VO's assigned  Outputs: \n-   Private key, to filename specified by '-o' or ./hostkey.pem by default\n-   Request Id, to stdout  Usage : osg-cert-request [options]\n     Options :    -h, --help            show this help message and exit\n  -c CSR, --csr=CSR     Specify CSR name (default = gennew.csr)\n  -o Output Keyfile, --outkeyfile=Output Keyfile\n                        Specify the output filename for the retrieved user\n                        certificate.  Default is ./hostkey.pem\n  -y CC List, --cc=CC List\n                        Specify the CC list(the email id s to be CCed). Separate values by  , \n  -m Comment, --comment=Comment\n                        The comment to be added to the request\n  -H CN, --hostname=CN  Specify hostname for CSR (FQDN)\n  -e EMAIL, --email=EMAIL\n                        Email address to receive certificate\n  -n NAME, --name=NAME  Name of user receiving certificate\n  -p PHONE, --phone=PHONE\n                        Phone number of user receiving certificate\n  -v VO, --vo=VO\n                        VO name of requested hostname\n  -T, --test            Run in test mode\n  -q, --quiet           don t print status messages to stdout\n  -V, --version         Print the script version number and exit.", 
            "title": "Detailed description of the osg-cert-request usage"
        }, 
        {
            "location": "/security/host-certs/#retrieve-and-install-the-host-certificate", 
            "text": "Once the certificate has been approved by the Certificate Authority you will receive an e-mail for the GOC. Then to retrieve the host certificate we will execute  osg-cert-retrieve : and use the request ID we recorded earlier. Since certificates are public, no authentication of the user is required to retrieve them.  [user@host /]$   osg-cert-retrieve  570  -o hostcert.pem Using timeout of 5 minutes  Running in test mode  Connecting server to retrieve certificate...  Certificate written to hostcert.pem", 
            "title": "Retrieve and Install the Host Certificate"
        }, 
        {
            "location": "/security/host-certs/#detailed-description-of-the-osg-cert-retrive-usage", 
            "text": "This osg-cert-retrive script:\n-   Accepts a request Id from the user\n-   Connects to OIM and requests the certificate identified by the request ID\n-   Writes the certificate to disk (as specified by the user)  Inputs: \n-   Request ID\n-   Filename to store certificate [Optional, default is ./hostcert.pem]  Outputs: \n-   Host certificate as PEM, to filename specified or ./hostcert.pem  Usage :osg-cert-retrieve -h/--help [for detailed explanations of options]  Options:\n  -h, --help            show this help message and exit\n  -i ID, --id=ID        Specify ID# of certificate to retrieve[Required]\n  -o ID, --certfile=ID  Specify the output filename for the retrieved user\n                        certificate . Default is ./hostcert.pem\n  -T, --test            Run in test mode\n  -q, --quiet           don t print status messages to stdout\n  -V, --version         Print the script version number and exit.  The certificate consists of two files (default hostcert.pem and hostkey.pem) which have been placed into the current directory. Note, If you did not use the  --o  option the filenames will be what you provided.   Warning  Please note that these files represent a public and a private and should be treated accordingly!   Please take a moment to verify that the  certificate matches the hostname  of the resource where you intend to install it before you proceed:  [user@host /]$  grid-cert-info -file ./hostcert.pem -subject /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=host.opensciencegrid.org  [user@host /]$  hostname -f host.opensciencegrid.org   Finally, install the certificate in the default location  /etc/grid-security/ :  [root@host /]$  cp ./host.opensciencegrid.orgcert.pem /etc/grid-security/hostcert.pem [root@host /]$  chmod  444  /etc/grid-security/hostcert.pem [root@host /]$  cp ./host.opensciencegrid.orgkey.pem /etc/grid-security/hostkey.pem [root@host /]$  chmod  400  /etc/grid-security/hostkey.pem", 
            "title": "Detailed description of the osg-cert-retrive usage"
        }, 
        {
            "location": "/security/host-certs/#request-a-service-certificate", 
            "text": "You can use the same  osg-cert-request  and  osg-cert-retrieve  commands to request and service certificates just like any host certificate. Just use service/hostname for the -t parameter.  [user@host /]$  osg-cert-request -H http/host.opensciencegrid.org -e emailaddress@domain.com -n  Your Name  -p  9999999999   ( Ph No )  -v  Your VO  -y  xyz@domain.com,abc@domain.com   ( CC list )  -m  This is my comment  -o hostkey.pem Writing key to ./hostkey.pem  Connecting to server...  Succesfully submitted  Request Id#: 571   Please note down the request ID.  Once the certificate has been approved by the Certificate Authority you will receive an e-mail for the GOC. Then to retrieve the host certificate we will execute  osg-cert-retrieve : and use the request ID we recorded earlier. Since certificates are public, no authentication of the user is required to retrieve them.  [user@host /]$   osg-cert-retrieve  571  -o hostcert.pem Using timeout of 5 minutes  Running in test mode  Connecting server to retrieve certificate...  Certificate written to hostcert.pem    Warning  Please note that these files represent a public and a private and should be treated accordingly!", 
            "title": "Request a Service Certificate"
        }, 
        {
            "location": "/security/host-certs/#install-the-service-certificate", 
            "text": "The  Service Certificate  should be installed under a subdirectory in  /etc/grid-security  indicating the name of the service. The next step will install the service certificate in the default location  /etc/grid-security/http :  [root@host /]$  cp ./host.opensciencegrid.org-httpcert.pem /etc/grid-security/http/httpcert.pem [root@host /]$  chmod  444  /etc/grid-security/http/httpcert.pem [root@host /]$  cp ./host.opensciencegrid.org-httpkey.pem /etc/grid-security/http/httpkey.pem [root@host /]$  chmod  400  /etc/grid-security/http/httpkey.pem   Warning  Please note that the service certificate must also be owned by the unix user who runs the service. For  Apache/Tomcat  this is the tomcat user:   [root@host /]$  chown tomcat.tomcat /etc/grid-security/http/httpcert.pem [root@host /]$  chown tomcat.tomcat /etc/grid-security/http/httpkey.pem  Please refer to  Operations/OSGPKICommandlineClients  for full documentation of the Client package", 
            "title": "Install the Service Certificate"
        }, 
        {
            "location": "/security/host-certs/#information-for-grid-admins", 
            "text": "If you are a grid admin then you can use a single command to request and retrieve a certificate immediately. For getting grid admin privileges, request enrollment  here  after obtaining your  user certificate .  Request and retrieve multliple host certificates from OIM. Authenticates to OIM and is only for use by Grid Admins for certificates they are authorized to approve. This script is only supported with all hosts being in the same domain (so we ensure they go to the same Grid Admin). The certificates are stored with the format of 'hostname-requestid.pem' (i.e. the id generated from the request for the certificate). The key is stored as 'hostname-serial-key.pem'.  Examples:  [user@host /]$  osg-gridadmin-cert-request -H host.opensciencegrid.org  If you want to request more then one certificate you can list them in a file (one host per line) and use the following command  [user@host /]$  osg-gridadmin-cert-request -f hostfile", 
            "title": "Information for Grid Admins"
        }, 
        {
            "location": "/security/host-certs/#detailed-description-of-the-osg-gridadmin-cert-request-usage", 
            "text": "This osg-gridadmin-cert-request script does the following in the process of acquiring certificates for the hostnames specified:  Reads a list of fully-qualified hostnames from a file specified by the user. For reach hostname: Generates a new private key and CSR Only important part of CSR is CN= component Writes the private key to a file with filename: /-key.pem Prompts the user for their private key pass phrase Pass phrase is cached so user is not re-prompted Authenticates to OIM and posts the CSRs as a single request to OIM Request id is returned and subsequently used Authenticates to OIM and approves the request Waits one minute for request to be processed by OIM Connects to OIM and attempts to retrieve certificates Writes out any certificates it retrieves with filename of /- .pem if all certificates have been retrieved, exits loop Wait 5 seconds and repeat.  Inputs:  filename of list of hostnames prefix path in which to write private keys and certificares [default: .] path to user's certificate [Optional, default is path specified by $X509_USER_CERT environment variable, ~/.globus/usercert.pem] path to user's private key [Optional, default is path specified by $X509_USER_KEY environment variable, ~/.globus/userkey.pem] Passphrase for user's private key via non-echoing prompt.  Outputs:  N host certificates in PEM format N private keys in PEM format  Usage:osg-gridadmin-cert-request -h/--help [for detailed explanations of options]  Options : -h, --help show this help message and exit -k PKEY, --pkey=PKEY Specify Requestor's private key (PEM Format). If not specified will take the value of X509_USER_KEY or $HOME/.globus/userkey.pem -c CERT, --cert=CERT Specify Requestor's certificate (PEM Format). If not specified will take the value of X509_USER_CERT or $HOME/.globus/usercert.pem -T, --test Run in test mode -q, --quiet don't print status messages to stdout -V, --version Print the script version number and exit.  Hostname Options : Use either of these options. Specify hostname as a single hostname using -H/--hostname or specify from a file using -f/--hostfile.  -H HOSTNAME, --hostname=HOSTNAME Specify the hostname or service/hostname for which you want to request the certificate for. If specified -f/--hostfile will be ignored -f HOSTFILE, --hostfile=HOSTFILE Filename with one hostname or service/hostname per line", 
            "title": "Detailed description of the osg-gridadmin-cert-request usage"
        }, 
        {
            "location": "/security/host-certs/#frequently-asked-questions", 
            "text": "", 
            "title": "Frequently Asked Questions"
        }, 
        {
            "location": "/security/host-certs/#can-i-use-any-host-to-request-a-certificate-for-a-different-host", 
            "text": "YES, you can use any host to create a certificate request as long as the hostname for the certificate is a fully qualified domain name.", 
            "title": "Can I use any host to request a certificate for a different host?"
        }, 
        {
            "location": "/security/host-certs/#may-i-reuse-my-host-certificate-as-a-service-certificate", 
            "text": "NO! For security reasons, please do not use clones of your host certificate for additional certificates even though it's technically possible.", 
            "title": "May I reuse my host certificate as a service certificate?"
        }, 
        {
            "location": "/security/host-certs/#how-do-i-renew-a-hostservice-certificate", 
            "text": "There is no separate procedure. Simply ask for a new certificate the same way you asked for it the previous time.", 
            "title": "How do I renew a host/service certificate?"
        }, 
        {
            "location": "/security/host-certs/#i-get-a-gss-authentication-failure-when-users-try-to-authenticate-with-my-site", 
            "text": "You likely used an  alias  for the host instead of the fully qualified domain name when you generated the certificate request. This can cause the GSS authentication failures similar to the following when a user tries to authenticate to the host after your certificate is installed:  GSS authentication failure   GSS Major Status: General failure   GSS Minor Status Error Chain:   accept_sec_context.c:gss_accept_sec_context:403:   Error during delegation: Delegation protocol violation   Failure: GSS failed Major:000d0000 Minor:00000001 Token:00000000", 
            "title": "I get a \"GSS authentication failure\" when users try to authenticate with my site?"
        }, 
        {
            "location": "/security/host-certs/#how-can-i-check-if-i-have-a-host-certificate-installed-already", 
            "text": "By default the host certificate key pair will be installed in  /etc/grid-security/hostcert.pem  and  /etc/grid-security/hostkey.pem . You can use  openssl  to access basic information about the certificate:  [root@osg-se robert]#  openssl x509 -in /etc/grid-security/hostcert.pem -subject -issuer -dates -noout subject= /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=host.opensciencegrid.org  issuer= /DC=org/DC=cilogon/C=US/O=CILogon/CN=CILogon OSG CA 1  notBefore=Apr  8 00:00:00 2013 GMT  notAfter=May 17 12:00:00 2014 GMT", 
            "title": "How can I check if I have a host certificate installed already?"
        }, 
        {
            "location": "/security/host-certs/#how-can-i-check-the-expiration-time-of-my-installed-host-certificate", 
            "text": "If you installed the Certificates Script Package you can use  grid-cert-info  to retrieve information about the certificate:  [root@osg-se robert]#  grid-cert-info -file /etc/grid-security/hostcert.pem -startdate -enddate Jan  4 21:08:41 2010 GMT  Jan  4 21:08:41 2011 GMT   Alternatively you can use  openssl :  [root@osg-se robert]#  openssl x509 -in /etc/grid-security/hostcert.pem -dates -noout notBefore=Jan  4 21:08:41 2010 GMT  notAfter=Jan  4 21:08:41 2011 GMT", 
            "title": "How can I check the expiration time of my installed host certificate?"
        }, 
        {
            "location": "/security/host-certs/#references", 
            "text": "Useful OpenSSL commands (from NCSA)  - e.g. how to convert the format of your certificate.", 
            "title": "References"
        }, 
        {
            "location": "/security/user-certs/", 
            "text": "How do I get a PKI/X509 Personal Certificate?\n\n\n\n\nNote\n\n\nFor instructions on getting \nservice\n and \nhost\n certificates, \nyou're in the wrong place\n. See \nGet Host and Service Certificates\n.\n\n\n\n\nThis document describes how to get and set up a \npersonal\n certificate (also called a grid user certificate), from the OSG Certificate Authority (OSG CA). The OSG CA is recognized by all OSG resources. Other CAs may be used; if your virtual organization (VO) requires that you get a certificate from a different CA, \ncontact your VO Support Center\n for instructions.\n\n\nKnow your Responsibilities\n\n\nWhen you request a certificate your provide some public personal information about yourself; \nname, email address, phone number\n, and which \nVO\n (virtual organization) you belong to. If any of this information changes after you have your certificate you should notify OSG (and probably VO) of the changes. For the OSG RA send email to osg-ra at opensciencegrid dot org.\n\n\nYour \nname\n and \nemail\n address are encoded into the signed certificate and \nif either of those change\n (usually email address) then you should \nrequest a new certificate\n with the correct information and \nrevoke the old certificate\n.\n\n\nGetting your Certificate\n\n\nThere are two different ways to obtain your certificate, either via a command line interface, or through a web browser. The links below will guide you through whichever method you choose. The command line method is generally simpler and less prone to errors than using a web browser but it may require extra setup and package installation the first time you do it. Most people end up needing their certificate for both command line grid use and web browser use and it is generally easier to import your certificate into your browser (and email client) from the command line certificate files than to export your certificate from your web browser to use on the command line. This is because of the large variety of web browsers each with it's own way to deal with certificates. If you will use the certificate ONLY in your web browser then requesting it from your browser is probably the easiest method.\n\n\nGet or renew a certificate with command line interface.\n\n\nPKCS12 (.p12) vs PEM format\n\n\nYour certificate and key pair can be stored as separate files, by default named \n~/.globus/usercert.pem\n and \n~/.globus/userkey.pem\n, or they can be bundled in a single file in PKCS12 (Public Key Cryptography Standard #12), by default named \n~/.globus/usercred.p12\n. All OSG user tools works with both formats. Unless you specify a name in the command, they look first for the certificate/key pair default names, then for the PKCS12 default name and use the first one they find. PKCS 12 is a single file, convenient to move and used by many softwares, e.g. the Web browsers (so you don't need conversions).\n\n\nAnyway note that the PKCS12 bundle includes normally the certificate, the key and also the CA certificate (or a certificate chain) validating your certificate. In the rare (but possible) case where the CA certificate expires before your certificate this may lead to the creation of a proxy without VOMS attributes and confusing error messages where it is not clear if the problem is with the host, remote server or the certificate, e.g. \"Error: Error during SSL handshake:Either proxy or user certificate are expired.\" or \"error:80066423:lib(128):verify_callback:remote certificate has expired:sslutils.c:1963 remote certificate has expired\".\n\n\nuser@host $\n  voms-proxy-init -debug\n\nDetected Globus version: 2.2\n\n\nUnspecified proxy version, settling on Globus version: 2\n\n\nNumber of bits in key :1024\n\n\nFiles being used:\n\n\n CA certificate file: none\n\n\n Trusted certificates directory : /etc/grid-security/certificates\n\n\n Proxy certificate file : /tmp/x509up_u20003\n\n\n User certificate file: /home/user/.globus/usercred.p12\n\n\n User key file: /home/user/.globus/usercred.p12\n\n\nOutput to /tmp/x509up_u20003\n\n\nEnter GRID pass phrase for this identity:\n\n\nYour identity: /DC=org/DC=doegrids/OU=People/CN=First Lastname 12345\n\n\nCreating proxy to /tmp/x509up_u20003 .............++++++\n\n\n.....................++++++\n\n\n Done\n\n\n\nYour proxy is valid until Fri Aug  9 00:18:07 2013\n\n\nError: Certificate verify failed.\n\n\nerror:80066423:lib(128):verify_callback:remote certificate has expired:sslutils.c:1963\n\n\nremote certificate has expired\n\n\nFunction: verify_callback\n\n\nerror:80066412:lib(128):verify_callback:certificate::sslutils.c:2283\n\n\n        subject=/DC=org/DC=DOEGrids/OU=Certificate Authorities/CN=DOEGrids CA 1\n\n\n        issuer =/DC=net/DC=ES/O=ESnet/OU=Certificate Authorities/CN=ESnet Root CA 1\n\n\ncertificate:\n\n\n        subject=/DC=org/DC=DOEGrids/OU=Certificate Authorities/CN=DOEGrids CA 1\n\n\n        issuer =/DC=net/DC=ES/O=ESnet/OU=Certificate Authorities/CN=ESnet Root CA 1\n\n\nFunction: verify_callback\n\n\nuser@host $\n voms-proxy-init -debug -voms osg\n\nDetected Globus version: 2.2\n\n\nUnspecified proxy version, settling on Globus version: 2\n\n\nNumber of bits in key :1024\n\n\nFiles being used:\n\n\n CA certificate file: none\n\n\n Trusted certificates directory : /etc/grid-security/certificates\n\n\n Proxy certificate file : /tmp/x509up_u20003\n\n\n User certificate file: /home/user/.globus/usercred.p12\n\n\n User key file: /home/user/.globus/usercred.p12\n\n\nOutput to /tmp/x509up_u20003\n\n\nEnter GRID pass phrase for this identity:\n\n\nYour identity: /DC=org/DC=doegrids/OU=People/CN=First Lastname 12345\n\n\nUsing configuration file /home/marco/.glite/vomses\n\n\nUsing configuration file /etc/vomses\n\n\nCreating temporary proxy to /tmp/tmp_x509up_u20003_17448 ..........................++++++\n\n\n...................++++++\n\n\n Done\n\n\nContacting  voms.opensciencegrid.org:15027 [/DC=org/DC=doegrids/OU=Services/CN=host/voms.opensciencegrid.org] \nosg\n Failed\n\n\n\nError: Error during SSL handshake:Either proxy or user certificate are expired.\n\n\n\nNone of the contacted servers for osg were capable\n\n\nof returning a valid AC for the user.\n\n\n\n\n\n\nTo solve the problem split the bundle in the two PEM files as documented in the \nreferenced document\n, so that the embedded CA chain is not used.\n\n\nRevoke your Certificate if Compromised\n\n\nIf the security of your certificate or private key has been compromised, you have a responsibility to revoke the certificate. You can follow the steps \nhere\n to revoke your certificate. The same instructions apply if you need to revoke a certificate because your email address changed or your name has changed.\n\n\nReferences\n\n\n\n\nUseful Documentation.OpenSSL commands (from NCSA)\n - e.g. how to convert the format of your certificate.", 
            "title": "User Certificates"
        }, 
        {
            "location": "/security/user-certs/#how-do-i-get-a-pkix509-personal-certificate", 
            "text": "Note  For instructions on getting  service  and  host  certificates,  you're in the wrong place . See  Get Host and Service Certificates .   This document describes how to get and set up a  personal  certificate (also called a grid user certificate), from the OSG Certificate Authority (OSG CA). The OSG CA is recognized by all OSG resources. Other CAs may be used; if your virtual organization (VO) requires that you get a certificate from a different CA,  contact your VO Support Center  for instructions.", 
            "title": "How do I get a PKI/X509 Personal Certificate?"
        }, 
        {
            "location": "/security/user-certs/#know-your-responsibilities", 
            "text": "When you request a certificate your provide some public personal information about yourself;  name, email address, phone number , and which  VO  (virtual organization) you belong to. If any of this information changes after you have your certificate you should notify OSG (and probably VO) of the changes. For the OSG RA send email to osg-ra at opensciencegrid dot org.  Your  name  and  email  address are encoded into the signed certificate and  if either of those change  (usually email address) then you should  request a new certificate  with the correct information and  revoke the old certificate .", 
            "title": "Know your Responsibilities"
        }, 
        {
            "location": "/security/user-certs/#getting-your-certificate", 
            "text": "There are two different ways to obtain your certificate, either via a command line interface, or through a web browser. The links below will guide you through whichever method you choose. The command line method is generally simpler and less prone to errors than using a web browser but it may require extra setup and package installation the first time you do it. Most people end up needing their certificate for both command line grid use and web browser use and it is generally easier to import your certificate into your browser (and email client) from the command line certificate files than to export your certificate from your web browser to use on the command line. This is because of the large variety of web browsers each with it's own way to deal with certificates. If you will use the certificate ONLY in your web browser then requesting it from your browser is probably the easiest method.  Get or renew a certificate with command line interface.", 
            "title": "Getting your Certificate"
        }, 
        {
            "location": "/security/user-certs/#pkcs12-p12-vs-pem-format", 
            "text": "Your certificate and key pair can be stored as separate files, by default named  ~/.globus/usercert.pem  and  ~/.globus/userkey.pem , or they can be bundled in a single file in PKCS12 (Public Key Cryptography Standard #12), by default named  ~/.globus/usercred.p12 . All OSG user tools works with both formats. Unless you specify a name in the command, they look first for the certificate/key pair default names, then for the PKCS12 default name and use the first one they find. PKCS 12 is a single file, convenient to move and used by many softwares, e.g. the Web browsers (so you don't need conversions).  Anyway note that the PKCS12 bundle includes normally the certificate, the key and also the CA certificate (or a certificate chain) validating your certificate. In the rare (but possible) case where the CA certificate expires before your certificate this may lead to the creation of a proxy without VOMS attributes and confusing error messages where it is not clear if the problem is with the host, remote server or the certificate, e.g. \"Error: Error during SSL handshake:Either proxy or user certificate are expired.\" or \"error:80066423:lib(128):verify_callback:remote certificate has expired:sslutils.c:1963 remote certificate has expired\".  user@host $   voms-proxy-init -debug Detected Globus version: 2.2  Unspecified proxy version, settling on Globus version: 2  Number of bits in key :1024  Files being used:   CA certificate file: none   Trusted certificates directory : /etc/grid-security/certificates   Proxy certificate file : /tmp/x509up_u20003   User certificate file: /home/user/.globus/usercred.p12   User key file: /home/user/.globus/usercred.p12  Output to /tmp/x509up_u20003  Enter GRID pass phrase for this identity:  Your identity: /DC=org/DC=doegrids/OU=People/CN=First Lastname 12345  Creating proxy to /tmp/x509up_u20003 .............++++++  .....................++++++   Done  Your proxy is valid until Fri Aug  9 00:18:07 2013  Error: Certificate verify failed.  error:80066423:lib(128):verify_callback:remote certificate has expired:sslutils.c:1963  remote certificate has expired  Function: verify_callback  error:80066412:lib(128):verify_callback:certificate::sslutils.c:2283          subject=/DC=org/DC=DOEGrids/OU=Certificate Authorities/CN=DOEGrids CA 1          issuer =/DC=net/DC=ES/O=ESnet/OU=Certificate Authorities/CN=ESnet Root CA 1  certificate:          subject=/DC=org/DC=DOEGrids/OU=Certificate Authorities/CN=DOEGrids CA 1          issuer =/DC=net/DC=ES/O=ESnet/OU=Certificate Authorities/CN=ESnet Root CA 1  Function: verify_callback  user@host $  voms-proxy-init -debug -voms osg Detected Globus version: 2.2  Unspecified proxy version, settling on Globus version: 2  Number of bits in key :1024  Files being used:   CA certificate file: none   Trusted certificates directory : /etc/grid-security/certificates   Proxy certificate file : /tmp/x509up_u20003   User certificate file: /home/user/.globus/usercred.p12   User key file: /home/user/.globus/usercred.p12  Output to /tmp/x509up_u20003  Enter GRID pass phrase for this identity:  Your identity: /DC=org/DC=doegrids/OU=People/CN=First Lastname 12345  Using configuration file /home/marco/.glite/vomses  Using configuration file /etc/vomses  Creating temporary proxy to /tmp/tmp_x509up_u20003_17448 ..........................++++++  ...................++++++   Done  Contacting  voms.opensciencegrid.org:15027 [/DC=org/DC=doegrids/OU=Services/CN=host/voms.opensciencegrid.org]  osg  Failed  Error: Error during SSL handshake:Either proxy or user certificate are expired.  None of the contacted servers for osg were capable  of returning a valid AC for the user.   To solve the problem split the bundle in the two PEM files as documented in the  referenced document , so that the embedded CA chain is not used.", 
            "title": "PKCS12 (.p12) vs PEM format"
        }, 
        {
            "location": "/security/user-certs/#revoke-your-certificate-if-compromised", 
            "text": "If the security of your certificate or private key has been compromised, you have a responsibility to revoke the certificate. You can follow the steps  here  to revoke your certificate. The same instructions apply if you need to revoke a certificate because your email address changed or your name has changed.", 
            "title": "Revoke your Certificate if Compromised"
        }, 
        {
            "location": "/security/user-certs/#references", 
            "text": "Useful Documentation.OpenSSL commands (from NCSA)  - e.g. how to convert the format of your certificate.", 
            "title": "References"
        }, 
        {
            "location": "/security/osg-ca-manage/", 
            "text": "Managing CAs\n\n\nThe osg-ca-manage tool provides a unified interface to manage the CA Certificate installations. This page provides the instructions on using this command. It provides status commands that allows you to list the CAs and the validity of the CAs and CRLs included in the installation. The manage commands allow you to fetch CAs and CRLs, change the distribution URL, as well as add and remove CAs from your local installation.\n\n\nSyntax\n\n\n   osg-ca-manage [global_options] command \n    global_options =\n        [--verbose]\n        [--force]\n        [--cert-dir \nlocation\n]\n        [--help | --usage]\n        [--version]\n        [--auto-refresh]\n\n    command = [manage_command | status_command]\n\n    status_command = [\n        showCAURL |\n        listCA [--pattern \npattern\n] |\n        verify [--hash \nCA hash\n  | --pattern \npattern\n] |\n        diffCAPackage |\n        show [--certfile \ncert_file\n | --hash \nCA hash\n] |\n        showChain [--certfile \ncert_file\n | --hash \nCA hash\n]\n    ]\n\n    manage_command = [\n        setupCA --location \nroot|PATH\n [--url \nosg|igtf|URL\n --no-update --force] |\n        refreshCA |\n        fetchCRL |\n        setCAURL [--url \nosg|igtf|URL\n] |\n        add [--cadir \nlocaldir\n | --caname \nCA\n]\n        remove [--cadir \nlocaldir\n | --caname \nCA\n]\n    ]\n\n\n\n\n\nExplanation of global options\n\n\nZero or more of these options may be used during an execution of ca_manage.\n\n\n\n\n--verbose\n Provides you with more information depending on the command context.\n\n\n--force\n Forces the command to run ignoring any checks/warnings. The actual effect is context dependent, and this behavior is noted in the command details below.\n\n\n--cert-dir \nlocation\n This location specifies the path CA directory. If this option is not specified then the command will look for \n$X509_CERT_DIR\n, and \n$VDT_LOCATION/globus/TRUSTED_CA\n respectively. If none of these directories can be found, the command will exit with an error.\n\n\n--auto-refresh\n This option will indicate if this permissible to fetch CAs and CRLs as deemed necessary by this tool. For example at the end of an addCA/removeCA it would be advisable to refresh the CA list and the corresponding CRLs. Default is \nnot\n to refresh, unless the admin requests it by specifying this option.\n\n\n--version\n Prints the version of the vdt-ca-certs-manager tool.\n\n\n--help | --usage\n Print usage information. Show a brief explanatory text for using osg-ca-manage.\n\n\n\n\nExplanation of commands\n\n\nExactly one command is to be specified during an execution of osg-ca-manage\n\n\nStatus commands\n\n\n\n\nshowCAURL\n This will print out the distribution location specified in the config file. This command will read osg-update-certs.conf and output cacerts_url.\n\n\nlistCA [--pattern \npattern\n]\n This command will use openssl x509 command on the files in the --dir to provide hash, the subject and whether a CA is IGTF or TeraGrid accredited and distribution package which was used to download CAs into the directory. --verbose option will provide additional information like issuer (of CA) and all associated dates (CA cert issuance date, and CRL issuance date, and expiry dates). The command will look for CA files in the -certDir. The \npattern\n specified in the option will be matched, using perl regex, against the subject field of the certificate (but we might also expand it include issuer if needed) and all CAs are listed if no pattern is given.\n\n\nverify [--hash \nCA_hash\n | --pattern \npattern\n]\n The verify command will check all CAs (or if specified only the \nCA_hash\n) in the \ncertDir\n directory, to see if any CA/CRL have expired or are about to do so. If any expired CA/CRL are found, an error is issued along with the hash, date when CA cert/CRL expired. A warning is issued if either the CA cert or CRL is about the expire within the next 24 Hrs. The --verbose option provides the CA Name, date the CA certs and CRL files are created (by the CA), and when they will expire. In addition to hash value we will also consider providing an option of verify using \npattern\n \n\n\ndiffCAPackage\n This command will compare the hash of certificates included in the certificate directory against the latest VDT/OSG distribution (based on your cacerts_url) and outputs the difference. \n\n\nshow [--certfile \ncert_file\n | --hash \nCA_hash\n]\n This command will essentially provide a condensed output of openssl x509 command. --verbose option will provide the full output. If --hash option is used we will look for the \nCA_hash\n.o\n file in the \ncertDir\n. The --certfile option can also take in a user proxy. \n\n\nshowChain [--certfile \ncert_file\n | --hash \nCA_hash\n]\n This command will output the trust chain of the certificate. \ncertDir\n will be used as the directory in which search for ancestor certs will be conducted. This command can also be used to trace the trust chain of a user proxy.\n\n\n\n\nManage commands\n\n\n\n\nsetupCA --location \nroot|PATH\n [--url \nosg|igtf|URL\n --no-update --force]\n This command is used for the inital setup of the CA package. The CA package can be setup to download CAs from any URL. Keywords are provided for various distributions. For the location to specify, keywords are provided to install into 'root' (/etc/grid-security). A --no-update option is available. Setting this flag instructs just setup the symlinks only and not to run configure osg-update-certs to be run automatically. This option is for installations that will not manage their own certificates, but will rely on updates through another method (such as RPM, or using osg-update-certs from a different OSG installation). A common use case for this is to have worker-node installations rely on the CA certificates being available on an NFS share, and the updating will happen on a single node.\n\n\nrefreshCA\n This command run osg-update-certs to check for a new version of the CA distribution. If you already have the latest version, but wish to force an update anyways, use the --force option. 1.\nfetchCRL\n It retrieves CRLs for all CAs within the directory. This will involve invoking fetch-crl, with appropriate arguments. NOTE: If vdt's fetch-crl service has not been enabled (i.e. no fetch-crl entry in crontab), then this command will not execute. This is a safety mechanism to prevent crls from being downloaded using this tool if they are not scheduled to be updated.\n\n\nsetCAURL [--url \nosg|igtf|URL\n]\n This command sets the location from where the CA files. This command will modify vdt-update-certs.conf and set the cacerts_url as \nURL_location\n. Only if --auto-refresh is specified both CA and CRLs are refreshed once the URL change has been made. The distribution \nURL_location\n will be required to conform to the VDT CA distribution format (e.g. similar to \nhttp://vdt.cs.wisc.edu/software/certificates/ca-certs-version\n). If the \nURL_location\n cannot be reached or if it is valid syntactically (i.e. does not conform to the format requirements) a warning will be issues and no changes will be made. The --force option can be used to force a change ignoring the warning. If URL location is left unspecified the \nURL_location\n will be set to OSG default. We define keywords for OSG, IGTF as shortcuts for OSG wide well-known CA URL_locations.\n\n\nadd [--cadir \nlocaldir\n | --caname \nCA\n]\n The --hash argument is required. If --dir is not specified we will assume that the user wants to include a CA he has previously excluded and will remove the corresponding exclude lines from the config. If \nCA_hash\n is not known to us or it is already included we will provide appropriate error/warning information. In the common case this command will add include lines for \nlocal_dir\n/\nCA_hash\n.*, into the vdt-update-certs.conf file. Lastly the command will invoke functions refresh the CAs and fetch CRLs. This command will also do some preliminary error checks, e.g. make sure that \u201c.0\u201d, \u201c.crl_url\u201d, \u201c.signing_policy\u201d files exist and that --dir is different than --certDir.\n\n\nremove [--cadir \nlocaldir\n | --caname \nCA\n]\n This command will be complementary to add and would either add an exclude or remove an include depending on the scenario. This command will also refresh CA and CRLs. vdt-update-certs do the job of removing cert files, we will still do the preliminary error checks to make sure that the certs that are being removed are included in the first place. For both addCA and removeCA, new CAs will be included/removed and CRLs will be refreshed only if --auto-refresh is set.\n\n\n\n\nUsage Examples\n\n\n\n\nNote\n\n\nThese commands will not work if of the osg-ca-certs (or igtf-ca-certs) RPM packages are installed.\n\n\n\n\nInstall a Certificate Authority Package\n\n\nBefore you proceed to install a Certificate Authority Package you should decide which of the available packages to install.\n\n\n\n\nosg\n, the package recommended to be used by production resources on the OSG. It is based on the CA distribution from the IGTF, but it may differ slightly as decided by the \nSecurity Team\n. \n\n\nigtf\n, the package is a redistribution of the unchanged CA distribution from the IGTF\n\n\nurl\n a package provided at a given URL\n\n\n\n\n\n\nNote\n\n\nIf in doubt, please consult the policies of your home institution and get in contact with the \nSecurity Team\n.\n\n\n\n\nNext decide at what location to install the Certificate Authority Package:\n\n\n\n\non the \nroot\n file system in a system directory \n/etc/grid-security/certificates\n\n\nin a \ncustom\n directory that can also be shared\n\n\n\n\nSetup the CA Certificates\n\n\nThe Certificate Authority Package is preferably be used by grid users without root privileges \nor\n if the CA certificates will not be shared by other installations on the same host.\n\n\nroot@host #\n osg-ca-manage setupca --location \nroot\n --url osg\n\nSetting CA Certificates for at \n/etc/grid-security/certificates\n\n\n\nSetup completed successfully.\n\n\n\n\n\n\nAfter a successful installation the certificates will be installed in (\n/etc/grid-security/certificates\n in this example). Typically to write into this default location you will need root privileges.\n\n\nIf you need to need to install it with out root privileges use\n\n\nuser@host $\n osg-ca-manage setupca --location \n$HOME\n/certificates\n --url osg\n\nSetting CA Certificates for at \n$HOME/certificates\n\n\n\nSetup completed successfully.\n\n\n\n\n\n\nAdding/Removing a directory of local CAs\n\n\nAdding\n\n\nroot@host #\nosg-ca-manage add --cadir /etc/grid-security/localca\n\nNOTE:\n\n\n    You did not specify the --auto-refresh flag.\n\n\n    So the changes made to the configuration will not be reflected till the next time\n\n\n    when CAs and CRLs are updated respectively by osg-update-certs and fetch-crl running from cron.\n\n\n    Run `osg-ca-manage refreshCA` and `osg-ca-manage fetchCRL` to commit your changes immediately.\n\n\n\n\n\n\nHere is the resulting file after add\n\n\n##cat /etc/osg/osg-update-certs.conf\n# Configuration file for osg-update-certs\n\n# This file has been regenerated by osg-ca-manage, which removes most\n# comments.  You can still manually modify it, any manual change will\n# be preserved if osg-ca-manage is used again.\n\n## The parent location certificates will be installed at.\ninstall_dir = /etc/grid-security\n\n## cacerts_url is the URL of your certificate distribution\ncacerts_url = https://repo.grid.iu.edu/pacman/cadist/ca-certs-version-igtf-new\n\n## log specifies where logging output will go\nlog = /var/log/osg-update-certs.log\n\n## include specifies files (full pathnames) that should be copied\n## into the certificates installation after an update has occured.\ninclude=/etc/grid-security/localca/*\n\n## exclude_ca specifies a CA (not full pathnames, but just the hash\n## of the CA you want to exclude) that should be removed from the\n## certificates installation after an update has occured.\n\ndebug = 0\n\n\n\n\n\nRemoving\n\n\nroot@host #\nosg-ca-manage remove --cadir /etc/grid-security/localca\n\nNOTE:\n\n\n    You did not specify the --auto-refresh flag.\n\n\n    So the changes made to the configuration will not be reflected till the next time\n\n\n    when CAs and CRLs are updated respectively by osg-update-certs and fetch-crl running from cron.\n\n\n    Run `osg-ca-manage refreshCA` and `osg-ca-manage fetchCRL` to commit your changes immediately.\n\n\n\n\n\n\nRemoving/Adding a particular CA included in OSG CA package\n\n\nRemoving\n\n\nroot@host #\nosg-ca-manage remove --caname ce33db76\n\nSymlink detected for hash: We have determided that the hash value you entered belong to the CA \nIRAN-GRID.pem\n. If you wish to add this CA back you will have to use this name is the parameter.\n\n\nNOTE:\n\n\n    You did not specify the --auto-refresh flag.\n\n\n    So the changes made to the configuration will not be reflected till the next time\n\n\n    when CAs and CRLs are updated respectively by osg-update-certs and fetch-crl running from cron.\n\n\n    Run `osg-ca-manage refreshCA` and `osg-ca-manage fetchCRL` to commit your changes immediately.\n\n\n\n\n\n\nThe resulting config file after the remove is as follows\n\n\n##cat /etc/osg/osg-update-certs.conf\n# Configuration file for osg-update-certs\n\n# This file has been regenerated by osg-ca-manage, which removes most\n# comments.  You can still manually modify it, any manual change will\n# be preserved if osg-ca-manage is used again.\n\n## The parent location certificates will be installed at.\ninstall_dir = /etc/grid-security\n\n## cacerts_url is the URL of your certificate distribution\ncacerts_url = https://repo.grid.iu.edu/pacman/cadist/ca-certs-version-igtf-new\n\n## log specifies where logging output will go\nlog = /var/log/osg-update-certs.log\n\n## include specifies files (full pathnames) that should be copied\n## into the certificates installation after an update has occured.\n\n## exclude_ca specifies a CA (not full pathnames, but just the hash\n## of the CA you want to exclude) that should be removed from the\n## certificates installation after an update has occured.\nexclude_ca = IRAN-GRID\n\ndebug = 0\n\n\n\n\n\nAdding back the removed CA\n\n\nroot@host #\nosg-ca-manage add --caname IRAN-GRID\n\nNOTE:\n\n\n    You did not specify the --auto-refresh flag.\n\n\n    So the changes made to the configuration will not be reflected till the next time\n\n\n    when CAs and CRLs are updated respectively by osg-update-certs and fetch-crl running from cron.\n\n\n    Run `osg-ca-manage refreshCA` and `osg-ca-manage fetchCRL` to commit your changes immediately.\n\n\n\n\n\n\nInspect Installed CA Certificates\n\n\nYou can inspect the list of CA Certificates that have been installed:\n\n\nuser@host $\n osg-ca-manage listCA\n\nHash=09ff08b7; Subject= /C=FR/O=CNRS/CN=CNRS2-Projets; Issuer= /C=FR/O=CNRS/CN=CNRS2; Accreditation=Unknown; Status=https://repo.grid.iu.edu/pacman/cadist/ca-certs-version\n\n\nHash=0a12b607; Subject= /DC=org/DC=ugrid/CN=UGRID CA; Issuer= /DC=org/DC=ugrid/CN=UGRID CA; Accreditation=Unknown; Status=https://repo.grid.iu.edu/pacman/cadist/ca-certs-version\n\n\n[...]\n\n\n\n\n\n\nAny certificate issued by any of the Certificate Authorities listed will be trusted. If in doubt please contact the \nOSG Security Team\n and review the policies of your home institution.\n\n\nTroubleshooting\n\n\nUseful configuration and log files\n\n\nLogs and configuration:\n\n\n\n\n\n\n\n\nFile Description\n\n\nLocation\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nConfiguration File for osg-update-certs\n\n\n/etc/osg/osg-update-certs.conf\n\n\nThis file may be edited by hand, though it is recommended to use osg-ca-manage to set configuration parameters.\n\n\n\n\n\n\nLog file of osg-update-certs\n\n\n/var/log/osg-update-certs.log\n\n\n\n\n\n\n\n\nStdout of osg-update-certs\n\n\n/var/log/osg-ca-certs-status.system.out\n\n\n\n\n\n\n\n\nStdout of osg-ca-manage\n\n\n/var/log/osg-ca-manage.system.out\n\n\n\n\n\n\n\n\nStdout of initial CA setup\n\n\n/var/log/osg-setup-ca-certificates.system.out\n\n\n\n\n\n\n\n\n\n\nReferences\n\n\n\n\nInstalling the Certificate Authorities Certificates and the related RPMs", 
            "title": "Managing CAs"
        }, 
        {
            "location": "/security/osg-ca-manage/#managing-cas", 
            "text": "The osg-ca-manage tool provides a unified interface to manage the CA Certificate installations. This page provides the instructions on using this command. It provides status commands that allows you to list the CAs and the validity of the CAs and CRLs included in the installation. The manage commands allow you to fetch CAs and CRLs, change the distribution URL, as well as add and remove CAs from your local installation.", 
            "title": "Managing CAs"
        }, 
        {
            "location": "/security/osg-ca-manage/#syntax", 
            "text": "osg-ca-manage [global_options] command \n    global_options =\n        [--verbose]\n        [--force]\n        [--cert-dir  location ]\n        [--help | --usage]\n        [--version]\n        [--auto-refresh]\n\n    command = [manage_command | status_command]\n\n    status_command = [\n        showCAURL |\n        listCA [--pattern  pattern ] |\n        verify [--hash  CA hash   | --pattern  pattern ] |\n        diffCAPackage |\n        show [--certfile  cert_file  | --hash  CA hash ] |\n        showChain [--certfile  cert_file  | --hash  CA hash ]\n    ]\n\n    manage_command = [\n        setupCA --location  root|PATH  [--url  osg|igtf|URL  --no-update --force] |\n        refreshCA |\n        fetchCRL |\n        setCAURL [--url  osg|igtf|URL ] |\n        add [--cadir  localdir  | --caname  CA ]\n        remove [--cadir  localdir  | --caname  CA ]\n    ]", 
            "title": "Syntax"
        }, 
        {
            "location": "/security/osg-ca-manage/#explanation-of-global-options", 
            "text": "Zero or more of these options may be used during an execution of ca_manage.   --verbose  Provides you with more information depending on the command context.  --force  Forces the command to run ignoring any checks/warnings. The actual effect is context dependent, and this behavior is noted in the command details below.  --cert-dir  location  This location specifies the path CA directory. If this option is not specified then the command will look for  $X509_CERT_DIR , and  $VDT_LOCATION/globus/TRUSTED_CA  respectively. If none of these directories can be found, the command will exit with an error.  --auto-refresh  This option will indicate if this permissible to fetch CAs and CRLs as deemed necessary by this tool. For example at the end of an addCA/removeCA it would be advisable to refresh the CA list and the corresponding CRLs. Default is  not  to refresh, unless the admin requests it by specifying this option.  --version  Prints the version of the vdt-ca-certs-manager tool.  --help | --usage  Print usage information. Show a brief explanatory text for using osg-ca-manage.", 
            "title": "Explanation of global options"
        }, 
        {
            "location": "/security/osg-ca-manage/#explanation-of-commands", 
            "text": "Exactly one command is to be specified during an execution of osg-ca-manage", 
            "title": "Explanation of commands"
        }, 
        {
            "location": "/security/osg-ca-manage/#status-commands", 
            "text": "showCAURL  This will print out the distribution location specified in the config file. This command will read osg-update-certs.conf and output cacerts_url.  listCA [--pattern  pattern ]  This command will use openssl x509 command on the files in the --dir to provide hash, the subject and whether a CA is IGTF or TeraGrid accredited and distribution package which was used to download CAs into the directory. --verbose option will provide additional information like issuer (of CA) and all associated dates (CA cert issuance date, and CRL issuance date, and expiry dates). The command will look for CA files in the -certDir. The  pattern  specified in the option will be matched, using perl regex, against the subject field of the certificate (but we might also expand it include issuer if needed) and all CAs are listed if no pattern is given.  verify [--hash  CA_hash  | --pattern  pattern ]  The verify command will check all CAs (or if specified only the  CA_hash ) in the  certDir  directory, to see if any CA/CRL have expired or are about to do so. If any expired CA/CRL are found, an error is issued along with the hash, date when CA cert/CRL expired. A warning is issued if either the CA cert or CRL is about the expire within the next 24 Hrs. The --verbose option provides the CA Name, date the CA certs and CRL files are created (by the CA), and when they will expire. In addition to hash value we will also consider providing an option of verify using  pattern    diffCAPackage  This command will compare the hash of certificates included in the certificate directory against the latest VDT/OSG distribution (based on your cacerts_url) and outputs the difference.   show [--certfile  cert_file  | --hash  CA_hash ]  This command will essentially provide a condensed output of openssl x509 command. --verbose option will provide the full output. If --hash option is used we will look for the  CA_hash .o  file in the  certDir . The --certfile option can also take in a user proxy.   showChain [--certfile  cert_file  | --hash  CA_hash ]  This command will output the trust chain of the certificate.  certDir  will be used as the directory in which search for ancestor certs will be conducted. This command can also be used to trace the trust chain of a user proxy.", 
            "title": "Status commands"
        }, 
        {
            "location": "/security/osg-ca-manage/#manage-commands", 
            "text": "setupCA --location  root|PATH  [--url  osg|igtf|URL  --no-update --force]  This command is used for the inital setup of the CA package. The CA package can be setup to download CAs from any URL. Keywords are provided for various distributions. For the location to specify, keywords are provided to install into 'root' (/etc/grid-security). A --no-update option is available. Setting this flag instructs just setup the symlinks only and not to run configure osg-update-certs to be run automatically. This option is for installations that will not manage their own certificates, but will rely on updates through another method (such as RPM, or using osg-update-certs from a different OSG installation). A common use case for this is to have worker-node installations rely on the CA certificates being available on an NFS share, and the updating will happen on a single node.  refreshCA  This command run osg-update-certs to check for a new version of the CA distribution. If you already have the latest version, but wish to force an update anyways, use the --force option. 1. fetchCRL  It retrieves CRLs for all CAs within the directory. This will involve invoking fetch-crl, with appropriate arguments. NOTE: If vdt's fetch-crl service has not been enabled (i.e. no fetch-crl entry in crontab), then this command will not execute. This is a safety mechanism to prevent crls from being downloaded using this tool if they are not scheduled to be updated.  setCAURL [--url  osg|igtf|URL ]  This command sets the location from where the CA files. This command will modify vdt-update-certs.conf and set the cacerts_url as  URL_location . Only if --auto-refresh is specified both CA and CRLs are refreshed once the URL change has been made. The distribution  URL_location  will be required to conform to the VDT CA distribution format (e.g. similar to  http://vdt.cs.wisc.edu/software/certificates/ca-certs-version ). If the  URL_location  cannot be reached or if it is valid syntactically (i.e. does not conform to the format requirements) a warning will be issues and no changes will be made. The --force option can be used to force a change ignoring the warning. If URL location is left unspecified the  URL_location  will be set to OSG default. We define keywords for OSG, IGTF as shortcuts for OSG wide well-known CA URL_locations.  add [--cadir  localdir  | --caname  CA ]  The --hash argument is required. If --dir is not specified we will assume that the user wants to include a CA he has previously excluded and will remove the corresponding exclude lines from the config. If  CA_hash  is not known to us or it is already included we will provide appropriate error/warning information. In the common case this command will add include lines for  local_dir / CA_hash .*, into the vdt-update-certs.conf file. Lastly the command will invoke functions refresh the CAs and fetch CRLs. This command will also do some preliminary error checks, e.g. make sure that \u201c.0\u201d, \u201c.crl_url\u201d, \u201c.signing_policy\u201d files exist and that --dir is different than --certDir.  remove [--cadir  localdir  | --caname  CA ]  This command will be complementary to add and would either add an exclude or remove an include depending on the scenario. This command will also refresh CA and CRLs. vdt-update-certs do the job of removing cert files, we will still do the preliminary error checks to make sure that the certs that are being removed are included in the first place. For both addCA and removeCA, new CAs will be included/removed and CRLs will be refreshed only if --auto-refresh is set.", 
            "title": "Manage commands"
        }, 
        {
            "location": "/security/osg-ca-manage/#usage-examples", 
            "text": "Note  These commands will not work if of the osg-ca-certs (or igtf-ca-certs) RPM packages are installed.", 
            "title": "Usage Examples"
        }, 
        {
            "location": "/security/osg-ca-manage/#install-a-certificate-authority-package", 
            "text": "Before you proceed to install a Certificate Authority Package you should decide which of the available packages to install.   osg , the package recommended to be used by production resources on the OSG. It is based on the CA distribution from the IGTF, but it may differ slightly as decided by the  Security Team .   igtf , the package is a redistribution of the unchanged CA distribution from the IGTF  url  a package provided at a given URL    Note  If in doubt, please consult the policies of your home institution and get in contact with the  Security Team .   Next decide at what location to install the Certificate Authority Package:   on the  root  file system in a system directory  /etc/grid-security/certificates  in a  custom  directory that can also be shared", 
            "title": "Install a Certificate Authority Package"
        }, 
        {
            "location": "/security/osg-ca-manage/#setup-the-ca-certificates", 
            "text": "The Certificate Authority Package is preferably be used by grid users without root privileges  or  if the CA certificates will not be shared by other installations on the same host.  root@host #  osg-ca-manage setupca --location  root  --url osg Setting CA Certificates for at  /etc/grid-security/certificates  Setup completed successfully.   After a successful installation the certificates will be installed in ( /etc/grid-security/certificates  in this example). Typically to write into this default location you will need root privileges.  If you need to need to install it with out root privileges use  user@host $  osg-ca-manage setupca --location  $HOME /certificates  --url osg Setting CA Certificates for at  $HOME/certificates  Setup completed successfully.", 
            "title": "Setup the CA Certificates"
        }, 
        {
            "location": "/security/osg-ca-manage/#addingremoving-a-directory-of-local-cas", 
            "text": "", 
            "title": "Adding/Removing a directory of local CAs"
        }, 
        {
            "location": "/security/osg-ca-manage/#adding", 
            "text": "root@host # osg-ca-manage add --cadir /etc/grid-security/localca NOTE:      You did not specify the --auto-refresh flag.      So the changes made to the configuration will not be reflected till the next time      when CAs and CRLs are updated respectively by osg-update-certs and fetch-crl running from cron.      Run `osg-ca-manage refreshCA` and `osg-ca-manage fetchCRL` to commit your changes immediately.   Here is the resulting file after add  ##cat /etc/osg/osg-update-certs.conf\n# Configuration file for osg-update-certs\n\n# This file has been regenerated by osg-ca-manage, which removes most\n# comments.  You can still manually modify it, any manual change will\n# be preserved if osg-ca-manage is used again.\n\n## The parent location certificates will be installed at.\ninstall_dir = /etc/grid-security\n\n## cacerts_url is the URL of your certificate distribution\ncacerts_url = https://repo.grid.iu.edu/pacman/cadist/ca-certs-version-igtf-new\n\n## log specifies where logging output will go\nlog = /var/log/osg-update-certs.log\n\n## include specifies files (full pathnames) that should be copied\n## into the certificates installation after an update has occured.\ninclude=/etc/grid-security/localca/*\n\n## exclude_ca specifies a CA (not full pathnames, but just the hash\n## of the CA you want to exclude) that should be removed from the\n## certificates installation after an update has occured.\n\ndebug = 0", 
            "title": "Adding"
        }, 
        {
            "location": "/security/osg-ca-manage/#removing", 
            "text": "root@host # osg-ca-manage remove --cadir /etc/grid-security/localca NOTE:      You did not specify the --auto-refresh flag.      So the changes made to the configuration will not be reflected till the next time      when CAs and CRLs are updated respectively by osg-update-certs and fetch-crl running from cron.      Run `osg-ca-manage refreshCA` and `osg-ca-manage fetchCRL` to commit your changes immediately.", 
            "title": "Removing"
        }, 
        {
            "location": "/security/osg-ca-manage/#removingadding-a-particular-ca-included-in-osg-ca-package", 
            "text": "", 
            "title": "Removing/Adding a particular CA included in OSG CA package"
        }, 
        {
            "location": "/security/osg-ca-manage/#removing_1", 
            "text": "root@host # osg-ca-manage remove --caname ce33db76 Symlink detected for hash: We have determided that the hash value you entered belong to the CA  IRAN-GRID.pem . If you wish to add this CA back you will have to use this name is the parameter.  NOTE:      You did not specify the --auto-refresh flag.      So the changes made to the configuration will not be reflected till the next time      when CAs and CRLs are updated respectively by osg-update-certs and fetch-crl running from cron.      Run `osg-ca-manage refreshCA` and `osg-ca-manage fetchCRL` to commit your changes immediately.   The resulting config file after the remove is as follows  ##cat /etc/osg/osg-update-certs.conf\n# Configuration file for osg-update-certs\n\n# This file has been regenerated by osg-ca-manage, which removes most\n# comments.  You can still manually modify it, any manual change will\n# be preserved if osg-ca-manage is used again.\n\n## The parent location certificates will be installed at.\ninstall_dir = /etc/grid-security\n\n## cacerts_url is the URL of your certificate distribution\ncacerts_url = https://repo.grid.iu.edu/pacman/cadist/ca-certs-version-igtf-new\n\n## log specifies where logging output will go\nlog = /var/log/osg-update-certs.log\n\n## include specifies files (full pathnames) that should be copied\n## into the certificates installation after an update has occured.\n\n## exclude_ca specifies a CA (not full pathnames, but just the hash\n## of the CA you want to exclude) that should be removed from the\n## certificates installation after an update has occured.\nexclude_ca = IRAN-GRID\n\ndebug = 0", 
            "title": "Removing"
        }, 
        {
            "location": "/security/osg-ca-manage/#adding-back-the-removed-ca", 
            "text": "root@host # osg-ca-manage add --caname IRAN-GRID NOTE:      You did not specify the --auto-refresh flag.      So the changes made to the configuration will not be reflected till the next time      when CAs and CRLs are updated respectively by osg-update-certs and fetch-crl running from cron.      Run `osg-ca-manage refreshCA` and `osg-ca-manage fetchCRL` to commit your changes immediately.", 
            "title": "Adding back the removed CA"
        }, 
        {
            "location": "/security/osg-ca-manage/#inspect-installed-ca-certificates", 
            "text": "You can inspect the list of CA Certificates that have been installed:  user@host $  osg-ca-manage listCA Hash=09ff08b7; Subject= /C=FR/O=CNRS/CN=CNRS2-Projets; Issuer= /C=FR/O=CNRS/CN=CNRS2; Accreditation=Unknown; Status=https://repo.grid.iu.edu/pacman/cadist/ca-certs-version  Hash=0a12b607; Subject= /DC=org/DC=ugrid/CN=UGRID CA; Issuer= /DC=org/DC=ugrid/CN=UGRID CA; Accreditation=Unknown; Status=https://repo.grid.iu.edu/pacman/cadist/ca-certs-version  [...]   Any certificate issued by any of the Certificate Authorities listed will be trusted. If in doubt please contact the  OSG Security Team  and review the policies of your home institution.", 
            "title": "Inspect Installed CA Certificates"
        }, 
        {
            "location": "/security/osg-ca-manage/#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/security/osg-ca-manage/#useful-configuration-and-log-files", 
            "text": "Logs and configuration:     File Description  Location  Comment      Configuration File for osg-update-certs  /etc/osg/osg-update-certs.conf  This file may be edited by hand, though it is recommended to use osg-ca-manage to set configuration parameters.    Log file of osg-update-certs  /var/log/osg-update-certs.log     Stdout of osg-update-certs  /var/log/osg-ca-certs-status.system.out     Stdout of osg-ca-manage  /var/log/osg-ca-manage.system.out     Stdout of initial CA setup  /var/log/osg-setup-ca-certificates.system.out", 
            "title": "Useful configuration and log files"
        }, 
        {
            "location": "/security/osg-ca-manage/#references", 
            "text": "Installing the Certificate Authorities Certificates and the related RPMs", 
            "title": "References"
        }, 
        {
            "location": "/common/install-best-practices/", 
            "text": "Installation Best Practices\n\n\nAbout This Document\n\n\nThis document covers best practices to be used in installing software packages or security certificates on large numbers of hosts. It assumes you understand \nYUM/RPM Basics\n.\n\n\nInstall yum-priorities\n\n\nWe use \nyum\n priorities to ensure that we get the right packages from the OSG repositories. Please install it:\n\n\nroot@host #\n yum install yum-priorities\n\n\n\n\n\nThere is \ngood documentation\n in the wild about yum priorities, but we'll summarize the essential bits for you.\n\n\nThe basic idea of yum priorities is that if a package is found in two repositories, priorities will influence how yum chooses which package to install. We want the OSG software repository to be chosen instead of the EPEL repository so software, such as Globus, comes from our repository instead of the EPEL repository. (This is important because you will have installation errors if you get the EPEL Globus.)\n\n\nYum priorities is a \nyum plugin\n. Once enabled, you can set the priority of each repository (located in /etc/yum.repos.d) The lower the numerical priority, the better the priority. The default priority for a repository when it's not specified is 99. We set the priority for the OSG repository to be 98, as you can see here:\n\n\nroot@host # cat /etc/yum.repos.d/osg.repo\n[osg]\nname=OSG Software for Enterprise Linux 5 - $basearch\nmirrorlist=http://repo.grid.iu.edu/mirror/osg-release/$basearch\nfailovermethod=priority\npriority=\n98\n\nenabled=0\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\n\n\n\n\n\nYou can adjust these priorities if you have special needs at your site.\n\n\nAutomatic Updates\n\n\nWe strongly recommend against automatic updates for production services. You want to only change software versions during a controlled downtime (or, at least, while a human is watching); we strive to thoroughly test software updates, but cannot guarantee new version of software will not be problematic for your site.\n\n\nFor testbeds, automatic updates are suggested.\n\n\nTroubleshooting\n\n\nSometimes when installing, you will get an error like this:\n\n\nhttp://ftp1.scientificlinux.org/linux/scientific/54/x86_64/updates/security/repodata/filelists.sqlite.bz2: \n[\nErrno -1\n]\n Metadata file does not match checksum\n\n\n\n\n\n\nThis often indicates that you have out of date information, cached by \nyum\n. The following command will clear the out of date information, and you can try again:\n\n\nroot@host #\n yum --enablerepo\n=\n*\n clean all\n\n\n\n\n\nConsiderations for Large Sites\n\n\nWhile \nyum\n is a wonderful tool for installing software on a single server, it's a poor tool to install the same version of the software on many hosts. We \nstrongly\n recommend a cluster management tool; as far as we know, all cluster management tools provide a mechanism to create a local yum repository and have all your worker nodes use that.\n\n\nIf you have more than 20 worker nodes, we have the following advice:\n\n\n\n\nDo \nnot\n use one of the OSG repositories directly for worker node installations; build a local mirror instead. (See below).\n\n\nDistribute CRLs to the worker nodes using an HTTP proxy.\n\n\n\n\nBoth items are covered in this document.\n\n\nIn the future, we will be providing a mechanism for distributing CAs and CRLs via a shared file system; this is not quite finished. If you choose to do this, remember that your security infrastructure will only be as safe and reliable as the shared filesystem!\n\n\nRepository Mirrors\n\n\nA local yum mirror allows you to reduce the amount of external bandwidth used when updating or installing packages.\n\n\nAdd the following to a file in \n/etc/cron.d\n:\n\n\nRANDOM\n * * * * root rsync -aH rsync://repo.opensciencegrid.org/osg/ /var/www/html/osg/\n\n\n\n\n\n\nOr, to mirror only a single repository:\n\n\nRANDOM\n * * * * root rsync -aH rsync://repo.opensciencegrid.org/osg/\nOSG_RELEASE\n/el6/development /var/www/html/osg/\nOSG_RELEASE\n/el6\n\n\n\n\n\n\nReplace \nRANDOM\n with a number between 0 and 59.\n\n\nReplace \nOSG_RELEASE\n with the OSG release you want to use (e.g. '3.3', or '3.4').\n\n\nOn your worker node, you can replace the \nbaseurl\n line of \n/etc/yum.repos.d/osg.repo\n with the appropriate URL for your mirror.\n\n\nIf you are interested in having your mirror be part of the OSG's default set of mirrors, \nplease file a GOC ticket\n.\n\n\nCA Certificate Installation Considerations\n\n\nCAs are distributed in two ways:\n\n\n\n\nAs an RPM that contains the set of CAs. There are several such RPMs corresponding to different sets of CAs.\n\n\nThrough direct downloads from GOC with the \nosg-update-certs\n tool (provided by the \nosg-ca-scripts\n RPM).\n\n\n\n\nAs long as you use one of these two mechanisms, the OSG software will install successfully.\n\n\nCertificate Revocation List (CRL) Installation/Update\n\n\nCRLs are not distributed via \nyum\n. Instead, we provide the \nfetch-crl\n \ntool\n that downloads CRLs to the CA directory.", 
            "title": "Install Best Practices"
        }, 
        {
            "location": "/common/install-best-practices/#installation-best-practices", 
            "text": "", 
            "title": "Installation Best Practices"
        }, 
        {
            "location": "/common/install-best-practices/#about-this-document", 
            "text": "This document covers best practices to be used in installing software packages or security certificates on large numbers of hosts. It assumes you understand  YUM/RPM Basics .", 
            "title": "About This Document"
        }, 
        {
            "location": "/common/install-best-practices/#install-yum-priorities", 
            "text": "We use  yum  priorities to ensure that we get the right packages from the OSG repositories. Please install it:  root@host #  yum install yum-priorities  There is  good documentation  in the wild about yum priorities, but we'll summarize the essential bits for you.  The basic idea of yum priorities is that if a package is found in two repositories, priorities will influence how yum chooses which package to install. We want the OSG software repository to be chosen instead of the EPEL repository so software, such as Globus, comes from our repository instead of the EPEL repository. (This is important because you will have installation errors if you get the EPEL Globus.)  Yum priorities is a  yum plugin . Once enabled, you can set the priority of each repository (located in /etc/yum.repos.d) The lower the numerical priority, the better the priority. The default priority for a repository when it's not specified is 99. We set the priority for the OSG repository to be 98, as you can see here:  root@host # cat /etc/yum.repos.d/osg.repo\n[osg]\nname=OSG Software for Enterprise Linux 5 - $basearch\nmirrorlist=http://repo.grid.iu.edu/mirror/osg-release/$basearch\nfailovermethod=priority\npriority= 98 \nenabled=0\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG  You can adjust these priorities if you have special needs at your site.", 
            "title": "Install yum-priorities"
        }, 
        {
            "location": "/common/install-best-practices/#automatic-updates", 
            "text": "We strongly recommend against automatic updates for production services. You want to only change software versions during a controlled downtime (or, at least, while a human is watching); we strive to thoroughly test software updates, but cannot guarantee new version of software will not be problematic for your site.  For testbeds, automatic updates are suggested.", 
            "title": "Automatic Updates"
        }, 
        {
            "location": "/common/install-best-practices/#troubleshooting", 
            "text": "Sometimes when installing, you will get an error like this:  http://ftp1.scientificlinux.org/linux/scientific/54/x86_64/updates/security/repodata/filelists.sqlite.bz2:  [ Errno -1 ]  Metadata file does not match checksum   This often indicates that you have out of date information, cached by  yum . The following command will clear the out of date information, and you can try again:  root@host #  yum --enablerepo = *  clean all", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/common/install-best-practices/#considerations-for-large-sites", 
            "text": "While  yum  is a wonderful tool for installing software on a single server, it's a poor tool to install the same version of the software on many hosts. We  strongly  recommend a cluster management tool; as far as we know, all cluster management tools provide a mechanism to create a local yum repository and have all your worker nodes use that.  If you have more than 20 worker nodes, we have the following advice:   Do  not  use one of the OSG repositories directly for worker node installations; build a local mirror instead. (See below).  Distribute CRLs to the worker nodes using an HTTP proxy.   Both items are covered in this document.  In the future, we will be providing a mechanism for distributing CAs and CRLs via a shared file system; this is not quite finished. If you choose to do this, remember that your security infrastructure will only be as safe and reliable as the shared filesystem!", 
            "title": "Considerations for Large Sites"
        }, 
        {
            "location": "/common/install-best-practices/#repository-mirrors", 
            "text": "A local yum mirror allows you to reduce the amount of external bandwidth used when updating or installing packages.  Add the following to a file in  /etc/cron.d :  RANDOM  * * * * root rsync -aH rsync://repo.opensciencegrid.org/osg/ /var/www/html/osg/   Or, to mirror only a single repository:  RANDOM  * * * * root rsync -aH rsync://repo.opensciencegrid.org/osg/ OSG_RELEASE /el6/development /var/www/html/osg/ OSG_RELEASE /el6   Replace  RANDOM  with a number between 0 and 59.  Replace  OSG_RELEASE  with the OSG release you want to use (e.g. '3.3', or '3.4').  On your worker node, you can replace the  baseurl  line of  /etc/yum.repos.d/osg.repo  with the appropriate URL for your mirror.  If you are interested in having your mirror be part of the OSG's default set of mirrors,  please file a GOC ticket .", 
            "title": "Repository Mirrors"
        }, 
        {
            "location": "/common/install-best-practices/#ca-certificate-installation-considerations", 
            "text": "CAs are distributed in two ways:   As an RPM that contains the set of CAs. There are several such RPMs corresponding to different sets of CAs.  Through direct downloads from GOC with the  osg-update-certs  tool (provided by the  osg-ca-scripts  RPM).   As long as you use one of these two mechanisms, the OSG software will install successfully.", 
            "title": "CA Certificate Installation Considerations"
        }, 
        {
            "location": "/common/install-best-practices/#certificate-revocation-list-crl-installationupdate", 
            "text": "CRLs are not distributed via  yum . Instead, we provide the  fetch-crl   tool  that downloads CRLs to the CA directory.", 
            "title": "Certificate Revocation List (CRL) Installation/Update"
        }, 
        {
            "location": "/common/ca/", 
            "text": "Installing Certificate Authorities Certificates and related RPMs\n\n\nThis document provides you with details of various options to install the Certificate Authority (CA) certificates and have up-to-date certificate revocation list (CRL).\n\n\nWhen installing software with RPMs, you need to decide how you want to install the Certificate Authority (CA) certificates. You might ask \"why do I care? Can\u2019t you just give them to me?\" We can, but you have a few things to consider:\n\n\n\n\nWhat set of CA certificates do you want? How much control do you want over the set of CA certificates? (Some sites might not want to install specific CAs for policy or security reasons.)\n\n\nHow do you want to update them?\n\n\nDo you want to centrally manage the CA certificates or install them on each computer at your site?\n\n\n\n\nYou have four options for installing CA certificates:\n\n\n\n\nInstall an RPM for a specific set of CA certificates.\n\n\nInstall \nosg-update-certs\n, a program that lets you install/update a predefined set of CA certificates, then adjust the set by adding or deleting specific CAs.\n\n\nInstall an RPM that installs \nno\n CAs. This is useful when you want your RPM installations to succeed (because our RPMs require CA certificates, and this RPM satisfies that dependency) but you want to manage them with your own technique.\n\n\nMake no choice, let \nyum\n decide for you.\n\n\n\n\nAdditionally this page also provides instruction on installation of a tool (fetch-crl) to ensure your site has up-to-date certificate revocation list (CRL) from the CA.\n\n\nPrior to following the instructions on this page, you must enable our \nyum repositories\n\n\nInstall CA certificates: Options\n\n\nPlease choose one of the four options to install the CA certificates.\n\n\nOption 1: Install an RPM for a specific set of CA certificates\n\n\nIf you want to install an RPM for one of our predefined CA certificates, you have two choices to make:\n\n\nWhich set of CAs?\n\n\n\n\n(\nrecommended\n) The OSG CA certificates. This is similar to the IGTF set, but may have a small number of additions or deletions. (See \nhere\n for details)\n\n\nThe default \nIGTF\n CA certificates.\n\n\n\n\nDepending on your choice, you select one of two RPMs:\n\n\n\n\n\n\n\n\nSet of CAs\n\n\nFormat\n --\n\n\nRPM name\n\n\nInstallation command (as root)\n\n\n\n\n\n\n\n\n\n\nOSG\n\n\nOpenSSL-both\n\n\nosg-ca-certs\n\n\nyum install osg-ca-certs\n\n\n\n\n\n\nIGTF\n\n\nOpenSSL-both\n\n\nigtf-ca-certs\n\n\nyum install igtf-ca-certs\n\n\n\n\n\n\n\n\nHow do I keep CAs updated?\n\n\nPlease follow the \nupdate instructions\n to make sure that the CAs are kept updated.\n\n\nOption 2: Install osg-update-certs\n\n\nInstall this with:\n\n\nroot@host #\n yum install osg-ca-scripts\n\n\n\n\n\nYou have the same choices for CA certificates as above. In order to choose, you will run \nosg-ca-manage\n, which will install the CA certificates. Then (if desired) you need to enable periodic updating of the CA certificates.\n\n\n\n\n\n\n\n\nSet of CAs\n\n\nFormat\n\n\nCA certs name\n\n\nInstallation command (as root)\n\n\n\n\n\n\n\n\n\n\nOSG\n\n\nOpenSSL-both\n\n\nosg\n\n\n/usr/sbin/osg-ca-manage setupCA --location root --url osg\n\n\n\n\n\n\nIGTF\n\n\nOpenSSL-both\n\n\nigtf\n\n\n/usr/sbin/osg-ca-manage setupCA --location root --url igtf\n\n\n\n\n\n\n\n\nHere is an example:\n\n\nroot@host #\n /usr/sbin/osg-ca-manage setupCA --location root --url osg\n\nSetting up CA Certificates for OSG installation\n\n\nCA Certificates will be installed into /etc/grid-security/certificates\n\n\nosg-update-certs\n\n\n  Log file: /var/log/osg-update-certs.log\n\n\n  Updates from: https://repo.opensciencegrid.org/pacman/cadist/ca-certs-version-new\n\n\n\nWill update CA certificates from version unknown to version 1.21NEW.\n\n\nUpdate successful.\n\n\n\nSetup completed successfully.\n\n\n\n\n\n\nInitially the CA certificates will not be updated. You can tell by looking at:\n\n\nroot@host #\n /sbin/service osg-update-certs-cron  status\n\nPeriodic osg-update-certs is disabled.\n\n\n\n\n\n\nYou can enable the \ncron\n job that updates the CA certs with:\n\n\nroot@host #\n /sbin/service osg-update-certs-cron  start\n\nEnabling periodic osg-update-certs:                        [  \nOK\n  ]\n\n\n\n\n\n\nA complete set of options available though \nosg-ca-manage\n command, including your interface to adding and removing CAs, could be found at \nosg-ca-manage documentation\n\n\nOption 3: Install an RPM that installs no CAs\n\n\nInstall this with:\n\n\nyum install empty-ca-certs \u2013-enablerepo=osg-empty\n\n\n\n\n\n\n\n\nWarning\n\n\nIf you choose this option, you are responsible for installing the CA certificates yourself. You must install them in \n/etc/grid-security/certificates\n, or make a symlink from that location to the directory that contains the CA certificates.\n\n\n\n\nOption 4: Make no choice, let yum decide for you\n\n\nIf you use \nyum\n to install software that requires CA certificates but you haven\u2019t made one of these choices, yum will choose a default. Right now, it is Option #1 from above (\nInstall an RPM for a specific set of CA certificates\n), and the osg-ca-certs RPM is chosen.\n\n\nInstall other CAs\n\n\nIn addition to the above CAs, you can install other CAs via RPM. These only work with the RPMs that provide CAs (that is, \nosg-ca-certs\n and the like, but not \nosg-ca-scripts\n.) They are in addition to the above RPMs, so do not only install these extra CAs.\n\n\n\n\n\n\n\n\nSet of CAs\n\n\nFormat\n\n\nRPM name\n\n\nInstallation command (as root)\n\n\n\n\n\n\n\n\n\n\ncilogon-basic \n cilogon-openid\n\n\nOpenSSL-both\n\n\ncilogon-ca-certs\n\n\nyum install cilogon-ca-certs\n\n\n\n\n\n\n\n\nManaging Certificate Revocation Lists\n\n\nIn addition to CA certificates, you normally need to have updated Certificate Revocation Lists (CRLs) which are are lists of certificates that have been revoked for any reason. Software in the OSG Software Stack uses these to ensure that you are talking to valid clients or servers.\n\n\nWe use a tool named \nfetch-crl\n that periodically updates the CRLs. Fetch CRL is a utility that updates Certificate Authority (CA) Certificate Revocation Lists (CRLs). These are lists of certificates that were granted by the CA, but have since been revoked. It is good practice to regularly update the CRL list for each CA to ensure that you do not authenticate any certificate that has been revoked.\n\n\nfetch-crl\n is installed as two different system services. The fetch-crl-boot service runs only\nat boot time. The \nfetch-crl-cron\n service runs \nfetch-crl\n every 6 hours (with a random sleep\ntime included) by default. Both services are disabled by default. At the very minimum, the\n\nfetch-crl-cron\n service needs to be enabled otherwise services will begin to fail as the\nexisting CRLs expire.\n\n\nInstall \nfetch-crl\n\n\nNormally \nfetch-crl\n is installed when you install the rest of the software and you do not need\nto specifically install it. If you do wish to install it, you can install it as:\n\n\nroot@host #\n yum install fetch-crl\n\n\n\n\n\nEnable and Start \nfetch-crl\n\n\nTo enable fetch-crl (fetch Certificate Revocation Lists) services by default on the node:\n\n\nroot@host #\n /sbin/chkconfig fetch-crl-boot on\n\nroot@host #\n /sbin/chkconfig fetch-crl-cron on\n\n\n\n\n\nTo start fetch-crl:\n\n\nroot@host #\n /sbin/service fetch-crl-boot start\n\nroot@host #\n /sbin/service fetch-crl-cron start\n\n\n\n\n\n\n\nNote\n\n\nWhile it is necessary to start \nfetch-crl-cron\n in order to have it active, \nfetch-crl-boot\n is started automatically at boot time if enabled. The start command will run \nfetch-crl-boot\n at the moment when it is invoked and it may take some time to complete.\n\n\n\n\nConfigure \nfetch-crl\n\n\nTo modify the times that fetch-crl-cron runs, edit \n/etc/cron.d/fetch-crl\n.\n\n\nBy default, \nfetch-crl\n connects directly to the remote CA; this is\ninefficient and potentially harmful if done simultaneously by many nodes\n(e.g. all the worker nodes of a big cluster). We recommend you provide a\nHTTP proxy (such as \nsquid\n) the worker nodes can utilize; OSG provides\n\npackaging of squid\n.\n\n\nTo configure fetch-crl to use an HTTP proxy server:\n\n\nCreate or edit the file \n/etc/fetch-crl.conf\n and add the following line:\n\n\nhttp_proxy\n=\nhttp://your.squid.fqdn:port\n\n\n\n\n\n\nAgain, adjust the URL appropriately for your proxy server.\n\n\nNote that the \nnosymlinks\n option in the configuration files refers\nto ignoring links within the certificates directory (e.g. two different\nnames for the same file). It is perfectly fine if the path of the CA\ncertificates directory itself (\ninfodir\n) is a link to a directory.\n\n\nAny modifications to the configuration file will be preserved during an RPM update.\n\n\nCurrent versions of \nfetch-crl\n produce more output.\nIt is possible to send the output to syslog instead of the default email system. To do so:\n\n\n\n\n\n\nChange the configuration file to enable syslog:\n\n\nlogmode = syslog\nsyslogfacility = daemon\n\n\n\n\n\n\n\n\n\nMake sure the file \n/var/log/daemon\n exists, e.g. touching the file\n\n\n\n\nChange \n/etc/logrotate.d\n files to rotate it\n\n\n\n\nStart/Stop fetch-crl: A quick guide\n\n\nYou need to fetch the latest CA Certificate Revocation Lists (CRLs) and you should enable the fetch-crl service to keep the CRLs up to date:\n\n\nroot@host #\n /usr/sbin/fetch-crl \n# This fetches the CRLs\n\n\nroot@host #\n /sbin/service fetch-crl-boot start\n\nroot@host #\n /sbin/service fetch-crl-cron start\n\n\n\n\n\nTo enable the \nfetch-crl\n service to keep the CRLs up to date after reboots:\n\n\nroot@host #\n /sbin/chkconfig fetch-crl-boot on\n\nroot@host #\n /sbin/chkconfig fetch-crl-cron on\n\n\n\n\n\nTo stop \nfetch-crl\n:\n\n\nroot@host #\n /sbin/service fetch-crl-boot stop\n\nroot@host #\n /sbin/service fetch-crl-cron stop\n\n\n\n\n\nTo disable the fetch-crl service:\n\n\nroot@host #\n /sbin/chkconfig fetch-crl-boot off\n\nroot@host #\n /sbin/chkconfig fetch-crl-cron off\n\n\n\n\n\nUpdating CAs/CRLs\n\n\nWhy maintain up-to-date Trusted CA /CRL information\n\n\nThe Trusted Certificate Authority (CA) certificates, and their associated Certificate Revocation Lists (CRLs), are used for every transaction on a resource that establishes an authenticated network connection based on end user\u2019s certificate. In order for the authentication to succeed, the user\u2019s certificate must have been issued by one of the CAs in the Trusted CA directory, and the user\u2019s certificate must not be listed in the CRL for that CA.\n\n\nCRLs can be thought of as a black list of certificates. CAs are the trust authorities, similar to DMV that issues you the driving license. (Another way of thinking CRLs is the do-not-fly lists at the airports. if your certificate shows up in CRLs, you are not allowed access.)\n\n\nThis is handled at the certificate validation stage even before the authorization check (which will provide the mapping of an authenticated user to a local account UID/GID). So you do not need to do worry about it; the grid software will do this for you.\n\n\nHowever, you should make sure that your site has the most up-to-date list of Trusted CAs. There are multiple trust authorities in OSG (think of it as a different DMV for each state). If you do not have an up-to-date list of CAs it is possible that some of your users transactions at your site will start to fail. A current CRL list for each CA is also necessary, since without one transactions for users of that CA will fail.\n\n\nHow to ensure you are get up-to-date CA/CRL information\n\n\n\n\nIf you installed CAs using rpm packages (\nosg-ca-certs\n,\nigtf-ca-certs\n) (Options 1, 4), you will need to install the software described in \nthe CA update document\n, and enable \nosg-ca-certs-updater\n service to keep the CAs automatically updated. If you do not install the updater, you will have to regularly run yum update to keep the CAs updated.\n\n\n\n\nIf you use Option 2 (i.e. \nosg-update-certs\n) then make sure that you have the corresponding service enabled.\n\n\nroot@host #\n /sbin/service osg-update-certs-cron  status\n\nPeriodic osg-update-certs is enabled.\n\n\n\n\n\n\n\n\n\n\nEnsure that fetch-crl cron is enabled\n\n\nroot@host #\n /sbin/service fetch-crl-cron  status\n\nPeriodic fetch-crl is enabled.\n\n\n\n\n\n\n\n\n\n\nTroubleshooting\n\n\nUseful configuration and log files\n\n\nConfiguration files:\n\n\n\n\n\n\n\n\nPackage\n\n\nFile Description\n\n\nLocation\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nAll CA Packages\n\n\nCA File Location\n\n\n/etc/grid-security/certificates\n\n\n\n\n\n\n\n\nAll CA Packages\n\n\nIndex files\n\n\n/etc/grid-security/certificates/INDEX.html\n or \n/etc/grid-security/certificates/INDEX.txt\n\n\nLatest version also available at \nhttp://repo.opensciencegrid.org/pacman/cadist/\n\n\n\n\n\n\nAll CA Packages\n\n\nChange Log\n\n\n/etc/grid-security/certificates/CHANGES\n\n\nLatest version also available at \nhttp://repo.opensciencegrid.org/pacman/cadist/CHANGES\n\n\n\n\n\n\nosg-ca-certs or igtf-ca-certs\n\n\ncontain only CA files\n\n\n\n\n\n\n\n\n\n\nosg-ca-scripts\n\n\nConfiguration File for osg-update-certs\n\n\n/etc/osg/osg-update-certs.conf\n\n\nThis file may be edited by hand, though it is recommended to use osg-ca-manage to set configuration parameters.\n\n\n\n\n\n\nfetch-crl-3.x\n\n\nConfiguration file\n\n\n/etc/fetch-crl.conf\n\n\n\n\n\n\n\n\n\n\nThe index and change log files contain a summary of all the CA distributed and their version.\n\n\nLogs files:\n\n\n\n\n\n\n\n\nPackage\n\n\nFile Description\n\n\nLocation\n\n\n\n\n\n\n\n\n\n\nosg-ca-scripts\n\n\nLog file of osg-update-certs\n\n\n/var/log/osg-update-certs.log\n\n\n\n\n\n\nosg-ca-scripts\n\n\nStdout of osg-update-certs\n\n\n/var/log/osg-ca-certs-status.system.out\n\n\n\n\n\n\nosg-ca-scripts\n\n\nStdout of osg-ca-manage\n\n\n/var/log/osg-ca-manage.system.out\n\n\n\n\n\n\nosg-ca-scripts\n\n\nStdout of initial CA setup\n\n\n/var/log/osg-setup-ca-certificates.system.out\n\n\n\n\n\n\n\n\nTests\n\n\nTo test the host certificate of a server \nopenssl s_client\n can be used. Here is an example with the gatekeeper:\n\n\nuser@host $\n openssl s_client -showcerts \n\\\n\n    -cert /etc/grid-security/hostcert.pem \n\\\n\n    -key /etc/grid-security/hostkey.pem \n\\\n\n    -CApath /etc/grid-security/certificates/ \n\\\n\n    -debug -connect osg-gk.mwt2.org:2119\n\n\n\n\n\nFrequently Asked Questions\n\n\nLocation of Certificates?\n\n\n /etc/grid-security/certificates\n\n\n\n\n\nWhat is the version of OSG CA package I have installed and what are its contents?\n\n\nThe version of the CA package ca be found at \n/etc/grid-security/certificates/INDEX.html\n or \n/etc/grid-security/certificates/INDEX.txt\n. The changes file can be found at \n/etc/grid-security/certificates/CHANGES\n.\n\n\nContents of OSG CA package?\n\n\nThe OSG CA Distribution contains:\n\n\n\n\nIGTF Distribution of Authority Root Certificates\n (CAs accredited by the \nInternational Grid Trust Federation\n)\n\n\n\n\nDetails of CAs in OSG distribution can be found \nhere\n. For additional details what is in the current release, see the \ndistribution site\n and \nchange log\n.\n\n\nHow can I add or remove a particular CA file?\n\n\nAdd and remove of CA files are supported only if you CA files are being installed using \nosg-update-certs\n, which is included in the \nosg-ca-scripts\n package (option 2), for all other options no support for adding and removing a particular CA file is provided by OSG. The preferred approach to add or remove a CA is to use \nosg-ca-manage\n. For adding a new CA \nosg-ca-manage add [--dir \nlocal_dir\n] --hash \nCA_hash\n may be used, while a CA is removed using \nosg-ca-manage remove --hash \nCA_hash\n.\n\n\nAre there any log files or configuration files associated with CA certificate package?\n\n\nIf CA files are installed using \nosg-ca-certs\n or \nigtf-ca-certs\n rpms (i.e. options 1, 4) no log or configuration files are present.\n\n\nLog and configuration files are however present for \nosg-ca-scripts\n rpm package (option 2).\n\n\nConfig files: \n/etc/osg/osg-update-certs.conf\n Log files: \n/var/log/osg-update-certs.log\n, \n/var/log/osg-ca-certs-status.system.out\n, \n/var/log/osg-ca-manage.system.out\n, \n/var/log/osg-setup-ca-certificates.system.out\n\n\nAre CA packages automatically updated?\n\n\nIf CA files are installed using \nosg-ca-certs\n or \nigtf-ca-certs\n rpms (i.e. options 1, 4), you will need to install the software described in \nOSG CA certs updater\n, and enable osg-ca-certs-updater service to keep the CAs automatically updated.\n\n\nIf CA files are being installed using \nosg-ca-scripts\n rpm package (option 2), CA files are kept up-to-date as long as \nosg-update-certs-cron\n service the package provides has been started.\n\n\nHow do I manually update my CA package?\n\n\nFor Option 1: run one of the following \nyum update osg-ca-certs\n or \nyum update igtf-ca-certs\n depending on the rpm package you installed.\n\n\nFor Option 4: run \nyum update osg-ca-certs\n\n\nFor Option 2: You do not need to do a manual update, make sure \nosg-update-certs-cron\n is enabled using\n\n\nroot@host #\n /sbin/service osg-update-certs-cron  status\n\n\n\n\n\nIf the service is disabled, enable it using\n\n\nroot@host #\n /sbin/service osg-update-certs-cron  start\n\n\n\n\n\nIf for some extraordinary reason you need to manually update the CA you could run \nosg-ca-manage [--force] refreshCA\n.\n\n\nWhere are the configuration files for fetch-crl?\n\n\n/etc/fetch-crl.conf\n\n\nReferences\n\n\nSome guides on x509 certificates:\n\n\n\n\nUseful commands: \nhttp://security.ncsa.illinois.edu/research/grid-howtos/usefulopenssl.html\n\n\nInstall GSI authentication on a server: \nhttp://security.ncsa.illinois.edu/research/wssec/gsihttps/\n\n\nCertificates how-to: \nhttp://www.nordugrid.org/documents/certificate_howto.html\n\n\n\n\nSome examples about verifying the certificates:\n\n\n\n\nhttp://gagravarr.org/writing/openssl-certs/others.shtml\n\n\nhttp://www.cyberciti.biz/faq/test-ssl-certificates-diagnosis-ssl-certificate/\n\n\nhttp://www.cyberciti.biz/tips/debugging-ssl-communications-from-unix-shell-prompt.html\n\n\n\n\nRelated software:\n\n\n\n\nDescription, manual and examples of \nosg-ca-manage\n\n\nosg-ca-certs-updater", 
            "title": "CA Certificates"
        }, 
        {
            "location": "/common/ca/#installing-certificate-authorities-certificates-and-related-rpms", 
            "text": "This document provides you with details of various options to install the Certificate Authority (CA) certificates and have up-to-date certificate revocation list (CRL).  When installing software with RPMs, you need to decide how you want to install the Certificate Authority (CA) certificates. You might ask \"why do I care? Can\u2019t you just give them to me?\" We can, but you have a few things to consider:   What set of CA certificates do you want? How much control do you want over the set of CA certificates? (Some sites might not want to install specific CAs for policy or security reasons.)  How do you want to update them?  Do you want to centrally manage the CA certificates or install them on each computer at your site?   You have four options for installing CA certificates:   Install an RPM for a specific set of CA certificates.  Install  osg-update-certs , a program that lets you install/update a predefined set of CA certificates, then adjust the set by adding or deleting specific CAs.  Install an RPM that installs  no  CAs. This is useful when you want your RPM installations to succeed (because our RPMs require CA certificates, and this RPM satisfies that dependency) but you want to manage them with your own technique.  Make no choice, let  yum  decide for you.   Additionally this page also provides instruction on installation of a tool (fetch-crl) to ensure your site has up-to-date certificate revocation list (CRL) from the CA.  Prior to following the instructions on this page, you must enable our  yum repositories", 
            "title": "Installing Certificate Authorities Certificates and related RPMs"
        }, 
        {
            "location": "/common/ca/#install-ca-certificates-options", 
            "text": "Please choose one of the four options to install the CA certificates.", 
            "title": "Install CA certificates: Options"
        }, 
        {
            "location": "/common/ca/#option-1-install-an-rpm-for-a-specific-set-of-ca-certificates", 
            "text": "If you want to install an RPM for one of our predefined CA certificates, you have two choices to make:", 
            "title": "Option 1: Install an RPM for a specific set of CA certificates"
        }, 
        {
            "location": "/common/ca/#which-set-of-cas", 
            "text": "( recommended ) The OSG CA certificates. This is similar to the IGTF set, but may have a small number of additions or deletions. (See  here  for details)  The default  IGTF  CA certificates.   Depending on your choice, you select one of two RPMs:     Set of CAs  Format  --  RPM name  Installation command (as root)      OSG  OpenSSL-both  osg-ca-certs  yum install osg-ca-certs    IGTF  OpenSSL-both  igtf-ca-certs  yum install igtf-ca-certs", 
            "title": "Which set of CAs?"
        }, 
        {
            "location": "/common/ca/#how-do-i-keep-cas-updated", 
            "text": "Please follow the  update instructions  to make sure that the CAs are kept updated.", 
            "title": "How do I keep CAs updated?"
        }, 
        {
            "location": "/common/ca/#option-2-install-osg-update-certs", 
            "text": "Install this with:  root@host #  yum install osg-ca-scripts  You have the same choices for CA certificates as above. In order to choose, you will run  osg-ca-manage , which will install the CA certificates. Then (if desired) you need to enable periodic updating of the CA certificates.     Set of CAs  Format  CA certs name  Installation command (as root)      OSG  OpenSSL-both  osg  /usr/sbin/osg-ca-manage setupCA --location root --url osg    IGTF  OpenSSL-both  igtf  /usr/sbin/osg-ca-manage setupCA --location root --url igtf     Here is an example:  root@host #  /usr/sbin/osg-ca-manage setupCA --location root --url osg Setting up CA Certificates for OSG installation  CA Certificates will be installed into /etc/grid-security/certificates  osg-update-certs    Log file: /var/log/osg-update-certs.log    Updates from: https://repo.opensciencegrid.org/pacman/cadist/ca-certs-version-new  Will update CA certificates from version unknown to version 1.21NEW.  Update successful.  Setup completed successfully.   Initially the CA certificates will not be updated. You can tell by looking at:  root@host #  /sbin/service osg-update-certs-cron  status Periodic osg-update-certs is disabled.   You can enable the  cron  job that updates the CA certs with:  root@host #  /sbin/service osg-update-certs-cron  start Enabling periodic osg-update-certs:                        [   OK   ]   A complete set of options available though  osg-ca-manage  command, including your interface to adding and removing CAs, could be found at  osg-ca-manage documentation", 
            "title": "Option 2: Install osg-update-certs"
        }, 
        {
            "location": "/common/ca/#option-3-install-an-rpm-that-installs-no-cas", 
            "text": "Install this with:  yum install empty-ca-certs \u2013-enablerepo=osg-empty    Warning  If you choose this option, you are responsible for installing the CA certificates yourself. You must install them in  /etc/grid-security/certificates , or make a symlink from that location to the directory that contains the CA certificates.", 
            "title": "Option 3: Install an RPM that installs no CAs"
        }, 
        {
            "location": "/common/ca/#option-4-make-no-choice-let-yum-decide-for-you", 
            "text": "If you use  yum  to install software that requires CA certificates but you haven\u2019t made one of these choices, yum will choose a default. Right now, it is Option #1 from above ( Install an RPM for a specific set of CA certificates ), and the osg-ca-certs RPM is chosen.", 
            "title": "Option 4: Make no choice, let yum decide for you"
        }, 
        {
            "location": "/common/ca/#install-other-cas", 
            "text": "In addition to the above CAs, you can install other CAs via RPM. These only work with the RPMs that provide CAs (that is,  osg-ca-certs  and the like, but not  osg-ca-scripts .) They are in addition to the above RPMs, so do not only install these extra CAs.     Set of CAs  Format  RPM name  Installation command (as root)      cilogon-basic   cilogon-openid  OpenSSL-both  cilogon-ca-certs  yum install cilogon-ca-certs", 
            "title": "Install other CAs"
        }, 
        {
            "location": "/common/ca/#managing-certificate-revocation-lists", 
            "text": "In addition to CA certificates, you normally need to have updated Certificate Revocation Lists (CRLs) which are are lists of certificates that have been revoked for any reason. Software in the OSG Software Stack uses these to ensure that you are talking to valid clients or servers.  We use a tool named  fetch-crl  that periodically updates the CRLs. Fetch CRL is a utility that updates Certificate Authority (CA) Certificate Revocation Lists (CRLs). These are lists of certificates that were granted by the CA, but have since been revoked. It is good practice to regularly update the CRL list for each CA to ensure that you do not authenticate any certificate that has been revoked.  fetch-crl  is installed as two different system services. The fetch-crl-boot service runs only\nat boot time. The  fetch-crl-cron  service runs  fetch-crl  every 6 hours (with a random sleep\ntime included) by default. Both services are disabled by default. At the very minimum, the fetch-crl-cron  service needs to be enabled otherwise services will begin to fail as the\nexisting CRLs expire.", 
            "title": "Managing Certificate Revocation Lists"
        }, 
        {
            "location": "/common/ca/#install-fetch-crl", 
            "text": "Normally  fetch-crl  is installed when you install the rest of the software and you do not need\nto specifically install it. If you do wish to install it, you can install it as:  root@host #  yum install fetch-crl", 
            "title": "Install fetch-crl"
        }, 
        {
            "location": "/common/ca/#enable-and-start-fetch-crl", 
            "text": "To enable fetch-crl (fetch Certificate Revocation Lists) services by default on the node:  root@host #  /sbin/chkconfig fetch-crl-boot on root@host #  /sbin/chkconfig fetch-crl-cron on  To start fetch-crl:  root@host #  /sbin/service fetch-crl-boot start root@host #  /sbin/service fetch-crl-cron start   Note  While it is necessary to start  fetch-crl-cron  in order to have it active,  fetch-crl-boot  is started automatically at boot time if enabled. The start command will run  fetch-crl-boot  at the moment when it is invoked and it may take some time to complete.", 
            "title": "Enable and Start fetch-crl"
        }, 
        {
            "location": "/common/ca/#configure-fetch-crl", 
            "text": "To modify the times that fetch-crl-cron runs, edit  /etc/cron.d/fetch-crl .  By default,  fetch-crl  connects directly to the remote CA; this is\ninefficient and potentially harmful if done simultaneously by many nodes\n(e.g. all the worker nodes of a big cluster). We recommend you provide a\nHTTP proxy (such as  squid ) the worker nodes can utilize; OSG provides packaging of squid .  To configure fetch-crl to use an HTTP proxy server:  Create or edit the file  /etc/fetch-crl.conf  and add the following line:  http_proxy = http://your.squid.fqdn:port   Again, adjust the URL appropriately for your proxy server.  Note that the  nosymlinks  option in the configuration files refers\nto ignoring links within the certificates directory (e.g. two different\nnames for the same file). It is perfectly fine if the path of the CA\ncertificates directory itself ( infodir ) is a link to a directory.  Any modifications to the configuration file will be preserved during an RPM update.  Current versions of  fetch-crl  produce more output.\nIt is possible to send the output to syslog instead of the default email system. To do so:    Change the configuration file to enable syslog:  logmode = syslog\nsyslogfacility = daemon    Make sure the file  /var/log/daemon  exists, e.g. touching the file   Change  /etc/logrotate.d  files to rotate it", 
            "title": "Configure fetch-crl"
        }, 
        {
            "location": "/common/ca/#startstop-fetch-crl-a-quick-guide", 
            "text": "You need to fetch the latest CA Certificate Revocation Lists (CRLs) and you should enable the fetch-crl service to keep the CRLs up to date:  root@host #  /usr/sbin/fetch-crl  # This fetches the CRLs  root@host #  /sbin/service fetch-crl-boot start root@host #  /sbin/service fetch-crl-cron start  To enable the  fetch-crl  service to keep the CRLs up to date after reboots:  root@host #  /sbin/chkconfig fetch-crl-boot on root@host #  /sbin/chkconfig fetch-crl-cron on  To stop  fetch-crl :  root@host #  /sbin/service fetch-crl-boot stop root@host #  /sbin/service fetch-crl-cron stop  To disable the fetch-crl service:  root@host #  /sbin/chkconfig fetch-crl-boot off root@host #  /sbin/chkconfig fetch-crl-cron off", 
            "title": "Start/Stop fetch-crl: A quick guide"
        }, 
        {
            "location": "/common/ca/#updating-cascrls", 
            "text": "", 
            "title": "Updating CAs/CRLs"
        }, 
        {
            "location": "/common/ca/#why-maintain-up-to-date-trusted-ca-crl-information", 
            "text": "The Trusted Certificate Authority (CA) certificates, and their associated Certificate Revocation Lists (CRLs), are used for every transaction on a resource that establishes an authenticated network connection based on end user\u2019s certificate. In order for the authentication to succeed, the user\u2019s certificate must have been issued by one of the CAs in the Trusted CA directory, and the user\u2019s certificate must not be listed in the CRL for that CA.  CRLs can be thought of as a black list of certificates. CAs are the trust authorities, similar to DMV that issues you the driving license. (Another way of thinking CRLs is the do-not-fly lists at the airports. if your certificate shows up in CRLs, you are not allowed access.)  This is handled at the certificate validation stage even before the authorization check (which will provide the mapping of an authenticated user to a local account UID/GID). So you do not need to do worry about it; the grid software will do this for you.  However, you should make sure that your site has the most up-to-date list of Trusted CAs. There are multiple trust authorities in OSG (think of it as a different DMV for each state). If you do not have an up-to-date list of CAs it is possible that some of your users transactions at your site will start to fail. A current CRL list for each CA is also necessary, since without one transactions for users of that CA will fail.", 
            "title": "Why maintain up-to-date Trusted CA /CRL information"
        }, 
        {
            "location": "/common/ca/#how-to-ensure-you-are-get-up-to-date-cacrl-information", 
            "text": "If you installed CAs using rpm packages ( osg-ca-certs , igtf-ca-certs ) (Options 1, 4), you will need to install the software described in  the CA update document , and enable  osg-ca-certs-updater  service to keep the CAs automatically updated. If you do not install the updater, you will have to regularly run yum update to keep the CAs updated.   If you use Option 2 (i.e.  osg-update-certs ) then make sure that you have the corresponding service enabled.  root@host #  /sbin/service osg-update-certs-cron  status Periodic osg-update-certs is enabled.     Ensure that fetch-crl cron is enabled  root@host #  /sbin/service fetch-crl-cron  status Periodic fetch-crl is enabled.", 
            "title": "How to ensure you are get up-to-date CA/CRL information"
        }, 
        {
            "location": "/common/ca/#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/common/ca/#useful-configuration-and-log-files", 
            "text": "Configuration files:     Package  File Description  Location  Comment      All CA Packages  CA File Location  /etc/grid-security/certificates     All CA Packages  Index files  /etc/grid-security/certificates/INDEX.html  or  /etc/grid-security/certificates/INDEX.txt  Latest version also available at  http://repo.opensciencegrid.org/pacman/cadist/    All CA Packages  Change Log  /etc/grid-security/certificates/CHANGES  Latest version also available at  http://repo.opensciencegrid.org/pacman/cadist/CHANGES    osg-ca-certs or igtf-ca-certs  contain only CA files      osg-ca-scripts  Configuration File for osg-update-certs  /etc/osg/osg-update-certs.conf  This file may be edited by hand, though it is recommended to use osg-ca-manage to set configuration parameters.    fetch-crl-3.x  Configuration file  /etc/fetch-crl.conf      The index and change log files contain a summary of all the CA distributed and their version.  Logs files:     Package  File Description  Location      osg-ca-scripts  Log file of osg-update-certs  /var/log/osg-update-certs.log    osg-ca-scripts  Stdout of osg-update-certs  /var/log/osg-ca-certs-status.system.out    osg-ca-scripts  Stdout of osg-ca-manage  /var/log/osg-ca-manage.system.out    osg-ca-scripts  Stdout of initial CA setup  /var/log/osg-setup-ca-certificates.system.out", 
            "title": "Useful configuration and log files"
        }, 
        {
            "location": "/common/ca/#tests", 
            "text": "To test the host certificate of a server  openssl s_client  can be used. Here is an example with the gatekeeper:  user@host $  openssl s_client -showcerts  \\ \n    -cert /etc/grid-security/hostcert.pem  \\ \n    -key /etc/grid-security/hostkey.pem  \\ \n    -CApath /etc/grid-security/certificates/  \\ \n    -debug -connect osg-gk.mwt2.org:2119", 
            "title": "Tests"
        }, 
        {
            "location": "/common/ca/#frequently-asked-questions", 
            "text": "", 
            "title": "Frequently Asked Questions"
        }, 
        {
            "location": "/common/ca/#location-of-certificates", 
            "text": "/etc/grid-security/certificates", 
            "title": "Location of Certificates?"
        }, 
        {
            "location": "/common/ca/#what-is-the-version-of-osg-ca-package-i-have-installed-and-what-are-its-contents", 
            "text": "The version of the CA package ca be found at  /etc/grid-security/certificates/INDEX.html  or  /etc/grid-security/certificates/INDEX.txt . The changes file can be found at  /etc/grid-security/certificates/CHANGES .", 
            "title": "What is the version of OSG CA package I have installed and what are its contents?"
        }, 
        {
            "location": "/common/ca/#contents-of-osg-ca-package", 
            "text": "The OSG CA Distribution contains:   IGTF Distribution of Authority Root Certificates  (CAs accredited by the  International Grid Trust Federation )   Details of CAs in OSG distribution can be found  here . For additional details what is in the current release, see the  distribution site  and  change log .", 
            "title": "Contents of OSG CA package?"
        }, 
        {
            "location": "/common/ca/#how-can-i-add-or-remove-a-particular-ca-file", 
            "text": "Add and remove of CA files are supported only if you CA files are being installed using  osg-update-certs , which is included in the  osg-ca-scripts  package (option 2), for all other options no support for adding and removing a particular CA file is provided by OSG. The preferred approach to add or remove a CA is to use  osg-ca-manage . For adding a new CA  osg-ca-manage add [--dir  local_dir ] --hash  CA_hash  may be used, while a CA is removed using  osg-ca-manage remove --hash  CA_hash .", 
            "title": "How can I add or remove a particular CA file?"
        }, 
        {
            "location": "/common/ca/#are-there-any-log-files-or-configuration-files-associated-with-ca-certificate-package", 
            "text": "If CA files are installed using  osg-ca-certs  or  igtf-ca-certs  rpms (i.e. options 1, 4) no log or configuration files are present.  Log and configuration files are however present for  osg-ca-scripts  rpm package (option 2).  Config files:  /etc/osg/osg-update-certs.conf  Log files:  /var/log/osg-update-certs.log ,  /var/log/osg-ca-certs-status.system.out ,  /var/log/osg-ca-manage.system.out ,  /var/log/osg-setup-ca-certificates.system.out", 
            "title": "Are there any log files or configuration files associated with CA certificate package?"
        }, 
        {
            "location": "/common/ca/#are-ca-packages-automatically-updated", 
            "text": "If CA files are installed using  osg-ca-certs  or  igtf-ca-certs  rpms (i.e. options 1, 4), you will need to install the software described in  OSG CA certs updater , and enable osg-ca-certs-updater service to keep the CAs automatically updated.  If CA files are being installed using  osg-ca-scripts  rpm package (option 2), CA files are kept up-to-date as long as  osg-update-certs-cron  service the package provides has been started.", 
            "title": "Are CA packages automatically updated?"
        }, 
        {
            "location": "/common/ca/#how-do-i-manually-update-my-ca-package", 
            "text": "For Option 1: run one of the following  yum update osg-ca-certs  or  yum update igtf-ca-certs  depending on the rpm package you installed.  For Option 4: run  yum update osg-ca-certs  For Option 2: You do not need to do a manual update, make sure  osg-update-certs-cron  is enabled using  root@host #  /sbin/service osg-update-certs-cron  status  If the service is disabled, enable it using  root@host #  /sbin/service osg-update-certs-cron  start  If for some extraordinary reason you need to manually update the CA you could run  osg-ca-manage [--force] refreshCA .", 
            "title": "How do I manually update my CA package?"
        }, 
        {
            "location": "/common/ca/#where-are-the-configuration-files-for-fetch-crl", 
            "text": "/etc/fetch-crl.conf", 
            "title": "Where are the configuration files for fetch-crl?"
        }, 
        {
            "location": "/common/ca/#references", 
            "text": "Some guides on x509 certificates:   Useful commands:  http://security.ncsa.illinois.edu/research/grid-howtos/usefulopenssl.html  Install GSI authentication on a server:  http://security.ncsa.illinois.edu/research/wssec/gsihttps/  Certificates how-to:  http://www.nordugrid.org/documents/certificate_howto.html   Some examples about verifying the certificates:   http://gagravarr.org/writing/openssl-certs/others.shtml  http://www.cyberciti.biz/faq/test-ssl-certificates-diagnosis-ssl-certificate/  http://www.cyberciti.biz/tips/debugging-ssl-communications-from-unix-shell-prompt.html   Related software:   Description, manual and examples of  osg-ca-manage  osg-ca-certs-updater", 
            "title": "References"
        }, 
        {
            "location": "/common/osg-ca-certs-updater/", 
            "text": "OSG CA Certificates Updater\n\n\nAbout this Document\n\n\nThis document explains the installation and use of \nosg-ca-certs-updater\n, a package that provides automatic updates of CA certificates.\n\n\nRequirements\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare the \nrequired Yum repositories\n\n\nInstall \nCA certificates\n\n\n\n\nInstall instructions\n\n\nRun the following command to install the latest version of the updater.\n\n\nroot@host#\n yum install osg-ca-certs-updater\n\n\n\n\n\nServices\n\n\nStarting and Enabling Services\n\n\nRun the following to enable the updater. This will persist until the machine is rebooted.\n\n\nroot@host#\n service osg-ca-certs-updater-cron start\n\n\n\n\n\nRun the following to enable the updater when the machine is rebooted.\n\n\nroot@host#\n chkconfig osg-ca-certs-updater-cron on\n\n\n\n\n\nRun both commands if you wish for the service to activate immediately and remain active throughout reboots.\n\n\nStopping and Disabling Services\n\n\nEnter the following to disable the updater. This will persist until the machine is rebooted.\n\n\nroot@host#\n service osg-ca-certs-updater-cron stop\n\n\n\n\n\nEnter the following to disable the updater when the machine is rebooted.\n\n\nroot@host#\n chkconfig osg-ca-certs-updater-cron off\n\n\n\n\n\nRun both commands if you wish for the service to deactivate immediately and not get reactivated during reboots.\n\n\nConfiguration\n\n\nWhile there is no configuration file, the behavior of the updater can be adjusted by command-line arguments that are specified in the \ncron\n entry of the service. This entry is located in the file \n/etc/cron.d/osg-ca-certs-updater\n. Please see the Unix manual page for \ncrontab\n in section 5 for an explanation of the format. The manual page can be accessed by the command \nman 5 crontab\n. The valid command-line arguments can be listed by running \nosg-ca-certs-updater --help\n. Reasonable defaults have been provided, namely:\n\n\n\n\nAttempt an update no more often than every 23 hours. Due to the random wait (see below), having a 24-hour minimum time between updates would cause the update time to slowly slide back every day.\n\n\nRun the script every 6 hours. We run the script more often than we update so that downtime at the wrong moment does not cause the update to be delayed for a full day.\n\n\nDelay for a random amount of time up to 30 minutes before updating, to reduce load spikes on OSG repositories.\n\n\nDo not warn the administrator about update failures that have happened less than 72 hours since the last successful update.\n\n\nLog errors only.\n\n\n\n\nTroubleshooting\n\n\nUseful configuration and log files\n\n\nConfiguration file\n\n\n\n\n\n\n\n\nPackage\n\n\nFile Description\n\n\nLocation\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nosg-ca-certs-updater\n\n\nCron entry for periodically launching the updater\n\n\n/etc/cron.d/osg-ca-certs-updater\n\n\nCommand-line arguments to the updater can be specified here\n\n\n\n\n\n\nosg-release\n\n\nRepo definition files for production OSG repositories\n\n\n/etc/yum.repos.d/osg.repo\n or \n/etc/yum.repos.d/osg-el6.repo\n\n\nMake sure these repositories are enabled and reachable from the host you are trying to update\n\n\n\n\n\n\n\n\nLog files\n\n\nLogging is performed to the console by default. Please see the manual for your \ncron\n daemon to find out how it handles console output.\n\n\nA logfile can be specified via the \n-l\n / \n--logfile\n command-line option.\n\n\nIf logging to syslog via the \n-s\n / \n--log-to-syslog\n option, the updater will write to the \nuser\n section of the syslog. The file \n/etc/syslog.conf\n determines where syslog messages are saved.\n\n\nHow to get Help?\n\n\nTo get assistance please use \nHelp Procedure\n.\n\n\nReferences\n\n\nSome guides on X.509 certificates:\n\n\n\n\nUseful commands: \nhttp://security.ncsa.illinois.edu/research/grid-howtos/usefulopenssl.html\n\n\nInstall GSI authentication on a server: \nhttp://security.ncsa.illinois.edu/research/wssec/gsihttps/\n\n\nCertificates how-to: \nhttp://www.nordugrid.org/documents/certificate_howto.html\n\n\n\n\nSome examples about verifying the certificates:\n\n\n\n\nhttp://gagravarr.org/writing/openssl-certs/others.shtml\n\n\nhttp://www.cyberciti.biz/faq/test-ssl-certificates-diagnosis-ssl-certificate/\n\n\nhttp://www.cyberciti.biz/tips/debugging-ssl-communications-from-unix-shell-prompt.html", 
            "title": "OSG CA Certificates Updater"
        }, 
        {
            "location": "/common/osg-ca-certs-updater/#osg-ca-certificates-updater", 
            "text": "", 
            "title": "OSG CA Certificates Updater"
        }, 
        {
            "location": "/common/osg-ca-certs-updater/#about-this-document", 
            "text": "This document explains the installation and use of  osg-ca-certs-updater , a package that provides automatic updates of CA certificates.", 
            "title": "About this Document"
        }, 
        {
            "location": "/common/osg-ca-certs-updater/#requirements", 
            "text": "As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system  Obtain root access to the host  Prepare the  required Yum repositories  Install  CA certificates", 
            "title": "Requirements"
        }, 
        {
            "location": "/common/osg-ca-certs-updater/#install-instructions", 
            "text": "Run the following command to install the latest version of the updater.  root@host#  yum install osg-ca-certs-updater", 
            "title": "Install instructions"
        }, 
        {
            "location": "/common/osg-ca-certs-updater/#services", 
            "text": "", 
            "title": "Services"
        }, 
        {
            "location": "/common/osg-ca-certs-updater/#starting-and-enabling-services", 
            "text": "Run the following to enable the updater. This will persist until the machine is rebooted.  root@host#  service osg-ca-certs-updater-cron start  Run the following to enable the updater when the machine is rebooted.  root@host#  chkconfig osg-ca-certs-updater-cron on  Run both commands if you wish for the service to activate immediately and remain active throughout reboots.", 
            "title": "Starting and Enabling Services"
        }, 
        {
            "location": "/common/osg-ca-certs-updater/#stopping-and-disabling-services", 
            "text": "Enter the following to disable the updater. This will persist until the machine is rebooted.  root@host#  service osg-ca-certs-updater-cron stop  Enter the following to disable the updater when the machine is rebooted.  root@host#  chkconfig osg-ca-certs-updater-cron off  Run both commands if you wish for the service to deactivate immediately and not get reactivated during reboots.", 
            "title": "Stopping and Disabling Services"
        }, 
        {
            "location": "/common/osg-ca-certs-updater/#configuration", 
            "text": "While there is no configuration file, the behavior of the updater can be adjusted by command-line arguments that are specified in the  cron  entry of the service. This entry is located in the file  /etc/cron.d/osg-ca-certs-updater . Please see the Unix manual page for  crontab  in section 5 for an explanation of the format. The manual page can be accessed by the command  man 5 crontab . The valid command-line arguments can be listed by running  osg-ca-certs-updater --help . Reasonable defaults have been provided, namely:   Attempt an update no more often than every 23 hours. Due to the random wait (see below), having a 24-hour minimum time between updates would cause the update time to slowly slide back every day.  Run the script every 6 hours. We run the script more often than we update so that downtime at the wrong moment does not cause the update to be delayed for a full day.  Delay for a random amount of time up to 30 minutes before updating, to reduce load spikes on OSG repositories.  Do not warn the administrator about update failures that have happened less than 72 hours since the last successful update.  Log errors only.", 
            "title": "Configuration"
        }, 
        {
            "location": "/common/osg-ca-certs-updater/#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/common/osg-ca-certs-updater/#useful-configuration-and-log-files", 
            "text": "", 
            "title": "Useful configuration and log files"
        }, 
        {
            "location": "/common/osg-ca-certs-updater/#configuration-file", 
            "text": "Package  File Description  Location  Comment      osg-ca-certs-updater  Cron entry for periodically launching the updater  /etc/cron.d/osg-ca-certs-updater  Command-line arguments to the updater can be specified here    osg-release  Repo definition files for production OSG repositories  /etc/yum.repos.d/osg.repo  or  /etc/yum.repos.d/osg-el6.repo  Make sure these repositories are enabled and reachable from the host you are trying to update", 
            "title": "Configuration file"
        }, 
        {
            "location": "/common/osg-ca-certs-updater/#log-files", 
            "text": "Logging is performed to the console by default. Please see the manual for your  cron  daemon to find out how it handles console output.  A logfile can be specified via the  -l  /  --logfile  command-line option.  If logging to syslog via the  -s  /  --log-to-syslog  option, the updater will write to the  user  section of the syslog. The file  /etc/syslog.conf  determines where syslog messages are saved.", 
            "title": "Log files"
        }, 
        {
            "location": "/common/osg-ca-certs-updater/#how-to-get-help", 
            "text": "To get assistance please use  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/common/osg-ca-certs-updater/#references", 
            "text": "Some guides on X.509 certificates:   Useful commands:  http://security.ncsa.illinois.edu/research/grid-howtos/usefulopenssl.html  Install GSI authentication on a server:  http://security.ncsa.illinois.edu/research/wssec/gsihttps/  Certificates how-to:  http://www.nordugrid.org/documents/certificate_howto.html   Some examples about verifying the certificates:   http://gagravarr.org/writing/openssl-certs/others.shtml  http://www.cyberciti.biz/faq/test-ssl-certificates-diagnosis-ssl-certificate/  http://www.cyberciti.biz/tips/debugging-ssl-communications-from-unix-shell-prompt.html", 
            "title": "References"
        }, 
        {
            "location": "/common/yum/", 
            "text": "YUM Repositories\n\n\nAbout This Document\n\n\nThis document introduces YUM repositories and how OSG uses them.\n\n\nRepositories\n\n\nOSG hosts four public-facing repositories at \nrepo.opensciencegrid.org\n:\n\n\n\n\nrelease\n: This repository contains software that we are willing to support and can be used by the general community.\n\n\ncontrib\n: RPMs contributed from outside the OSG.\n\n\ntesting\n: This repository contains software ready for testing. If you install packages from here, they may be buggy, but we will provide limited assistance in providing a migration path to a fixed version.\n\n\ndevelopment\n: This repository is the bleeding edge. Installing from this repository may cause the host to stop functioning, and we will not assist in undoing any damage.\n\n\n\n\nOSG's RPM packages rely also on external packages provided by supported OSes and EPEL. You must have the following repositories available and enabled:\n\n\n\n\nyour OS repositories (SL 6/7, CentOS 6/7, or RHEL 6/7 repositories)\n\n\nEPEL repositories\n\n\nthe OSG repositories you'd like to use\n\n\n\n\nIf one of these repositories is missing you may have missing dependencies.\n\n\n\n\nWarning\n\n\nWe did not test other repositories. If you use packages from other repositories, like \njpackage\n, \ndag\n, or \nrpmforge\n, you may encounter problems.\n\n\n\n\nEnabling Repositories\n\n\nIn \nour advice on using yum\n you will learn many tricks and tips on using yum.\n\n\nTo use the packages in a repository without adding special options to the yum command the repository must be enabled.\n\n\nInstall the Yum Repositories required by OSG\n\n\nThe OSG RPMs currently support Red Hat Enterprise Linux 6, 7, and variants.\n\n\nOSG RPMs are distributed via the OSG yum repositories. Some packages depend on packages distributed via the \nEPEL\n repositories. So both repositories must be enabled.\n\n\nInstall EPEL\n\n\n\n\nInstall the EPEL repository, if not already present. \nNote:\n This enables EPEL by default. Choose the right version to match your OS version.\n#\n EPEL \n6\n \n(\nFor RHEL \n6\n, CentOS \n6\n, and SL \n6\n)\n\n\nroot@host #\n rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-6.noarch.rpm\n\n#\n EPEL \n7\n \n(\nFor RHEL \n7\n, CentOS \n7\n, and SL \n7\n)\n \n\nroot@host #\n rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nif you have your own mirror or configuration of the EPEL repository, you \nMUST\n verify that the OSG repository has a better yum priority than EPEL (\ndetails\n). Otherwise, you will have strange dependency resolution (\ndepsolving\n) issues.\n\n\n\n\nInstall the Yum priorities package\n\n\nFor packages that exist in both OSG and EPEL repositories, it is important to prefer the OSG ones or else OSG software installs may fail. Installing the Yum priorities package enables the repository priority system to work.\n\n\n\n\n\n\nInstall the Yum priorities package:\n\n\nroot@host #\n yum install yum-plugin-priorities\n\n\n\n\n\n\n\n\n\nEnsure that \n/etc/yum.conf\n has the following line in the \n[main]\n section (particularly when using ROCKS), thereby enabling Yum plugins, including the priorities one:\n\n\nplugins=1\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nIf you do not have a required key you can force the installation using \n--nogpgcheck=\n; e.g., \nyum install --nogpgcheck yum-priorities\n.\n\n\n\n\nInstall OSG Repositories\n\n\nIf you are upgrading from one OSG series to another, remove the old OSG repository definition files and clean the Yum cache:\n\n\nroot@host #\n yum clean all \n\nroot@host #\n rpm -e osg-release\n\n\n\n\n\nThis step ensures that local changes to \n*.repo\n files will not block the installation of the new OSG repositories. After this step, \n*.repo\n files that have been changed will exist in \n/etc/yum.repos.d/\n with the \n*.rpmsave\n extension. After installing the new OSG repositories (the next step) you may want to apply any changes made in the \n*.rpmsave\n files to the new \n*.repo\n files.\n\n\nInstall the OSG repositories:\n\n\nroot@host #\n rpm -Uvh \nURL\n\n\n\n\n\n\nWhere \nURL\n is one of the following:\n\n\n\n\n\n\n\n\nSeries\n\n\nEL6 URL (for RHEL 6, CentOS 6, or SL 6)\n\n\nEL7 URL (for RHEL 7, CentOS 7, or SL 7)\n\n\n\n\n\n\n\n\n\n\nOSG 3.3\n\n\nhttps://repo.opensciencegrid.org/osg/3.3/osg-3.3-el6-release-latest.rpm\n\n\nhttps://repo.opensciencegrid.org/osg/3.3/osg-3.3-el7-release-latest.rpm\n\n\n\n\n\n\nOSG 3.4\n\n\nhttps://repo.opensciencegrid.org/osg/3.4/osg-3.4-el6-release-latest.rpm\n\n\nhttps://repo.opensciencegrid.org/osg/3.4/osg-3.4-el7-release-latest.rpm\n\n\n\n\n\n\n\n\nPriorities\n\n\n\n\nNote\n\n\nMake sure you installed the Yum priorities plugin, as described above. Not doing so is a common mistake that causes failed installations.\n\n\n\n\nThe only OSG repository enabled by default is the release one. If you want to enable another one, such as \nosg-testing\n, then edit its file (e.g. \n/etc/yum.repos.d/osg-testing.repo\n) and change the enabled option from 0 to 1:\n\n\n[osg-testing]\n\n\nname\n=\nOSG Software for Enterprise Linux 7 - Testing - $basearch\n\n\n#baseurl=http://repo.grid.iu.edu/osg/3.4/el7/testing/$basearch\n\n\nmirrorlist\n=\nhttp://repo.grid.iu.edu/mirror/osg/3.4/el7/testing/$basearch\n\n\nfailovermethod\n=\npriority\n\n\npriority\n=\n98\n\n\nenabled\n=\n1\n\n\ngpgcheck\n=\n1\n\n\ngpgkey\n=\nfile:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\n\n\n\n\n\n\n\n\nWarning\n\n\nif you have your own mirror or configuration of the EPEL repository, you \nMUST\n verify that the OSG repository has a better yum priority than EPEL. Otherwise, you will have strange dependency resolution issues.\n\n\n\n\nReference\n\n\n\n\nBasic use of Yum\n\n\nBest practices in using Yum", 
            "title": "Yum Repos"
        }, 
        {
            "location": "/common/yum/#yum-repositories", 
            "text": "", 
            "title": "YUM Repositories"
        }, 
        {
            "location": "/common/yum/#about-this-document", 
            "text": "This document introduces YUM repositories and how OSG uses them.", 
            "title": "About This Document"
        }, 
        {
            "location": "/common/yum/#repositories", 
            "text": "OSG hosts four public-facing repositories at  repo.opensciencegrid.org :   release : This repository contains software that we are willing to support and can be used by the general community.  contrib : RPMs contributed from outside the OSG.  testing : This repository contains software ready for testing. If you install packages from here, they may be buggy, but we will provide limited assistance in providing a migration path to a fixed version.  development : This repository is the bleeding edge. Installing from this repository may cause the host to stop functioning, and we will not assist in undoing any damage.   OSG's RPM packages rely also on external packages provided by supported OSes and EPEL. You must have the following repositories available and enabled:   your OS repositories (SL 6/7, CentOS 6/7, or RHEL 6/7 repositories)  EPEL repositories  the OSG repositories you'd like to use   If one of these repositories is missing you may have missing dependencies.   Warning  We did not test other repositories. If you use packages from other repositories, like  jpackage ,  dag , or  rpmforge , you may encounter problems.", 
            "title": "Repositories"
        }, 
        {
            "location": "/common/yum/#enabling-repositories", 
            "text": "In  our advice on using yum  you will learn many tricks and tips on using yum.  To use the packages in a repository without adding special options to the yum command the repository must be enabled.", 
            "title": "Enabling Repositories"
        }, 
        {
            "location": "/common/yum/#install-the-yum-repositories-required-by-osg", 
            "text": "The OSG RPMs currently support Red Hat Enterprise Linux 6, 7, and variants.  OSG RPMs are distributed via the OSG yum repositories. Some packages depend on packages distributed via the  EPEL  repositories. So both repositories must be enabled.", 
            "title": "Install the Yum Repositories required by OSG"
        }, 
        {
            "location": "/common/yum/#install-epel", 
            "text": "Install the EPEL repository, if not already present.  Note:  This enables EPEL by default. Choose the right version to match your OS version. #  EPEL  6   ( For RHEL  6 , CentOS  6 , and SL  6 )  root@host #  rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-6.noarch.rpm #  EPEL  7   ( For RHEL  7 , CentOS  7 , and SL  7 )   root@host #  rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm     Warning  if you have your own mirror or configuration of the EPEL repository, you  MUST  verify that the OSG repository has a better yum priority than EPEL ( details ). Otherwise, you will have strange dependency resolution ( depsolving ) issues.", 
            "title": "Install EPEL"
        }, 
        {
            "location": "/common/yum/#install-the-yum-priorities-package", 
            "text": "For packages that exist in both OSG and EPEL repositories, it is important to prefer the OSG ones or else OSG software installs may fail. Installing the Yum priorities package enables the repository priority system to work.    Install the Yum priorities package:  root@host #  yum install yum-plugin-priorities    Ensure that  /etc/yum.conf  has the following line in the  [main]  section (particularly when using ROCKS), thereby enabling Yum plugins, including the priorities one:  plugins=1     Note  If you do not have a required key you can force the installation using  --nogpgcheck= ; e.g.,  yum install --nogpgcheck yum-priorities .", 
            "title": "Install the Yum priorities package"
        }, 
        {
            "location": "/common/yum/#install-osg-repositories", 
            "text": "If you are upgrading from one OSG series to another, remove the old OSG repository definition files and clean the Yum cache:  root@host #  yum clean all  root@host #  rpm -e osg-release  This step ensures that local changes to  *.repo  files will not block the installation of the new OSG repositories. After this step,  *.repo  files that have been changed will exist in  /etc/yum.repos.d/  with the  *.rpmsave  extension. After installing the new OSG repositories (the next step) you may want to apply any changes made in the  *.rpmsave  files to the new  *.repo  files.  Install the OSG repositories:  root@host #  rpm -Uvh  URL   Where  URL  is one of the following:     Series  EL6 URL (for RHEL 6, CentOS 6, or SL 6)  EL7 URL (for RHEL 7, CentOS 7, or SL 7)      OSG 3.3  https://repo.opensciencegrid.org/osg/3.3/osg-3.3-el6-release-latest.rpm  https://repo.opensciencegrid.org/osg/3.3/osg-3.3-el7-release-latest.rpm    OSG 3.4  https://repo.opensciencegrid.org/osg/3.4/osg-3.4-el6-release-latest.rpm  https://repo.opensciencegrid.org/osg/3.4/osg-3.4-el7-release-latest.rpm", 
            "title": "Install OSG Repositories"
        }, 
        {
            "location": "/common/yum/#priorities", 
            "text": "Note  Make sure you installed the Yum priorities plugin, as described above. Not doing so is a common mistake that causes failed installations.   The only OSG repository enabled by default is the release one. If you want to enable another one, such as  osg-testing , then edit its file (e.g.  /etc/yum.repos.d/osg-testing.repo ) and change the enabled option from 0 to 1:  [osg-testing]  name = OSG Software for Enterprise Linux 7 - Testing - $basearch  #baseurl=http://repo.grid.iu.edu/osg/3.4/el7/testing/$basearch  mirrorlist = http://repo.grid.iu.edu/mirror/osg/3.4/el7/testing/$basearch  failovermethod = priority  priority = 98  enabled = 1  gpgcheck = 1  gpgkey = file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG    Warning  if you have your own mirror or configuration of the EPEL repository, you  MUST  verify that the OSG repository has a better yum priority than EPEL. Otherwise, you will have strange dependency resolution issues.", 
            "title": "Priorities"
        }, 
        {
            "location": "/common/yum/#reference", 
            "text": "Basic use of Yum  Best practices in using Yum", 
            "title": "Reference"
        }, 
        {
            "location": "/common/openjdk7/", 
            "text": "Java Installation Guide for OSG Software\n\n\nSeveral OSG software components (e.g., \nBeStMan\n, \nGUMS\n, \nVOMS Admin\n, and their dependencies) are written in the \nJava programming language\n and therefore need a Java Virtual Machine (JVM) implementation in which to run. Currently, OSG software is supported under the OpenJDK\u00a07 implementation. Other implementations (such as the Oracle JDK) may also work, but there is considerable risk in using one; for instance, the IBM implementation is known to cause failures in most OSG Java software.\n\n\nThis document explains how to install Java for OSG software components in a way that allows other Java implementations to be present on a system. If you want to remove all other Java implementations besides the supported OpenJDK\u00a07, there are optional instructions for doing so. This document is only about installing Java as a preliminary step for other OSG software installations; other OSG install guides refer to this document when necessary.\n\n\nInstalling Java\n\n\nOSG strongly recommends installing OpenJDK\u00a07 as the primary Java implementation on each OSG host that needs Java. Instructions for doing so are below. If you have specific technical reasons to install or use an alternate, unsupported Java implementation, there are separate instructions for doing so. Choose one of the following subsections based on your needs.\n\n\nInstalling OpenJDK 7 for OSG software\n\n\nTo install OpenJDK\u00a07 as the primary Java implementation on an OSG host:\n\n\n\n\n\n\nRemove any conflicting symbolic links from an Oracle JDK installation (will not break Oracle JDK) \n\n\nroot@host #\n rm -f /usr/bin/java /usr/bin/javac /usr/bin/javadoc /usr/bin/jar\n\n\n\n\n\n\n\n\n\nInstall OpenJDK\u00a07 and OSG Java\u00a07 package compatability layer \n\n\nroot@host #\n yum install java-1.7.0-openjdk java-1.7.0-openjdk-devel osg-java7-compat osg-java7-devel-compat\n\n\n\n\n\n\n\n\n\nMake OpenJDK the preferred Java runtime environment (JRE) \n\n\nroot@host #\n alternatives --config java Select \n`\njre-1.7.0-openjdk\n`\n.\n\n\n\n\n\n\n\n\n\nVerify that OpenJDK is the preferred JRE \n\n\nroot@host #\n java -version The version number should start with \n`\n1\n.7\n`\n.\n\n\n\n\n\n\n\n\n\nMake OpenJDK the preferred Java development kit (JDK) \n\n\nroot@host #\n alternatives --config javac Select \n`\njava-1.7.0-openjdk\n`\n.\n\n\n\n\n\n\n\n\n\nVerify that OpenJDK is the preferred JDK \n\n\nroot@host #\n javac -version The version number should start with \n`\n1\n.7\n`\n.\n\n\n\n\n\n\n\n\n\nInstalling another Java implementation for OSG software\n\n\nUnless you have specific technical reasons otherwise, you should use OpenJDK\u00a07 for OSG Software; see the installation instructions above and skip this section. To install an alternate Java implementation as the primary Java implementation on an OSG host, use this section.\n\n\n\n\nNote\n\n\nThe IBM Java implementation (\njava-1.7.0-ibm\n or \njava-1.7.1-ibm\n) is known to cause severe failures in BeStMan 2 and GUMS (at least).\n\n\n\n\n\n\nInstall the Java implementation of your choice\n\n\nInstall the OSG Java\u00a07 package compatability layer \nroot@host #\n yum install osg-java7-compat osg-java7-devel-compat\n\n\n\n\n\n\n\n\n\nMake your Java implementation the preferred Java runtime environment (JRE) \n\n\n:::console\nroot@host # alternatives --config java\n4.  Verify that your Java implementation is the preferred JRE \n\n\n:::console\nroot@host # java -version The version number should start with \n1.7\n.\n5.  Make your Java implementation the preferred Java development kit (JDK) \n\n\n:::console\nroot@host # alternatives --config javac\n6.  Verify that your Java implementation is the preferred JDK \n\n\n:::console\nroot@host # javac -version The version number should start with \n1.7\n.\n\n\n\n\n\n\n\n\n\n\nFixing OpenJDK After Installing Other Software\n\n\nIf you happened to install OSG or other Java-dependent software \nbefore\n installing OpenJDK, there may be issues with some stale symbolic links to the Oracle Java implementation. To see whether the issue exists:\n\n\n\n\n\n\nCheck for an Oracle Java installation \n\n\nroot@host #\n rpm -q jdk\n\n\n\n\n\nIf this command outputs a specific RPM name and version (e.g., \njdk-1.7.0\n), then Oracle Java is installed and you should continue with the next step. Otherwise, there are no issues and you are done\n\n\n\n\n\n\nCheck for stale symbolic links \n\n\nroot@host #\n **file /usr/bin/java**\n\n\n\n\n\n\n\n\n\n/usr/bin/java: symbolic link to `/etc/alternatives/java' If the output of the \nfile\n command above differs from the sample output \u2014 if \n/usr/bin/java\n does not exist, if it is not a symbolic link, or if it is a symbolic link to a different path \u2014 then your OpenJDK installation has issues and must be fixed; continue with the next repair procedure below. Otherwise, there are no issues and you are done\n\n\nTo fix the OpenJDK installation:\n\n\n\n\n\n\nRemove incorrect symbolic links: \n\n\nroot@host #\n rm -f /usr/bin/java /usr/bin/javac /usr/bin/javadoc /usr/bin/jar\n\n\n\n\n\n\n\n\n\nReinstall OpenJDK to create the symbolic links: \n\n\nroot@host #\n yum reinstall java-1.7.0-openjdk java-1.7.0-openjdk-devel\n\n\n\n\n\n\n\n\n\nMake OpenJDK the preferred Java runtime environment (JRE): \n\n\nroot@host #\n alternatives --config java Select \n`\njre-1.7.0-openjdk\n`\n.\n\n\n\n\n\n\n\n\n\nVerify that OpenJDK is the preferred JRE: \n\n\nroot@host #\n java -version The version number should start with \n`\n1\n.7\n`\n.\n\n\n\n\n\n\n\n\n\nMake OpenJDK the preferred Java development kit (JDK): \n\n\nroot@host #\n alternatives --config javac Select \n`\njava-1.7.0-openjdk\n`\n.\n\n\n\n\n\n\n\n\n\nVerify that OpenJDK is the preferred JDK: \n\n\nroot@host #\n javac -version The version number should start with \n`\n1\n.7\n`\n.\n\n\n\n\n\n\n\n\n\nFixing Tomcat\n\n\nIf you have a pre-existing Tomcat installation, you must ensure that that it is using your preferred Java implementation.\n\n\n\n\n\n\nCheck if tomcat6 is installed: \n\n\nroot@host #\n rpm -q tomcat6\n\n\n\n\n\nIf the RPM is installed, the command will output the specific RPM name and version. If there is no output: Tomcat is not installed, there is no issue, and you are done\n\n\n\n\n\n\nOpen \n/etc/sysconfig/tomcat6\n.\n\n\n\n\nCheck if there is a line setting \nJAVA_HOME\n and that line is uncommented. If so, change that line to: \nJAVA_HOME=\"/etc/alternatives/jre\" If not, then you do not need to do anything.\n\n\nDo the same check for \n/etc/tomcat6/tomcat6.conf\n.\n\n\nRestart the Tomcat service: \nroot@host #\n service tomcat6 restart\n\n\n\n\n\n\n\n\n\nFixing BeStMan\n\n\nIf you have a pre-existing BeStMan installation, you must ensure that it is using your preferred Java implementation.\n\n\n\n\n\n\nCheck if any bestman2 packages are installed: \n\n\nroot@host #\n rpm -qa \n|\n grep ^bestman2\n\n\n\n\n\nIf any bestman2 RPMs are installed, the command will output the specific names and versions. If there is no output: BeStMan is not installed, there is no issue, and you are done\n2.  Open \n/etc/sysconfig/bestman2\n.\n3.  Check if there is a line setting \nJAVA_HOME\n and that line is uncommented. If so, change that line to: \nJAVA_HOME=/etc/alternatives/java_sdk If not, then you do not need to do anything.\n4.  Restart the BeStMan service: \n\n\nroot@host #\n service bestman2 restart\n\n\n\n\n\n\n\n\n\nFixing SSL problems / Disabling SSLv3\n\n\nWe have received reports that the latest version of Firefox (39.0) keeps users from being able to access the web UIs of GUMS, Gratia, or VOMS-Admin. Attempts to access will result in this error: \n\n\n      SSL received a weak ephemeral Diffie-Hellman key in Server Key Exchange handshake message.\n      (Error code: ssl_error_weak_server_ephemeral_dh_key)\n\n\n\n\n\nThis affects all known versions of OpenJDK 1.7.0.   The recommended workaround for this is server-side:\n\n\n\n\nFind the file: \n/usr/lib/jvm/java-1.7.0-openjdk-1.7.0-*/jre/lib/security/java.security\n\n\nFind the setting called \njdk.tls.disabledAlgorithms\n (or add it if missing)\n\n\nIf not already present, add \nSSLv3\n and \nDHE\n to the list\n\n\nIf using EL6, run \nservice tomcat6 restart\n. If using EL7, run \nservice tomcat restart\n\n\n\n\nThis will cause the webapp to use a different algorithm, which Firefox will not complain about.   We do not have an ETA for when a future version of OpenJDK 1.7.0 will contain a fix. OpenJDK 1.8.0 does not have this problem, but since we have not verified that it works with OSG software, we do not recommend switching to that yet. \n\n\nOptional: Preventing Oracle JDK From Being Installed\n\n\nTo prevent Oracle's java implementation from being installed, you can configure yum to exclude it from all future installs: Edit \n/etc/yum.conf\n and add the line \nexclude=jdk java-1.6.0-sun-compat\n to the end of the file. If a line starting with \nexclude=\n already exists in \n/etc/yum.conf\n, then add \njdk\n and \njava-1.6.0-sun-compat\n to the end of that line, separated by spaces.\n\n\nGetting Help with Java and OSG Software\n\n\nFor further assistance please use \nthis page\n.", 
            "title": "Java 7"
        }, 
        {
            "location": "/common/openjdk7/#java-installation-guide-for-osg-software", 
            "text": "Several OSG software components (e.g.,  BeStMan ,  GUMS ,  VOMS Admin , and their dependencies) are written in the  Java programming language  and therefore need a Java Virtual Machine (JVM) implementation in which to run. Currently, OSG software is supported under the OpenJDK\u00a07 implementation. Other implementations (such as the Oracle JDK) may also work, but there is considerable risk in using one; for instance, the IBM implementation is known to cause failures in most OSG Java software.  This document explains how to install Java for OSG software components in a way that allows other Java implementations to be present on a system. If you want to remove all other Java implementations besides the supported OpenJDK\u00a07, there are optional instructions for doing so. This document is only about installing Java as a preliminary step for other OSG software installations; other OSG install guides refer to this document when necessary.", 
            "title": "Java Installation Guide for OSG Software"
        }, 
        {
            "location": "/common/openjdk7/#installing-java", 
            "text": "OSG strongly recommends installing OpenJDK\u00a07 as the primary Java implementation on each OSG host that needs Java. Instructions for doing so are below. If you have specific technical reasons to install or use an alternate, unsupported Java implementation, there are separate instructions for doing so. Choose one of the following subsections based on your needs.", 
            "title": "Installing Java"
        }, 
        {
            "location": "/common/openjdk7/#installing-openjdk-7-for-osg-software", 
            "text": "To install OpenJDK\u00a07 as the primary Java implementation on an OSG host:    Remove any conflicting symbolic links from an Oracle JDK installation (will not break Oracle JDK)   root@host #  rm -f /usr/bin/java /usr/bin/javac /usr/bin/javadoc /usr/bin/jar    Install OpenJDK\u00a07 and OSG Java\u00a07 package compatability layer   root@host #  yum install java-1.7.0-openjdk java-1.7.0-openjdk-devel osg-java7-compat osg-java7-devel-compat    Make OpenJDK the preferred Java runtime environment (JRE)   root@host #  alternatives --config java Select  ` jre-1.7.0-openjdk ` .    Verify that OpenJDK is the preferred JRE   root@host #  java -version The version number should start with  ` 1 .7 ` .    Make OpenJDK the preferred Java development kit (JDK)   root@host #  alternatives --config javac Select  ` java-1.7.0-openjdk ` .    Verify that OpenJDK is the preferred JDK   root@host #  javac -version The version number should start with  ` 1 .7 ` .", 
            "title": "Installing OpenJDK 7 for OSG software"
        }, 
        {
            "location": "/common/openjdk7/#installing-another-java-implementation-for-osg-software", 
            "text": "Unless you have specific technical reasons otherwise, you should use OpenJDK\u00a07 for OSG Software; see the installation instructions above and skip this section. To install an alternate Java implementation as the primary Java implementation on an OSG host, use this section.   Note  The IBM Java implementation ( java-1.7.0-ibm  or  java-1.7.1-ibm ) is known to cause severe failures in BeStMan 2 and GUMS (at least).    Install the Java implementation of your choice  Install the OSG Java\u00a07 package compatability layer  root@host #  yum install osg-java7-compat osg-java7-devel-compat    Make your Java implementation the preferred Java runtime environment (JRE)   :::console\nroot@host # alternatives --config java\n4.  Verify that your Java implementation is the preferred JRE   :::console\nroot@host # java -version The version number should start with  1.7 .\n5.  Make your Java implementation the preferred Java development kit (JDK)   :::console\nroot@host # alternatives --config javac\n6.  Verify that your Java implementation is the preferred JDK   :::console\nroot@host # javac -version The version number should start with  1.7 .", 
            "title": "Installing another Java implementation for OSG software"
        }, 
        {
            "location": "/common/openjdk7/#fixing-openjdk-after-installing-other-software", 
            "text": "If you happened to install OSG or other Java-dependent software  before  installing OpenJDK, there may be issues with some stale symbolic links to the Oracle Java implementation. To see whether the issue exists:    Check for an Oracle Java installation   root@host #  rpm -q jdk  If this command outputs a specific RPM name and version (e.g.,  jdk-1.7.0 ), then Oracle Java is installed and you should continue with the next step. Otherwise, there are no issues and you are done    Check for stale symbolic links   root@host #  **file /usr/bin/java**    /usr/bin/java: symbolic link to `/etc/alternatives/java' If the output of the  file  command above differs from the sample output \u2014 if  /usr/bin/java  does not exist, if it is not a symbolic link, or if it is a symbolic link to a different path \u2014 then your OpenJDK installation has issues and must be fixed; continue with the next repair procedure below. Otherwise, there are no issues and you are done  To fix the OpenJDK installation:    Remove incorrect symbolic links:   root@host #  rm -f /usr/bin/java /usr/bin/javac /usr/bin/javadoc /usr/bin/jar    Reinstall OpenJDK to create the symbolic links:   root@host #  yum reinstall java-1.7.0-openjdk java-1.7.0-openjdk-devel    Make OpenJDK the preferred Java runtime environment (JRE):   root@host #  alternatives --config java Select  ` jre-1.7.0-openjdk ` .    Verify that OpenJDK is the preferred JRE:   root@host #  java -version The version number should start with  ` 1 .7 ` .    Make OpenJDK the preferred Java development kit (JDK):   root@host #  alternatives --config javac Select  ` java-1.7.0-openjdk ` .    Verify that OpenJDK is the preferred JDK:   root@host #  javac -version The version number should start with  ` 1 .7 ` .", 
            "title": "Fixing OpenJDK After Installing Other Software"
        }, 
        {
            "location": "/common/openjdk7/#fixing-tomcat", 
            "text": "If you have a pre-existing Tomcat installation, you must ensure that that it is using your preferred Java implementation.    Check if tomcat6 is installed:   root@host #  rpm -q tomcat6  If the RPM is installed, the command will output the specific RPM name and version. If there is no output: Tomcat is not installed, there is no issue, and you are done    Open  /etc/sysconfig/tomcat6 .   Check if there is a line setting  JAVA_HOME  and that line is uncommented. If so, change that line to:  JAVA_HOME=\"/etc/alternatives/jre\" If not, then you do not need to do anything.  Do the same check for  /etc/tomcat6/tomcat6.conf .  Restart the Tomcat service:  root@host #  service tomcat6 restart", 
            "title": "Fixing Tomcat"
        }, 
        {
            "location": "/common/openjdk7/#fixing-bestman", 
            "text": "If you have a pre-existing BeStMan installation, you must ensure that it is using your preferred Java implementation.    Check if any bestman2 packages are installed:   root@host #  rpm -qa  |  grep ^bestman2  If any bestman2 RPMs are installed, the command will output the specific names and versions. If there is no output: BeStMan is not installed, there is no issue, and you are done\n2.  Open  /etc/sysconfig/bestman2 .\n3.  Check if there is a line setting  JAVA_HOME  and that line is uncommented. If so, change that line to:  JAVA_HOME=/etc/alternatives/java_sdk If not, then you do not need to do anything.\n4.  Restart the BeStMan service:   root@host #  service bestman2 restart", 
            "title": "Fixing BeStMan"
        }, 
        {
            "location": "/common/openjdk7/#fixing-ssl-problems-disabling-sslv3", 
            "text": "We have received reports that the latest version of Firefox (39.0) keeps users from being able to access the web UIs of GUMS, Gratia, or VOMS-Admin. Attempts to access will result in this error:         SSL received a weak ephemeral Diffie-Hellman key in Server Key Exchange handshake message.\n      (Error code: ssl_error_weak_server_ephemeral_dh_key)  This affects all known versions of OpenJDK 1.7.0.   The recommended workaround for this is server-side:   Find the file:  /usr/lib/jvm/java-1.7.0-openjdk-1.7.0-*/jre/lib/security/java.security  Find the setting called  jdk.tls.disabledAlgorithms  (or add it if missing)  If not already present, add  SSLv3  and  DHE  to the list  If using EL6, run  service tomcat6 restart . If using EL7, run  service tomcat restart   This will cause the webapp to use a different algorithm, which Firefox will not complain about.   We do not have an ETA for when a future version of OpenJDK 1.7.0 will contain a fix. OpenJDK 1.8.0 does not have this problem, but since we have not verified that it works with OSG software, we do not recommend switching to that yet.", 
            "title": "Fixing SSL problems / Disabling SSLv3"
        }, 
        {
            "location": "/common/openjdk7/#optional-preventing-oracle-jdk-from-being-installed", 
            "text": "To prevent Oracle's java implementation from being installed, you can configure yum to exclude it from all future installs: Edit  /etc/yum.conf  and add the line  exclude=jdk java-1.6.0-sun-compat  to the end of the file. If a line starting with  exclude=  already exists in  /etc/yum.conf , then add  jdk  and  java-1.6.0-sun-compat  to the end of that line, separated by spaces.", 
            "title": "Optional: Preventing Oracle JDK From Being Installed"
        }, 
        {
            "location": "/common/openjdk7/#getting-help-with-java-and-osg-software", 
            "text": "For further assistance please use  this page .", 
            "title": "Getting Help with Java and OSG Software"
        }, 
        {
            "location": "/common/pki-cli/", 
            "text": "OSG PKI Command Line Clients\n\n\nOverview\n\n\nThe OSG PKI Command Line Clients provide a command-line interface for requesting and issuing host certificates from the OSG PKI. They complement the \nOIM Web Interface\n.\n\n\nPrerequisites\n\n\nIf you have not already done so, you need to \nconfigure the OSG software repositories\n.\n\n\nInstallation\n\n\nThe command-line scripts have been packaged as an RPM and are available from the OSG repositories.\n\n\nTo install the RPM, run:\n\n\nroot@host #\n yum install osg-pki-tools\n\n\n\n\n\nUsage\n\n\nConfiguration Files\n\n\nThis configuration file contains information divided into two sections for testing and production.  Configuration variables include:\n\n\n\n\nRequest URL\n\n\nApprove URL\n\n\nRetrieve URL\n\n\nHost URL\n\n\n\n\nThese parameters are used as input to the script depending upon the mode of execution of the script (test or OIM). The command-line utilities check for configuration files in the following order:\n\n\n\n\n$HOME/.osg-pki/OSG_PKI.ini\n\n\n./pki-clients.ini\n\n\n/etc/pki-clients.ini\n\n\n\n\nosg-cert-request\n\n\nSends a request for a host certificate.\n\n\nThis script generates a private key and submits a request for a certificate to the OSG PKI. The request will be approved by an appropriate Grid Admin. You will receive an email when this approval has been completed containing directions on how to run \nosg-cert-retreive\n to retrieve the certificate. It works in two modes:\n\n\n\n\nCSR is provided by the user: the CSR provided is sent to the OIM.\n\n\nCSR is not provided by the user: the script generates a private key for the user. Writes it to default key file name or the one specified by \n-o\n.\n\n\n\n\nThis script:\n\n\n\n\nGenerates a new host private key and CSR (the only important part of CSR is \nCN=\nHOSTNAME\n component).\n\n\nSaves the host private key to disk (as specified by the user).\n\n\nAuthenticates to OIM and posts the CSR as a request to OIM.\n\n\nReturns the request ID to the user.\n\n\n\n\nIf the user provides the CSR, then this script would just send the same CSR to OIM.\n\n\nInputs:\n\n\n\n\nfully-qualified hostname\n\n\nfilename to store private key (optional; default is \n./hostkey.pem\n).\n\n\npath to user's certificate (optional: default is path specified by \n$X509_USER_CERT\n environment variable, then \n~/.globus/usercert.pem\n).\n\n\npath to user's private key (optional: default is path specified by \n$X509_USER_KEY\n environment variable, then \n~/.globus/userkey.pem\n).\n\n\nPassphrase for user's private key (via non-echoing prompt).\n\n\nUser needs to provide VO name if the requested hostname has multiple VO's assigned.\n\n\n\n\nOutputs:\n\n\n\n\nPrivate key, to filename specified by \n-o\n or \n./hostkey.pem\n by default.\n\n\nRequest Id, to \nstdout\n.\n\n\n\n\nroot@host #\n osg-cert-request --help\n\nUsage: osg-cert-request [options]\n\n\n\nOptions:\n\n\n  -h, --help            show this help message and exit\n\n\n  -c CSR, --csr=CSR     Specify CSR name (default = gennew.csr)\n\n\n  -o OUTPUT KEYFILE, --outkeyfile=OUTPUT KEYFILE\n\n\n                        Specify the output filename for the retrieved user certificate.\n\n\n                        Default is ./hostkey.pem\n\n\n  -v VO name, --vo=VO name\n\n\n                        Specify the VO for the host request\n\n\n  -y CC LIST, --cc=CC LIST\n\n\n                        Specify the CC list(the email id\ns to be CCed).\n\n\n                        Separate values by \n,\n\n\n  -m COMMENT, --comment=COMMENT\n\n\n                        The comment to be added to the request\n\n\n  -H CN, --hostname=CN  Specify a hostname for CSR (FQDN)\n\n\n  -a HOSTNAME, --altname=HOSTNAME\n\n\n                        Specify an alternative hostname for the CSR (FQDN). May be used more than once\n\n\n  -e EMAIL, --email=EMAIL\n\n\n                        Email address to receive certificate\n\n\n  -n NAME, --name=NAME  Name of user receiving certificate\n\n\n  -p PHONE, --phone=PHONE\n\n\n                        Phone number of user receiving certificate\n\n\n  -t TIMEOUT, --timeout=TIMEOUT\n\n\n                        Specify the timeout in minutes\n\n\n  -T, --test            Run in test mode\n\n\n  -q, --quiet           don\nt print status messages to stdout\n\n\n  -d WRITE_DIRECTORY, --directory=WRITE_DIRECTORY\n\n\n                        Write the output files to this directory\n\n\n  -V, --version         Print version information and exit\n\n\n\n\n\n\nExamples.\n\n\nOSG generates the key pair for the request.\n\n\nroot@host #\n osg-cert-request -t hostname.domain.com -e emailaddress@domain.com -n \nYour Name\n -p \n9999999999\n -y \nxyz@domain.com,abc@domain.com\n -m \nThis is my comment\n\n\n\n\n\n\nIf you want to request a service certificate, you need to escape backslash for service name inside CN like following.\n\n\nroot@host #\n osg-cert-request -t hostname.domain.com -e emailaddress@domain.com -n \nYour Name\n -p \n9999999999\n -y \nrsv\\/xyz@domain.com\n -m \nThis is my comment\n\n\n\n\n\n\nYou can create your CSR on your target hosts using tools such as \nopenssl\n.\n\n\nroot@host #\n \numask\n \n077\n;\n openssl req -new -newkey rsa:2048 -nodes -keyout hostkey.pem -subj \n/CN=osg-ce.example.edu\n -out csr.pem\n\n\n\n\n\nNote that the DN will be overriden by the OSG PKI except for the CN component.\n\n\nSubmitting the request:\n\n\nroot@host #\n osg-cert-request -t hostname.domain.com -e emailaddress@domain.com -n \nYour Name\n -p \n9999999999\n -y \nxyz@domain.com,abc@domain.com\n -m \nThis is my comment\n --csr csr.pem\n\n\n\n\n\nosg-cert-retrieve\n\n\nRetrieve a certificate (host or user) from OIM given a request Id. Typically you will run this script after submitting a request with \nosg-cert-request\n and receiving an email telling you your certificate has been approved.\n\n\nYou can also use this script to retrieve other certificates that have been previously issued (assuming you know their request ID number).\n\n\nSince certificates are public, no authentication of the user is required.\n\n\nThis script:\n\n\n\n\nAccepts a request Id from the user\n\n\nConnects to OIM and requests the certificate identified by the request ID\n\n\nWrites the certificate to disk (as specified by the user)\n\n\n\n\nInputs:\n\n\n\n\nRequest ID\n\n\nFilename to store certificate (optional: default is \n./hostcert.pem\n).\n\n\n\n\nOutputs:\n\n\n\n\nHost certificate as PEM, to filename specified or \n./hostcert.pem\n.\n\n\n\n\nroot@host #\n osg-cert-retrieve --help\n\nUsage: osg-cert-retrieve [options] \nRequest ID\n\n\nUsage: osg-cert-retrieve -h/--help [for detailed explanations of options]\n\n\n\nOptions:\n\n\n  -h, --help            show this help message and exit\n\n\n  -o ID, --certfile=ID  Specify the output filename for the retrieved user\n\n\n                        certificate . Default is ./hostcert.pem\n\n\n  -T, --test            Run in test mode\n\n\n  -t TIMEOUT, --timeout=TIMEOUT\n\n\n                        Specify the timeout in minutes\n\n\n  -q, --quiet           don\nt print status messages to stdout\n\n\n  -d WRITE_DIRECTORY, --directory=WRITE_DIRECTORY\n\n\n                        Write the output files to this directory\n\n\n  -V, --version         Print version information and exit\n\n\n  -i, --id              Specify ID# of certificate to be retrieved\n\n\n                        [deprecated]\n\n\n\n\n\n\nExample:\n\n\nroot@host #\n osg-cert-retrieve -i \n555\n\n\n\n\n\n\nosg-gridadmin-cert-request\n\n\nRequest and retrieve multliple host certificates from OIM. Authenticates to OIM and is only for use by Grid Admins for certificates they are authorized to approve. This script is only supported with all hosts being in the same domain (so we ensure they go to the same Grid Admin). The certificates are stored with the format of \nhostname-requestid.pem\n (i.e. the id generated from the request for the certificate). The key is stored as \nhostname-serial-key.pem\n.\n\n\nThis script does the following in the process of acquiring certificates for the hostnames specified:\n\n\n\n\nReads a list of fully-qualified hostnames from a file specified by the user.\n\n\nFor each hostname:\n\n\nGenerates a new private key and CSR.\n\n\nOnly important part of CSR is \nCN=\nHOSTNAME\n component.\n\n\nWrites the private key to a file with filename: \nPREFIX\n/\nHOSTNAME\n-key.pem\n.\n\n\nPrompts the user for their private key pass phrase (the pass phrase is cached so user is not re-prompted).\n\n\nAuthenticates to OIM and posts the CSRs as a single request to OIM.\n\n\nRequest ID is returned and subsequently used.\n\n\nAuthenticates to OIM and approves the request.\n\n\nWaits one minute for request to be processed by OIM.\n\n\nConnects to OIM and attempts to retrieve certificates.\n\n\nWrites out any certificates it retrieves with filename of \nPREFIX\n/\nHOSTNAME\n-\nID\n.pem\n.\n\n\nIf all certificates have been retrieved, exits loop.\n\n\nWait 5 seconds and proceeds with the next repeat.\n\n\n\n\n\n\n\n\nInputs:\n\n\n\n\nfilename of list of hostnames.\n\n\nprefix path in which to write private keys and certificates (default: \n.\n.)\n\n\npath to user's certificate (optional: default is path specified by \n$X509_USER_CERT\n environment variable, then \n~/.globus/usercert.pem\n).\n\n\npath to user's private key (optional, default is path specified by \n$X509_USER_KEY\n environment variable, then \n~/.globus/userkey.pem\n).\n\n\nPassphrase for user's private key via non-echoing prompt.\n\n\n\n\nOutputs:\n\n\n\n\nN\n host certificates in PEM format.\n\n\nN\n private keys in PEM format.\n\n\n\n\nroot@host #\n osg-gridadmin-cert-request --help\n\nUsage: osg-gridadmin-cert-request [options] arg\n\n\nUsage: osg-gridadmin-cert-request -h/--help [for detailed explanations of options]\n\n\n\nOptions:\n\n\n  -h, --help            show this help message and exit\n\n\n  -k PKEY, --pkey=PKEY  Specify Requestor\ns private key (PEM Format). If not\n\n\n                        specifiedwill take the value of X509_USER_KEY or\n\n\n                        $\nHOME/.globus/userkey.pem\n\n  -c CERT, --cert=CERT  Specify Requestor\ns certificate (PEM Format). If not\n\n\n                        specified, will take the value of X509_USER_CERT or\n\n\n                        $\nHOME/.globus/usercert.pem\n\n  -a HOSTNAME, --altname=HOSTNAME\n\n\n                        Specify an alternative hostname for CSR (FQDN). May be\n\n\n                        used more than once and if specified, -f/--hostfile\n\n\n                        will be ignored\n\n\n  -v VO name, --vo=VO name\n\n\n                        Specify the VO for the host request\n\n\n  -y CC List, --cc=CC List\n\n\n                        Specify the CC list(the email id\ns to be\n\n\n                        CCed).Separate values by \n,\n\n\n  -T, --test            Run in test mode\n\n\n  -t TIMEOUT, --timeout=TIMEOUT\n\n\n                        Specify the timeout in minutes\n\n\n  -q, --quiet           don\nt print status messages to stdout\n\n\n  -d WRITE_DIRECTORY, --directory=WRITE_DIRECTORY\n\n\n                        Write the output files to this directory\n\n\n  -V, --version         Print version information and exit\n\n\n\n  Hostname Options:\n\n\n    Use either of these options. Specify hostname as a single hostname\n\n\n    using -H/--hostname or specify from a file using -f/--hostfile.\n\n\n\n    -H HOSTNAME, --hostname=HOSTNAME\n\n\n                        Specify the hostname or service/hostname for which you\n\n\n                        want to request the certificate for. If specified,\n\n\n                        -f/--hostfile will be ignored\n\n\n    -f HOSTFILE, --hostfile=HOSTFILE\n\n\n                        Filename with one host (hostname or service/hostname\n\n\n                        and its optional,alternative hostnames, separated by\n\n\n                        spaces) per line\n\n\n\n\n\n\nExamples:\n\n\nroot@host #\n osg-gridadmin-cert-request -f filename -k privatekeyfile -c certificatefile\n\n\n\n\n\nroot@host #\n osg-gridadmin-cert-request -H hostname.domain.com -k privatekeyfile -c certificatefile\n\n\n\n\n\nosg-user-cert-renew\n\n\nSends a request for renewing a user certificate and if the certificate can be renewed, fetches and writes the renewed certificate.\n\n\nThe script generates request for renewing user certificate to the OIM, and if the certificate is renewed, it fetches the renewed user certificate. The user is authenticated before making such a request. If the user certificate is renewed, the user gets email notification regarding the same and the renewed certificate is saved by the name of the existing certificate suffixed by \n-renewed.pem\n (e.g. when we renew \nmy-cert.pem\n, the renewed certificate is named \nmy-cert-renewed.pem\n).\n\n\nInputs:\n\n\n\n\npath to user's certificate (optional: default is path specified by \n$X509_USER_CERT\n environment variable, then \n~/.globus/usercert.pem\n).\n\n\npath to user's private key (optional: default is path specified by \n$X509_USER_KEY\n environment variable, then \n~/.globus/userkey.pem\n).\n\n\nPassphrase for user's private key via non-echoing prompt.\n\n\nUser needs to provide VO name if the requested hostname has multiple VO's assigned\n\n\n\n\nOutputs:\n\n\n\n\nOn Renewal, the renewed certificate is stored with the filename of the older user certificate suffixed with \n-renewed.pem\n.  The certificate name for renewed certificate is sent to \nstdout\n.\n\n\n\n\n\n\nNote\n\n\nIf the retrieval of user certificate fails for some reason, the user can download the renewed certificate from OIM web interface using the following URL (where \nREQID\n is the request ID number for the user certificate on OIM):\n\n\nhttps://oim.opensciencegrid.org/oim/certificateuser?id=\nREQID\n\n\n\n\n\n\n\n\nroot@host #\n osg-user-cert-renew --help\n\nUsage: osg-user-cert-renew [options]\n\n\n\nOptions:\n\n\n  -h, --help            show this help message and exit\n\n\n  -k PKEY, --pkey=PKEY  Specify Requestor\ns private key (PEM Format). If not\n\n\n                        specified  will take the value of X509_USER_KEY or\n\n\n                        $\nHOME/.globus/userkey.pem\n\n  -c CERT, --cert=CERT  Specify Requestor\ns certificate (PEM Format).  If not\n\n\n                        specified will take the value of X509_USER_CERT or\n\n\n                        $\nHOME/.globus/usercert.pem\n\n  -T, --test            Run in test mode\n\n\n  -t TIMEOUT, --timeout=TIMEOUT\n\n\n                        Specify the timeout in minutes\n\n\n  -d WRITE_DIRECTORY, --directory=WRITE_DIRECTORY\n\n\n                        Write the output files to this directory\n\n\n  -q, --quiet           don\nt print status messages to stdout\n\n\n  -V, --version         Print version information and exit\n\n\n\n\n\n\nExample\n\n\nroot@host #\n osg-user-cert-renew -k userkey.pem -c usercert.pem\n\n\n\n\n\nosg-user-cert-revoke\n\n\nRevoke a user certificate from OIM given a request ID or certificate ID. Usually the script is run when a user wants to revoke user certificate.\n\n\nFor revoking user certificate, user authentication is done and if the user is authorized to revoke the user certificate, the certificate is immediately revoked and an email notification is sent informing the user that the user certificate is revoked.\n\n\nThe script:\n\n\n\n\nAccepts a Request ID or Certificate ID from the user.\n\n\nAuthenticates user and connects to OIM to revoke the user certificate identified by the request ID or the certificate ID.\n\n\n\n\nInputs:\n\n\n\n\nPrivate key for the user requesting host certificate revocation.\n\n\nUser certificate for the user requesting host certificate revocation.\n\n\nMessage for requesting the user certificate revocation.\n\n\nRequest ID OR the certificate ID for the user certificate to be revoked.\n\n\n\n\nOutputs:\n\n\n\n\nMessage if the revocation was successful along with the host cert request ID/certificate ID on \nstdout\n.\n\n\nError message if the revocation was unsuccessful on \nstdout\n.\n\n\n\n\nIf the user's private key and certificate are not provided, the script takes the private key and user certificate from the \n~/.globus\n folder using the default names (\nuserkey.pem\n and \nusercert.pem\n, respectively)\n\n\nroot@host #\n osg-user-cert-revoke --help\n\nUsage: osg-user-cert-revoke [options] \nRequest ID\n \nmessage\n\n\nUsage: osg-user-cert-revoke -h/--help [for detailed explanations of options]\n\n\n\nOptions:\n\n\n  -h, --help            show this help message and exit\n\n\n  -n, --certid          Treat the ID argument as the serial ID# for the\n\n\n                        certificate to be revoked\n\n\n  -u, --user            Certificate to be revoked is a user certificate.\n\n\n                        Redundant when using `osg-user-cert-revoke`.\n\n\n  -k PKEY, --pkey=PKEY  Specify Requestor\ns private key (PEM Format). If not\n\n\n                        specified, this takes the value of X509_USER_KEY or\n\n\n                        $\nHOME/.globus/userkey.pem\n\n  -c CERT, --cert=CERT  Specify Requestor\ns certificate (PEM Format). If not\n\n\n                        specified, this takes the value of X509_USER_CERT or\n\n\n                        $\nHOME/.globus/usercert.pem\n\n  -T, --test            Run in test mode\n\n\n  -t TIMEOUT, --timeout=TIMEOUT\n\n\n                        Specify the timeout in minutes\n\n\n  -q, --quiet           don\nt print status messages to stdout\n\n\n  -V, --version         Print version information and exit\n\n\n  -m REASON, --message=REASON\n\n\n                        Specify the reason for certificate revocation\n\n\n                        [deprecated]\n\n\n  -i, --id              Specify ID# of certificate to be retrieved\n\n\n                        [deprecated]\n\n\n\n\n\n\nExample:\n\n\nroot@host #\n osg-user-cert-revoke -i \n999\n -m \nTesting user cert revocation\n -k privatekeyfile -c usercertfile\n\n\n\n\n\nosg-cert-revoke\n\n\nRevoke a host certificate from OIM given a request ID. Usually the script is run when a user wants to revoke host/service certificate.\n\n\nFor revoking host certificate, user authentication is done and if the user is authorized to revoke the host certificate, the certificate is immediately revoked and an email notification is sent informing the user that the host certificate is revoked.\n\n\nThe script:\n\n\n\n\nAccepts a request ID from the user.\n\n\nAuthenticates user and connects to OIM to revoke the host certificate identified by the request ID.\n\n\n\n\nInputs:\n\n\n\n\nPrivate key for the user requesting host certificate revocation.\n\n\ncertificate for the user requesting host certificate revocation.\n\n\nMessage for requesting the host certificate revocation.\n\n\nRequest ID for the host certificate to be revoked.\n\n\n\n\nOutputs:\n\n\n\n\nMessage if the revocation was successful along with the host cert request ID on \nstdout\n.\n\n\nError message if the revocation was unsuccessful on \nstdout\n.\n\n\n\n\nIf the user's private key and certificate are not provided, the script takes the private key and user certificate from the \n~/.globus\n folder using the default names (\nuserkey.pem\n and \nusercert.pem\n, respectively).\n\n\nroot@host #\n osg-user-cert-revoke --help\n\nUsage: osg-cert-revoke [options] \nRequest ID\n \nmessage\n\n\nUsage: osg-cert-revoke -h/--help [for detailed explanations of options]\n\n\n\nOptions:\n\n\n  -h, --help            show this help message and exit\n\n\n  -n, --certid          Treat the ID argument as the serial ID# for the\n\n\n                        certificate to be revoked\n\n\n  -u, --user            Certificate to be revoked is a user certificate.\n\n\n                        Redundant when using `osg-user-cert-revoke`.\n\n\n  -k PKEY, --pkey=PKEY  Specify Requestor\ns private key (PEM Format). If not\n\n\n                        specified, this takes the value of X509_USER_KEY or\n\n\n                        $\nHOME/.globus/userkey.pem\n\n  -c CERT, --cert=CERT  Specify Requestor\ns certificate (PEM Format). If not\n\n\n                        specified, this takes the value of X509_USER_CERT or\n\n\n                        $\nHOME/.globus/usercert.pem\n\n  -T, --test            Run in test mode\n\n\n  -t TIMEOUT, --timeout=TIMEOUT\n\n\n                        Specify the timeout in minutes\n\n\n  -q, --quiet           don\nt print status messages to stdout\n\n\n  -V, --version         Print version information and exit\n\n\n  -m REASON, --message=REASON\n\n\n                        Specify the reason for certificate revocation\n\n\n                        [deprecated]\n\n\n  -i, --id              Specify ID# of certificate to be retrieved\n\n\n                        [deprecated]\n\n\n\n\n\n\nExample:\n\n\nroot@host #\n osg-cert-revoke -i \n999\n -m \nTesting host cert revocation\n -k privatekeyfile -c usercertfile\n\n\n\n\n\nTest Mode\n\n\nThe scripts have two modes of execution.\n\n\nIn the normal mode of execution, the script connects to the production server and generated certificates are from default OSG CA.\n\n\nIf the user provides a \n-T\n parameter on the command-line, the scripts connect to the OIM-ITB server and any generated certificates are issued by the OSG test CAs. This mode is intended for testing and training. The resulting certificates are not usable in a production environment.\n\n\nCurrent Limitations and Bugs\n\n\n\n\nNote that Common Names (CNs) are limited to 64 characters. This is a limitation of OpenSSL and the PKI standard. For details see \nOSGPKI-252\n.", 
            "title": "Certificate Tools"
        }, 
        {
            "location": "/common/pki-cli/#osg-pki-command-line-clients", 
            "text": "", 
            "title": "OSG PKI Command Line Clients"
        }, 
        {
            "location": "/common/pki-cli/#overview", 
            "text": "The OSG PKI Command Line Clients provide a command-line interface for requesting and issuing host certificates from the OSG PKI. They complement the  OIM Web Interface .", 
            "title": "Overview"
        }, 
        {
            "location": "/common/pki-cli/#prerequisites", 
            "text": "If you have not already done so, you need to  configure the OSG software repositories .", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/common/pki-cli/#installation", 
            "text": "The command-line scripts have been packaged as an RPM and are available from the OSG repositories.  To install the RPM, run:  root@host #  yum install osg-pki-tools", 
            "title": "Installation"
        }, 
        {
            "location": "/common/pki-cli/#usage", 
            "text": "", 
            "title": "Usage"
        }, 
        {
            "location": "/common/pki-cli/#configuration-files", 
            "text": "This configuration file contains information divided into two sections for testing and production.  Configuration variables include:   Request URL  Approve URL  Retrieve URL  Host URL   These parameters are used as input to the script depending upon the mode of execution of the script (test or OIM). The command-line utilities check for configuration files in the following order:   $HOME/.osg-pki/OSG_PKI.ini  ./pki-clients.ini  /etc/pki-clients.ini", 
            "title": "Configuration Files"
        }, 
        {
            "location": "/common/pki-cli/#osg-cert-request", 
            "text": "Sends a request for a host certificate.  This script generates a private key and submits a request for a certificate to the OSG PKI. The request will be approved by an appropriate Grid Admin. You will receive an email when this approval has been completed containing directions on how to run  osg-cert-retreive  to retrieve the certificate. It works in two modes:   CSR is provided by the user: the CSR provided is sent to the OIM.  CSR is not provided by the user: the script generates a private key for the user. Writes it to default key file name or the one specified by  -o .   This script:   Generates a new host private key and CSR (the only important part of CSR is  CN= HOSTNAME  component).  Saves the host private key to disk (as specified by the user).  Authenticates to OIM and posts the CSR as a request to OIM.  Returns the request ID to the user.   If the user provides the CSR, then this script would just send the same CSR to OIM.  Inputs:   fully-qualified hostname  filename to store private key (optional; default is  ./hostkey.pem ).  path to user's certificate (optional: default is path specified by  $X509_USER_CERT  environment variable, then  ~/.globus/usercert.pem ).  path to user's private key (optional: default is path specified by  $X509_USER_KEY  environment variable, then  ~/.globus/userkey.pem ).  Passphrase for user's private key (via non-echoing prompt).  User needs to provide VO name if the requested hostname has multiple VO's assigned.   Outputs:   Private key, to filename specified by  -o  or  ./hostkey.pem  by default.  Request Id, to  stdout .   root@host #  osg-cert-request --help Usage: osg-cert-request [options]  Options:    -h, --help            show this help message and exit    -c CSR, --csr=CSR     Specify CSR name (default = gennew.csr)    -o OUTPUT KEYFILE, --outkeyfile=OUTPUT KEYFILE                          Specify the output filename for the retrieved user certificate.                          Default is ./hostkey.pem    -v VO name, --vo=VO name                          Specify the VO for the host request    -y CC LIST, --cc=CC LIST                          Specify the CC list(the email id s to be CCed).                          Separate values by  ,    -m COMMENT, --comment=COMMENT                          The comment to be added to the request    -H CN, --hostname=CN  Specify a hostname for CSR (FQDN)    -a HOSTNAME, --altname=HOSTNAME                          Specify an alternative hostname for the CSR (FQDN). May be used more than once    -e EMAIL, --email=EMAIL                          Email address to receive certificate    -n NAME, --name=NAME  Name of user receiving certificate    -p PHONE, --phone=PHONE                          Phone number of user receiving certificate    -t TIMEOUT, --timeout=TIMEOUT                          Specify the timeout in minutes    -T, --test            Run in test mode    -q, --quiet           don t print status messages to stdout    -d WRITE_DIRECTORY, --directory=WRITE_DIRECTORY                          Write the output files to this directory    -V, --version         Print version information and exit", 
            "title": "osg-cert-request"
        }, 
        {
            "location": "/common/pki-cli/#examples", 
            "text": "OSG generates the key pair for the request.  root@host #  osg-cert-request -t hostname.domain.com -e emailaddress@domain.com -n  Your Name  -p  9999999999  -y  xyz@domain.com,abc@domain.com  -m  This is my comment   If you want to request a service certificate, you need to escape backslash for service name inside CN like following.  root@host #  osg-cert-request -t hostname.domain.com -e emailaddress@domain.com -n  Your Name  -p  9999999999  -y  rsv\\/xyz@domain.com  -m  This is my comment   You can create your CSR on your target hosts using tools such as  openssl .  root@host #   umask   077 ;  openssl req -new -newkey rsa:2048 -nodes -keyout hostkey.pem -subj  /CN=osg-ce.example.edu  -out csr.pem  Note that the DN will be overriden by the OSG PKI except for the CN component.  Submitting the request:  root@host #  osg-cert-request -t hostname.domain.com -e emailaddress@domain.com -n  Your Name  -p  9999999999  -y  xyz@domain.com,abc@domain.com  -m  This is my comment  --csr csr.pem", 
            "title": "Examples."
        }, 
        {
            "location": "/common/pki-cli/#osg-cert-retrieve", 
            "text": "Retrieve a certificate (host or user) from OIM given a request Id. Typically you will run this script after submitting a request with  osg-cert-request  and receiving an email telling you your certificate has been approved.  You can also use this script to retrieve other certificates that have been previously issued (assuming you know their request ID number).  Since certificates are public, no authentication of the user is required.  This script:   Accepts a request Id from the user  Connects to OIM and requests the certificate identified by the request ID  Writes the certificate to disk (as specified by the user)   Inputs:   Request ID  Filename to store certificate (optional: default is  ./hostcert.pem ).   Outputs:   Host certificate as PEM, to filename specified or  ./hostcert.pem .   root@host #  osg-cert-retrieve --help Usage: osg-cert-retrieve [options]  Request ID  Usage: osg-cert-retrieve -h/--help [for detailed explanations of options]  Options:    -h, --help            show this help message and exit    -o ID, --certfile=ID  Specify the output filename for the retrieved user                          certificate . Default is ./hostcert.pem    -T, --test            Run in test mode    -t TIMEOUT, --timeout=TIMEOUT                          Specify the timeout in minutes    -q, --quiet           don t print status messages to stdout    -d WRITE_DIRECTORY, --directory=WRITE_DIRECTORY                          Write the output files to this directory    -V, --version         Print version information and exit    -i, --id              Specify ID# of certificate to be retrieved                          [deprecated]   Example:  root@host #  osg-cert-retrieve -i  555", 
            "title": "osg-cert-retrieve"
        }, 
        {
            "location": "/common/pki-cli/#osg-gridadmin-cert-request", 
            "text": "Request and retrieve multliple host certificates from OIM. Authenticates to OIM and is only for use by Grid Admins for certificates they are authorized to approve. This script is only supported with all hosts being in the same domain (so we ensure they go to the same Grid Admin). The certificates are stored with the format of  hostname-requestid.pem  (i.e. the id generated from the request for the certificate). The key is stored as  hostname-serial-key.pem .  This script does the following in the process of acquiring certificates for the hostnames specified:   Reads a list of fully-qualified hostnames from a file specified by the user.  For each hostname:  Generates a new private key and CSR.  Only important part of CSR is  CN= HOSTNAME  component.  Writes the private key to a file with filename:  PREFIX / HOSTNAME -key.pem .  Prompts the user for their private key pass phrase (the pass phrase is cached so user is not re-prompted).  Authenticates to OIM and posts the CSRs as a single request to OIM.  Request ID is returned and subsequently used.  Authenticates to OIM and approves the request.  Waits one minute for request to be processed by OIM.  Connects to OIM and attempts to retrieve certificates.  Writes out any certificates it retrieves with filename of  PREFIX / HOSTNAME - ID .pem .  If all certificates have been retrieved, exits loop.  Wait 5 seconds and proceeds with the next repeat.     Inputs:   filename of list of hostnames.  prefix path in which to write private keys and certificates (default:  . .)  path to user's certificate (optional: default is path specified by  $X509_USER_CERT  environment variable, then  ~/.globus/usercert.pem ).  path to user's private key (optional, default is path specified by  $X509_USER_KEY  environment variable, then  ~/.globus/userkey.pem ).  Passphrase for user's private key via non-echoing prompt.   Outputs:   N  host certificates in PEM format.  N  private keys in PEM format.   root@host #  osg-gridadmin-cert-request --help Usage: osg-gridadmin-cert-request [options] arg  Usage: osg-gridadmin-cert-request -h/--help [for detailed explanations of options]  Options:    -h, --help            show this help message and exit    -k PKEY, --pkey=PKEY  Specify Requestor s private key (PEM Format). If not                          specifiedwill take the value of X509_USER_KEY or                          $ HOME/.globus/userkey.pem   -c CERT, --cert=CERT  Specify Requestor s certificate (PEM Format). If not                          specified, will take the value of X509_USER_CERT or                          $ HOME/.globus/usercert.pem   -a HOSTNAME, --altname=HOSTNAME                          Specify an alternative hostname for CSR (FQDN). May be                          used more than once and if specified, -f/--hostfile                          will be ignored    -v VO name, --vo=VO name                          Specify the VO for the host request    -y CC List, --cc=CC List                          Specify the CC list(the email id s to be                          CCed).Separate values by  ,    -T, --test            Run in test mode    -t TIMEOUT, --timeout=TIMEOUT                          Specify the timeout in minutes    -q, --quiet           don t print status messages to stdout    -d WRITE_DIRECTORY, --directory=WRITE_DIRECTORY                          Write the output files to this directory    -V, --version         Print version information and exit    Hostname Options:      Use either of these options. Specify hostname as a single hostname      using -H/--hostname or specify from a file using -f/--hostfile.      -H HOSTNAME, --hostname=HOSTNAME                          Specify the hostname or service/hostname for which you                          want to request the certificate for. If specified,                          -f/--hostfile will be ignored      -f HOSTFILE, --hostfile=HOSTFILE                          Filename with one host (hostname or service/hostname                          and its optional,alternative hostnames, separated by                          spaces) per line   Examples:  root@host #  osg-gridadmin-cert-request -f filename -k privatekeyfile -c certificatefile  root@host #  osg-gridadmin-cert-request -H hostname.domain.com -k privatekeyfile -c certificatefile", 
            "title": "osg-gridadmin-cert-request"
        }, 
        {
            "location": "/common/pki-cli/#osg-user-cert-renew", 
            "text": "Sends a request for renewing a user certificate and if the certificate can be renewed, fetches and writes the renewed certificate.  The script generates request for renewing user certificate to the OIM, and if the certificate is renewed, it fetches the renewed user certificate. The user is authenticated before making such a request. If the user certificate is renewed, the user gets email notification regarding the same and the renewed certificate is saved by the name of the existing certificate suffixed by  -renewed.pem  (e.g. when we renew  my-cert.pem , the renewed certificate is named  my-cert-renewed.pem ).  Inputs:   path to user's certificate (optional: default is path specified by  $X509_USER_CERT  environment variable, then  ~/.globus/usercert.pem ).  path to user's private key (optional: default is path specified by  $X509_USER_KEY  environment variable, then  ~/.globus/userkey.pem ).  Passphrase for user's private key via non-echoing prompt.  User needs to provide VO name if the requested hostname has multiple VO's assigned   Outputs:   On Renewal, the renewed certificate is stored with the filename of the older user certificate suffixed with  -renewed.pem .  The certificate name for renewed certificate is sent to  stdout .    Note  If the retrieval of user certificate fails for some reason, the user can download the renewed certificate from OIM web interface using the following URL (where  REQID  is the request ID number for the user certificate on OIM):  https://oim.opensciencegrid.org/oim/certificateuser?id= REQID    root@host #  osg-user-cert-renew --help Usage: osg-user-cert-renew [options]  Options:    -h, --help            show this help message and exit    -k PKEY, --pkey=PKEY  Specify Requestor s private key (PEM Format). If not                          specified  will take the value of X509_USER_KEY or                          $ HOME/.globus/userkey.pem   -c CERT, --cert=CERT  Specify Requestor s certificate (PEM Format).  If not                          specified will take the value of X509_USER_CERT or                          $ HOME/.globus/usercert.pem   -T, --test            Run in test mode    -t TIMEOUT, --timeout=TIMEOUT                          Specify the timeout in minutes    -d WRITE_DIRECTORY, --directory=WRITE_DIRECTORY                          Write the output files to this directory    -q, --quiet           don t print status messages to stdout    -V, --version         Print version information and exit   Example  root@host #  osg-user-cert-renew -k userkey.pem -c usercert.pem", 
            "title": "osg-user-cert-renew"
        }, 
        {
            "location": "/common/pki-cli/#osg-user-cert-revoke", 
            "text": "Revoke a user certificate from OIM given a request ID or certificate ID. Usually the script is run when a user wants to revoke user certificate.  For revoking user certificate, user authentication is done and if the user is authorized to revoke the user certificate, the certificate is immediately revoked and an email notification is sent informing the user that the user certificate is revoked.  The script:   Accepts a Request ID or Certificate ID from the user.  Authenticates user and connects to OIM to revoke the user certificate identified by the request ID or the certificate ID.   Inputs:   Private key for the user requesting host certificate revocation.  User certificate for the user requesting host certificate revocation.  Message for requesting the user certificate revocation.  Request ID OR the certificate ID for the user certificate to be revoked.   Outputs:   Message if the revocation was successful along with the host cert request ID/certificate ID on  stdout .  Error message if the revocation was unsuccessful on  stdout .   If the user's private key and certificate are not provided, the script takes the private key and user certificate from the  ~/.globus  folder using the default names ( userkey.pem  and  usercert.pem , respectively)  root@host #  osg-user-cert-revoke --help Usage: osg-user-cert-revoke [options]  Request ID   message  Usage: osg-user-cert-revoke -h/--help [for detailed explanations of options]  Options:    -h, --help            show this help message and exit    -n, --certid          Treat the ID argument as the serial ID# for the                          certificate to be revoked    -u, --user            Certificate to be revoked is a user certificate.                          Redundant when using `osg-user-cert-revoke`.    -k PKEY, --pkey=PKEY  Specify Requestor s private key (PEM Format). If not                          specified, this takes the value of X509_USER_KEY or                          $ HOME/.globus/userkey.pem   -c CERT, --cert=CERT  Specify Requestor s certificate (PEM Format). If not                          specified, this takes the value of X509_USER_CERT or                          $ HOME/.globus/usercert.pem   -T, --test            Run in test mode    -t TIMEOUT, --timeout=TIMEOUT                          Specify the timeout in minutes    -q, --quiet           don t print status messages to stdout    -V, --version         Print version information and exit    -m REASON, --message=REASON                          Specify the reason for certificate revocation                          [deprecated]    -i, --id              Specify ID# of certificate to be retrieved                          [deprecated]   Example:  root@host #  osg-user-cert-revoke -i  999  -m  Testing user cert revocation  -k privatekeyfile -c usercertfile", 
            "title": "osg-user-cert-revoke"
        }, 
        {
            "location": "/common/pki-cli/#osg-cert-revoke", 
            "text": "Revoke a host certificate from OIM given a request ID. Usually the script is run when a user wants to revoke host/service certificate.  For revoking host certificate, user authentication is done and if the user is authorized to revoke the host certificate, the certificate is immediately revoked and an email notification is sent informing the user that the host certificate is revoked.  The script:   Accepts a request ID from the user.  Authenticates user and connects to OIM to revoke the host certificate identified by the request ID.   Inputs:   Private key for the user requesting host certificate revocation.  certificate for the user requesting host certificate revocation.  Message for requesting the host certificate revocation.  Request ID for the host certificate to be revoked.   Outputs:   Message if the revocation was successful along with the host cert request ID on  stdout .  Error message if the revocation was unsuccessful on  stdout .   If the user's private key and certificate are not provided, the script takes the private key and user certificate from the  ~/.globus  folder using the default names ( userkey.pem  and  usercert.pem , respectively).  root@host #  osg-user-cert-revoke --help Usage: osg-cert-revoke [options]  Request ID   message  Usage: osg-cert-revoke -h/--help [for detailed explanations of options]  Options:    -h, --help            show this help message and exit    -n, --certid          Treat the ID argument as the serial ID# for the                          certificate to be revoked    -u, --user            Certificate to be revoked is a user certificate.                          Redundant when using `osg-user-cert-revoke`.    -k PKEY, --pkey=PKEY  Specify Requestor s private key (PEM Format). If not                          specified, this takes the value of X509_USER_KEY or                          $ HOME/.globus/userkey.pem   -c CERT, --cert=CERT  Specify Requestor s certificate (PEM Format). If not                          specified, this takes the value of X509_USER_CERT or                          $ HOME/.globus/usercert.pem   -T, --test            Run in test mode    -t TIMEOUT, --timeout=TIMEOUT                          Specify the timeout in minutes    -q, --quiet           don t print status messages to stdout    -V, --version         Print version information and exit    -m REASON, --message=REASON                          Specify the reason for certificate revocation                          [deprecated]    -i, --id              Specify ID# of certificate to be retrieved                          [deprecated]   Example:  root@host #  osg-cert-revoke -i  999  -m  Testing host cert revocation  -k privatekeyfile -c usercertfile", 
            "title": "osg-cert-revoke"
        }, 
        {
            "location": "/common/pki-cli/#test-mode", 
            "text": "The scripts have two modes of execution.  In the normal mode of execution, the script connects to the production server and generated certificates are from default OSG CA.  If the user provides a  -T  parameter on the command-line, the scripts connect to the OIM-ITB server and any generated certificates are issued by the OSG test CAs. This mode is intended for testing and training. The resulting certificates are not usable in a production environment.", 
            "title": "Test Mode"
        }, 
        {
            "location": "/common/pki-cli/#current-limitations-and-bugs", 
            "text": "Note that Common Names (CNs) are limited to 64 characters. This is a limitation of OpenSSL and the PKI standard. For details see  OSGPKI-252 .", 
            "title": "Current Limitations and Bugs"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/", 
            "text": "Configuration with OSG-Configure\n\n\n\n\nAbout this document\n\n\nInvocation and script usage\n\n\nSyntax and layout\n\n\nConfiguration sections\n\n\nJob managers (batch systems):\n\n\nBosco\n\n\nCondor\n\n\nLSF\n\n\nPBS\n\n\nSGE\n\n\nSlurm\n\n\n\n\n\n\nMonitoring/reporting:\n\n\nGratia\n\n\nInfo Services\n\n\nRSV\n\n\nSubcluster / Resource Entry\n\n\n\n\n\n\nGateway\n\n\nLocal Settings\n\n\nMisc Services\n\n\nSite Information\n\n\nSquid\n\n\nStorage\n\n\n\n\n\n\n\n\nAbout this document\n\n\nOSG-Configure and the INI files in \n/etc/osg/config.d\n allow a high level configuration of OSG services.\nThis document outlines the settings and options found in the INI files for system administers that are installing and configuring OSG software.\n\n\nThis page gives an overview of the options for each of the sections of the configuration files that \nosg-configure\n uses.\n\n\nInvocation and script usage\n\n\nThe \nosg-configure\n script is used to process the INI files and apply changes to the system.\n\nosg-configure\n must be run as root.\n\n\nThe typical workflow of OSG-Configure is to first edit the INI files, then verify them, then apply the changes.\n\n\nTo verify the config files, run:\n\n\n[root@server] osg-configure -v\n\n\n\n\n\n\nOSG-Configure will list any errors in your configuration, usually including the section and option where the problem is.\nPotential problems are:\n\n\n\n\nRequired option not filled in\n\n\nInvalid value\n\n\nSyntax error\n\n\nInconsistencies between options\n\n\n\n\nTo apply changes, run:\n\n\n[root@server] osg-configure -c\n\n\n\n\n\n\nIf your INI files do not change, then re-running \nosg-configure -c\n will result in the same configuration as when you ran it the last time.\nThis allows you to experiment with your settings without having to worry about messing up your system.\n\n\nOSG-Configure is split up into modules. Normally, all modules are run when calling \nosg-configure\n.\nHowever, it is possible to run specific modules separately.\nTo see a list of modules, including whether they can be run separately, run:\n\n\n[root@server] osg-configure -l\n\n\n\n\n\n\nIf the module can be run separately, specify it with the \n-m \nMODULE\n option:\n\n\n[root@server] osg-configure -c -m \nMODULE\n\n\n\n\n\n\nOptions may be specified in multiple INI files, which may make it hard to determine which value OSG-Configure uses.\nYou may query the final value of an option via one of these methods:\n\n\n[root@server] osg-configure -o \nOPTION\n\n\n[root@server] osg-configure -o \nSECTION\n.\nOPTION\n\n\n\n\n\n\nLogs are written to \n/var/log/osg/osg-configure.log\n.\nIf something goes wrong, specify the \n-d\n flag to add more verbose output to \nosg-configure.log\n.\n\n\nThe rest of this document will detail what to specify in the INI files.\n\n\nConventions\n\n\nIn the tables below:\n\n\n\n\nMandatory options for a section are given in \nbold\n type. Sometime the default value may be OK and no edit required, but the variable has to be in the file.\n\n\nOptions that are not found in the default ini file are in \nitalics\n.\n\n\n\n\nSyntax and layout\n\n\nThe configuration files used by \nosg-configure\n are the one supported by Python's \nSafeConfigParser\n, similar in format to the \nINI configuration file\n used by MS Windows:\n\n\n\n\nConfig files are separated into sections, specified by a section name in square brackets (e.g. \n[Section 1]\n)\n\n\nOptions should be set using \nname = value\n pairs\n\n\nLines that begin with \n;\n or \n#\n are comments\n\n\nLong lines can be split up using continutations: each white space character can be preceded by a newline to fold/continue the field on a new line (same syntax as specified in \nemail RFC 822\n)\n\n\nVariable substitutions are supported -- \nsee below\n\n\n\n\nosg-configure\n reads and uses all of the files in \n/etc/osg/config.d\n that have a \".ini\" suffix. The files in this directory are ordered with a numeric prefix with higher numbers being applied later and thus having higher precedence (e.g. 00-foo.ini has a lower precedence than 99-local-site-settings.ini). Configuration sections and options can be specified multiple times in different files. E.g. a section called \n[PBS]\n can be given in \n20-pbs.ini\n as well as \n99-local-site-settings.ini\n.\n\n\nEach of the files are successively read and merged to create a final configuration that is then used to configure OSG software. Options and settings in files read later override the ones in previous files. This allows admins to create a file with local settings (e.g. \n99-local-site-settings.ini\n) that can be read last and which will be take precedence over the default settings in configuration files installed by various RPMs and which will not be overwritten if RPMs are updated.\n\n\nVariable substitution\n\n\nThe osg-configure parser allows variables to be defined and used in the configuration file:\nany option set in a given section can be used as a variable in that section.  Assuming that you have set an option with the name \nmyoption\n in the section, you can substitute the value of that option elsewhere in the section by referring to it as \n%(myoption)s\n.\n\n\n\n\nNote\n\n\nThe trailing \ns\n is required. Also, option names cannot have a variable subsitution in them.\n\n\n\n\nSpecial Settings\n\n\nIf a setting is set to UNAVAILABLE or DEFAULT or left blank, osg-configure will try to use a sensible default for setting if possible.\n\n\nIgnore setting\n\n\nThe \nenabled\n option, specifying whether a service is enabled or not, is a boolean but also accepts \nIgnore\n as a possible value. Using Ignore, results in the service associated with the section being ignored entirely (and any configuration is skipped). This differs from using \nFalse\n (or the \n%(disabled)s\n variable), because using \nFalse\n results in the service associated with the section being disabled. \nosg-configure\n will not change the configuration of the service if the \nenabled\n is set to \nIgnore\n.\n\n\nThis is useful, if you have a complex configuration for a given that can't be set up using the ini configuration files. You can manually configure that service by hand editing config files, manually start/stop the service and then use the \nIgnore\n setting so that \nosg-configure\n does not alter the service's configuration and status.\n\n\nConfiguration sections\n\n\nThe OSG configuration is divided into sections with each section starting with a section name in square brackets (e.g. \n[Section 1]\n). The configuration is split in multiple files and options form one section can be in more than one files.\n\n\nThe following sections give an overview of the options for each of the sections of the configuration files that \nosg-configure\n uses.\n\n\nBosco\n\n\nThis section is contained in \n/etc/osg/config.d/20-bosco.ini\n which is provided by the \nosg-configure-bosco\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nTrue\n, \nFalse\n, \nIgnore\n\n\nThis indicates whether the Bosco jobmanager is being used or not.\n\n\n\n\n\n\nusers\n\n\nString\n\n\nA comma separated string. The existing usernames on the CE for which to install Bosco and allow submissions. In order to have separate usernames per VO, for example the CMS VO to have the cms username, each user must have Bosco installed. The osg-configure service will install Bosco on each of the users listed here.\n\n\n\n\n\n\nendpoint\n\n\nString\n\n\nThe remote cluster submission host for which Bosco will submit jobs to the scheduler. This is in the form of \n, exactly as you would use to ssh into the remote cluster.\n\n\n\n\n\n\nbatch\n\n\nString\n\n\nThe type of scheduler installed on the remote cluster.\n\n\n\n\n\n\nssh_key\n\n\nString\n\n\nThe location of the ssh key, as created above.\n\n\n\n\n\n\n\n\nCondor\n\n\nThis section describes the parameters for a Condor jobmanager if it's being used in the current CE installation. If Condor is not being used, the \nenabled\n setting should be set to \nFalse\n.\n\n\nThis section is contained in \n/etc/osg/config.d/20-condor.ini\n which is provided by the \nosg-configure-condor\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nTrue\n, \nFalse\n, \nIgnore\n\n\nThis indicates whether the Condor jobmanager is being used or not.\n\n\n\n\n\n\ncondor_location\n\n\nString\n\n\nThis should be set to be directory where condor is installed. If this is set to a blank variable, DEFAULT or UNAVAILABLE, the \nosg-configure\n script will try to get this from the CONDOR_LOCATION environment variable if available otherwise it will use \n/usr\n which works for the RPM installation.\n\n\n\n\n\n\ncondor_config\n\n\nString\n\n\nThis should be set to be path where the condor_config file is located. If this is set to a blank variable, DEFAULT or UNAVAILABLE, the \nosg-configure\n script will try to get this from the CONDOR_CONFIG environment variable if available otherwise it will use \n/etc/condor/condor_config\n, the default for the RPM installation.\n\n\n\n\n\n\n\n\nLSF\n\n\nThis section describes the parameters for a LSF jobmanager if it's being used in the current CE installation. If LSF is not being used, the \nenabled\n setting should be set to \nFalse\n.\n\n\nThis section is contained in \n/etc/osg/config.d/20-lsf.ini\n which is provided by the \nosg-configure-lsf\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nTrue\n, \nFalse\n, \nIgnore\n\n\nThis indicates whether the LSF jobmanager is being used or not.\n\n\n\n\n\n\nlsf_location\n\n\nString\n\n\nThis should be set to be directory where lsf is installed\n\n\n\n\n\n\n\n\nPBS\n\n\nThis section describes the parameters for a pbs jobmanager if it's being used in the current CE installation. If PBS is not being used, the \nenabled\n setting should be set to \nFalse\n.\n\n\nThis section is contained in \n/etc/osg/config.d/20-pbs.ini\n which is provided by the \nosg-configure-pbs\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nTrue\n, \nFalse\n, \nIgnore\n\n\nThis indicates whether the PBS jobmanager is being used or not.\n\n\n\n\n\n\npbs_location\n\n\nString\n\n\nThis should be set to be directory where pbs is installed. osg-configure will try to loocation for the pbs binaries in pbs_location/bin.\n\n\n\n\n\n\naccounting_log_directory\n\n\nString\n\n\nThis setting is used to tell Gratia where to find your accounting log files, and it is required for proper accounting.\n\n\n\n\n\n\npbs_server\n\n\nString\n\n\nThis setting is optional and should point to your PBS server node if it is different from your OSG CE\n\n\n\n\n\n\n\n\nSGE\n\n\nThis section describes the parameters for a SGE jobmanager if it's being used in the current CE installation. If SGE is not being used, the \nenabled\n setting should be set to \nFalse\n.\n\n\nThis section is contained in \n/etc/osg/config.d/20-sge.ini\n which is provided by the \nosg-configure-sge\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nTrue\n, \nFalse\n, \nIgnore\n\n\nThis indicates whether the SGE jobmanager is being used or not.\n\n\n\n\n\n\nsge_root\n\n\nString\n\n\nThis should be set to be directory where sge is installed (e.g. same as \n$SGE_ROOT\n variable).\n\n\n\n\n\n\nsge_cell\n\n\nString\n\n\nThe sge_cell setting should be set to the value of $SGE_CELL for your SGE install.\n\n\n\n\n\n\ndefault_queue\n\n\nString\n\n\nThis setting determines queue that jobs should be placed in if the job description does not specify a queue.\n\n\n\n\n\n\navailable_queues\n\n\nString\n\n\nThis setting indicates which queues are available on the cluster and should be used for validation when \nvalidate_queues\n is set.\n\n\n\n\n\n\nvalidate_queues\n\n\nString\n\n\nThis setting determines whether the globus jobmanager should check the job RSL and verify that any queue specified matches a queue available on the cluster. See note.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nvalidate_queues\n:\n\nIf \navailable_queues\n is set, that list of queues will be used for\nvalidation, otherwise SGE will be queried for available queues.\n\n\n\n\nSlurm\n\n\nThis section describes the parameters for a Slurm jobmanager if it's being used in the current CE installation. If Slurm is not being used, the \nenabled\n setting should be set to \nFalse\n.\n\n\nThis section is contained in \n/etc/osg/config.d/20-slurm.ini\n which is provided by the \nosg-configure-slurm\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nTrue\n, \nFalse\n, \nIgnore\n\n\nThis indicates whether the Slurm jobmanager is being used or not.\n\n\n\n\n\n\nslurm_location\n\n\nString\n\n\nThis should be set to be directory where slurm is installed. osg-configure will try to location for the slurm binaries in slurm_location/bin.\n\n\n\n\n\n\ndb_host\n\n\nString\n\n\nHostname of the machine hosting the SLURM database. This information is needed to configure the SLURM gratia probe.\n\n\n\n\n\n\ndb_port\n\n\nString\n\n\nPort of where the SLURM database is listening. This information is needed to configure the SLURM gratia probe.\n\n\n\n\n\n\ndb_user\n\n\nString\n\n\nUsername used to access the SLURM database. This information is needed to configure the SLURM gratia probe.\n\n\n\n\n\n\ndb_pass\n\n\nString\n\n\nThe location of a file containing the password used to access the SLURM database. This information is needed to configure the SLURM gratia probe.\n\n\n\n\n\n\ndb_name\n\n\nString\n\n\nName of the SLURM database. This information is needed to configure the SLURM gratia probe.\n\n\n\n\n\n\nslurm_cluster\n\n\nString\n\n\nThe name of the Slurm cluster\n\n\n\n\n\n\n\n\nGratia\n\n\nThis section configures Gratia. If \nprobes\n is set to \nUNAVAILABLE\n, then \nosg-configure\n will use appropriate default values. If you need to specify custom reporting (e.g. a local gratia collector) in addition to the default probes, \n%(osg-jobmanager-gratia)s\n, \n%(osg-gridftp-gratia)s\n, \n%(osg-metric-gratia)s\n, \n%(itb-jobmanager-gratia)s\n, \n%(itb-gridftp-gratia)s\n, \n%(itb-metric-gratia)s\n are defined in the default configuration files to make it easier to specify the standard osg reporting.\n\n\nThis section is contained in \n/etc/osg/config.d/30-gratia.ini\n which is provided by the \nosg-configure-gratia\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nTrue\n , \nFalse\n, \nIgnore\n\n\nThis should be set to True if gratia should be configured and enabled on the installation being configured.\n\n\n\n\n\n\nresource\n\n\nString\n\n\nThis should be set to the resource name as given in the OIM registration\n\n\n\n\n\n\nprobes\n\n\nString\n\n\nThis should be set to the gratia probes that should be enabled. A probe is specified by using as \n[probe_type]:server:port\n. See note\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nprobes\n:\n\nLegal values for \nprobe_type\n are:\n\n\n\n\nmetric\n (for RSV)\n\n\njobmanager\n (for the appropriate jobmanager probe)\n\n\ngridftp\n (for the GridFTP transfer probe)\n\n\n\n\n\n\nInfo Services\n\n\nReporting to the central CE Collectors is configured in this section.  In the majority of cases, this file can be left untouched; you only need to configure this section if you wish to report to your own CE Collector instead of the ones run by OSG Operations.\n\n\nThis section is contained in \n/etc/osg/config.d/30-infoservices.ini\n, which is provided by the \nosg-configure-infoservices\n RPM. (This is for historical reasons.)\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nTrue\n, \nFalse\n, \nIgnore\n\n\nTrue if reporting should be configured and enabled\n\n\n\n\n\n\nce_collectors\n\n\nString\n\n\nThe server(s) HTCondor-CE information should be sent to. See note\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nce_collectors\n:\n\n\n\n\nSet this to \nDEFAULT\n to report to the OSG Production or ITB servers (depending on your \nSite Information\n configuration).\n\n\nSet this to \nPRODUCTION\n to report to the OSG Production servers\n\n\nSet this to \nITB\n to report to the OSG ITB servers\n\n\nOtherwise, set this to the \nhostname:port\n of a host running a \ncondor-ce-collector\n daemon\n\n\n\n\n\n\nRSV\n\n\nThis section handles the configuration and setup of the RSV services.\n\n\nThis section is contained in \n/etc/osg/config.d/30-rsv.ini\n which is provided by the \nosg-configure-rsv\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nTrue\n, \nFalse\n, \nIgnore\n\n\nThis indicates whether the rsv  service is being used or not.\n\n\n\n\n\n\nrsv_user\n\n\nString\n\n\nThis gives username that rsv will run under.  If this is blank or set to \nUNAVAILABLE\n, it will default to rsv.\n\n\n\n\n\n\ngratia_probes\n\n\nString\n\n\nThis settings indicates which rsv gratia probes should be used.  It is a list of probes separated by a comma.  Valid probes are metric, condor, pbs, lsf, sge, managedfork, hadoop-transfer, and gridftp-transfer\n\n\n\n\n\n\nce_hosts\n\n\nString\n\n\nThis option lists the serviceURI of the CEs that generic RSV CE probes should check.  This should be a list of serviceURIs (\nhostname[:port/service]\n) separated by a comma (e.g. \nmy.host,my.host2,my.host3:2812\n).\n\n\n\n\n\n\nhtcondor_ce_hosts\n\n\nString\n\n\nThis option lists the serviceURI of the HTCondor-CE-based CEs that the RSV HTCondor-CE probes should check. This should be a list of serviceURIs (\nhostname[:port/service]\n) separated by a comma (e.g. \nmy.host,my.host2,my.host3:2812\n).\n\n\n\n\n\n\ngums_hosts\n\n\nString\n\n\nThis option lists the serviceURI or FQDN of the CEs or SEs, using GUMS for authentication, that the RSV GUMS probes should check.  This should be a list of \nCE\n or \nSE\n FQDNs (and \nnot a GUMS server FQDN\n) separated by a comma (e.g. \nmy.host,my.host2,my.host3\n).\n\n\n\n\n\n\ngridftp_hosts\n\n\nString\n\n\nThis option lists the serviceURI of the gridftp servers that the RSV gridftp probes should check.  This should be a list of serviceURIs (\nhostname[:port/service]\n) separated by a comma (e.g. \nmy.host.iu.edu:2812,my.host2,my.host3\n).\n\n\n\n\n\n\ngridftp_dir\n\n\nString\n\n\nThis should be the directory that the gridftp probes should use during testing.  This defaults to \n/tmp\n if left blank or set to \nUNAVAILABLE\n.\n\n\n\n\n\n\nsrm_hosts\n\n\nString\n\n\nThis option lists the serviceURI of the srm servers that the RSV srm probes should check.  This should be a list of serviceURIs (\nhostname[:port/service]\n) separated by a comma (e.g. \nmy.host,my.host2,my.host3:8444\n).\n\n\n\n\n\n\nsrm_dir\n\n\nString\n\n\nThis should be the directory that the srm probes should use during testing.\n\n\n\n\n\n\nsrm_webservice_path\n\n\nString\n\n\nThis option gives the webservice path that SRM probes need to use along with the host:port. See note.\n\n\n\n\n\n\nservice_cert\n\n\nString\n\n\nThis option should point to the public key file (pem) for your service  certificate. If this is left blank or set to \nUNAVAILABLE\n and the \nuser_proxy\n setting is set, it will default to \n/etc/grid-security/rsvcert.pem\n\n\n\n\n\n\nservice_key\n\n\nString\n\n\nThis option should point to the private key file (pem) for your service  certificate. If this is left blank or set to \nUNAVAILABLE\n and the \nservice_cert\n setting is enabled, it will default to \n/etc/grid-security/rsvkey.pem\n .\n\n\n\n\n\n\nservice_proxy\n\n\nString\n\n\nThis should point to the location of the rsv proxy file. If this is left blank or set to \nUNAVAILABLE\n and the use_service_cert  setting is enabled, it will default to \n/tmp/rsvproxy\n.\n\n\n\n\n\n\nuser_proxy\n\n\nString\n\n\nIf you don't use a service certificate for rsv, you will need to specify a  proxy file that RSV should use in the proxy_file setting.  If this is set, then  service_cert, service_key, and service_proxy should be left blank, or set to \nUNAVAILABE\n or \nDEFAULT\n.\n\n\n\n\n\n\nenable_gratia\n\n\nTrue\n, \nFalse\n\n\nThis option will enable RSV record uploading to central RSV collector at the GOC.   This should be set to True on all OSG resources (and to False on non-OSG resources).\n\n\n\n\n\n\nsetup_rsv_nagios\n\n\nTrue\n, \nFalse\n\n\nThis option indicates whether rsv should upload results to a local  nagios server instance. This should be set to True or False.\n This plugin is provided as an experimental component, and admins are recommend \nnot to enable\n it on production resources.\n\n\n\n\n\n\nrsv_nagios_conf_file\n\n\nString\n\n\nThis option indicates the location of the rsv nagios  file to use for configuration details. This file \nneeds to be configured locally for RSV-Nagios forwarding to work\n -- see inline comments in file for more information.\n\n\n\n\n\n\ncondor_location\n\n\nString\n\n\nIf you installed Condor in a non-standard location (somewhere other than /usr, which is where the RPM puts it)  you must specify the path to the install dir here.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nsrm_webservice_path\n:\n\nFor dcache installations, this should work if left blank. However\nBestman-xrootd SEs normally use \nsrm/v2/server\n as web service path, and so\nBestman-xrootd admins will have to pass this option with the appropriate\nvalue (for example: \nsrm/v2/server\n) for the SRM probes to pass on their\nSE.\n\n\n\n\nSubcluster / Resource Entry\n\n\nSubcluster and Resource Entry configuration is for reporting about the worker resources on your site. A \nsubcluster\n is a homogeneous set of worker node hardware; a \nresource\n is a set of subcluster(s) with common capabilities that will be reported to the ATLAS AGIS system.\n\n\nAt least one Subcluster or Resource Entry section\n is required on a CE; please populate the information for all your subclusters. This information will be reported to a central collector and will be used to send GlideIns / pilot jobs to your site; having accurate information is necessary for OSG jobs to effectively use your resources.\n\n\nThis section is contained in \n/etc/osg/config.d/30-gip.ini\n which is provided by the \nosg-configure-gip\n RPM. (This is for historical reasons.)\n\n\nThis configuration uses multiple sections of the OSG configuration files:\n\n\n\n\nSubcluster*\n: options about homogeneous subclusters\n\n\nResource Entry*\n: options for specifying ATLAS queues for AGIS\n\n\n\n\nNotes for multi-CE sites.\n\n\nIf you would like to properly advertise multiple CEs per cluster, make sure that you:\n\n\n\n\nSet the value of site_name in the \"Site Information\" section to be the same for each CE.\n\n\nHave the \nexact\n same configuration values for the Subcluster* and Resource Entry* sections in each CE.\n\n\n\n\nSubcluster Configuration\n\n\nEach homogeneous set of worker node hardware is called a \nsubcluster\n. For each subcluster in your cluster, fill in the information about the worker node hardware by creating a new Subcluster section with a unique name in the following format: \n[Subcluster CHANGEME]\n, where CHANGEME is the globally unique subcluster name (yes, it must be a \nglobally\n unique name for the whole grid, not just unique to your site. Get creative.)\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nname\n\n\nString\n\n\nThe same name that is in the Section label; it should be \nglobally unique\n\n\n\n\n\n\nram_mb\n\n\nPositive Integer\n\n\nMegabytes of RAM per node\n\n\n\n\n\n\ncores_per_node\n\n\nPositive Integer\n\n\nNumber of cores per node\n\n\n\n\n\n\nallowed_vos\n\n\nComma-separated List or \n*\n\n\nThe VOs that are allowed to run jobs on this subcluster (autodetected if \n*\n). Optional on OSG 3.3\n\n\n\n\n\n\n\n\nThe following attributes are optional:\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nmax_wall_time\n\n\nPositive Integer\n\n\nMaximum wall-clock time, in minutes, that a job is allowed to run on this subcluster (the default is one day or 1440 mins)\n\n\n\n\n\n\nqueue\n\n\nString\n\n\nThe queue to which jobs should be submitted in order to run on this subcluster\n\n\n\n\n\n\nextra_transforms\n\n\nClassad\n\n\nTransformation attributes which the HTCondor Job Router should apply to incoming jobs so they can run on this subcluster\n\n\n\n\n\n\n\n\nOSG 3.4 changes:\n\n\n\n\nallowed_vos\n is mandatory\n\n\n\n\nResource Entry Configuration (ATLAS only)\n\n\nIf you are configuring a CE for the ATLAS VO, you must provide hardware information to advertise the queues that are available to AGIS. For each queue, create a new \nResource Entry\n section with a unique name in the following format: \n[Resource Entry RESOURCE]\n where RESOURCE is a globally unique resource name (it must be a \nglobally\n unique name for the whole grid, not just unique to your site). The following options are required for the \nResource Entry\n section and are used to generate the data required by AGIS:\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nname\n\n\nString\n\n\nThe same name that is in the \nResource Entry\n label; it must be \nglobally unique\n\n\n\n\n\n\nmax_wall_time\n\n\nPositive Integer\n\n\nMaximum wall-clock time, in minutes, that a job is allowed to run on this resource\n\n\n\n\n\n\nqueue\n\n\nString\n\n\nThe queue to which jobs should be submitted to run on this resource\n\n\n\n\n\n\ncpucount\n (alias \ncores_per_node\n)\n\n\nPositive Integer\n\n\nNumber of cores that a job using this resource can get\n\n\n\n\n\n\nmaxmemory\n (alias \nram_mb\n)\n\n\nPositive Integer\n\n\nMaximum amount of memory (in MB) that a job using this resource can get\n\n\n\n\n\n\nallowed_vos\n\n\nComma-separated List or \n*\n\n\nThe VOs that are allowed to run jobs on this resource (autodetected if \n*\n). Optional on OSG 3.3\n\n\n\n\n\n\n\n\nThe following attributes are optional:\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nsubclusters\n\n\nComma-separated List\n\n\nThe physical subclusters the resource entry refers to; must be defined as Subcluster sections elsewhere in the file\n\n\n\n\n\n\nvo_tag\n\n\nString\n\n\nAn arbitrary label that is added to jobs routed through this resource\n\n\n\n\n\n\n\n\nOSG 3.4 changes:\n\n\n\n\nallowed_vos\n is mandatory\n\n\n\n\nGateway\n\n\nThis section gives information about the options in the Gateway section of the configuration files. These options control the behavior of job gateways on the CE. CEs are based on HTCondor-CE, which uses \ncondor-ce\n as the gateway.\n\n\nThis section is contained in \n/etc/osg/config.d/10-gateway.ini\n which is provided by the \nosg-configure-gateway\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nhtcondor_gateway_enabled\n\n\nTrue\n, \nFalse\n\n\n(default True). True if the CE is using HTCondor-CE, False otherwise. HTCondor-CE will be configured to support enabled batch systems. RSV will use HTCondor-CE to launch remote probes.\n\n\n\n\n\n\njob_envvar_path\n\n\nString\n\n\nThe value of the PATH environment variable to put into HTCondor jobs running with HTCondor-CE. This value is ignored if not using that batch system/gateway combination.\n\n\n\n\n\n\n\n\nLocal Settings\n\n\nThis section differs from other sections in that there are no set options in this section. Rather, the options set in this section will be placed in the \nosg-local-job-environment.conf\n verbatim. The options in this section are case sensitive and the case will be preserved when they are converted to environment variables. The \nosg-local-job-environment.conf\n file gets sourced by jobs run on your cluster so any variables set in this section will appear in the environment of jobs run on your system.\n\n\nAdding a line such as \nMy_Setting = my_Value\n would result in the an environment variable called \nMy_Setting\n set to \nmy_Value\n in the job's environment. \nmy_Value\n can also be defined in terms of an environment variable (i.e \nMy_Setting = $my_Value\n) that will be evaluated on the worker node. For example, to add a variable \nMY_PATH\n set to \n/usr/local/myapp\n, you'd have the following:\n\n\n[Local Settings]\n\n\n\nMY_PATH\n \n=\n \n/usr/local/myapp\n\n\n\n\n\n\nThis section is contained in \n/etc/osg/config.d/40-localsettings.ini\n which is provided by the \nosg-configure-ce\n RPM.\n\n\nMisc Services\n\n\nThis section handles the configuration of services that do not have a dedicated section for their configuration.\n\n\nThis section is contained in \n/etc/osg/config.d/10-misc.ini\n which is provided by the \nosg-configure-misc\n RPM.\n\n\nThis section primarily deals with authentication/authorization. For information on suggested settings for your CE, see the \nauthentication section of the HTCondor-CE install documents\n.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nglexec_location\n\n\nString\n\n\nThis gives the location of the glExec installation on the worker nodes, if it is present. Can be defined in terms of an environment variable (e.g. \n$FOO\n) that will be evaluated on the worker node. If it is not installed, set this to \nUNAVAILABLE\n. glExec does not work with the \nvomsmap\n authorization method on OSG 3.3 and \nis entirely unsupported starting in OSG 3.4\n\n\n\n\n\n\ngums_host\n\n\nString\n\n\nThis setting is used to indicate the hostname of the GUMS host that should be used for authentication, if the authorization method below is set to \nxacml\n. If GUMS is not used, this should be set to \nUNAVAILABLE\n. \nGUMS is deprecated in OSG 3.4\n\n\n\n\n\n\nauthorization_method\n\n\ngridmap\n, \nxacml\n, \nlocal-gridmap\n, \nvomsmap\n\n\nThis indicates which authorization method your site uses. \nxacml\n \nis deprecated in OSG 3.4\n\n\n\n\n\n\nedit_lcmaps_db\n\n\nTrue\n, \nFalse\n\n\n(Optional, default True) If true, osg-configure will overwrite \n/etc/lcmaps.db\n to set your authorization method. The previous version will be backed up to \n/etc/lcmaps.db.pre-configure\n\n\n\n\n\n\nall_fqans\n\n\nTrue\n, \nFalse\n\n\n(Optional, default False) If true, vomsmap auth will use all VOMS FQANs of a proxy for mapping -- see \ndocumentation\n\n\n\n\n\n\ncopy_host_cert_for_service_certs\n\n\nTrue\n, \nFalse\n\n\n(Optional, default False) If true, osg-configure will create a copy or copies of your host cert and key as service certs for RSV and (on OSG 3.3) GUMS\n\n\n\n\n\n\n\n\nOSG 3.4 changes:\n\n\n\n\nglexec_location\n must be \nUNAVAILABLE\n or unset\n\n\nauthorization_method\n defaults to \nvomsmap\n\n\nauthorization_method\n will raise a warning if set to \nxacml\n\n\n\n\nSite Information\n\n\nThe settings found in the \nSite Information\n section are described below. This section is used to give information about a resource such as resource name, site sponsors, administrators, etc.\n\n\nThis section is contained in \n/etc/osg/config.d/40-siteinfo.ini\n which is provided by the \nosg-configure-ce\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ngroup\n\n\nOSG\n , \nOSG-ITB\n\n\nThis should be set to either OSG or OSG-ITB depending on whether your resource is in the OSG or OSG-ITB group. Most sites should specify OSG\n\n\n\n\n\n\nhost_name\n\n\nString\n\n\nThis should be set to be hostname of the CE that is being configured\n\n\n\n\n\n\nresource\n\n\nString\n\n\nThe resource name of this CE endpoint as registered in OIM.\n\n\n\n\n\n\nresource_group\n\n\nString\n\n\nThe resource_group of this CE as registered in OIM.\n\n\n\n\n\n\nsponsor\n\n\nString\n\n\nThis should be set to the sponsor of the resource. See note.\n\n\n\n\n\n\nsite_policy\n\n\nUrl\n\n\nThis should be a url pointing to the resource's usage policy\n\n\n\n\n\n\ncontact\n\n\nString\n\n\nThis should be the name of the resource's admin contact\n\n\n\n\n\n\nemail\n\n\nEmail address\n\n\nThis should be the email address of the admin contact for the resource\n\n\n\n\n\n\ncity\n\n\nString\n\n\nThis should be the city that the resource is located in\n\n\n\n\n\n\ncountry\n\n\nString\n\n\nThis should be two letter country code for the country that the resource is located in.\n\n\n\n\n\n\nlongitude\n\n\nNumber\n\n\nThis should be the longitude of the resource. It should be a number between -180 and 180.\n\n\n\n\n\n\nlatitude\n\n\nNumber\n\n\nThis should be the latitude of the resource. It should be a number between -90 and 90.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nsponsor\n:\n\nIf your resource has multiple sponsors, you can separate them using commas\nor specify the percentage using the following format 'osg, atlas, cms' or\n'osg:10, atlas:45, cms:45'. The percentages must add up to 100 if multiple\nsponsors are used. If you have a sponsor that is not an OSG VO, you can\nindicate this by using 'local' as the VO.\n\n\n\n\nSquid\n\n\nThis section handles the configuration and setup of the squid web caching and proxy service.\n\n\nThis section is contained in \n/etc/osg/config.d/01-squid.ini\n which is provided by the \nosg-configure-squid\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nTrue\n, \nFalse\n, \nIgnore\n\n\nThis indicates whether the squid service is being used or not.\n\n\n\n\n\n\nlocation\n\n\nString\n\n\nThis should be set to the \nhostname:port\n of the squid server.\n\n\n\n\n\n\n\n\nStorage\n\n\nThis section gives information about the options in the Storage section of the configuration file.\nSeveral of these values are constrained and need to be set in a way that is consistent with one of the OSG storage models.\nPlease review the Storage Related Parameters section of the\n\nEnvironment Variables\n\ndescription and \nSite Planning\n discussions for explanations of the various storage models and the requirements for them.\n\n\nThis section is contained in \n/etc/osg/config.d/10-storage.ini\n which is provided by the \nosg-configure-ce\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nse_available\n\n\nTrue\n, \nFalse\n\n\nThis indicates whether there is an associated SE available.\n\n\n\n\n\n\ndefault_se\n\n\nString\n\n\nIf an SE is available at your cluster, set default_se to the hostname of this SE, otherwise set default_se to UNAVAILABLE.\n\n\n\n\n\n\ngrid_dir\n\n\nString\n\n\nThis setting should point to the directory which holds the files from the OSG worker node package. See note\n\n\n\n\n\n\napp_dir\n\n\nString\n\n\nThis setting should point to the directory which contains the VO specific applications. See note\n\n\n\n\n\n\ndata_dir\n\n\nString\n\n\nThis setting should point to a directory that can be used to store and stage data in and out of the cluster. See note\n\n\n\n\n\n\nworker_node_temp\n\n\nString\n\n\nThis directory should point to a directory that can be used as scratch space on compute nodes. If not set, the default is UNAVAILABLE. See note\n\n\n\n\n\n\nsite_read\n\n\nString\n\n\nThis setting should be the location or url to a directory that can be read to stage in data via the variable \n$OSG_SITE_READ\n. This is an url if you are using a SE. If not set, the default is UNAVAILABLE\n\n\n\n\n\n\nsite_write\n\n\nString\n\n\nThis setting should be the location or url to a directory that can be write to stage out data via the variable \n$OSG_SITE_WRITE\n. This is an url if you are using a SE. If not set, the default is UNAVAILABLE\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nAll of these can be defined in terms of an environment variable\n(e.g. \n$FOO\n) that will be evaluated on the worker node.\n\n\n\n\ngrid_dir\n:\n\nIf you have installed the worker node client via RPM (the normal case) it\nshould be \n/etc/osg/wn-client\n.  If you have installed the worker node in a\nspecial location (perhaps via the worker node client tarball or via OASIS),\nit should be the location of that directory.\n\n\nThis directory will be accessed via the \n$OSG_GRID\n environment variable.\nIt should be visible on all of the compute nodes. Read access is required,\nthough worker nodes don't need write access.\n\n\napp_dir\n:\n\nThis directory will be accesed via the \n$OSG_APP\n environment variable. It\nshould be visible on both the CE and worker nodes. Only the CE needs to\nhave write access to this directory. This directory must also contain a\nsub-directory \netc/\n with 1777 permissions.\n\n\ndata_dir\n:\n\nThis directory can be accessed via the \n$OSG_DATA\n environment variable. It\nshould be readable and writable on both the CE and worker nodes.\n\n\nworker_node_temp\n:\n\nThis directory will be accessed via the \n$OSG_WN_TMP\n environment variable.\nIt should allow read and write access on a worker node and can be visible\nto just that worker node.", 
            "title": "Configuration with OSG-Configure"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#configuration-with-osg-configure", 
            "text": "About this document  Invocation and script usage  Syntax and layout  Configuration sections  Job managers (batch systems):  Bosco  Condor  LSF  PBS  SGE  Slurm    Monitoring/reporting:  Gratia  Info Services  RSV  Subcluster / Resource Entry    Gateway  Local Settings  Misc Services  Site Information  Squid  Storage", 
            "title": "Configuration with OSG-Configure"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#about-this-document", 
            "text": "OSG-Configure and the INI files in  /etc/osg/config.d  allow a high level configuration of OSG services.\nThis document outlines the settings and options found in the INI files for system administers that are installing and configuring OSG software.  This page gives an overview of the options for each of the sections of the configuration files that  osg-configure  uses.", 
            "title": "About this document"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#invocation-and-script-usage", 
            "text": "The  osg-configure  script is used to process the INI files and apply changes to the system. osg-configure  must be run as root.  The typical workflow of OSG-Configure is to first edit the INI files, then verify them, then apply the changes.  To verify the config files, run:  [root@server] osg-configure -v   OSG-Configure will list any errors in your configuration, usually including the section and option where the problem is.\nPotential problems are:   Required option not filled in  Invalid value  Syntax error  Inconsistencies between options   To apply changes, run:  [root@server] osg-configure -c   If your INI files do not change, then re-running  osg-configure -c  will result in the same configuration as when you ran it the last time.\nThis allows you to experiment with your settings without having to worry about messing up your system.  OSG-Configure is split up into modules. Normally, all modules are run when calling  osg-configure .\nHowever, it is possible to run specific modules separately.\nTo see a list of modules, including whether they can be run separately, run:  [root@server] osg-configure -l   If the module can be run separately, specify it with the  -m  MODULE  option:  [root@server] osg-configure -c -m  MODULE   Options may be specified in multiple INI files, which may make it hard to determine which value OSG-Configure uses.\nYou may query the final value of an option via one of these methods:  [root@server] osg-configure -o  OPTION  [root@server] osg-configure -o  SECTION . OPTION   Logs are written to  /var/log/osg/osg-configure.log .\nIf something goes wrong, specify the  -d  flag to add more verbose output to  osg-configure.log .  The rest of this document will detail what to specify in the INI files.", 
            "title": "Invocation and script usage"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#conventions", 
            "text": "In the tables below:   Mandatory options for a section are given in  bold  type. Sometime the default value may be OK and no edit required, but the variable has to be in the file.  Options that are not found in the default ini file are in  italics .", 
            "title": "Conventions"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#syntax-and-layout", 
            "text": "The configuration files used by  osg-configure  are the one supported by Python's  SafeConfigParser , similar in format to the  INI configuration file  used by MS Windows:   Config files are separated into sections, specified by a section name in square brackets (e.g.  [Section 1] )  Options should be set using  name = value  pairs  Lines that begin with  ;  or  #  are comments  Long lines can be split up using continutations: each white space character can be preceded by a newline to fold/continue the field on a new line (same syntax as specified in  email RFC 822 )  Variable substitutions are supported --  see below   osg-configure  reads and uses all of the files in  /etc/osg/config.d  that have a \".ini\" suffix. The files in this directory are ordered with a numeric prefix with higher numbers being applied later and thus having higher precedence (e.g. 00-foo.ini has a lower precedence than 99-local-site-settings.ini). Configuration sections and options can be specified multiple times in different files. E.g. a section called  [PBS]  can be given in  20-pbs.ini  as well as  99-local-site-settings.ini .  Each of the files are successively read and merged to create a final configuration that is then used to configure OSG software. Options and settings in files read later override the ones in previous files. This allows admins to create a file with local settings (e.g.  99-local-site-settings.ini ) that can be read last and which will be take precedence over the default settings in configuration files installed by various RPMs and which will not be overwritten if RPMs are updated.", 
            "title": "Syntax and layout"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#variable-substitution", 
            "text": "The osg-configure parser allows variables to be defined and used in the configuration file:\nany option set in a given section can be used as a variable in that section.  Assuming that you have set an option with the name  myoption  in the section, you can substitute the value of that option elsewhere in the section by referring to it as  %(myoption)s .   Note  The trailing  s  is required. Also, option names cannot have a variable subsitution in them.", 
            "title": "Variable substitution"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#special-settings", 
            "text": "If a setting is set to UNAVAILABLE or DEFAULT or left blank, osg-configure will try to use a sensible default for setting if possible.", 
            "title": "Special Settings"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#ignore-setting", 
            "text": "The  enabled  option, specifying whether a service is enabled or not, is a boolean but also accepts  Ignore  as a possible value. Using Ignore, results in the service associated with the section being ignored entirely (and any configuration is skipped). This differs from using  False  (or the  %(disabled)s  variable), because using  False  results in the service associated with the section being disabled.  osg-configure  will not change the configuration of the service if the  enabled  is set to  Ignore .  This is useful, if you have a complex configuration for a given that can't be set up using the ini configuration files. You can manually configure that service by hand editing config files, manually start/stop the service and then use the  Ignore  setting so that  osg-configure  does not alter the service's configuration and status.", 
            "title": "Ignore setting"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#configuration-sections", 
            "text": "The OSG configuration is divided into sections with each section starting with a section name in square brackets (e.g.  [Section 1] ). The configuration is split in multiple files and options form one section can be in more than one files.  The following sections give an overview of the options for each of the sections of the configuration files that  osg-configure  uses.", 
            "title": "Configuration sections"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#bosco", 
            "text": "This section is contained in  /etc/osg/config.d/20-bosco.ini  which is provided by the  osg-configure-bosco  RPM.     Option  Values Accepted  Explanation      enabled  True ,  False ,  Ignore  This indicates whether the Bosco jobmanager is being used or not.    users  String  A comma separated string. The existing usernames on the CE for which to install Bosco and allow submissions. In order to have separate usernames per VO, for example the CMS VO to have the cms username, each user must have Bosco installed. The osg-configure service will install Bosco on each of the users listed here.    endpoint  String  The remote cluster submission host for which Bosco will submit jobs to the scheduler. This is in the form of  , exactly as you would use to ssh into the remote cluster.    batch  String  The type of scheduler installed on the remote cluster.    ssh_key  String  The location of the ssh key, as created above.", 
            "title": "Bosco"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#condor", 
            "text": "This section describes the parameters for a Condor jobmanager if it's being used in the current CE installation. If Condor is not being used, the  enabled  setting should be set to  False .  This section is contained in  /etc/osg/config.d/20-condor.ini  which is provided by the  osg-configure-condor  RPM.     Option  Values Accepted  Explanation      enabled  True ,  False ,  Ignore  This indicates whether the Condor jobmanager is being used or not.    condor_location  String  This should be set to be directory where condor is installed. If this is set to a blank variable, DEFAULT or UNAVAILABLE, the  osg-configure  script will try to get this from the CONDOR_LOCATION environment variable if available otherwise it will use  /usr  which works for the RPM installation.    condor_config  String  This should be set to be path where the condor_config file is located. If this is set to a blank variable, DEFAULT or UNAVAILABLE, the  osg-configure  script will try to get this from the CONDOR_CONFIG environment variable if available otherwise it will use  /etc/condor/condor_config , the default for the RPM installation.", 
            "title": "Condor"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#lsf", 
            "text": "This section describes the parameters for a LSF jobmanager if it's being used in the current CE installation. If LSF is not being used, the  enabled  setting should be set to  False .  This section is contained in  /etc/osg/config.d/20-lsf.ini  which is provided by the  osg-configure-lsf  RPM.     Option  Values Accepted  Explanation      enabled  True ,  False ,  Ignore  This indicates whether the LSF jobmanager is being used or not.    lsf_location  String  This should be set to be directory where lsf is installed", 
            "title": "LSF"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#pbs", 
            "text": "This section describes the parameters for a pbs jobmanager if it's being used in the current CE installation. If PBS is not being used, the  enabled  setting should be set to  False .  This section is contained in  /etc/osg/config.d/20-pbs.ini  which is provided by the  osg-configure-pbs  RPM.     Option  Values Accepted  Explanation      enabled  True ,  False ,  Ignore  This indicates whether the PBS jobmanager is being used or not.    pbs_location  String  This should be set to be directory where pbs is installed. osg-configure will try to loocation for the pbs binaries in pbs_location/bin.    accounting_log_directory  String  This setting is used to tell Gratia where to find your accounting log files, and it is required for proper accounting.    pbs_server  String  This setting is optional and should point to your PBS server node if it is different from your OSG CE", 
            "title": "PBS"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#sge", 
            "text": "This section describes the parameters for a SGE jobmanager if it's being used in the current CE installation. If SGE is not being used, the  enabled  setting should be set to  False .  This section is contained in  /etc/osg/config.d/20-sge.ini  which is provided by the  osg-configure-sge  RPM.     Option  Values Accepted  Explanation      enabled  True ,  False ,  Ignore  This indicates whether the SGE jobmanager is being used or not.    sge_root  String  This should be set to be directory where sge is installed (e.g. same as  $SGE_ROOT  variable).    sge_cell  String  The sge_cell setting should be set to the value of $SGE_CELL for your SGE install.    default_queue  String  This setting determines queue that jobs should be placed in if the job description does not specify a queue.    available_queues  String  This setting indicates which queues are available on the cluster and should be used for validation when  validate_queues  is set.    validate_queues  String  This setting determines whether the globus jobmanager should check the job RSL and verify that any queue specified matches a queue available on the cluster. See note.      Note  validate_queues : \nIf  available_queues  is set, that list of queues will be used for\nvalidation, otherwise SGE will be queried for available queues.", 
            "title": "SGE"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#slurm", 
            "text": "This section describes the parameters for a Slurm jobmanager if it's being used in the current CE installation. If Slurm is not being used, the  enabled  setting should be set to  False .  This section is contained in  /etc/osg/config.d/20-slurm.ini  which is provided by the  osg-configure-slurm  RPM.     Option  Values Accepted  Explanation      enabled  True ,  False ,  Ignore  This indicates whether the Slurm jobmanager is being used or not.    slurm_location  String  This should be set to be directory where slurm is installed. osg-configure will try to location for the slurm binaries in slurm_location/bin.    db_host  String  Hostname of the machine hosting the SLURM database. This information is needed to configure the SLURM gratia probe.    db_port  String  Port of where the SLURM database is listening. This information is needed to configure the SLURM gratia probe.    db_user  String  Username used to access the SLURM database. This information is needed to configure the SLURM gratia probe.    db_pass  String  The location of a file containing the password used to access the SLURM database. This information is needed to configure the SLURM gratia probe.    db_name  String  Name of the SLURM database. This information is needed to configure the SLURM gratia probe.    slurm_cluster  String  The name of the Slurm cluster", 
            "title": "Slurm"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#gratia", 
            "text": "This section configures Gratia. If  probes  is set to  UNAVAILABLE , then  osg-configure  will use appropriate default values. If you need to specify custom reporting (e.g. a local gratia collector) in addition to the default probes,  %(osg-jobmanager-gratia)s ,  %(osg-gridftp-gratia)s ,  %(osg-metric-gratia)s ,  %(itb-jobmanager-gratia)s ,  %(itb-gridftp-gratia)s ,  %(itb-metric-gratia)s  are defined in the default configuration files to make it easier to specify the standard osg reporting.  This section is contained in  /etc/osg/config.d/30-gratia.ini  which is provided by the  osg-configure-gratia  RPM.     Option  Values Accepted  Explanation      enabled  True  ,  False ,  Ignore  This should be set to True if gratia should be configured and enabled on the installation being configured.    resource  String  This should be set to the resource name as given in the OIM registration    probes  String  This should be set to the gratia probes that should be enabled. A probe is specified by using as  [probe_type]:server:port . See note      Note  probes : \nLegal values for  probe_type  are:   metric  (for RSV)  jobmanager  (for the appropriate jobmanager probe)  gridftp  (for the GridFTP transfer probe)", 
            "title": "Gratia"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#info-services", 
            "text": "Reporting to the central CE Collectors is configured in this section.  In the majority of cases, this file can be left untouched; you only need to configure this section if you wish to report to your own CE Collector instead of the ones run by OSG Operations.  This section is contained in  /etc/osg/config.d/30-infoservices.ini , which is provided by the  osg-configure-infoservices  RPM. (This is for historical reasons.)     Option  Values Accepted  Explanation      enabled  True ,  False ,  Ignore  True if reporting should be configured and enabled    ce_collectors  String  The server(s) HTCondor-CE information should be sent to. See note      Note  ce_collectors :   Set this to  DEFAULT  to report to the OSG Production or ITB servers (depending on your  Site Information  configuration).  Set this to  PRODUCTION  to report to the OSG Production servers  Set this to  ITB  to report to the OSG ITB servers  Otherwise, set this to the  hostname:port  of a host running a  condor-ce-collector  daemon", 
            "title": "Info Services"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#rsv", 
            "text": "This section handles the configuration and setup of the RSV services.  This section is contained in  /etc/osg/config.d/30-rsv.ini  which is provided by the  osg-configure-rsv  RPM.     Option  Values Accepted  Explanation      enabled  True ,  False ,  Ignore  This indicates whether the rsv  service is being used or not.    rsv_user  String  This gives username that rsv will run under.  If this is blank or set to  UNAVAILABLE , it will default to rsv.    gratia_probes  String  This settings indicates which rsv gratia probes should be used.  It is a list of probes separated by a comma.  Valid probes are metric, condor, pbs, lsf, sge, managedfork, hadoop-transfer, and gridftp-transfer    ce_hosts  String  This option lists the serviceURI of the CEs that generic RSV CE probes should check.  This should be a list of serviceURIs ( hostname[:port/service] ) separated by a comma (e.g.  my.host,my.host2,my.host3:2812 ).    htcondor_ce_hosts  String  This option lists the serviceURI of the HTCondor-CE-based CEs that the RSV HTCondor-CE probes should check. This should be a list of serviceURIs ( hostname[:port/service] ) separated by a comma (e.g.  my.host,my.host2,my.host3:2812 ).    gums_hosts  String  This option lists the serviceURI or FQDN of the CEs or SEs, using GUMS for authentication, that the RSV GUMS probes should check.  This should be a list of  CE  or  SE  FQDNs (and  not a GUMS server FQDN ) separated by a comma (e.g.  my.host,my.host2,my.host3 ).    gridftp_hosts  String  This option lists the serviceURI of the gridftp servers that the RSV gridftp probes should check.  This should be a list of serviceURIs ( hostname[:port/service] ) separated by a comma (e.g.  my.host.iu.edu:2812,my.host2,my.host3 ).    gridftp_dir  String  This should be the directory that the gridftp probes should use during testing.  This defaults to  /tmp  if left blank or set to  UNAVAILABLE .    srm_hosts  String  This option lists the serviceURI of the srm servers that the RSV srm probes should check.  This should be a list of serviceURIs ( hostname[:port/service] ) separated by a comma (e.g.  my.host,my.host2,my.host3:8444 ).    srm_dir  String  This should be the directory that the srm probes should use during testing.    srm_webservice_path  String  This option gives the webservice path that SRM probes need to use along with the host:port. See note.    service_cert  String  This option should point to the public key file (pem) for your service  certificate. If this is left blank or set to  UNAVAILABLE  and the  user_proxy  setting is set, it will default to  /etc/grid-security/rsvcert.pem    service_key  String  This option should point to the private key file (pem) for your service  certificate. If this is left blank or set to  UNAVAILABLE  and the  service_cert  setting is enabled, it will default to  /etc/grid-security/rsvkey.pem  .    service_proxy  String  This should point to the location of the rsv proxy file. If this is left blank or set to  UNAVAILABLE  and the use_service_cert  setting is enabled, it will default to  /tmp/rsvproxy .    user_proxy  String  If you don't use a service certificate for rsv, you will need to specify a  proxy file that RSV should use in the proxy_file setting.  If this is set, then  service_cert, service_key, and service_proxy should be left blank, or set to  UNAVAILABE  or  DEFAULT .    enable_gratia  True ,  False  This option will enable RSV record uploading to central RSV collector at the GOC.   This should be set to True on all OSG resources (and to False on non-OSG resources).    setup_rsv_nagios  True ,  False  This option indicates whether rsv should upload results to a local  nagios server instance. This should be set to True or False.  This plugin is provided as an experimental component, and admins are recommend  not to enable  it on production resources.    rsv_nagios_conf_file  String  This option indicates the location of the rsv nagios  file to use for configuration details. This file  needs to be configured locally for RSV-Nagios forwarding to work  -- see inline comments in file for more information.    condor_location  String  If you installed Condor in a non-standard location (somewhere other than /usr, which is where the RPM puts it)  you must specify the path to the install dir here.      Note  srm_webservice_path : \nFor dcache installations, this should work if left blank. However\nBestman-xrootd SEs normally use  srm/v2/server  as web service path, and so\nBestman-xrootd admins will have to pass this option with the appropriate\nvalue (for example:  srm/v2/server ) for the SRM probes to pass on their\nSE.", 
            "title": "RSV"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#subcluster-resource-entry", 
            "text": "Subcluster and Resource Entry configuration is for reporting about the worker resources on your site. A  subcluster  is a homogeneous set of worker node hardware; a  resource  is a set of subcluster(s) with common capabilities that will be reported to the ATLAS AGIS system.  At least one Subcluster or Resource Entry section  is required on a CE; please populate the information for all your subclusters. This information will be reported to a central collector and will be used to send GlideIns / pilot jobs to your site; having accurate information is necessary for OSG jobs to effectively use your resources.  This section is contained in  /etc/osg/config.d/30-gip.ini  which is provided by the  osg-configure-gip  RPM. (This is for historical reasons.)  This configuration uses multiple sections of the OSG configuration files:   Subcluster* : options about homogeneous subclusters  Resource Entry* : options for specifying ATLAS queues for AGIS", 
            "title": "Subcluster / Resource Entry"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#notes-for-multi-ce-sites", 
            "text": "If you would like to properly advertise multiple CEs per cluster, make sure that you:   Set the value of site_name in the \"Site Information\" section to be the same for each CE.  Have the  exact  same configuration values for the Subcluster* and Resource Entry* sections in each CE.", 
            "title": "Notes for multi-CE sites."
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#subcluster-configuration", 
            "text": "Each homogeneous set of worker node hardware is called a  subcluster . For each subcluster in your cluster, fill in the information about the worker node hardware by creating a new Subcluster section with a unique name in the following format:  [Subcluster CHANGEME] , where CHANGEME is the globally unique subcluster name (yes, it must be a  globally  unique name for the whole grid, not just unique to your site. Get creative.)     Option  Values Accepted  Explanation      name  String  The same name that is in the Section label; it should be  globally unique    ram_mb  Positive Integer  Megabytes of RAM per node    cores_per_node  Positive Integer  Number of cores per node    allowed_vos  Comma-separated List or  *  The VOs that are allowed to run jobs on this subcluster (autodetected if  * ). Optional on OSG 3.3     The following attributes are optional:     Option  Values Accepted  Explanation      max_wall_time  Positive Integer  Maximum wall-clock time, in minutes, that a job is allowed to run on this subcluster (the default is one day or 1440 mins)    queue  String  The queue to which jobs should be submitted in order to run on this subcluster    extra_transforms  Classad  Transformation attributes which the HTCondor Job Router should apply to incoming jobs so they can run on this subcluster     OSG 3.4 changes:   allowed_vos  is mandatory", 
            "title": "Subcluster Configuration"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#resource-entry-configuration-atlas-only", 
            "text": "If you are configuring a CE for the ATLAS VO, you must provide hardware information to advertise the queues that are available to AGIS. For each queue, create a new  Resource Entry  section with a unique name in the following format:  [Resource Entry RESOURCE]  where RESOURCE is a globally unique resource name (it must be a  globally  unique name for the whole grid, not just unique to your site). The following options are required for the  Resource Entry  section and are used to generate the data required by AGIS:     Option  Values Accepted  Explanation      name  String  The same name that is in the  Resource Entry  label; it must be  globally unique    max_wall_time  Positive Integer  Maximum wall-clock time, in minutes, that a job is allowed to run on this resource    queue  String  The queue to which jobs should be submitted to run on this resource    cpucount  (alias  cores_per_node )  Positive Integer  Number of cores that a job using this resource can get    maxmemory  (alias  ram_mb )  Positive Integer  Maximum amount of memory (in MB) that a job using this resource can get    allowed_vos  Comma-separated List or  *  The VOs that are allowed to run jobs on this resource (autodetected if  * ). Optional on OSG 3.3     The following attributes are optional:     Option  Values Accepted  Explanation      subclusters  Comma-separated List  The physical subclusters the resource entry refers to; must be defined as Subcluster sections elsewhere in the file    vo_tag  String  An arbitrary label that is added to jobs routed through this resource     OSG 3.4 changes:   allowed_vos  is mandatory", 
            "title": "Resource Entry Configuration (ATLAS only)"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#gateway", 
            "text": "This section gives information about the options in the Gateway section of the configuration files. These options control the behavior of job gateways on the CE. CEs are based on HTCondor-CE, which uses  condor-ce  as the gateway.  This section is contained in  /etc/osg/config.d/10-gateway.ini  which is provided by the  osg-configure-gateway  RPM.     Option  Values Accepted  Explanation      htcondor_gateway_enabled  True ,  False  (default True). True if the CE is using HTCondor-CE, False otherwise. HTCondor-CE will be configured to support enabled batch systems. RSV will use HTCondor-CE to launch remote probes.    job_envvar_path  String  The value of the PATH environment variable to put into HTCondor jobs running with HTCondor-CE. This value is ignored if not using that batch system/gateway combination.", 
            "title": "Gateway"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#local-settings", 
            "text": "This section differs from other sections in that there are no set options in this section. Rather, the options set in this section will be placed in the  osg-local-job-environment.conf  verbatim. The options in this section are case sensitive and the case will be preserved when they are converted to environment variables. The  osg-local-job-environment.conf  file gets sourced by jobs run on your cluster so any variables set in this section will appear in the environment of jobs run on your system.  Adding a line such as  My_Setting = my_Value  would result in the an environment variable called  My_Setting  set to  my_Value  in the job's environment.  my_Value  can also be defined in terms of an environment variable (i.e  My_Setting = $my_Value ) that will be evaluated on the worker node. For example, to add a variable  MY_PATH  set to  /usr/local/myapp , you'd have the following:  [Local Settings]  MY_PATH   =   /usr/local/myapp   This section is contained in  /etc/osg/config.d/40-localsettings.ini  which is provided by the  osg-configure-ce  RPM.", 
            "title": "Local Settings"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#misc-services", 
            "text": "This section handles the configuration of services that do not have a dedicated section for their configuration.  This section is contained in  /etc/osg/config.d/10-misc.ini  which is provided by the  osg-configure-misc  RPM.  This section primarily deals with authentication/authorization. For information on suggested settings for your CE, see the  authentication section of the HTCondor-CE install documents .     Option  Values Accepted  Explanation      glexec_location  String  This gives the location of the glExec installation on the worker nodes, if it is present. Can be defined in terms of an environment variable (e.g.  $FOO ) that will be evaluated on the worker node. If it is not installed, set this to  UNAVAILABLE . glExec does not work with the  vomsmap  authorization method on OSG 3.3 and  is entirely unsupported starting in OSG 3.4    gums_host  String  This setting is used to indicate the hostname of the GUMS host that should be used for authentication, if the authorization method below is set to  xacml . If GUMS is not used, this should be set to  UNAVAILABLE .  GUMS is deprecated in OSG 3.4    authorization_method  gridmap ,  xacml ,  local-gridmap ,  vomsmap  This indicates which authorization method your site uses.  xacml   is deprecated in OSG 3.4    edit_lcmaps_db  True ,  False  (Optional, default True) If true, osg-configure will overwrite  /etc/lcmaps.db  to set your authorization method. The previous version will be backed up to  /etc/lcmaps.db.pre-configure    all_fqans  True ,  False  (Optional, default False) If true, vomsmap auth will use all VOMS FQANs of a proxy for mapping -- see  documentation    copy_host_cert_for_service_certs  True ,  False  (Optional, default False) If true, osg-configure will create a copy or copies of your host cert and key as service certs for RSV and (on OSG 3.3) GUMS     OSG 3.4 changes:   glexec_location  must be  UNAVAILABLE  or unset  authorization_method  defaults to  vomsmap  authorization_method  will raise a warning if set to  xacml", 
            "title": "Misc Services"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#site-information", 
            "text": "The settings found in the  Site Information  section are described below. This section is used to give information about a resource such as resource name, site sponsors, administrators, etc.  This section is contained in  /etc/osg/config.d/40-siteinfo.ini  which is provided by the  osg-configure-ce  RPM.     Option  Values Accepted  Description      group  OSG  ,  OSG-ITB  This should be set to either OSG or OSG-ITB depending on whether your resource is in the OSG or OSG-ITB group. Most sites should specify OSG    host_name  String  This should be set to be hostname of the CE that is being configured    resource  String  The resource name of this CE endpoint as registered in OIM.    resource_group  String  The resource_group of this CE as registered in OIM.    sponsor  String  This should be set to the sponsor of the resource. See note.    site_policy  Url  This should be a url pointing to the resource's usage policy    contact  String  This should be the name of the resource's admin contact    email  Email address  This should be the email address of the admin contact for the resource    city  String  This should be the city that the resource is located in    country  String  This should be two letter country code for the country that the resource is located in.    longitude  Number  This should be the longitude of the resource. It should be a number between -180 and 180.    latitude  Number  This should be the latitude of the resource. It should be a number between -90 and 90.      Note  sponsor : \nIf your resource has multiple sponsors, you can separate them using commas\nor specify the percentage using the following format 'osg, atlas, cms' or\n'osg:10, atlas:45, cms:45'. The percentages must add up to 100 if multiple\nsponsors are used. If you have a sponsor that is not an OSG VO, you can\nindicate this by using 'local' as the VO.", 
            "title": "Site Information"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#squid", 
            "text": "This section handles the configuration and setup of the squid web caching and proxy service.  This section is contained in  /etc/osg/config.d/01-squid.ini  which is provided by the  osg-configure-squid  RPM.     Option  Values Accepted  Explanation      enabled  True ,  False ,  Ignore  This indicates whether the squid service is being used or not.    location  String  This should be set to the  hostname:port  of the squid server.", 
            "title": "Squid"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#storage", 
            "text": "This section gives information about the options in the Storage section of the configuration file.\nSeveral of these values are constrained and need to be set in a way that is consistent with one of the OSG storage models.\nPlease review the Storage Related Parameters section of the Environment Variables \ndescription and  Site Planning  discussions for explanations of the various storage models and the requirements for them.  This section is contained in  /etc/osg/config.d/10-storage.ini  which is provided by the  osg-configure-ce  RPM.     Option  Values Accepted  Explanation      se_available  True ,  False  This indicates whether there is an associated SE available.    default_se  String  If an SE is available at your cluster, set default_se to the hostname of this SE, otherwise set default_se to UNAVAILABLE.    grid_dir  String  This setting should point to the directory which holds the files from the OSG worker node package. See note    app_dir  String  This setting should point to the directory which contains the VO specific applications. See note    data_dir  String  This setting should point to a directory that can be used to store and stage data in and out of the cluster. See note    worker_node_temp  String  This directory should point to a directory that can be used as scratch space on compute nodes. If not set, the default is UNAVAILABLE. See note    site_read  String  This setting should be the location or url to a directory that can be read to stage in data via the variable  $OSG_SITE_READ . This is an url if you are using a SE. If not set, the default is UNAVAILABLE    site_write  String  This setting should be the location or url to a directory that can be write to stage out data via the variable  $OSG_SITE_WRITE . This is an url if you are using a SE. If not set, the default is UNAVAILABLE      Note  All of these can be defined in terms of an environment variable\n(e.g.  $FOO ) that will be evaluated on the worker node.   grid_dir : \nIf you have installed the worker node client via RPM (the normal case) it\nshould be  /etc/osg/wn-client .  If you have installed the worker node in a\nspecial location (perhaps via the worker node client tarball or via OASIS),\nit should be the location of that directory.  This directory will be accessed via the  $OSG_GRID  environment variable.\nIt should be visible on all of the compute nodes. Read access is required,\nthough worker nodes don't need write access.  app_dir : \nThis directory will be accesed via the  $OSG_APP  environment variable. It\nshould be visible on both the CE and worker nodes. Only the CE needs to\nhave write access to this directory. This directory must also contain a\nsub-directory  etc/  with 1777 permissions.  data_dir : \nThis directory can be accessed via the  $OSG_DATA  environment variable. It\nshould be readable and writable on both the CE and worker nodes.  worker_node_temp : \nThis directory will be accessed via the  $OSG_WN_TMP  environment variable.\nIt should allow read and write access on a worker node and can be visible\nto just that worker node.", 
            "title": "Storage"
        }, 
        {
            "location": "/other/gsissh/", 
            "text": "Installing GSI OpenSSH\n\n\nThis document gives instructions on installing and using the GSI OpenSSH server available in the OSG repository and configuring it so that you can use on your cluster.\n\n\nRequirements\n\n\nHost and OS\n\n\nThe GSI OpenSSH rpms will require an user account and group in order for the privilege separation to work.\n\n\nUsers and Groups\n\n\nThe RPM installation will try to create the \ngsisshd\n user and group and the \n/var/empty/gsisshd\n directory with the correct ownership if they are not present. If you are using a configuration management system or ROCKS, you should make sure that these users and groups are created before installing the RPMs to avoid potential issues. The gsisshd user should have an empty home directory. By default, this is home directory set to \n/var/empty/gsisshd\n and belongs to the \ngsisshd\n user and group. You may change it if needed to something else as long as the ownerships remain the same.\n\n\nInstallation procedure\n\n\nPrior to install, make sure you have:\n\n \nYum repositories correctly configured\n for OSG.\n\n \nCA certificates installed\n\n\nGSI OpenSSH Installation\n\n\nStart with installing GSI OpenSSH from the repository\n\n\n[root@server]#\n yum install gsi-openssh-server gsi-openssh-clients\n\n\n\n\n\nIn addition, you'll need to install CA certificates in order for GSIOpenSSH to work. You can follow the instructions below in order to install them:\n\n\nConfiguration and Operations\n\n\nUseful configuration and log files\n\n\nConfiguration Files\n\n\n\n\n\n\n\n\nService or Process\n\n\nConfiguration File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ngsisshd\n\n\n/etc/gsissh/sshd_config\n\n\nConfiguration file\n\n\n\n\n\n\ngsisshd\n\n\n/etc/sysconfig/gsisshd\n\n\nEnvironment variables for gsisshd\n\n\n\n\n\n\ngsisshd\n\n\n/etc/lcmaps.db\n\n\nLCMAPS configuration\n\n\n\n\n\n\n\n\nLog Files\n\n\n\n\n\n\n\n\nService or Process\n\n\nLog File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ngsisshd\n\n\n/var/log/messages\n\n\nAll log messages\n\n\n\n\n\n\n\n\nOther Files\n\n\n\n\n\n\n\n\nService or Process\n\n\nFile\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ngsisshd\n\n\n/etc/grid-security/hostcert.pem\n\n\nHost certificate\n\n\n\n\n\n\ngsisshd\n\n\n/etc/grid-security/hostkey.pem\n\n\nX.509 host key\n\n\n\n\n\n\ngsisshd\n\n\n/etc/gsissh/ssh_host_rsa_key\n\n\nRSA Host key\n\n\n\n\n\n\n\n\nConfiguration\n\n\nIn order to get a running instance of the GSI OpenSSH server, you'll\nneed to change the default configuration. However, before you go any\nfurther, you'll need to decide whether you want GSI OpenSSH to be your \nprimary ssh service or not (e.g. whether the GSI OpenSSH service will \nreplace your existing SSH service). If you choose not to replace your \nexisting service, you'll need to change the port setting in the GSI \nOpenSSH configuration to another port (e.g. 2222) so that you can run \nboth SSH services at the same time. Regardless of your choice, you \nshould probably have both services use the same host key. In order \nto do this, symlink \n/etc/gsissh/ssh_host_dsa_key\n and \n/etc/gsissh/ssh_host_rsa_key\n \nto \n/etc/ssh/ssh_host_dsa_key\n and \n/etc/ssh/ssh_host_rsa_key\n respectively. \n\n\n\n\nNote\n\n\nRegardless of the authorization method used for the user, any \naccount that will be used with GSI OpenSSH must have a shell \nassigned to it and not be locked (have ! in the password field of \n/etc/shadow\n).\n\n\n\n\nUsing a gridmap file for authorization\n\n\nIn order to use gsissh, you'll need to create mappings in your \n\n/etc/grid-security/grid-mapfile\n for the DNs that you will \nallow to login. The mappings should be entered one to a line, \nwith each line consisting of DN followed by the account the DN \nshould map to. Also, you should ensure that the \n\n/etc/grid-security/gsi-authz.conf\n file is empty or that all \nof the lines in the file are commented out using a \n#\n at the beginning of the line.\n\n\n\n\nNote\n\n\nThe mappings will not consider VOMS extensions so the first mapping that matches will be used regardless of the VO role or VO present in the users proxy\n\n\n\n\nAn example of the \n/etc/grid-security/grid-mapfile\n follows:\n\n\n/DC=org/DC=doegrids/OU=People/CN=USER NAME 123456\n useraccount\n\n\n\n\n\nUsing LCMAPS and GUMS for authorization\n\n\nIn order to use LCMAPS callouts with GSI OpenSSH, you'll first need to edit \n/etc/grid-security/gsi-authz.conf\n to indicate that Globus should do a GSI callout for authorization. The file should contain the following:\n\n\nglobus_mapping liblcas_lcmaps_gt4_mapping.so lcmaps_callout\n\n\n\n\n\nso that LCMAPS is used. Next, install the lcmaps rpms:\n\n\n[root@server]#\n yum install lcmaps lcas-lcmaps-gt4-interface\n\n\n\n\n\nFinally, you'll need to modify \n/etc/lcmaps.db\n so that the \ngumsclient\n entry has the correct endpoint for your gums server.\n\n\nStarting and Enabling Services\n\n\nTo start the services:\n\n\n\n\nTo start GSI OpenSSH you can use the service command, e.g.:\n[root@server]#\n service gsisshd start\n\n\n\n\n\n\n\n\n\nYou should also enable the appropriate services so that they are automatically started when your system is powered on:\n\n\n\n\nTo enable OpenSSH by default on the node:\n[root@server]#\n chkconfig gsisshd on\n\n\n\n\n\n\n\n\n\nStopping and Disabling Services\n\n\nTo stop the services:\n\n\n\n\nTo stop OpenSSH you can use:\n[root@server]#\n service gsisshd stop\n\n\n\n\n\n\n\n\n\nIn addition, you can disable services by running the following commands. However, you don't need to do this normally.\n\n\n\n\nOptionally, to disable OpenSSH:\n[root@server]#\n chkconfig gsisshd off\n\n\n\n\n\n\n\n\n\nTroubleshooting\n\n\nYou can get information on troubleshooting errors on the \nNCSA page\n.\n\n\nTo troubleshoot LCMAPS authorization, you can add the following to \n/etc/sysconfig/gsisshd\n and choose a higher debug level:\n\n\n# level 0: no messages, 1: errors, 2: also warnings, 3: also notices,\n\n\n#  4: also info, 5: maximum debug\n\n\nLCMAPS_DEBUG_LEVEL\n=\n2\n\n\n\n\n\n\nOutput goes to \n/var/log/messages\n by default.\n\n\nTest GSI OpenSSH\n\n\nAfter starting the \ngsisshd\n service you can check if it is running correctly\n\n\n$\n grid-proxy-init\n\nYour identity: /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=User Name\n\n\nEnter GRID pass phrase for this identity:\n\n\nCreating proxy ............................................................................................... Done\n\n\nYour proxy is valid until: Sat Apr 23 08:18:27 2016\n\n\n$\n gsissh localhost -p \n2222\n\n\nLast login: Tue Sep 18 16:08:03 2012 from itb4.uchicago.edu\n\n\n$\n\n\n\n\n\n\nHow to get Help?\n\n\nTo get assistance please use this \nHelp Procedure\n.", 
            "title": "GSI-enabled SSH"
        }, 
        {
            "location": "/other/gsissh/#installing-gsi-openssh", 
            "text": "This document gives instructions on installing and using the GSI OpenSSH server available in the OSG repository and configuring it so that you can use on your cluster.", 
            "title": "Installing GSI OpenSSH"
        }, 
        {
            "location": "/other/gsissh/#requirements", 
            "text": "", 
            "title": "Requirements"
        }, 
        {
            "location": "/other/gsissh/#host-and-os", 
            "text": "The GSI OpenSSH rpms will require an user account and group in order for the privilege separation to work.", 
            "title": "Host and OS"
        }, 
        {
            "location": "/other/gsissh/#users-and-groups", 
            "text": "The RPM installation will try to create the  gsisshd  user and group and the  /var/empty/gsisshd  directory with the correct ownership if they are not present. If you are using a configuration management system or ROCKS, you should make sure that these users and groups are created before installing the RPMs to avoid potential issues. The gsisshd user should have an empty home directory. By default, this is home directory set to  /var/empty/gsisshd  and belongs to the  gsisshd  user and group. You may change it if needed to something else as long as the ownerships remain the same.", 
            "title": "Users and Groups"
        }, 
        {
            "location": "/other/gsissh/#installation-procedure", 
            "text": "Prior to install, make sure you have:   Yum repositories correctly configured  for OSG.   CA certificates installed", 
            "title": "Installation procedure"
        }, 
        {
            "location": "/other/gsissh/#gsi-openssh-installation", 
            "text": "Start with installing GSI OpenSSH from the repository  [root@server]#  yum install gsi-openssh-server gsi-openssh-clients  In addition, you'll need to install CA certificates in order for GSIOpenSSH to work. You can follow the instructions below in order to install them:", 
            "title": "GSI OpenSSH Installation"
        }, 
        {
            "location": "/other/gsissh/#configuration-and-operations", 
            "text": "", 
            "title": "Configuration and Operations"
        }, 
        {
            "location": "/other/gsissh/#useful-configuration-and-log-files", 
            "text": "Configuration Files     Service or Process  Configuration File  Description      gsisshd  /etc/gsissh/sshd_config  Configuration file    gsisshd  /etc/sysconfig/gsisshd  Environment variables for gsisshd    gsisshd  /etc/lcmaps.db  LCMAPS configuration     Log Files     Service or Process  Log File  Description      gsisshd  /var/log/messages  All log messages     Other Files     Service or Process  File  Description      gsisshd  /etc/grid-security/hostcert.pem  Host certificate    gsisshd  /etc/grid-security/hostkey.pem  X.509 host key    gsisshd  /etc/gsissh/ssh_host_rsa_key  RSA Host key", 
            "title": "Useful configuration and log files"
        }, 
        {
            "location": "/other/gsissh/#configuration", 
            "text": "In order to get a running instance of the GSI OpenSSH server, you'll\nneed to change the default configuration. However, before you go any\nfurther, you'll need to decide whether you want GSI OpenSSH to be your \nprimary ssh service or not (e.g. whether the GSI OpenSSH service will \nreplace your existing SSH service). If you choose not to replace your \nexisting service, you'll need to change the port setting in the GSI \nOpenSSH configuration to another port (e.g. 2222) so that you can run \nboth SSH services at the same time. Regardless of your choice, you \nshould probably have both services use the same host key. In order \nto do this, symlink  /etc/gsissh/ssh_host_dsa_key  and  /etc/gsissh/ssh_host_rsa_key  \nto  /etc/ssh/ssh_host_dsa_key  and  /etc/ssh/ssh_host_rsa_key  respectively.    Note  Regardless of the authorization method used for the user, any \naccount that will be used with GSI OpenSSH must have a shell \nassigned to it and not be locked (have ! in the password field of  /etc/shadow ).", 
            "title": "Configuration"
        }, 
        {
            "location": "/other/gsissh/#using-a-gridmap-file-for-authorization", 
            "text": "In order to use gsissh, you'll need to create mappings in your  /etc/grid-security/grid-mapfile  for the DNs that you will \nallow to login. The mappings should be entered one to a line, \nwith each line consisting of DN followed by the account the DN \nshould map to. Also, you should ensure that the  /etc/grid-security/gsi-authz.conf  file is empty or that all \nof the lines in the file are commented out using a  #  at the beginning of the line.   Note  The mappings will not consider VOMS extensions so the first mapping that matches will be used regardless of the VO role or VO present in the users proxy   An example of the  /etc/grid-security/grid-mapfile  follows:  /DC=org/DC=doegrids/OU=People/CN=USER NAME 123456  useraccount", 
            "title": "Using a gridmap file for authorization"
        }, 
        {
            "location": "/other/gsissh/#using-lcmaps-and-gums-for-authorization", 
            "text": "In order to use LCMAPS callouts with GSI OpenSSH, you'll first need to edit  /etc/grid-security/gsi-authz.conf  to indicate that Globus should do a GSI callout for authorization. The file should contain the following:  globus_mapping liblcas_lcmaps_gt4_mapping.so lcmaps_callout  so that LCMAPS is used. Next, install the lcmaps rpms:  [root@server]#  yum install lcmaps lcas-lcmaps-gt4-interface  Finally, you'll need to modify  /etc/lcmaps.db  so that the  gumsclient  entry has the correct endpoint for your gums server.", 
            "title": "Using LCMAPS and GUMS for authorization"
        }, 
        {
            "location": "/other/gsissh/#starting-and-enabling-services", 
            "text": "To start the services:   To start GSI OpenSSH you can use the service command, e.g.: [root@server]#  service gsisshd start    You should also enable the appropriate services so that they are automatically started when your system is powered on:   To enable OpenSSH by default on the node: [root@server]#  chkconfig gsisshd on", 
            "title": "Starting and Enabling Services"
        }, 
        {
            "location": "/other/gsissh/#stopping-and-disabling-services", 
            "text": "To stop the services:   To stop OpenSSH you can use: [root@server]#  service gsisshd stop    In addition, you can disable services by running the following commands. However, you don't need to do this normally.   Optionally, to disable OpenSSH: [root@server]#  chkconfig gsisshd off", 
            "title": "Stopping and Disabling Services"
        }, 
        {
            "location": "/other/gsissh/#troubleshooting", 
            "text": "You can get information on troubleshooting errors on the  NCSA page .  To troubleshoot LCMAPS authorization, you can add the following to  /etc/sysconfig/gsisshd  and choose a higher debug level:  # level 0: no messages, 1: errors, 2: also warnings, 3: also notices,  #  4: also info, 5: maximum debug  LCMAPS_DEBUG_LEVEL = 2   Output goes to  /var/log/messages  by default.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/other/gsissh/#test-gsi-openssh", 
            "text": "After starting the  gsisshd  service you can check if it is running correctly  $  grid-proxy-init Your identity: /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=User Name  Enter GRID pass phrase for this identity:  Creating proxy ............................................................................................... Done  Your proxy is valid until: Sat Apr 23 08:18:27 2016  $  gsissh localhost -p  2222  Last login: Tue Sep 18 16:08:03 2012 from itb4.uchicago.edu  $", 
            "title": "Test GSI OpenSSH"
        }, 
        {
            "location": "/other/gsissh/#how-to-get-help", 
            "text": "To get assistance please use this  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/other/install-gwms-frontend/", 
            "text": "GlideinWMS VO Frontend Installation\n\n\nAbout This Document\n\n\nThis document describes how to install the Glidein Workflow Managment System (GlideinWMS) VO Frontend for use with the OSG glidein factory. This software is the minimum requirement for a VO to use glideinWMS.\n\n\nThis document assumes expertise with Condor and familiarity with the glideinWMS software. It \ndoes not\n cover anything but the simplest possible install. Please consult the \nGlidein WMS reference documentation\n for advanced topics, including non-\nroot\n, non-RPM-based installation.\n\n\nThis document covers three components of the GlideinWMS a VO needs to install:\n\n\n\n\nUser Pool Collectors\n: A set of \ncondor_collector\n processes. Pilots submitted by the factory will join to one of these collectors to form a Condor pool.\n\n\nUser Pool Schedd\n: A \ncondor_schedd\n. Users may submit Condor vanilla universe jobs to this schedd; it will run jobs in the Condor pool formed by the \nUser Pool Collectors\n.\n\n\nGlidein Frontend\n: The frontend will periodically query the \nUser Pool Schedd\n to determine the desired number of running job slots. If necessary, it will request the factory to launch additional pilots.\n\n\n\n\nThis guide covers installation of all three components on the same host: it is designed for small to medium VOs (see the Hardware Requirements below). Given a significant, large host, we have been able to scale the single-host install to 20,000 running jobs.\n\n\n\n\nRelease\n\n\nThis document reflects glideinWMS v3.2.17.\n\n\nHow to get Help?\n\n\nTo get assistance about the OSG software please use \nthis page\n.\n\n\nFor specific questions about the Frontend configuration (and how to add it in your HTCondor infrastructure) you can email the glideinWMS support \n\n\nTo request access the OSG Glidein Factory (e.g. the UCSD factory) you have to send an email to \n (see below).\n\n\nRequirements\n\n\nHost and OS\n\n\n\n\nA host to install the GlideinWMS Frontend (pristine node).\n\n\nOS is Red Hat Enterprise Linux 6, 7, and variants (see \ndetails\n). Currently most of our testing has been done on Scientific Linux 6.\n\n\nRoot access\n\n\n\n\nThe Glidein WMS VO Frontend has the following hardware requirements for a production host:\n\n\n\n\nCPU\n: Four cores, preferably no more than 2 years old.\n\n\nRAM\n: 3GB plus 2MB per running job. For example, to sustain 2000 running jobs, a host with 5GB is needed.\n\n\nDisk\n: 30GB will be plenty sufficient for all the binaries, config and log files related to glideinWMS. As this will be an interactive submit host, plan enough disk space for your users' jobs. Depending on your workflow, this might require 2MB to 2GB per job in a workflow.\n\n\n\n\nUsers\n\n\nThe Glidein WMS Frontend installation will create the following users unless they are already created.\n\n\n\n\n\n\n\n\nUser\n\n\nDefault uid\n\n\nComment\n\n\n\n\n\n\n\n\n\n\napache\n\n\n48\n\n\nRuns httpd to provide the monitoring page (installed via dependencies).\n\n\n\n\n\n\ncondor\n\n\nnone\n\n\nCondor user (installed via dependencies).\n\n\n\n\n\n\nfrontend\n\n\nnone\n\n\nThis user runs the glideinWMS VO frontend. It also owns the credentials forwarded to the factory to use for the glideins.\n\n\n\n\n\n\ngratia\n\n\nnone\n\n\nRuns the Gratia probes to collect accounting data (optional see \nthe Gratia section below\n)\n\n\n\n\n\n\n\n\nNote that if uid 48 is already taken but not used for the appropriate users, you will experience errors. \nDetails...\n\n\nCredentials and Proxies\n\n\nThe VO Frontend will use two credentials in its interactions with the the other glideinWMS services. At this time, these will be proxy files.\n\n\n\n\nthe \nVO Frontend proxy\n (used to authenticate with the other glideinWMS services).\n\n\none or more glideinWMS \npilot proxies\n (used/delegated to the factory services and submitted on the glideinWMS pilot jobs).\n\n\n\n\nThe \nVO Frontend proxy\n and the \n pilot proxy\n can be the same. By default, the VO Frontend will run as user \nfrontend\n (UID is machine dependent) so these proxies must be owned by the user \nfrontend\n.\n\n\nVO Frontend proxy\n\n\nThe use of a service certificate is recommended. Then you create a proxy from the certificate as explained in the \nproxy configuration section\n. This can be a plain grid proxy (from \ngrid-proxy-init\n), no VO extensions are required.\n\n\nYou must notify the Factory operation of the DN of this proxy when you initially setup the frontend and each time the DN changes\n.\n\n\nPilot proxies\n\n\nThis proxy is used by the factory to submit the glideinWMS pilot jobs. Therefore, they must be authorized to access to the CEs (factory entry points) where jobs are submitted. There is no need to notify the Factory operation about the DN of this proxy (neither at the initial registration nor for subsequent changes). This second proxy has no special requirement or controls added by the factory but will probably require VO attributes because of the CEs: if you are able to use this proxy to submit jobs to the CEs where the Factory runs glideinWMS pilots for you, then the proxy is OK. You can test your proxy using \nglobusrun\n or HTCondor-G\n\n\nTo check the important information about a pem certificate you can use: \nopenssl x509 -in /etc/grid-security/hostcert.pem -subject -issuer -dates -noout\n. You will need that to find out information for the configuration files and the request to the GlideinWMS factory.\n\n\nCertificates/Proxies configuration example\n\n\nThis document has a \nproxy configuration section\n that uses the host certificate/key and a user certificate to generate the required proxies.\n\n\n\n\n\n\n\n\nCertificate\n\n\nUser that owns certificate\n\n\nPath to certificate\n\n\n\n\n\n\n\n\n\n\nHost certificate\n\n\nroot\n\n\n/etc/grid-security/hostcert.pem\n            \n/etc/grid-security/hostkey.pem\n\n\n\n\n\n\n\n\nHere\n are instructions to request a host certificate.\n\n\nNetworking\n\n\n\n\n\n\n\n\nService Name\n\n\nProtocol\n\n\nPort Number\n\n\nInbound\n\n\nOutbound\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nHTCondor port range\n\n\ntcp\n\n\nLOWPORT, HIGHPORT\n\n\nYES\n\n\n\n\ncontiguous range of ports\n\n\n\n\n\n\nGlideinWMS Frontend\n\n\ntcp\n\n\n9618 to 9660\n\n\nYES\n\n\n\n\nHTCondor Collectors for the GlideinWMS Frontend (received ClassAds from resources and jobs)\n\n\n\n\n\n\n\n\nThe VO frontend must have reliable network connectivity, be on the public internet (no NAT), and preferably with no firewalls. Each running pilot requires 5 outgoing TCP ports. Incoming TCP ports 9618 to 9660 must be open.\n\n\n\n\nFor example, 2000 running jobs require about 10,100 TCP connections. This will overwhelm many firewalls; if you are unfamiliar with your network topology, you may want to warn your network administrator.\n\n\n\n\nBefore the installation\n\n\nOnce all requirements are satisfied you must take a couple of actions before installing the Frontend:\n\n\n\n\nyou need all the data to connect to a GWMS Factory\n\n\nRemember to install HTCondor BEFORE installing the Frontend (\ninstructions are below\n)\n\n\n\n\nOSG Factory access\n\n\nBefore installing the Glidein WMS VO Frontend you need the information about a \nGlidein Factory\n that you can access:\n\n\n\n\n(recommended) OSG is managing a factory at UCSD and one at GOC and you can request access to them\n\n\nYou have another Glidein Factory that you can access\n\n\nYou \ninstall your own Glidein Factory\n\n\n\n\nTo request access to the OSG Glidein Factory at UCSD you have to send an email to \n providing:\n\n\n\n\nYour Name\n\n\nThe VO that is utilizing the VO Frontend\n\n\nThe DN of the proxy you will use to communicate with the Factory (VO Frontend DN, e.g. the host certificate subject if you follow the \nproxy configuration section\n)\n\n\nYou can propose a security name that will have to be confirmed/changed by the Factory managers (see below)\n\n\n\n\nA list of sites where you want to run:\n\n\n\n\n\n\nYour VO must be supported on those sites\n\n\n\n\nYou can provide a list or piggy back on existing lists, e.g. all the sites supported for the VO. Check with the Factory managers\n\n\nYou can start with one single site\n\n\n\n\nIn the reply from the OSG Factory managers you will receive some information needed for the configuration of your VO Frontend\n\n\n\n\nThe exact spelling and capitalization of your VO name. Sometime is different from what is commonly used, e.g. OSG VO is \"OSGVO\".\n\n\nThe host of the Factory Collector: \ngfactory-1.t2.ucsd.edu\n\n\nThe DN os the factory, e.g. \n/DC=org/DC=doegrids/OU=Services/CN=gfactory-1.t2.ucsd.edu\n\n\nThe factory identity, e.g.: \ngfactory@gfactory-1.t2.ucsd.edu\n\n\nThe identity on the factory you will be mapped to. Something like: \nusername@gfactory-1.t2.ucsd.edu\n\n\nYour security name. A unique name, usually containing your VO name: \nMy_SecName\n\n\nA string to add in the main factory query_expr in the frontend configuration, e.g. \nstringListMember(\"\nVO\n\",GLIDEIN_Supported_VOs)\n. From there you get the correct name of the VO (above in this list).\n\n\n\n\nInstallation Procedure\n\n\nRefer to installation of the \nCA certificates\n\n\nInstall HTCondor\n\n\nMost required software is installed from the Frontend RPM installation. HTCondor is the only exception since there are \nmany different ways to install it\n, using the RPM system or not. You need to have HTCondor installed before installing the Glidein WMS Frontend. If yum cannot find a HTCondor RPM, it will install the dummy \nempty-condor\n RPM, assuming that you installed HTCondor using a tarball distribution.\n\n\nIf you don't have HTCondor already installed, you can install the HTCondor RPM from the OSG repository:\n\n\nroot@host #\n yum install condor.x86_64\n\n#\n If you have a \n32\n bit host use instead:\n\nroot@host #\n yum install condor.i386\n\n\n\n\n\nSee \nthis HTCondor document\n for more information on the different options.\n\n\nDownload and install the VO Frontend RPM\n\n\nThe RPM is available in the OSG repository:\n\n\nInstall the RPM and dependencies (be prepared for a lot of dependencies).\n\n\nroot@host #\n yum install glideinwms-vofrontend\n\n\n\n\n\nThis will install the current production release verified and tested by OSG with default condor configuration. This command will install the glideinwms vofrontend, condor, the OSG client, and all the required dependencies all on one node.\n\n\nIf you wish to install a different version of GlideinWMS, add the \"--enablerepo\" argument to the command as follows:\n\n\n\n\nyum install --enablerepo=osg-testing glideinwms-vofrontend\n: The most recent production release, still in testing phase. This will usually match the current tarball version on the GlideinWMS home page. (The osg-release production version may lag behind the tarball release by a few weeks as it is verified and packaged by OSG). Note that this will also take the osg-testing versions of all dependencies as well.\n\n\nyum install --enablerepo=osg-contrib glideinwms-vofrontend\n: The most recent development series release, ie version 3 release. This has newer features such as cloud submission support, but is less tested.\n\n\n\n\nNote that these commands will install default condor configurations with all services on one node.\n\n\nAdvanced: Multi-node Installation\n\n\nFor advanced users requiring heavy usage on their submit node, you may want to consider splitting the usercollector, user submit, and vo frontend services.\n\n\nThis can be doing using the following three commands (on different machines):\n\n\nroot@host #\n yum install glideinwms-vofrontend-standalone\n\nroot@host #\n yum install glideinwms-usercollector\n\nroot@host #\n yum install glideinwms-userschedd\n\n\n\n\n\nIn addition, you will need to perform the following steps:\n\n\n\n\nOn the vofrontend and userschedd, modify CONDOR_HOST to point to your usercollector. This is in \n/etc/condor/config.d/00_gwms_general.config\n. You can also override this value by placing it in a new config file. (For instance, \n/etc/condor/config.d/99_local_custom.config\n to avoid rpmsave/rpmnew conflicts on upgrades).\n\n\nIn \n/etc/condor/certs/condor_mapfile\n, you will need to all DNs for each machine (userschedd, usercollector, vofrontend). Take great care to escape all special characters. Alternatively, you can use the \nglidecondor_addDN\n to add these values.\n\n\nIn the \n/etc/gwms-frontend/frontend.xml\n file, change the schedd locations to match the correct server. Also change the collectors tags at the bottom of the file. More details on frontend xml are in the following sections.\n\n\n\n\nUpgrade Procedure\n\n\nIf you have a working installation of glideinwms-frontend you can just upgrade the frontend rpms and skip the most of the configuration procedure below. These general upgrade instructions apply when upgrading the glideinwms-frontend rpm within same major versions.\n\n\n%\nRED%# Update the glideinwms-vofrontend packages\n\n\nroot@host #\n yum update glideinwms\n\\*\n\n\n%\nRED%# Update the scripts in the working directory to the latest one\n\n\n%\nRED%# For RHEL \n7\n, CentOS \n7\n, and SL7\n\n\nroot@host #\n /usr/sbin/gwms-frontend upgrade\n\n%\nRED%# For RHEL \n6\n, CentOS \n6\n, and SL6\n\n\nroot@host #\n service gwms-frontend upgrade\n\n%\nRED%# Restart HTCondor because the configuration may be different\n\n\nroot@host #\n service condor restart\n\n\n\n\n\n\n\nNote\n\n\nThe \\*\n on the yum update is important.\n*\n\n\n\n\n\n\nWarning\n\n\nWhen you do a generic yum update that will update also condor, the upgrade may restore the personal condor config file that you have to remove with \nrm /etc/condor/config.d/00personal_condor.config\n\n\n\n\n\n\nNote\n\n\nWhen upgrading to GlideinWMS 3.2.7 the second schedd is removed from the default configuration. For a smooth transition:\n\n\n\n\nremove from \n/etc/gwms-frontend/frontend.xml\n the second schedd (the line containing \nschedd_jobs2@YOUR_HOST\n)\n\n\nreconfigure the frontend (\nservice gwms-frontend reconfig\n)\n\n\nrestart HTCondor (\nservice condor restart\n)\n\n\n\n\n\n\nConfiguration Procedure\n\n\nAfter installing the RPM, you need to configure the components of the glideinWMS VO Frontend:\n\n\n\n\nEdit Frontend configuration options\n\n\nEdit Condor configuration options\n\n\nCreate a Condor grid map file\n\n\nReconfigure and Start frontend\n\n\n\n\nConfiguring the Frontend\n\n\nThe VO Frontend configuration file is \n/etc/gwms-frontend/frontend.xml\n. The next steps will describe each line that you will need to edit if you are using the OSG Factory at UCSD. The portions to edit are highlighted in red font. If you are using a different Factory more changes are necessary, please check the VO Frontend configuration reference.\n\n\n\n\n\n\nThe VO you are affiliated with. This will identify those CEs that the glideinWMS pilot will be authorized to run on using the \npilot proxy\n described previously in the this \nsection\n. Sometimes the whole \nquery_expr\n is provided to you by the factory (see Factory access above):\n\n\nfactory query_expr=\n((stringListMember(\nVO\n, GLIDEIN_Supported_VOs)))\n\n\n\n\n\n\n\n\n\n\nFactory collector information. The \nusername\n that you are assigned by the factory (also called the identity you will be mapped to on the factory, see above) . Note that if you are using a factory different than the production factory, you will have to change also \nDN\n, \nfactory_identity\n and \nnode\n attributes. (refer to the information provided to you by the factory operator):\n\n\ncollector DN=\n/DC=org/DC=doegrids/OU=Services/CN=gfactory-1.t2.ucsd.edu\n\n           comment=\nDefine factory collector globally for simplicity\n\n           factory_identity=\ngfactory@gfactory-1.t2.ucsd.edu\n\n           my_identity=\nusername@gfactory-1.t2.ucsd.edu\n\n           node=\ngfactory-1.t2.ucsd.edu\n/\n\n\n\n\n\n\n\n\n\n\nFrontend security information.\n\n\n\n\nThe \nclassad_proxy\n in the security entry is the location of the VO Frontend proxy described previously \nhere\n.\n\n\nThe \nproxy_DN\n is the DN of the \nclassad_proxy\n above.\n\n\nThe \nsecurity_name\n identifies this VO Frontend to the the Factory, It is provided by the factory operator.\n\n\nThe \nabsfname\n in the credential (or proxy in v 2.x) entry is the location of the glideinWMS \npilot\n proxy described in the requirements section \nhere\n. There can be multiple pilot proxies, or even other kind of keys (e.g. if you use cloud resources). \nThe type and trust_domain of the credential must match respectively auth_method and trust_domain used in the entry definition in the factory. If there is no match, between these two attributes in one of the credentials and some entry in one of the factories, then this frontend cannot trigger glideins.\n\nBoth the \nclassad_proxy\n and \nabsfname\n files should be owned by \nfrontend\n user.\n# These lines are from the configuration of v 3.x\n\nsecurity\n \nclassad_proxy=\n/tmp/vo_proxy\n \nproxy_DN=\nDN of vo_proxy\n\n      \nproxy_selection_plugin=\nProxyAll\n\n      \nsecurity_name=\nThe security name, this is used by factory\n\n      \nsym_key=\naes_256_cbc\n\n      \ncredentials\n\n        \ncredential\n \nabsfname=\n/tmp/pilot_proxy\n \nsecurity_class=\nfrontend\n\n        \ntrust_domain=\nOSG\n \ntype=\ngrid_proxy\n/\n\n      \n/credentials\n\n\n/security\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe schedd information.\n\n\n\n\nThe \nDN\n of the \nVO Frontend Proxy\n described previously \nhere\n.\n\n\nThe \nfullname\n attribute is the fully qualified domain name of the host where you installed the VO Frontend (\nhostname --fqdn\n).\n\n\n\n\nA secondary schedd is optional. You will need to delete the secondary schedd line if you are not using it. Multiple schedds allow the frontend to service requests from multiple submit hosts.\n\n\n   \nschedds\n\n     \nschedd\n \nDN=\nCert DN used by the schedd at fullname:\n\n           \nfullname=\nHostname of the schedd\n/\n\n      \nschedd\n \nDN=\nCert DN used by the second Schedd at fullname:\n\n            \nfullname=\nschedd name@Hostname of second schedd\n/\n\n   \n/schedds\n\n\n\n\n\n\n\n\n\n\nThe User Collector information.\n\n\n\n\nThe \nDN\n of the \nVO Frontend Proxy\n described previously \nhere\n.\n\n\nThe \nnode\n attribute is the full hostname of the collectors (\nhostname --fqdn\n) and port\n\n\nThe \nsecondary\n attribute indicates whether the element is for the primary or secondary collectors (True/False).\n\n\n\n\nThe default Condor configuration of the VO Frontend starts multiple Collector processes on the host (\n/etc/condor/config.d/11_gwms_secondary_collectors.config\n). The \nDN\n and \nhostname\n on the first line are the hostname and the host certificate of the VO Frontend. The \nDN\n and \nhostname\n on the second line are the same as the ones in the first one. The hostname (e.g. hostname.domain.tld) is filled automatically during the installation. The secondary collector ports can be defined as a range, e.g., 9620-9660).\n\n\n    \ncollector DN=\nDN of main collector\n\n           node=\nhostname.domain.tld:9618\n secondary=\nFalse\n/\n\n    \ncollector DN=\nDN of secondary collectors (usually same as DN in line above)\n\n           node=\nhostname.domain.tld:9620-9660\n secondary=\nTrue\n/\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nThe Frontend configuration includes many knobs, some of which are conflicting with a RPM installation where there is only one version of the Frontend installed and it uses well known paths.     Do not change the following in the Frontend configuration (you must leave the default values coming with the RPM installation):\n\n\n\n\nfrontend_versioning='False' (in the first line of XML, versioning is useful to install multiple tarball versions)\n\n\nwork base_dir must be /var/lib/gwms-frontend/vofrontend/ (other scripts like /etc/init.d/gwms-frontend count on that value)\n\n\n\n\n\n\nIf you have a different Factory\n\n\nThe configuration above points to the OSG production Factory. If you are using a different Factory, then you have to:\n\n\n\n\nreplace \ngfactory@gfactory-1.t2.ucsd.edu\n and \ngfactory-1.t2.ucsd.edu\n with the correct values for your factory. And control also that the name used for the frontend () matches.\n\n\nmake sure that the factory is advertising the attributes used in the factory query expression (\nquery_expr\n).\n\n\n\n\nConfiguring Condor\n\n\nThe condor configuration for the frontend is placed in \n/etc/condor/config.d\n.\n\n\n\n\n00_gwms_general.config\n\n\n01_gwms_collectors.config\n\n\n02_gwms_schedds.config\n\n\n03_gwms_local.config\n\n\n11_gwms_secondary_collectors.config\n\n\n90_gwms_dns.config\n\n\n\n\nGet rid of the pre-loaded condor default to avoid conflicts in the configuration.\n\n\nroot@host #\n rm /etc/condor/config.d/00personal_condor.config\n\n\n\n\n\nFor most installations create a new file named \n/etc/condor/config.d/92_local_condor.config\n\n\nUsing other Condor RPMs, e.g. UW Madison HTCondor RPM\n\n\nThe above procedure will work if you are using the OSG HTCondor RPMS. You can verify that you used the OSG HTCondor RPM by using \nyum list condor\n. The version name should include \"osg\", e.g. \n7.8.6-3.osg.el5\n.\n\n\nIf you are using the UW Madison Condor RPMS, be aware of the following changes:\n\n\n\n\nThis Condor RPM uses a file \n/etc/condor/condor_config.local\n to add your local machine slot to the user pool.\n\n\nIf you want to disable this behavior (recommended), you should blank out that file or comment out the line in \n/etc/condor/condor_config\n for LOCAL_CONFIG_FILE. (Make sure that LOCAL_CONFIG_DIR is set to \n/etc/condor/config.d\n)\n\n\nNote that the variable LOCAL_DIR is set differently in UW Madison and OSG RPMs. This should not cause any more problems in the glideinwms RPMs, but please take note if you use this variable in your job submissions or other customizations.\n\n\n\n\nIn general if you are using a non OSG RPM or if you added custom configuration files for HTCondor please check the order of the configuration files:\n\n\nroot@host #\n condor_config_val -config\n\nConfiguration source:\n\n\n    /etc/condor/condor_config\n\n\nLocal configuration sources:\n\n\n    /etc/condor/config.d/00_gwms_general.config\n\n\n    /etc/condor/config.d/01_gwms_collectors.config\n\n\n    /etc/condor/config.d/02_gwms_schedds.config\n\n\n    /etc/condor/config.d/03_gwms_local.config\n\n\n    /etc/condor/config.d/11_gwms_secondary_collectors.config\n\n\n    /etc/condor/config.d/90_gwms_dns.config\n\n\n%\nRED%/etc/condor/condor_config.local\n\n\n\n\n\n\nIf, like in the example above, the GlideinWMS configuration files are not the last ones in the list please verify that important configuration options have not been overridden by the other configuration files.\n\n\nVerify your Condor configuration\n\n\n\n\n\n\nThe glideinWMS configuration files in \n/etc/condor/config.d\n should be the last ones in the list. If not, please verify that important configuration options have not been overridden by the other configuration files.\n\n\n\n\n\n\nVerify the alll the expected HTCondor daemons are running:\n\n\nroot@host #\n condor_config_val -verbose DAEMON_LIST DAEMON_LIST: MASTER, COLLECTOR, NEGOTIATOR, SCHEDD, SHARED_PORT, COLLECTOR0\n\nCOLLECTOR1 COLLECTOR2 COLLECTOR3 COLLECTOR4 COLLECTOR5 COLLECTOR6 COLLECTOR7 COLLECTOR8 COLLECTOR9 COLLECTOR10 , COLLECTOR11, COLLECTOR12,\n\n\nCOLLECTOR13, COLLECTOR14, COLLECTOR15, COLLECTOR16, COLLECTOR17, COLLECTOR18, COLLECTOR19, COLLECTOR20, COLLECTOR21, COLLECTOR22, COLLECTOR23,\n\n\nCOLLECTOR24, COLLECTOR25, COLLECTOR26, COLLECTOR27, COLLECTOR28, COLLECTOR29, COLLECTOR30, COLLECTOR31, COLLECTOR32, COLLECTOR33, COLLECTOR34,\n\n\nCOLLECTOR35, COLLECTOR36, COLLECTOR37, COLLECTOR38, COLLECTOR39, COLLECTOR40\n\n\nDefined in \n/etc/condor/config.d/11_gwms_secondary_collectors.config\n, line 193.\n\n\n\n\n\n\n\n\n\n\nIf you don't see all the collectors. shared port and the two schedd, then the configuration must be corrected. There should be \nno\n \nstartd\n daemons listed.\n\n\nCreate a Condor grid mapfile.\n\n\nThe Condor grid mapfile (\n/etc/condor/certs/condor_mapfile\n) is used for authentication between the glideinWMS pilot running on a remote worker node, and the local collector. Condor uses the mapfile to map certificates to pseudo-users on the local machine. It is important that you map the DN's of:\n\n\n\n\n\n\nEach schedd proxy\n: The \nDN\n of each schedd that the frontend talks to. Specified in the frontend.xml schedd element \nDN\n attribute:\n\n\nschedds\n\n  \nschedd\n \nDN=\n/DC=org/DC=doegrids/OU=Services/CN=YOUR_HOST\n \nfullname=\nYOUR_HOST\n/\n\n  \nschedd\n \nDN=\n/DC=org/DC=doegrids/OU=Services/CN=YOUR_HOST\n \nfullname=\nschedd_jobs2@YOUR_HOST\n/\n\n\n/schedds\n\n\n\n\n\n\n\n\n\n\nFrontend proxy\n: The DN of the proxy that the frontend uses to communicate with the other glideinWMS services. Specified in the frontend.xml security element \nproxy_DN\n attribute:\n\n\nsecurity classad_proxy=\n/tmp/vo_proxy\n proxy_DN=\nDN of vo_proxy\n ....\n\n\n\n\n\n\n\n\n\nEach pilot proxy\n The DN of \neach\n proxy that the frontend forwards to the factory to use with the glideinWMS pilots.  This allows the !glideinWMs pilot jobs to communicate with the User Collector. Specified in the frontend.xml proxy \nabsfname\n attribute (you need to specify the \nDN\n of each of those proxies:\n\n\nsecurity\n \n....\n\n\nproxies\n\n   \n proxy\n \nabsfname=\n/tmp/vo_proxy\n \n....\n\n   \n:\n\n\n/proxies\n\n\n\n\n\n\n\n\n\n\nBelow is an example mapfile, by default found in \n/etc/condor/certs/condor_mapfile\n. In this example there are lines for each of services mentioned above.\n\n\n\n\nNote\n\n\nThe \nexample_of_format\n entry as each DN should use this format for security purposes.\n\n\n\n\nGSI \nDN of schedd proxy\n schedd\nGSI \nDN of frontend proxy\n frontend\nGSI \nDN of pilot proxy\n$\n pilot_proxy\nGSI \n^\\/DC\\=org\\/DC\\=doegrids\\/OU\\=Services\\/CN\\=personal\\-submit\\-host2\\.mydomain\\.edu$\n \nexample_of_format\n\nGSI (.*) anonymous\nFS (.*) \\1\n\n\n\n\n\nRestart Condor\n\n\nAfter configuring condor, be sure to restart condor:\n\n\nroot@host #\n service condor restart\n\n\n\n\n\nProxy Configuration\n\n\nThere are 2 types of (or purposes for) proxies required for the VO Frontend: 1 the \nVO Frontend proxy\n (used to authenticate with the other glideinWMS services) 1 one or more glideinWMS \npilot proxies\n (used/delegated to the factory services and submitted on the glideinWMS pilot jobs) The \nVO Frontend proxy\n and the \npilot proxy\n can be the same. By default, the VO Frontend will run as user \nfrontend\n (UID is machine dependent) so these proxies must be owned by the user \nfrontend\n.\n\n\nManual proxy renewal\n\n\nVO Frontend proxy\n\nThe VO Frontend Proxy is used for communicating with the other glideinWMS services (Factory, User Collector and Schedd/Submit services). Create the proxy using the glidenWMS VO Frontend Host (or Service) cert and change ownership to the frontend user.\n\n\nroot@host #\n voms-proxy-init-valid \nhours_valid\n \n\\\n\n-cert /etc/grid-security/hostcert.pem \n\\\n\n-key /etc/grid-security/hostkey.pem \n\\\n\n-out \n/tmp/vofe_proxy\n\n\nroot@host #\n chown frontend \n/tmp/vofe_proxy\n\n\n\n\n\n\nPilot proxy\n\nThe pilot proxy is used on the glideinWMS pilot jobs submitted to the CEs. Create the proxy using the \npilot certificate\n and change ownership to the frontend user.\n\n\nroot@host #\n voms-proxy-init -valid \nhours_valid\n\\\n\n-voms \nvo\n\n\n-cert \npilot_cert\n \\\n\n\n-key \npilot_key\n \\\n\n\n-out \n/tmp/pilot_proxy\n\n\nroot@host #\n chown frontend \n/tmp/pilot_proxy\n\n\n\n\n\n\n\n\nWarning\n\n\nProxies do expire.\n You can extend the validity by using a longer time interval, e.g. \n-valid 3000:0\n. This sequence of commands will need to be renewed when the proxy expires or the machine reboots (if /tmp is used only).\n\n\n\n\nMake sure that this location is specified correctly in the \nfrontend.xml\n described in the \nConfiguring the Frontend\n section.\n\n\nYou may want to automate the procedure above (or part of it) by writing a script and adding it to crontab.\n\n\nExample of automatic proxy renewal\n\n\nThis example (user provided) uses the script \nmake-proxy.sh\n attached to this document. You still need to do some prep-work but this can be done only once a year and the script will warn you with an email.\n\n\nPreparation for the \nVO Frontend proxy\n. You'll have to redo this each time the Host (or Service) certificate and key are renewed:\n\n\n\n\n\n\nCopy the Host (or Service) certificate and key\n\n\nroot@host #\n cp /etc/grid-security/hostcert.pem /etc/grid-security/hostkey.pem /var/lib/gwms-frontend/\n\n\n\n\n\n\n\n\n\nChange ownership and permission of the certificate and key\n\n\nroot@host #\n chown frontend: /var/lib/gwms-frontend/host**.pem\n\nroot@host #\n chmod \n0600\n /var/lib/gwms-frontend/host**.pem\n\n\n\n\n\n\n\n\n\nPreparation for the \n pilot proxy\n. You'll have to redo this for each new or renewed pilot cert.\n\n\n\n\n\n\nCreate the proxy using the pilot certificate/key (as the user/submitter)\n\n\nroot@host #\n grid-proxy-init -valid \n8800\n:0 -out /tmp/tmp_proxy\n\n\n\n\n\n\n\n\n\nCopy the proxy to the correct name and change ownership and permissions (as root):\n\n\nroot@host #\n cp /tmp/tmp_proxy /var/lib/gwms-frontend/vofe_base_gi_delegated_proxy\n\nroot@host #\n chown frontend: /var/lib/gwms-frontend/vofe_base_gi_delegated_proxy\n\nroot@host #\n chmod \n0600\n /var/lib/gwms-frontend/vofe_base_gi_delegated_proxy\n\nroot@host #\n rm /tmp/tmp_proxy\n\n\n\n\n\n\n\n\n\nConfigure the script for the \nVO Frontend proxy\n\n\n\n\n\n\nDownload the \nattached script\n (the latest one is \nHere on Github\n) and save it as \n/var/lib/gwms-frontend/make-frontend-proxy.sh\n, make sure that it is executable.\n\n\n\n\n\n\nEdit the VARIABLES section to look something like (replace your email, host name and the paths that are different in your setup - the comments in the script will help):\n\n\nSETUP_FILE=\n\nCERT_FILE=\n/var/lib/gwms-frontend/hostcert.pem\n\nKEY_FILE=\n/var/lib/gwms-frontend/hostkey.pem\n\nIN_NAME=\n/var/lib/gwms-frontend/frontend_base_proxy\n\nOUT_NAME=\n/tmp/vofe_proxy\n\nOWNER_EMAIL=\nyour@email_here\n\nPROXY_DESCRIPTION=\nVO Fronted on \nhostname\n\nVOMS_OPTION=\n\n\n\n\n\n\n\n\n\n\nConfigure the script for the \npilot proxy\n:\n\n\n\n\n\n\nDownload the \nattached script\n (the latest one is \nHere on Github\n) and save it as \n/var/lib/gwms-frontend/make-pilot-proxy.sh\n, make sure that it is executable.\n\n\n\n\n\n\nEdit the VARIABLES section to look something like (replace your email, host name and the paths that are different in your setup - the comments in the script will help):\n\n\nSETUP_FILE=\n\nCERT_FILE=\n\nKEY_FILE=\n\nIN_NAME=\n/var/lib/gwms-frontend/vofe_base_gi_delegated_proxy\n\nOUT_NAME=\n/tmp/vofe_gi_delegated_proxy\n\nOWNER_EMAIL=\nyour@email_here\n\nPROXY_DESCRIPTION=\nVO Fronted glidein delegated on \nhostname\n\nVOMS_OPTION=\nosg:/osg\n\n\n\n\n\n\n\n\n\n\nBefore adding the scripts to the crontab I'd recommend to test them manually once to make sure that there are no errors. As user \nfrontend\n run the scripts (you can also use \nsh -x\n to debug them):\n\n\n/var/lib/gwms-frontend/make-frontend-proxy.sh --no-voms-proxy /var/lib/gwms-frontend/make-pilot-proxy.sh\n\n\n\n\n\n\nAdd the scripts to the crontab of the user \nfrontend\n with \ncrontab -e\n:\n\n\n10 * * * * /var/lib/gwms-frontend/make-frontend-proxy.sh --no-voms-proxy\n10 * * * * /var/lib/gwms-frontend/make-pilot-proxy.sh\n\n\n\n\n\nAn additional script like \nmake-proxy-control.sh\n (the latest one is \nHere on Github\n) can be used for an independent verification of the proxies. If you like, download it, fix the variables and add it to the crontab like the other two.\n\n\nReconfigure and verify installation\n\n\n\n\nWarning\n\n\nIn order to use the frontend, first you must reconfigure and upgrade it.\n\n\n# For RHEL 6, CentOS 6, and SL6\n\n\nroot\n@\nhost\n # \nservice\n \ngwms\n-\nfrontend\n \nreconfig\n\n\nroot\n@\nhost\n # \nservice\n \ngwms\n-\nfrontend\n \nupgrade\n\n\n\n# For RHEL 7, CentOS 7, and SL7\n\n\nroot\n@\nhost\n # \n/\nusr\n/\nsbin\n/\ngwms\n-\nfrontend\n \nreconfig\n\n\nroot\n@\nhost\n # \n/\nusr\n/\nsbin\n/\ngwms\n-\nfrontend\n \nupgrade\n\n\n\n\n\n\n\n\nAfter this initial reconfiguring/upgrading, you can start the frontend:\n\n\n \n# For RHEL 6, CentOS 6, and SL6\n\n \nroot\n@\nhost\n # \nservice\n \ngwms\n-\nfrontend\n \nstart\n\n \n# For RHEL 7, CentOS 7, and SL7\n\n \nroot\n@\nhost\n # \nsystemctl\n \nstart\n \ngwms\n-\nfrontend\n\n\n\n\n\n\nAdding Gratia Accounting and a Local Monitoring Page on a Production Server\n\n\nYou must report to Gratia if you are running on OSG more than a few test jobs.\n\n\nProbeConfigGlideinWMS\n explains how to instal and configure the HTCondor Gratia probe. If you are on a Campus Grid without x509 certificates pay attention to the \nUsers without Certificates part\n in the Unusual Use Cases section.\n\n\nOptional Configuration\n\n\nThe following configuration steps are optional and will likely not be required for setting up a small site. If you do not need any of the following special configurations, skip to \nthe section on service activation/deactivation\n.\n\n\n\n\nAllow users to specify where their jobs run\n\n\nCreating a group to test configuration changes\n\n\n\n\nAllow users to specify where their jobs run\n\n\nIn order to allow users to specify the sites at which their jobs want to run (or to test a specific site), a frontend can be configured to match on \nDESIRED_Sites\n or ignore it if not specified. Modify \n/etc/gwms-frontend/frontend.xml\n using the following instructions:\n\n\n\n\n\n\nIn the frontend's global \nmatch\n stanza, set the \nmatch_expr\n:\n\n\n((job.get(\nDESIRED_Sites\n,\nnosite\n)==\nnosite\n) or (glidein[\nattrs\n][\nGLIDEIN_Site\n] in job.get(\nDESIRED_Sites\n,\nnosite\n).split(\n,\n)))\n\n\n\n\n\n\n\n\n\n\nIn the same \nmatch\n stanza, set the \nstart_expr\n:\n\n\n(DESIRED_Sites=?=undefined || stringListMember(GLIDEIN_Site,DESIRED_Sites,\n,\n))\n\n\n\n\n\n\n\n\n\nAdd the \nDESIRED_Sites\n attribute to the match attributes list:\n\n\nmatch_attrs\n\n   \nmatch_attr\n \nname=\nDESIRED_Sites\n \ntype=\nstring\n/\n\n\n/match_attrs\n\n\n\n\n\n\n\n\n\n\nReconfigure the Frontend:\n\n\nroot@host #\n /etc/init.d/gwms-frontend reconfig\n\n\n\n\n\n\n\n\n\nCreating a group for testing configuration changes\n\n\nTo perform configuration changes without impacting production the recommended way is to create an ITB group in \n/etc/gwms-frontend/frontend.xml\n. This groupwould only match jobs that have the \n+is_itb=True\n ClassAd.\n\n\n\n\n\n\nCreate a \ngroup\n named itb.\n\n\n\n\n\n\nSet the group's \nstart_expr\n so that the group's glideins will only match user jobs with \n+is_itb=True\n:\n\n\nmatch match_expr=\nTrue\n start_expr=\n(is_itb)\n\n\n\n\n\n\n\n\n\n\nSet the \nfactory_query_expr\n so that this group only communicates with ITB factories:\n\n\nfactory query_expr=\nFactoryType=?=\nitb\n\n\n\n\n\n\n\n\n\n\nSet the group's \ncollector\n stanza to reference the ITB factory, replacing \nusername@gfactory-1.t2.ucsd.edu\n with your factory identity:\n\n\ncollector DN=\n/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=glidein-itb.grid.iu.edu\n \\\n          factory_identity=\ngfactory@glidein-itb.grid.iu.edu\n \\\n          my_identity=\nusername@gfactory-1.t2.ucsd.edu\n \\\n          node=\nglidein-itb.grid.iu.edu\n/\n\n\n\n\n\n\n\n\n\n\nSet the job \nquery_expr\n so that only ITB jobs appear in \ncondor_q\n:\n\n\njob query_expr=\n(!isUndefined(is_itb) \n is_itb)\n\n\n\n\n\n\n\n\n\n\nReconfigure the Frontend:\n\n\n/etc/init.d/gwms-frontend reconfig\n\n\n\n\n\n\n\n\n\nService Activation and Deactivation\n\n\nThe scripts updating your CA and CRLs plus three frontend services need to be running:\n\n\n\n\n\n\nYou need to fetch the latest CA Certificate Revocation Lists (CRLs) and you should enable the fetch-crl service to keep the CRLs up to date:\n\n\n%\nRED%# For RHEL \n6\n, CentOS \n6\n, and SL6, or OSG \n3\n _older_ than \n3\n.1.15\n\n\nroot@host #\n /usr/sbin/fetch-crl   \n# This fetches the CRLs\n\n\nroot@host #\n /sbin/service fetch-crl-boot start\n\nroot@host #\n /sbin/service fetch-crl-cron start\n\n%\nRED% \n# For RHEL 7, CentOS 7, and SL7 \n\n\nroot@host #\n /usr/sbin/fetch-crl   \n# This fetches the CRLs\n\n\nroot@host #\n systemctl start fetch-crl-boot\n\nroot@host #\n systemctl start fetch-crl-cron\n\n\n\n\n\n\n\n\n\nHTCondor, httpd, VO Frontend\n\n\n%\nRED%# For RHEL \n6\n, CentOS \n6\n, and SL6\n\n\nroot@host #\n service condor start\n\nroot@host #\n service httpd start\n\nroot@host #\n service gwms-frontend start\n\n%\nRED%# For RHEL \n7\n, CentOS \n7\n, and SL7\n\n\nroot@host #\n systemctl start condor\n\nroot@host #\n systemctl start gwms-frontend\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nOnce you successfully start using the frontend service, each time you change the configuration or want to upgrade, you run the following command\n\n\n%\nRED%# For RHEL \n6\n, CentOS \n6\n, and SL6\n\n\nroot@host #\n service gwms-frontend reconfig\n\n%\nRED%# And \nif\n you change also some code\n\n\nroot@host #\n service gwms-frontend upgrade\n\n\n%\nRED%# But the situation is a bit more complicated in RHEL \n7\n, CentOS \n7\n, and SL7 due to systemd restrictions\n\n\n%\nGREEN%# For reconfig:\n\n\n%\nRED%A. when the frontend is running\n\n\n%\nRED%A.1 without any additional options\n\n\nroot@host #\n /usr/sbin/gwms-frontend reconfig\n\n\nor\n\n\nroot@host #\n systemctl reload gwms-frontend\n\n\n%\nRED%A.2 \nif\n you want to give additional options \n\n\nsystemctl stop gwms-frontend\n\n\n/usr/sbin/gwms-frontend reconfig \nand your options\n\n\nsystemctl start gwms-frontend\n\n\n\n%\nRED%B. when the frontend is NOT running \n\n\nroot@host #\n /usr/sbin/gwms-frontend reconfig \n(\nand your options\n)\n\n\n\n%\nGREEN%# For upgrade:\n\n\n%\nRED%A. when the frontend is running \n\n\nsystemctl stop gwms-frontend\n\n\n/usr/sbin/gwms-frontend upgrade (\nand your options if any\n)\n\n\nsystemctl start gwms-frontend\n\n\n\n%\nRED%B. when the frontend is NOT running \n\n\n/usr/sbin/gwms-frontend upgrade (\nand your options if any\n)\n\n\n\n\n\n\n\n\nTo stop the frontend:\n\n\n%\nRED%# For RHEL \n6\n, CentOS \n6\n, and SL6\n\n\nroot@host #\n service gwms-frontend stop\n\n%\nRED%# For RHEL \n7\n, CentOS \n7\n, and SL7\n\n\nroot@host #\n systemctl stop gwms-frontend\n\n\n\n\n\nAnd you can stop also the other services if you are not using them independently form the frontend.\n\n\nValidation of Service Operation\n\n\nThe complete validation of the frontend is the submission of actual jobs. However, there are a few things that can be checked prior to submitting user jobs to Condor.\n\n\n\n\n\n\nVerify all Condor daemons are started.\n\n\nuser@host $ condor_config_val -verbose DAEMON_LIST\nDAEMON_LIST: MASTER,  COLLECTOR, NEGOTIATOR,  SCHEDD, SHARED_PORT, SCHEDDJOBS2 COLLECTOR0 COLLECTOR1 COLLECTOR2\nCOLLECTOR3 COLLECTOR4 COLLECTOR5 COLLECTOR6 COLLECTOR7 COLLECTOR8 COLLECTOR9 COLLECTOR10 , COLLECTOR11,\nCOLLECTOR12, COLLECTOR13, COLLECTOR14, COLLECTOR15, COLLECTOR16, COLLECTOR17, COLLECTOR18, COLLECTOR19, COLLECTOR20,\nCOLLECTOR21, COLLECTOR22, COLLECTOR23, COLLECTOR24, COLLECTOR25, COLLECTOR26, COLLECTOR27, COLLECTOR28, COLLECTOR29,\nCOLLECTOR30, COLLECTOR31, COLLECTOR32, COLLECTOR33, COLLECTOR34, COLLECTOR35, COLLECTOR36, COLLECTOR37, COLLECTOR38,\nCOLLECTOR39, COLLECTOR40\nDefined in \n/etc/condor/config.d/11_gwms_secondary_collectors.config\n, line 193.\n\n\n\n\n\nIf you don't see all the collectors and the two schedd, then the configuration must be corrected. There should be no startd daemons listed\n\n\n\n\n\n\nVerify all VO Frontend Condor services are communicating.\n\n\nuser@host $ condor_status -any\nMyType               TargetType           Name\nglideresource        None                 MM_fermicloud026@gfactory_inst\nScheduler            None                 fermicloud020.fnal.gov\nDaemonMaster         None                 fermicloud020.fnal.gov\nNegotiator           None                 fermicloud020.fnal.gov\nCollector            None                 frontend_service@fermicloud020\nScheduler            None                 schedd_jobs2@fermicloud020.fnal\n\n\n\n\n\n\n\n\n\nTo see the details of the glidein resource use \ncondor_status -subsystem glideresource -l\n, including the GlideFactoryName.\n\n\n\n\n\n\nVerify that the Factory is seeing correctly the Frontend using \ncondor_status -pool \nFACTORY_HOST\n -any -constraint 'FrontendName==\n\"FRONTEND_NAME_FROM_CONFIG\"\n' -l\n, including the GlideFactoryName.\n\n\n\n\n\n\nGlidein WMS Job submission\n\n\nCondor submit file \nglidein-job.sub\n. This is a simple job printing the hostname of the host where the job is running:\n\n\n#file glidein-job.sub\nuniverse = vanilla\nexecutable = /bin/hostname\noutput = glidein/test.out\nerror = glidein/test.err\nrequirements = IS_GLIDEIN == True\nlog = glidein/test.log\nShouldTransferFiles = YES\n\nwhen_to_transfer_output = ON_EXIT\nqueue\n\n\n\n\n\nTo submit the job:\n\n\nroot@host #\n condor_submit glidein-job.sub\n\n\n\n\n\nThen you can control the job like a normal condor job, e.g. to check the status of the job use \ncondor_q\n.\n\n\nMonitoring Web pages\n\n\nYou should be able to see the jobs also in the GWMS monitoring pages that are made available on the Web: \nhttp://gwms-frontend-host.domain/vofrontend/monitor/\n\n\nTroubleshooting\n\n\nFile Locations\n\n\n\n\n\n\n\n\nFile Description\n\n\nFile Location\n\n\n\n\n\n\n\n\n\n\nConfiguration file\n\n\n/etc/gwms-frontend/frontend.xml\n\n\n\n\n\n\nLogs\n\n\n/var/log/gwms-frontend/\n\n\n\n\n\n\nStartup script\n\n\n/etc/init.d/gwms-frontend\n\n\n\n\n\n\nWeb Directory\n\n\n/var/lib/gwms-frontend/web-area\n\n\n\n\n\n\nWeb Base\n\n\n/var/lib/gwms-frontend/web-base\n\n\n\n\n\n\nWeb configuration\n\n\n/etc/httpd/conf.d/gwms-frontend.conf\n\n\n\n\n\n\nWorking Directory\n\n\n/var/lib/gwms-frontend/vofrontend/\n\n\n\n\n\n\nLock files\n\n\n/etc/init.d/gwms-frontend/vofrontend/lock/frontend.lock /etc/init.d/gwms-frontend/vofrontend/group_*/lock/frontend.lock\n\n\n\n\n\n\nStatus files\n\n\n/var/lib/gwms-frontend/vofrontend/monitor/group_*/frontend_status.xml\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n/var/lib/gwms-frontend\n is also the home directory of the \nfrontend\n user\n\n\n\n\nCertificates brief\n\n\nHere a short list of files to check when you change the certificates. Note that if you renew a proxy or certificate and the DN remains the same no configuration file needs to change, just put the renewed certificate/proxy in place.\n\n\n\n\n\n\n\n\nFile Description\n\n\nFile Location\n\n\n\n\n\n\n\n\n\n\nConfiguration file\n\n\n/etc/gwms-frontend/frontend.xml\n\n\n\n\n\n\nHTCondor certificates map\n\n\n/etc/condor/creds/condor_mapfile (1)\n\n\n\n\n\n\nHost certificate and key (2)\n\n\n/etc/grid-security/hostcert.pem            /etc/grid-security/hostkey.pem\n\n\n\n\n\n\nVO Frontend proxy (from host certificate)\n\n\n/tmp/vofe_proxy (3)\n\n\n\n\n\n\nPilot proxy\n\n\n/tmp/vofe_proxy (3)\n\n\n\n\n\n\n\n\n\n\n\n\nIf using HTCondor RPM installation, e.g. the one coming from OSG. If you have separate/multiple HTCondor hosts (schedds, collectors, negotiators, ..) you may have to check this file on all of them to make sure that the HTCondor authentication works correctly.\n\n\n\n\n\n\nUsed to create the VO Frontend proxy if following the \ninstructions above\n\n\n\n\n\n\nIf using the scripts described \nabove in this document\n\n\n\n\n\n\nRemember also that when you change DN:\n\n\n\n\nThe VO Frontend certificate DN must be communicated to the GWMS Factory (\nsee above\n)\n\n\nThe pilot proxy must be able to run jobs at the sites you are using, e.g. by being added to the correct VO in OSG (the Factory forwards the proxy and does not care about the DN)\n\n\n\n\nIncrease the log level and change rotation policies\n\n\nYou can increase the log level of the frontend. To add a log file with all the log information add the following line with all the message types in the \nprocess_log\n section of \n/etc/gwms-frontend/frontend.xml\n:\n\n\nlog_retention\n\n   \nprocess_logs\n\n       \nprocess_log extension=\nall\n max_days=\n7.0\n max_mbytes=\n100.0\n min_days=\n3.0\n msg_types=\nDEBUG,EXCEPTION,INFO,ERROR,ERR\n/\n\n\n\n\n\n\nYou can also change the rotation policy and choose whether compress the rotated files, all in the same section of the config files:\n\n\n\n\nmax_bytes is the max size of the log files\n\n\nmax_days it will be rotated.\n\n\ncompression specifies if rotated files are compressed\n\n\nbackup_count is the number of rotated log files kept\n\n\n\n\nFurther details are in the \nreference documentation\n.\n\n\nFrontend reconfig failing\n\n\nIf \nservice gwms-frontend reconfig\n fails at the end with an error like \"Writing back config file failed, Reconfiguring the frontend [FAILED]\", make sure that \n/etc/gwms-frontend/\n belongs to the \nfrontend\n user. It must be able to write to update the configuration file.\n\n\nFrontend failing to start\n\n\nIf the startup script of the frontend is failing, check the log file for errors (probably \n/var/log/gwms-frontend/frontend/frontend.\nTODAY\n.err.log\n and \n.debug.log\n).\n\n\nIf you find errors like \n\"Exception occurred: ... 'ExpatError: no element found: line 1, column 0\\n']\"\n and \n\"IOError: [Errno 9] Bad file descriptor\"\n you may have an empty status file (\n/var/lib/gwms-frontend/vofrontend/monitor/group_*/frontend_status.xml\n) that causes Glidein WMS Frontend not to start. The glideinFrontend crashes after a XML parsing exception visible in the log file (\"Exception occurred: ... 'ExpatError: no element found: line 1, column 0\\n']\").\n\n\nRemove the status file. Then start the frontend. The fronten will be fixed in future versions to handle this automatically.\n\n\nCertificates not there\n\n\nThe scripts should send an email warning if there are problems and they fail to generate the proxies. Anyway something could go wrong and you want to check manually. If you are using the scripts to generate automatically the proxies but the proxies are not there (in \n/tmp\n or wherever you expect them):\n\n\n\n\nmake sure that the scripts are there and configured with the correct values\n\n\nmake sure that the scripts are executable\n\n\nmake sure that the scripts are in \nfrontend\n 's crontab\n\n\nmake sure that the certificates (or master proxy) used to generate the proxies is not expired\n\n\n\n\nFailed authentication\n\n\nIf you get a failed authentication error (e.g. \"Failed to talk to factory_pool gfactory-1.t2.ucsd.edu...) then:\n\n\n\n\ncheck that you have the right x509 certificates mentioned in the security section of \n/etc/gwms-frontend/frontend.xml\n\n\nthe owner must be \nfrontend\n (user running the frontend)\n\n\nthe permission must be 600\n\n\nthey must be valid for more than one hour (2/300 hours), at least the non VO part\n\n\n\n\n\n\ncheck that the clock is synchronized (see HostTimeSetup)\n\n\n\n\nFrontend doesn't trust factory\n\n\nIf your frontend complains in the debug log:\n\n\ncode 256:[\nError: communication error\\n\n, \nAUTHENTICATE:1003:Failed to authenticate with any method\\n\n, \nAUTHENTICATE:1004:Failed to authenticate using GSI\\n\n, \nGSI:5006:Failed to authenticate because the subject \n/DC=org/DC=doegrids/OU=Services/CN=devg-3.t2.ucsd.edu\n is not currently trusted by you.  If it should be, add it to GSI_DAEMON_NAME in the condor_config, or use the environment variable override (check the manual).\\n\n, \nGSI:5004:Failed to gss_assist_gridmap /DC=org/DC=doegrids/OU=Services/CN=devg-3.t2.ucsd.edu to a local user.\n\n\n\n\n\nA possible solution is to comment/remove the LOCAL_CONFIG_DIR in the file \n/var/lib/gwms-frontend/vofrontend/frontend.condor_config\n.\n\n\nNo security credentials match for factory pool ..., not advertising request\n\n\nYou may see a warning like \"No security credentials match for factory pool ..., not advertising request\", if the \ntrust_domain\n and \nauth_method\n of an entry in the Factory configuration is not matching any of the \ntrust_domain\n, \ntype\n couples in the credentials in the Frontend configuration. This causes the Frontend not to use some Factory entries (the ones not matching) and may end up without entries to send glideins to.\n\n\nTo fix the problem make sure that those attributes match as desired.\n\n\nJobs not running\n\n\nIf your jobs remain Idle\n\n\n\n\nCheck the frontend log files (see above)\n\n\nCheck the condor log files (\ncondor_config_val LOG\n will give you the correct log directory):\n\n\nSpecifically look the CollectorXXXLog files\n\n\n\n\n\n\n\n\nCommon causes of problems could be:\n\n\n\n\nx509 certificates\n\n\nmissing or expired or too short-lived proxy\n\n\nincorrect ownership or permission on the certificate/proxy file\n\n\nmissing certificates\n\n\n\n\n\n\nIf the frontend http server is down in the factory there will be errors like \"Failed to load file 'description.dbceCN.cfg' from \nhttp://FRONTEND_HOST/vofrontend/stage\n.\"\n\n\ncheck that the http server is running and you can reach the URL (\nhttp://FRONTEND_HOST/vofrontend/stage/description.dbceCN.cfg\n)\n\n\n\n\n\n\n\n\nAdvanced Configurations\n\n\n\n\nGlideinWMS Frontend on a Campus Grid\n\n\n\n\nReferences\n\n\nDefinitions:\n\n\n\n\nWhat is a \nVirtual Organisation\n\n\nIntroduction to the Grid for users/scientists\n\n\n\n\nDocuments about the Glidein-WMS system and the VO frontend:\n\n\n\n\nhttp://www.uscms.org/SoftwareComputing/Grid/WMS/glideinWMS/\n\n\nhttp://www.uscms.org/SoftwareComputing/Grid/WMS/glideinWMS/doc.prd/manual/\n\n\nHow to setup a Submit host flocking to the VO Frontend", 
            "title": "GlideinWMS Frontend"
        }, 
        {
            "location": "/other/install-gwms-frontend/#glideinwms-vo-frontend-installation", 
            "text": "", 
            "title": "GlideinWMS VO Frontend Installation"
        }, 
        {
            "location": "/other/install-gwms-frontend/#about-this-document", 
            "text": "This document describes how to install the Glidein Workflow Managment System (GlideinWMS) VO Frontend for use with the OSG glidein factory. This software is the minimum requirement for a VO to use glideinWMS.  This document assumes expertise with Condor and familiarity with the glideinWMS software. It  does not  cover anything but the simplest possible install. Please consult the  Glidein WMS reference documentation  for advanced topics, including non- root , non-RPM-based installation.  This document covers three components of the GlideinWMS a VO needs to install:   User Pool Collectors : A set of  condor_collector  processes. Pilots submitted by the factory will join to one of these collectors to form a Condor pool.  User Pool Schedd : A  condor_schedd . Users may submit Condor vanilla universe jobs to this schedd; it will run jobs in the Condor pool formed by the  User Pool Collectors .  Glidein Frontend : The frontend will periodically query the  User Pool Schedd  to determine the desired number of running job slots. If necessary, it will request the factory to launch additional pilots.   This guide covers installation of all three components on the same host: it is designed for small to medium VOs (see the Hardware Requirements below). Given a significant, large host, we have been able to scale the single-host install to 20,000 running jobs.", 
            "title": "About This Document"
        }, 
        {
            "location": "/other/install-gwms-frontend/#release", 
            "text": "This document reflects glideinWMS v3.2.17.", 
            "title": "Release"
        }, 
        {
            "location": "/other/install-gwms-frontend/#how-to-get-help", 
            "text": "To get assistance about the OSG software please use  this page .  For specific questions about the Frontend configuration (and how to add it in your HTCondor infrastructure) you can email the glideinWMS support   To request access the OSG Glidein Factory (e.g. the UCSD factory) you have to send an email to   (see below).", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/other/install-gwms-frontend/#requirements", 
            "text": "", 
            "title": "Requirements"
        }, 
        {
            "location": "/other/install-gwms-frontend/#host-and-os", 
            "text": "A host to install the GlideinWMS Frontend (pristine node).  OS is Red Hat Enterprise Linux 6, 7, and variants (see  details ). Currently most of our testing has been done on Scientific Linux 6.  Root access   The Glidein WMS VO Frontend has the following hardware requirements for a production host:   CPU : Four cores, preferably no more than 2 years old.  RAM : 3GB plus 2MB per running job. For example, to sustain 2000 running jobs, a host with 5GB is needed.  Disk : 30GB will be plenty sufficient for all the binaries, config and log files related to glideinWMS. As this will be an interactive submit host, plan enough disk space for your users' jobs. Depending on your workflow, this might require 2MB to 2GB per job in a workflow.", 
            "title": "Host and OS"
        }, 
        {
            "location": "/other/install-gwms-frontend/#users", 
            "text": "The Glidein WMS Frontend installation will create the following users unless they are already created.     User  Default uid  Comment      apache  48  Runs httpd to provide the monitoring page (installed via dependencies).    condor  none  Condor user (installed via dependencies).    frontend  none  This user runs the glideinWMS VO frontend. It also owns the credentials forwarded to the factory to use for the glideins.    gratia  none  Runs the Gratia probes to collect accounting data (optional see  the Gratia section below )     Note that if uid 48 is already taken but not used for the appropriate users, you will experience errors.  Details...", 
            "title": "Users"
        }, 
        {
            "location": "/other/install-gwms-frontend/#credentials-and-proxies", 
            "text": "The VO Frontend will use two credentials in its interactions with the the other glideinWMS services. At this time, these will be proxy files.   the  VO Frontend proxy  (used to authenticate with the other glideinWMS services).  one or more glideinWMS  pilot proxies  (used/delegated to the factory services and submitted on the glideinWMS pilot jobs).   The  VO Frontend proxy  and the   pilot proxy  can be the same. By default, the VO Frontend will run as user  frontend  (UID is machine dependent) so these proxies must be owned by the user  frontend .", 
            "title": "Credentials and Proxies"
        }, 
        {
            "location": "/other/install-gwms-frontend/#vo-frontend-proxy", 
            "text": "The use of a service certificate is recommended. Then you create a proxy from the certificate as explained in the  proxy configuration section . This can be a plain grid proxy (from  grid-proxy-init ), no VO extensions are required.  You must notify the Factory operation of the DN of this proxy when you initially setup the frontend and each time the DN changes .", 
            "title": "VO Frontend proxy"
        }, 
        {
            "location": "/other/install-gwms-frontend/#pilot-proxies", 
            "text": "This proxy is used by the factory to submit the glideinWMS pilot jobs. Therefore, they must be authorized to access to the CEs (factory entry points) where jobs are submitted. There is no need to notify the Factory operation about the DN of this proxy (neither at the initial registration nor for subsequent changes). This second proxy has no special requirement or controls added by the factory but will probably require VO attributes because of the CEs: if you are able to use this proxy to submit jobs to the CEs where the Factory runs glideinWMS pilots for you, then the proxy is OK. You can test your proxy using  globusrun  or HTCondor-G  To check the important information about a pem certificate you can use:  openssl x509 -in /etc/grid-security/hostcert.pem -subject -issuer -dates -noout . You will need that to find out information for the configuration files and the request to the GlideinWMS factory.", 
            "title": "Pilot proxies"
        }, 
        {
            "location": "/other/install-gwms-frontend/#certificatesproxies-configuration-example", 
            "text": "This document has a  proxy configuration section  that uses the host certificate/key and a user certificate to generate the required proxies.     Certificate  User that owns certificate  Path to certificate      Host certificate  root  /etc/grid-security/hostcert.pem              /etc/grid-security/hostkey.pem     Here  are instructions to request a host certificate.", 
            "title": "Certificates/Proxies configuration example"
        }, 
        {
            "location": "/other/install-gwms-frontend/#networking", 
            "text": "Service Name  Protocol  Port Number  Inbound  Outbound  Comment      HTCondor port range  tcp  LOWPORT, HIGHPORT  YES   contiguous range of ports    GlideinWMS Frontend  tcp  9618 to 9660  YES   HTCondor Collectors for the GlideinWMS Frontend (received ClassAds from resources and jobs)     The VO frontend must have reliable network connectivity, be on the public internet (no NAT), and preferably with no firewalls. Each running pilot requires 5 outgoing TCP ports. Incoming TCP ports 9618 to 9660 must be open.   For example, 2000 running jobs require about 10,100 TCP connections. This will overwhelm many firewalls; if you are unfamiliar with your network topology, you may want to warn your network administrator.", 
            "title": "Networking"
        }, 
        {
            "location": "/other/install-gwms-frontend/#before-the-installation", 
            "text": "Once all requirements are satisfied you must take a couple of actions before installing the Frontend:   you need all the data to connect to a GWMS Factory  Remember to install HTCondor BEFORE installing the Frontend ( instructions are below )", 
            "title": "Before the installation"
        }, 
        {
            "location": "/other/install-gwms-frontend/#osg-factory-access", 
            "text": "Before installing the Glidein WMS VO Frontend you need the information about a  Glidein Factory  that you can access:   (recommended) OSG is managing a factory at UCSD and one at GOC and you can request access to them  You have another Glidein Factory that you can access  You  install your own Glidein Factory   To request access to the OSG Glidein Factory at UCSD you have to send an email to   providing:   Your Name  The VO that is utilizing the VO Frontend  The DN of the proxy you will use to communicate with the Factory (VO Frontend DN, e.g. the host certificate subject if you follow the  proxy configuration section )  You can propose a security name that will have to be confirmed/changed by the Factory managers (see below)   A list of sites where you want to run:    Your VO must be supported on those sites   You can provide a list or piggy back on existing lists, e.g. all the sites supported for the VO. Check with the Factory managers  You can start with one single site   In the reply from the OSG Factory managers you will receive some information needed for the configuration of your VO Frontend   The exact spelling and capitalization of your VO name. Sometime is different from what is commonly used, e.g. OSG VO is \"OSGVO\".  The host of the Factory Collector:  gfactory-1.t2.ucsd.edu  The DN os the factory, e.g.  /DC=org/DC=doegrids/OU=Services/CN=gfactory-1.t2.ucsd.edu  The factory identity, e.g.:  gfactory@gfactory-1.t2.ucsd.edu  The identity on the factory you will be mapped to. Something like:  username@gfactory-1.t2.ucsd.edu  Your security name. A unique name, usually containing your VO name:  My_SecName  A string to add in the main factory query_expr in the frontend configuration, e.g.  stringListMember(\" VO \",GLIDEIN_Supported_VOs) . From there you get the correct name of the VO (above in this list).", 
            "title": "OSG Factory access"
        }, 
        {
            "location": "/other/install-gwms-frontend/#installation-procedure", 
            "text": "Refer to installation of the  CA certificates", 
            "title": "Installation Procedure"
        }, 
        {
            "location": "/other/install-gwms-frontend/#install-htcondor", 
            "text": "Most required software is installed from the Frontend RPM installation. HTCondor is the only exception since there are  many different ways to install it , using the RPM system or not. You need to have HTCondor installed before installing the Glidein WMS Frontend. If yum cannot find a HTCondor RPM, it will install the dummy  empty-condor  RPM, assuming that you installed HTCondor using a tarball distribution.  If you don't have HTCondor already installed, you can install the HTCondor RPM from the OSG repository:  root@host #  yum install condor.x86_64 #  If you have a  32  bit host use instead: root@host #  yum install condor.i386  See  this HTCondor document  for more information on the different options.", 
            "title": "Install HTCondor"
        }, 
        {
            "location": "/other/install-gwms-frontend/#download-and-install-the-vo-frontend-rpm", 
            "text": "The RPM is available in the OSG repository:  Install the RPM and dependencies (be prepared for a lot of dependencies).  root@host #  yum install glideinwms-vofrontend  This will install the current production release verified and tested by OSG with default condor configuration. This command will install the glideinwms vofrontend, condor, the OSG client, and all the required dependencies all on one node.  If you wish to install a different version of GlideinWMS, add the \"--enablerepo\" argument to the command as follows:   yum install --enablerepo=osg-testing glideinwms-vofrontend : The most recent production release, still in testing phase. This will usually match the current tarball version on the GlideinWMS home page. (The osg-release production version may lag behind the tarball release by a few weeks as it is verified and packaged by OSG). Note that this will also take the osg-testing versions of all dependencies as well.  yum install --enablerepo=osg-contrib glideinwms-vofrontend : The most recent development series release, ie version 3 release. This has newer features such as cloud submission support, but is less tested.   Note that these commands will install default condor configurations with all services on one node.", 
            "title": "Download and install the VO Frontend RPM"
        }, 
        {
            "location": "/other/install-gwms-frontend/#advanced-multi-node-installation", 
            "text": "For advanced users requiring heavy usage on their submit node, you may want to consider splitting the usercollector, user submit, and vo frontend services.  This can be doing using the following three commands (on different machines):  root@host #  yum install glideinwms-vofrontend-standalone root@host #  yum install glideinwms-usercollector root@host #  yum install glideinwms-userschedd  In addition, you will need to perform the following steps:   On the vofrontend and userschedd, modify CONDOR_HOST to point to your usercollector. This is in  /etc/condor/config.d/00_gwms_general.config . You can also override this value by placing it in a new config file. (For instance,  /etc/condor/config.d/99_local_custom.config  to avoid rpmsave/rpmnew conflicts on upgrades).  In  /etc/condor/certs/condor_mapfile , you will need to all DNs for each machine (userschedd, usercollector, vofrontend). Take great care to escape all special characters. Alternatively, you can use the  glidecondor_addDN  to add these values.  In the  /etc/gwms-frontend/frontend.xml  file, change the schedd locations to match the correct server. Also change the collectors tags at the bottom of the file. More details on frontend xml are in the following sections.", 
            "title": "Advanced: Multi-node Installation"
        }, 
        {
            "location": "/other/install-gwms-frontend/#upgrade-procedure", 
            "text": "If you have a working installation of glideinwms-frontend you can just upgrade the frontend rpms and skip the most of the configuration procedure below. These general upgrade instructions apply when upgrading the glideinwms-frontend rpm within same major versions.  % RED%# Update the glideinwms-vofrontend packages  root@host #  yum update glideinwms \\*  % RED%# Update the scripts in the working directory to the latest one  % RED%# For RHEL  7 , CentOS  7 , and SL7  root@host #  /usr/sbin/gwms-frontend upgrade % RED%# For RHEL  6 , CentOS  6 , and SL6  root@host #  service gwms-frontend upgrade % RED%# Restart HTCondor because the configuration may be different  root@host #  service condor restart   Note  The \\*  on the yum update is important. *    Warning  When you do a generic yum update that will update also condor, the upgrade may restore the personal condor config file that you have to remove with  rm /etc/condor/config.d/00personal_condor.config    Note  When upgrading to GlideinWMS 3.2.7 the second schedd is removed from the default configuration. For a smooth transition:   remove from  /etc/gwms-frontend/frontend.xml  the second schedd (the line containing  schedd_jobs2@YOUR_HOST )  reconfigure the frontend ( service gwms-frontend reconfig )  restart HTCondor ( service condor restart )", 
            "title": "Upgrade Procedure"
        }, 
        {
            "location": "/other/install-gwms-frontend/#configuration-procedure", 
            "text": "After installing the RPM, you need to configure the components of the glideinWMS VO Frontend:   Edit Frontend configuration options  Edit Condor configuration options  Create a Condor grid map file  Reconfigure and Start frontend", 
            "title": "Configuration Procedure"
        }, 
        {
            "location": "/other/install-gwms-frontend/#configuring-the-frontend", 
            "text": "The VO Frontend configuration file is  /etc/gwms-frontend/frontend.xml . The next steps will describe each line that you will need to edit if you are using the OSG Factory at UCSD. The portions to edit are highlighted in red font. If you are using a different Factory more changes are necessary, please check the VO Frontend configuration reference.    The VO you are affiliated with. This will identify those CEs that the glideinWMS pilot will be authorized to run on using the  pilot proxy  described previously in the this  section . Sometimes the whole  query_expr  is provided to you by the factory (see Factory access above):  factory query_expr= ((stringListMember( VO , GLIDEIN_Supported_VOs)))     Factory collector information. The  username  that you are assigned by the factory (also called the identity you will be mapped to on the factory, see above) . Note that if you are using a factory different than the production factory, you will have to change also  DN ,  factory_identity  and  node  attributes. (refer to the information provided to you by the factory operator):  collector DN= /DC=org/DC=doegrids/OU=Services/CN=gfactory-1.t2.ucsd.edu \n           comment= Define factory collector globally for simplicity \n           factory_identity= gfactory@gfactory-1.t2.ucsd.edu \n           my_identity= username@gfactory-1.t2.ucsd.edu \n           node= gfactory-1.t2.ucsd.edu /     Frontend security information.   The  classad_proxy  in the security entry is the location of the VO Frontend proxy described previously  here .  The  proxy_DN  is the DN of the  classad_proxy  above.  The  security_name  identifies this VO Frontend to the the Factory, It is provided by the factory operator.  The  absfname  in the credential (or proxy in v 2.x) entry is the location of the glideinWMS  pilot  proxy described in the requirements section  here . There can be multiple pilot proxies, or even other kind of keys (e.g. if you use cloud resources).  The type and trust_domain of the credential must match respectively auth_method and trust_domain used in the entry definition in the factory. If there is no match, between these two attributes in one of the credentials and some entry in one of the factories, then this frontend cannot trigger glideins. \nBoth the  classad_proxy  and  absfname  files should be owned by  frontend  user. # These lines are from the configuration of v 3.x security   classad_proxy= /tmp/vo_proxy   proxy_DN= DN of vo_proxy \n       proxy_selection_plugin= ProxyAll \n       security_name= The security name, this is used by factory \n       sym_key= aes_256_cbc \n       credentials \n         credential   absfname= /tmp/pilot_proxy   security_class= frontend \n         trust_domain= OSG   type= grid_proxy / \n       /credentials  /security       The schedd information.   The  DN  of the  VO Frontend Proxy  described previously  here .  The  fullname  attribute is the fully qualified domain name of the host where you installed the VO Frontend ( hostname --fqdn ).   A secondary schedd is optional. You will need to delete the secondary schedd line if you are not using it. Multiple schedds allow the frontend to service requests from multiple submit hosts.      schedds \n      schedd   DN= Cert DN used by the schedd at fullname: \n            fullname= Hostname of the schedd / \n       schedd   DN= Cert DN used by the second Schedd at fullname: \n             fullname= schedd name@Hostname of second schedd / \n    /schedds     The User Collector information.   The  DN  of the  VO Frontend Proxy  described previously  here .  The  node  attribute is the full hostname of the collectors ( hostname --fqdn ) and port  The  secondary  attribute indicates whether the element is for the primary or secondary collectors (True/False).   The default Condor configuration of the VO Frontend starts multiple Collector processes on the host ( /etc/condor/config.d/11_gwms_secondary_collectors.config ). The  DN  and  hostname  on the first line are the hostname and the host certificate of the VO Frontend. The  DN  and  hostname  on the second line are the same as the ones in the first one. The hostname (e.g. hostname.domain.tld) is filled automatically during the installation. The secondary collector ports can be defined as a range, e.g., 9620-9660).       collector DN= DN of main collector \n           node= hostname.domain.tld:9618  secondary= False / \n     collector DN= DN of secondary collectors (usually same as DN in line above) \n           node= hostname.domain.tld:9620-9660  secondary= True /      Warning  The Frontend configuration includes many knobs, some of which are conflicting with a RPM installation where there is only one version of the Frontend installed and it uses well known paths.     Do not change the following in the Frontend configuration (you must leave the default values coming with the RPM installation):   frontend_versioning='False' (in the first line of XML, versioning is useful to install multiple tarball versions)  work base_dir must be /var/lib/gwms-frontend/vofrontend/ (other scripts like /etc/init.d/gwms-frontend count on that value)", 
            "title": "Configuring the Frontend"
        }, 
        {
            "location": "/other/install-gwms-frontend/#if-you-have-a-different-factory", 
            "text": "The configuration above points to the OSG production Factory. If you are using a different Factory, then you have to:   replace  gfactory@gfactory-1.t2.ucsd.edu  and  gfactory-1.t2.ucsd.edu  with the correct values for your factory. And control also that the name used for the frontend () matches.  make sure that the factory is advertising the attributes used in the factory query expression ( query_expr ).", 
            "title": "If you have a different Factory"
        }, 
        {
            "location": "/other/install-gwms-frontend/#configuring-condor", 
            "text": "The condor configuration for the frontend is placed in  /etc/condor/config.d .   00_gwms_general.config  01_gwms_collectors.config  02_gwms_schedds.config  03_gwms_local.config  11_gwms_secondary_collectors.config  90_gwms_dns.config   Get rid of the pre-loaded condor default to avoid conflicts in the configuration.  root@host #  rm /etc/condor/config.d/00personal_condor.config  For most installations create a new file named  /etc/condor/config.d/92_local_condor.config", 
            "title": "Configuring Condor"
        }, 
        {
            "location": "/other/install-gwms-frontend/#using-other-condor-rpms-eg-uw-madison-htcondor-rpm", 
            "text": "The above procedure will work if you are using the OSG HTCondor RPMS. You can verify that you used the OSG HTCondor RPM by using  yum list condor . The version name should include \"osg\", e.g.  7.8.6-3.osg.el5 .  If you are using the UW Madison Condor RPMS, be aware of the following changes:   This Condor RPM uses a file  /etc/condor/condor_config.local  to add your local machine slot to the user pool.  If you want to disable this behavior (recommended), you should blank out that file or comment out the line in  /etc/condor/condor_config  for LOCAL_CONFIG_FILE. (Make sure that LOCAL_CONFIG_DIR is set to  /etc/condor/config.d )  Note that the variable LOCAL_DIR is set differently in UW Madison and OSG RPMs. This should not cause any more problems in the glideinwms RPMs, but please take note if you use this variable in your job submissions or other customizations.   In general if you are using a non OSG RPM or if you added custom configuration files for HTCondor please check the order of the configuration files:  root@host #  condor_config_val -config Configuration source:      /etc/condor/condor_config  Local configuration sources:      /etc/condor/config.d/00_gwms_general.config      /etc/condor/config.d/01_gwms_collectors.config      /etc/condor/config.d/02_gwms_schedds.config      /etc/condor/config.d/03_gwms_local.config      /etc/condor/config.d/11_gwms_secondary_collectors.config      /etc/condor/config.d/90_gwms_dns.config  % RED%/etc/condor/condor_config.local   If, like in the example above, the GlideinWMS configuration files are not the last ones in the list please verify that important configuration options have not been overridden by the other configuration files.", 
            "title": "Using other Condor RPMs, e.g. UW Madison HTCondor RPM"
        }, 
        {
            "location": "/other/install-gwms-frontend/#verify-your-condor-configuration", 
            "text": "The glideinWMS configuration files in  /etc/condor/config.d  should be the last ones in the list. If not, please verify that important configuration options have not been overridden by the other configuration files.    Verify the alll the expected HTCondor daemons are running:  root@host #  condor_config_val -verbose DAEMON_LIST DAEMON_LIST: MASTER, COLLECTOR, NEGOTIATOR, SCHEDD, SHARED_PORT, COLLECTOR0 COLLECTOR1 COLLECTOR2 COLLECTOR3 COLLECTOR4 COLLECTOR5 COLLECTOR6 COLLECTOR7 COLLECTOR8 COLLECTOR9 COLLECTOR10 , COLLECTOR11, COLLECTOR12,  COLLECTOR13, COLLECTOR14, COLLECTOR15, COLLECTOR16, COLLECTOR17, COLLECTOR18, COLLECTOR19, COLLECTOR20, COLLECTOR21, COLLECTOR22, COLLECTOR23,  COLLECTOR24, COLLECTOR25, COLLECTOR26, COLLECTOR27, COLLECTOR28, COLLECTOR29, COLLECTOR30, COLLECTOR31, COLLECTOR32, COLLECTOR33, COLLECTOR34,  COLLECTOR35, COLLECTOR36, COLLECTOR37, COLLECTOR38, COLLECTOR39, COLLECTOR40  Defined in  /etc/condor/config.d/11_gwms_secondary_collectors.config , line 193.     If you don't see all the collectors. shared port and the two schedd, then the configuration must be corrected. There should be  no   startd  daemons listed.", 
            "title": "Verify your Condor configuration"
        }, 
        {
            "location": "/other/install-gwms-frontend/#create-a-condor-grid-mapfile", 
            "text": "The Condor grid mapfile ( /etc/condor/certs/condor_mapfile ) is used for authentication between the glideinWMS pilot running on a remote worker node, and the local collector. Condor uses the mapfile to map certificates to pseudo-users on the local machine. It is important that you map the DN's of:    Each schedd proxy : The  DN  of each schedd that the frontend talks to. Specified in the frontend.xml schedd element  DN  attribute:  schedds \n   schedd   DN= /DC=org/DC=doegrids/OU=Services/CN=YOUR_HOST   fullname= YOUR_HOST / \n   schedd   DN= /DC=org/DC=doegrids/OU=Services/CN=YOUR_HOST   fullname= schedd_jobs2@YOUR_HOST /  /schedds     Frontend proxy : The DN of the proxy that the frontend uses to communicate with the other glideinWMS services. Specified in the frontend.xml security element  proxy_DN  attribute:  security classad_proxy= /tmp/vo_proxy  proxy_DN= DN of vo_proxy  ....    Each pilot proxy  The DN of  each  proxy that the frontend forwards to the factory to use with the glideinWMS pilots.  This allows the !glideinWMs pilot jobs to communicate with the User Collector. Specified in the frontend.xml proxy  absfname  attribute (you need to specify the  DN  of each of those proxies:  security   ....  proxies \n     proxy   absfname= /tmp/vo_proxy   .... \n    :  /proxies     Below is an example mapfile, by default found in  /etc/condor/certs/condor_mapfile . In this example there are lines for each of services mentioned above.   Note  The  example_of_format  entry as each DN should use this format for security purposes.   GSI  DN of schedd proxy  schedd\nGSI  DN of frontend proxy  frontend\nGSI  DN of pilot proxy $  pilot_proxy\nGSI  ^\\/DC\\=org\\/DC\\=doegrids\\/OU\\=Services\\/CN\\=personal\\-submit\\-host2\\.mydomain\\.edu$   example_of_format \nGSI (.*) anonymous\nFS (.*) \\1", 
            "title": "Create a Condor grid mapfile."
        }, 
        {
            "location": "/other/install-gwms-frontend/#restart-condor", 
            "text": "After configuring condor, be sure to restart condor:  root@host #  service condor restart", 
            "title": "Restart Condor"
        }, 
        {
            "location": "/other/install-gwms-frontend/#proxy-configuration", 
            "text": "There are 2 types of (or purposes for) proxies required for the VO Frontend: 1 the  VO Frontend proxy  (used to authenticate with the other glideinWMS services) 1 one or more glideinWMS  pilot proxies  (used/delegated to the factory services and submitted on the glideinWMS pilot jobs) The  VO Frontend proxy  and the  pilot proxy  can be the same. By default, the VO Frontend will run as user  frontend  (UID is machine dependent) so these proxies must be owned by the user  frontend .", 
            "title": "Proxy Configuration"
        }, 
        {
            "location": "/other/install-gwms-frontend/#manual-proxy-renewal", 
            "text": "VO Frontend proxy \nThe VO Frontend Proxy is used for communicating with the other glideinWMS services (Factory, User Collector and Schedd/Submit services). Create the proxy using the glidenWMS VO Frontend Host (or Service) cert and change ownership to the frontend user.  root@host #  voms-proxy-init-valid  hours_valid   \\ \n-cert /etc/grid-security/hostcert.pem  \\ \n-key /etc/grid-security/hostkey.pem  \\ \n-out  /tmp/vofe_proxy  root@host #  chown frontend  /tmp/vofe_proxy   Pilot proxy \nThe pilot proxy is used on the glideinWMS pilot jobs submitted to the CEs. Create the proxy using the  pilot certificate  and change ownership to the frontend user.  root@host #  voms-proxy-init -valid  hours_valid \\ \n-voms  vo  -cert  pilot_cert  \\  -key  pilot_key  \\  -out  /tmp/pilot_proxy  root@host #  chown frontend  /tmp/pilot_proxy    Warning  Proxies do expire.  You can extend the validity by using a longer time interval, e.g.  -valid 3000:0 . This sequence of commands will need to be renewed when the proxy expires or the machine reboots (if /tmp is used only).   Make sure that this location is specified correctly in the  frontend.xml  described in the  Configuring the Frontend  section.  You may want to automate the procedure above (or part of it) by writing a script and adding it to crontab.", 
            "title": "Manual proxy renewal"
        }, 
        {
            "location": "/other/install-gwms-frontend/#example-of-automatic-proxy-renewal", 
            "text": "This example (user provided) uses the script  make-proxy.sh  attached to this document. You still need to do some prep-work but this can be done only once a year and the script will warn you with an email.  Preparation for the  VO Frontend proxy . You'll have to redo this each time the Host (or Service) certificate and key are renewed:    Copy the Host (or Service) certificate and key  root@host #  cp /etc/grid-security/hostcert.pem /etc/grid-security/hostkey.pem /var/lib/gwms-frontend/    Change ownership and permission of the certificate and key  root@host #  chown frontend: /var/lib/gwms-frontend/host**.pem root@host #  chmod  0600  /var/lib/gwms-frontend/host**.pem    Preparation for the   pilot proxy . You'll have to redo this for each new or renewed pilot cert.    Create the proxy using the pilot certificate/key (as the user/submitter)  root@host #  grid-proxy-init -valid  8800 :0 -out /tmp/tmp_proxy    Copy the proxy to the correct name and change ownership and permissions (as root):  root@host #  cp /tmp/tmp_proxy /var/lib/gwms-frontend/vofe_base_gi_delegated_proxy root@host #  chown frontend: /var/lib/gwms-frontend/vofe_base_gi_delegated_proxy root@host #  chmod  0600  /var/lib/gwms-frontend/vofe_base_gi_delegated_proxy root@host #  rm /tmp/tmp_proxy    Configure the script for the  VO Frontend proxy    Download the  attached script  (the latest one is  Here on Github ) and save it as  /var/lib/gwms-frontend/make-frontend-proxy.sh , make sure that it is executable.    Edit the VARIABLES section to look something like (replace your email, host name and the paths that are different in your setup - the comments in the script will help):  SETUP_FILE= \nCERT_FILE= /var/lib/gwms-frontend/hostcert.pem \nKEY_FILE= /var/lib/gwms-frontend/hostkey.pem \nIN_NAME= /var/lib/gwms-frontend/frontend_base_proxy \nOUT_NAME= /tmp/vofe_proxy \nOWNER_EMAIL= your@email_here \nPROXY_DESCRIPTION= VO Fronted on  hostname \nVOMS_OPTION=     Configure the script for the  pilot proxy :    Download the  attached script  (the latest one is  Here on Github ) and save it as  /var/lib/gwms-frontend/make-pilot-proxy.sh , make sure that it is executable.    Edit the VARIABLES section to look something like (replace your email, host name and the paths that are different in your setup - the comments in the script will help):  SETUP_FILE= \nCERT_FILE= \nKEY_FILE= \nIN_NAME= /var/lib/gwms-frontend/vofe_base_gi_delegated_proxy \nOUT_NAME= /tmp/vofe_gi_delegated_proxy \nOWNER_EMAIL= your@email_here \nPROXY_DESCRIPTION= VO Fronted glidein delegated on  hostname \nVOMS_OPTION= osg:/osg     Before adding the scripts to the crontab I'd recommend to test them manually once to make sure that there are no errors. As user  frontend  run the scripts (you can also use  sh -x  to debug them):  /var/lib/gwms-frontend/make-frontend-proxy.sh --no-voms-proxy /var/lib/gwms-frontend/make-pilot-proxy.sh   Add the scripts to the crontab of the user  frontend  with  crontab -e :  10 * * * * /var/lib/gwms-frontend/make-frontend-proxy.sh --no-voms-proxy\n10 * * * * /var/lib/gwms-frontend/make-pilot-proxy.sh  An additional script like  make-proxy-control.sh  (the latest one is  Here on Github ) can be used for an independent verification of the proxies. If you like, download it, fix the variables and add it to the crontab like the other two.", 
            "title": "Example of automatic proxy renewal"
        }, 
        {
            "location": "/other/install-gwms-frontend/#reconfigure-and-verify-installation", 
            "text": "Warning  In order to use the frontend, first you must reconfigure and upgrade it.  # For RHEL 6, CentOS 6, and SL6  root @ host  #  service   gwms - frontend   reconfig  root @ host  #  service   gwms - frontend   upgrade  # For RHEL 7, CentOS 7, and SL7  root @ host  #  / usr / sbin / gwms - frontend   reconfig  root @ host  #  / usr / sbin / gwms - frontend   upgrade    After this initial reconfiguring/upgrading, you can start the frontend:    # For RHEL 6, CentOS 6, and SL6 \n  root @ host  #  service   gwms - frontend   start \n  # For RHEL 7, CentOS 7, and SL7 \n  root @ host  #  systemctl   start   gwms - frontend", 
            "title": "Reconfigure and verify installation"
        }, 
        {
            "location": "/other/install-gwms-frontend/#adding-gratia-accounting-and-a-local-monitoring-page-on-a-production-server", 
            "text": "You must report to Gratia if you are running on OSG more than a few test jobs.  ProbeConfigGlideinWMS  explains how to instal and configure the HTCondor Gratia probe. If you are on a Campus Grid without x509 certificates pay attention to the  Users without Certificates part  in the Unusual Use Cases section.", 
            "title": "Adding Gratia Accounting and a Local Monitoring Page on a Production Server"
        }, 
        {
            "location": "/other/install-gwms-frontend/#optional-configuration", 
            "text": "The following configuration steps are optional and will likely not be required for setting up a small site. If you do not need any of the following special configurations, skip to  the section on service activation/deactivation .   Allow users to specify where their jobs run  Creating a group to test configuration changes", 
            "title": "Optional Configuration"
        }, 
        {
            "location": "/other/install-gwms-frontend/#allow-users-to-specify-where-their-jobs-run", 
            "text": "In order to allow users to specify the sites at which their jobs want to run (or to test a specific site), a frontend can be configured to match on  DESIRED_Sites  or ignore it if not specified. Modify  /etc/gwms-frontend/frontend.xml  using the following instructions:    In the frontend's global  match  stanza, set the  match_expr :  ((job.get( DESIRED_Sites , nosite )== nosite ) or (glidein[ attrs ][ GLIDEIN_Site ] in job.get( DESIRED_Sites , nosite ).split( , )))     In the same  match  stanza, set the  start_expr :  (DESIRED_Sites=?=undefined || stringListMember(GLIDEIN_Site,DESIRED_Sites, , ))    Add the  DESIRED_Sites  attribute to the match attributes list:  match_attrs \n    match_attr   name= DESIRED_Sites   type= string /  /match_attrs     Reconfigure the Frontend:  root@host #  /etc/init.d/gwms-frontend reconfig", 
            "title": "Allow users to specify where their jobs run"
        }, 
        {
            "location": "/other/install-gwms-frontend/#creating-a-group-for-testing-configuration-changes", 
            "text": "To perform configuration changes without impacting production the recommended way is to create an ITB group in  /etc/gwms-frontend/frontend.xml . This groupwould only match jobs that have the  +is_itb=True  ClassAd.    Create a  group  named itb.    Set the group's  start_expr  so that the group's glideins will only match user jobs with  +is_itb=True :  match match_expr= True  start_expr= (is_itb)     Set the  factory_query_expr  so that this group only communicates with ITB factories:  factory query_expr= FactoryType=?= itb     Set the group's  collector  stanza to reference the ITB factory, replacing  username@gfactory-1.t2.ucsd.edu  with your factory identity:  collector DN= /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=glidein-itb.grid.iu.edu  \\\n          factory_identity= gfactory@glidein-itb.grid.iu.edu  \\\n          my_identity= username@gfactory-1.t2.ucsd.edu  \\\n          node= glidein-itb.grid.iu.edu /     Set the job  query_expr  so that only ITB jobs appear in  condor_q :  job query_expr= (!isUndefined(is_itb)   is_itb)     Reconfigure the Frontend:  /etc/init.d/gwms-frontend reconfig", 
            "title": "Creating a group for testing configuration changes"
        }, 
        {
            "location": "/other/install-gwms-frontend/#service-activation-and-deactivation", 
            "text": "The scripts updating your CA and CRLs plus three frontend services need to be running:    You need to fetch the latest CA Certificate Revocation Lists (CRLs) and you should enable the fetch-crl service to keep the CRLs up to date:  % RED%# For RHEL  6 , CentOS  6 , and SL6, or OSG  3  _older_ than  3 .1.15  root@host #  /usr/sbin/fetch-crl    # This fetches the CRLs  root@host #  /sbin/service fetch-crl-boot start root@host #  /sbin/service fetch-crl-cron start % RED%  # For RHEL 7, CentOS 7, and SL7   root@host #  /usr/sbin/fetch-crl    # This fetches the CRLs  root@host #  systemctl start fetch-crl-boot root@host #  systemctl start fetch-crl-cron    HTCondor, httpd, VO Frontend  % RED%# For RHEL  6 , CentOS  6 , and SL6  root@host #  service condor start root@host #  service httpd start root@host #  service gwms-frontend start % RED%# For RHEL  7 , CentOS  7 , and SL7  root@host #  systemctl start condor root@host #  systemctl start gwms-frontend     Note  Once you successfully start using the frontend service, each time you change the configuration or want to upgrade, you run the following command  % RED%# For RHEL  6 , CentOS  6 , and SL6  root@host #  service gwms-frontend reconfig % RED%# And  if  you change also some code  root@host #  service gwms-frontend upgrade % RED%# But the situation is a bit more complicated in RHEL  7 , CentOS  7 , and SL7 due to systemd restrictions  % GREEN%# For reconfig:  % RED%A. when the frontend is running  % RED%A.1 without any additional options  root@host #  /usr/sbin/gwms-frontend reconfig  or  root@host #  systemctl reload gwms-frontend % RED%A.2  if  you want to give additional options   systemctl stop gwms-frontend  /usr/sbin/gwms-frontend reconfig  and your options  systemctl start gwms-frontend  % RED%B. when the frontend is NOT running   root@host #  /usr/sbin/gwms-frontend reconfig  ( and your options )  % GREEN%# For upgrade:  % RED%A. when the frontend is running   systemctl stop gwms-frontend  /usr/sbin/gwms-frontend upgrade ( and your options if any )  systemctl start gwms-frontend  % RED%B. when the frontend is NOT running   /usr/sbin/gwms-frontend upgrade ( and your options if any )    To stop the frontend:  % RED%# For RHEL  6 , CentOS  6 , and SL6  root@host #  service gwms-frontend stop % RED%# For RHEL  7 , CentOS  7 , and SL7  root@host #  systemctl stop gwms-frontend  And you can stop also the other services if you are not using them independently form the frontend.", 
            "title": "Service Activation and Deactivation"
        }, 
        {
            "location": "/other/install-gwms-frontend/#validation-of-service-operation", 
            "text": "The complete validation of the frontend is the submission of actual jobs. However, there are a few things that can be checked prior to submitting user jobs to Condor.    Verify all Condor daemons are started.  user@host $ condor_config_val -verbose DAEMON_LIST\nDAEMON_LIST: MASTER,  COLLECTOR, NEGOTIATOR,  SCHEDD, SHARED_PORT, SCHEDDJOBS2 COLLECTOR0 COLLECTOR1 COLLECTOR2\nCOLLECTOR3 COLLECTOR4 COLLECTOR5 COLLECTOR6 COLLECTOR7 COLLECTOR8 COLLECTOR9 COLLECTOR10 , COLLECTOR11,\nCOLLECTOR12, COLLECTOR13, COLLECTOR14, COLLECTOR15, COLLECTOR16, COLLECTOR17, COLLECTOR18, COLLECTOR19, COLLECTOR20,\nCOLLECTOR21, COLLECTOR22, COLLECTOR23, COLLECTOR24, COLLECTOR25, COLLECTOR26, COLLECTOR27, COLLECTOR28, COLLECTOR29,\nCOLLECTOR30, COLLECTOR31, COLLECTOR32, COLLECTOR33, COLLECTOR34, COLLECTOR35, COLLECTOR36, COLLECTOR37, COLLECTOR38,\nCOLLECTOR39, COLLECTOR40\nDefined in  /etc/condor/config.d/11_gwms_secondary_collectors.config , line 193.  If you don't see all the collectors and the two schedd, then the configuration must be corrected. There should be no startd daemons listed    Verify all VO Frontend Condor services are communicating.  user@host $ condor_status -any\nMyType               TargetType           Name\nglideresource        None                 MM_fermicloud026@gfactory_inst\nScheduler            None                 fermicloud020.fnal.gov\nDaemonMaster         None                 fermicloud020.fnal.gov\nNegotiator           None                 fermicloud020.fnal.gov\nCollector            None                 frontend_service@fermicloud020\nScheduler            None                 schedd_jobs2@fermicloud020.fnal    To see the details of the glidein resource use  condor_status -subsystem glideresource -l , including the GlideFactoryName.    Verify that the Factory is seeing correctly the Frontend using  condor_status -pool  FACTORY_HOST  -any -constraint 'FrontendName== \"FRONTEND_NAME_FROM_CONFIG\" ' -l , including the GlideFactoryName.", 
            "title": "Validation of Service Operation"
        }, 
        {
            "location": "/other/install-gwms-frontend/#glidein-wms-job-submission", 
            "text": "Condor submit file  glidein-job.sub . This is a simple job printing the hostname of the host where the job is running:  #file glidein-job.sub\nuniverse = vanilla\nexecutable = /bin/hostname\noutput = glidein/test.out\nerror = glidein/test.err\nrequirements = IS_GLIDEIN == True\nlog = glidein/test.log\nShouldTransferFiles = YES\n\nwhen_to_transfer_output = ON_EXIT\nqueue  To submit the job:  root@host #  condor_submit glidein-job.sub  Then you can control the job like a normal condor job, e.g. to check the status of the job use  condor_q .", 
            "title": "Glidein WMS Job submission"
        }, 
        {
            "location": "/other/install-gwms-frontend/#monitoring-web-pages", 
            "text": "You should be able to see the jobs also in the GWMS monitoring pages that are made available on the Web:  http://gwms-frontend-host.domain/vofrontend/monitor/", 
            "title": "Monitoring Web pages"
        }, 
        {
            "location": "/other/install-gwms-frontend/#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/other/install-gwms-frontend/#file-locations", 
            "text": "File Description  File Location      Configuration file  /etc/gwms-frontend/frontend.xml    Logs  /var/log/gwms-frontend/    Startup script  /etc/init.d/gwms-frontend    Web Directory  /var/lib/gwms-frontend/web-area    Web Base  /var/lib/gwms-frontend/web-base    Web configuration  /etc/httpd/conf.d/gwms-frontend.conf    Working Directory  /var/lib/gwms-frontend/vofrontend/    Lock files  /etc/init.d/gwms-frontend/vofrontend/lock/frontend.lock /etc/init.d/gwms-frontend/vofrontend/group_*/lock/frontend.lock    Status files  /var/lib/gwms-frontend/vofrontend/monitor/group_*/frontend_status.xml      Note  /var/lib/gwms-frontend  is also the home directory of the  frontend  user", 
            "title": "File Locations"
        }, 
        {
            "location": "/other/install-gwms-frontend/#certificates-brief", 
            "text": "Here a short list of files to check when you change the certificates. Note that if you renew a proxy or certificate and the DN remains the same no configuration file needs to change, just put the renewed certificate/proxy in place.     File Description  File Location      Configuration file  /etc/gwms-frontend/frontend.xml    HTCondor certificates map  /etc/condor/creds/condor_mapfile (1)    Host certificate and key (2)  /etc/grid-security/hostcert.pem            /etc/grid-security/hostkey.pem    VO Frontend proxy (from host certificate)  /tmp/vofe_proxy (3)    Pilot proxy  /tmp/vofe_proxy (3)       If using HTCondor RPM installation, e.g. the one coming from OSG. If you have separate/multiple HTCondor hosts (schedds, collectors, negotiators, ..) you may have to check this file on all of them to make sure that the HTCondor authentication works correctly.    Used to create the VO Frontend proxy if following the  instructions above    If using the scripts described  above in this document    Remember also that when you change DN:   The VO Frontend certificate DN must be communicated to the GWMS Factory ( see above )  The pilot proxy must be able to run jobs at the sites you are using, e.g. by being added to the correct VO in OSG (the Factory forwards the proxy and does not care about the DN)", 
            "title": "Certificates brief"
        }, 
        {
            "location": "/other/install-gwms-frontend/#increase-the-log-level-and-change-rotation-policies", 
            "text": "You can increase the log level of the frontend. To add a log file with all the log information add the following line with all the message types in the  process_log  section of  /etc/gwms-frontend/frontend.xml :  log_retention \n    process_logs \n        process_log extension= all  max_days= 7.0  max_mbytes= 100.0  min_days= 3.0  msg_types= DEBUG,EXCEPTION,INFO,ERROR,ERR /   You can also change the rotation policy and choose whether compress the rotated files, all in the same section of the config files:   max_bytes is the max size of the log files  max_days it will be rotated.  compression specifies if rotated files are compressed  backup_count is the number of rotated log files kept   Further details are in the  reference documentation .", 
            "title": "Increase the log level and change rotation policies"
        }, 
        {
            "location": "/other/install-gwms-frontend/#frontend-reconfig-failing", 
            "text": "If  service gwms-frontend reconfig  fails at the end with an error like \"Writing back config file failed, Reconfiguring the frontend [FAILED]\", make sure that  /etc/gwms-frontend/  belongs to the  frontend  user. It must be able to write to update the configuration file.", 
            "title": "Frontend reconfig failing"
        }, 
        {
            "location": "/other/install-gwms-frontend/#frontend-failing-to-start", 
            "text": "If the startup script of the frontend is failing, check the log file for errors (probably  /var/log/gwms-frontend/frontend/frontend. TODAY .err.log  and  .debug.log ).  If you find errors like  \"Exception occurred: ... 'ExpatError: no element found: line 1, column 0\\n']\"  and  \"IOError: [Errno 9] Bad file descriptor\"  you may have an empty status file ( /var/lib/gwms-frontend/vofrontend/monitor/group_*/frontend_status.xml ) that causes Glidein WMS Frontend not to start. The glideinFrontend crashes after a XML parsing exception visible in the log file (\"Exception occurred: ... 'ExpatError: no element found: line 1, column 0\\n']\").  Remove the status file. Then start the frontend. The fronten will be fixed in future versions to handle this automatically.", 
            "title": "Frontend failing to start"
        }, 
        {
            "location": "/other/install-gwms-frontend/#certificates-not-there", 
            "text": "The scripts should send an email warning if there are problems and they fail to generate the proxies. Anyway something could go wrong and you want to check manually. If you are using the scripts to generate automatically the proxies but the proxies are not there (in  /tmp  or wherever you expect them):   make sure that the scripts are there and configured with the correct values  make sure that the scripts are executable  make sure that the scripts are in  frontend  's crontab  make sure that the certificates (or master proxy) used to generate the proxies is not expired", 
            "title": "Certificates not there"
        }, 
        {
            "location": "/other/install-gwms-frontend/#failed-authentication", 
            "text": "If you get a failed authentication error (e.g. \"Failed to talk to factory_pool gfactory-1.t2.ucsd.edu...) then:   check that you have the right x509 certificates mentioned in the security section of  /etc/gwms-frontend/frontend.xml  the owner must be  frontend  (user running the frontend)  the permission must be 600  they must be valid for more than one hour (2/300 hours), at least the non VO part    check that the clock is synchronized (see HostTimeSetup)", 
            "title": "Failed authentication"
        }, 
        {
            "location": "/other/install-gwms-frontend/#frontend-doesnt-trust-factory", 
            "text": "If your frontend complains in the debug log:  code 256:[ Error: communication error\\n ,  AUTHENTICATE:1003:Failed to authenticate with any method\\n ,  AUTHENTICATE:1004:Failed to authenticate using GSI\\n ,  GSI:5006:Failed to authenticate because the subject  /DC=org/DC=doegrids/OU=Services/CN=devg-3.t2.ucsd.edu  is not currently trusted by you.  If it should be, add it to GSI_DAEMON_NAME in the condor_config, or use the environment variable override (check the manual).\\n ,  GSI:5004:Failed to gss_assist_gridmap /DC=org/DC=doegrids/OU=Services/CN=devg-3.t2.ucsd.edu to a local user.  A possible solution is to comment/remove the LOCAL_CONFIG_DIR in the file  /var/lib/gwms-frontend/vofrontend/frontend.condor_config .", 
            "title": "Frontend doesn't trust factory"
        }, 
        {
            "location": "/other/install-gwms-frontend/#no-security-credentials-match-for-factory-pool-not-advertising-request", 
            "text": "You may see a warning like \"No security credentials match for factory pool ..., not advertising request\", if the  trust_domain  and  auth_method  of an entry in the Factory configuration is not matching any of the  trust_domain ,  type  couples in the credentials in the Frontend configuration. This causes the Frontend not to use some Factory entries (the ones not matching) and may end up without entries to send glideins to.  To fix the problem make sure that those attributes match as desired.", 
            "title": "No security credentials match for factory pool ..., not advertising request"
        }, 
        {
            "location": "/other/install-gwms-frontend/#jobs-not-running", 
            "text": "If your jobs remain Idle   Check the frontend log files (see above)  Check the condor log files ( condor_config_val LOG  will give you the correct log directory):  Specifically look the CollectorXXXLog files     Common causes of problems could be:   x509 certificates  missing or expired or too short-lived proxy  incorrect ownership or permission on the certificate/proxy file  missing certificates    If the frontend http server is down in the factory there will be errors like \"Failed to load file 'description.dbceCN.cfg' from  http://FRONTEND_HOST/vofrontend/stage .\"  check that the http server is running and you can reach the URL ( http://FRONTEND_HOST/vofrontend/stage/description.dbceCN.cfg )", 
            "title": "Jobs not running"
        }, 
        {
            "location": "/other/install-gwms-frontend/#advanced-configurations", 
            "text": "GlideinWMS Frontend on a Campus Grid", 
            "title": "Advanced Configurations"
        }, 
        {
            "location": "/other/install-gwms-frontend/#references", 
            "text": "Definitions:   What is a  Virtual Organisation  Introduction to the Grid for users/scientists   Documents about the Glidein-WMS system and the VO frontend:   http://www.uscms.org/SoftwareComputing/Grid/WMS/glideinWMS/  http://www.uscms.org/SoftwareComputing/Grid/WMS/glideinWMS/doc.prd/manual/  How to setup a Submit host flocking to the VO Frontend", 
            "title": "References"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/", 
            "text": "Install a CVMFS Stratum 1\n\n\nThis document describes how to install a CVMFS Stratum 1. There are many different variations on how to do that, but this document focuses on the configuration of the OSG GOC Stratum 1 oasis-replica.opensciencegrid.org. It is applicable to other Stratum 1s as well, very likely with modifications (some of which are suggested in the document below).\n\n\n\n\nApplicable versions\n\n\nThe applicable software versions for this document are cvmfs and cvmfs-server \n= 2.4.2.\n\n\n\n\nBefore Starting\n\n\nBefore starting the installation process, consider the following points:\n\n\n\n\nUser IDs and Group IDs:\n If your machine is also going to be a repository server like the OSG GOC, the installation will create the same user and group IDs as the \ncvmfs client\n.  If you are installing frontier-squid, the installation will also create the same user id as \nfrontier-squid\n.\n\n\nNetwork ports:\n This installation will host the stratum 1 on ports 80 and 8000 and, if squid is installed, it will host the uncached apache on port 8080.\n\n\nHost choice:\n -  Make sure there is adequate disk space for the repositories that will be served, at \n/srv/cvmfs\n. Do not use xfs as the filesystem type on operating systems older than EL7, because it has been demonstrated to perform poorly for CVMFS repositories; instead use ext3 or ext4. About 10GB should be reserved for apache and squid logs under /var/log on a production server, although they normally will not get that large. A Stratum 1 that is also a repository server should have at least 5GB available at \n/var/cache\n.\n\n\nSELinux\n - Ensure SELinux is disabled\n\n\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare the \nrequired Yum repositories\n\n\n\n\nInstalling\n\n\nAll CVMFS Stratum 1s require cvmfs-server software and apache (httpd). It is highly recommended to also install \nfrontier-squid\n and \nfrontier-awstats\n on the same machine to be able to easily join the WLCG \nMRTG\n and \nawstats\n monitoring systems. The recommended configuration for frontier-squid below only caches geo api lookups.  Other than that, it is primarily for monitoring.\n\n\nInstalling cvmfs-server and httpd\n\n\nThe OSG GOC Stratum 1 has to function as a repository server in addition to serving repository replications; most Stratum 1s serve only replications. Instructions are also provided for how to install cvmfs-server on Stratum 1s that do not have to be repository servers.  Choose the appropriate subsection.\n\n\nInstalling a CVMFS stratum 1 that is also a repository server\n\n\nEL6 does not support a CVMFS repository server with the standard kernel, so use EL7.  EL7.2 cannot be reliably used as a repository server, because of bugs in the union filesystem OverlayFS. The bugs are fixed in EL7.3, so use EL7.3 or later.\n\n\nroot@host #\n yum -y install cvmfs-server cvmfs mod_wsgi\n\n\n\n\n\nInstalling CVMFS stratum 1 that is not a repository server\n\n\nIf you're not installing for the OSG GOC or otherwise want to support serving repositories on the same machine as a Stratum 1, use this command on either EL6 or EL7:\n\n\nroot@host #\n yum -y install cvmfs-server cvmfs-config mod_wsgi\n\n\n\n\n\nInstalling frontier-squid and frontier-awstats\n\n\nfrontier-awstats\n is not distributed by OSG so these instructions get it from its original source.  Do these commands to install frontier-squid and frontier-awstats:\n\n\nroot@host #\n rpm -i http://frontier.cern.ch/dist/rpms/RPMS/noarch/frontier-release-1.1-1.noarch.rpm\n\nroot@host #\n yum -y install frontier-awstats\n\n\n\n\n\nConfiguring\n\n\nConfiguring the system\n\n\nIncrease the default number of open file descriptors:\n\n\nroot@host #\n \necho\n -e \n*\\t\\t-\\tnofile\\t\\t16384\n \n/etc/security/limits.conf \n\nroot@host #\n \nulimit\n -n \n16384\n\n\n\n\n\n\nIn order for this to apply also interactively when logging in over ssh, the option \nUsePAM\n has to be set to \nyes\n in \n/etc/ssh/sshd_config\n.\n\n\nConfiguring cron\n\n\nFirst, create the log directory: \n\n\nroot@host #\n mkdir -p /var/log/cvmfs\n\n\n\n\n\nPut the following in \n/etc/cron.d/cvmfs\n:\n\n\n0,15,30,45 * * * * root test -d /srv/cvmfs || exit;cvmfs_server snapshot -ai \n6 1 * * * root cvmfs_server gc -af 2\n/dev/null || true\n0 9 * * * root find /srv/cvmfs/*.*/data/txn -name \n*.*\n -mtime +2 2\n/dev/null|xargs rm -f\n\n\n\n\n\nAlso, put the following in \n/etc/logrotate.d/cvmfs\n:\n\n\n/var/log/cvmfs/*.log {\n    weekly\n    missingok\n    notifempty\n}\n\n\n\n\n\nConfiguring apache\n\n\nIf you are installing frontier-squid, create \n/etc/httpd/conf.d/cvmfs.conf\n and put the following lines into it:\n\n\nListen 8080 KeepAlive On\n\n\n\n\n\nIf you are not installing frontier-squid, instead put the following lines into that file:\n\n\nListen 8000 KeepAlive On\n\n\n\n\n\nIf you will be serving opensciencegrid.org repositories, you have to allow for old client configurations that access repositories without the domain name added. For that reason, you will need to remove each \n/etc/httpd/conf.d/cvmfs.\nrepositoryname\n.conf\n that adding a replica creates (this is included in the \nadd_osg_repository script\n), and instead add the following to \n/etc/httpd/conf.d/cvmfs.conf\n:\n\n\nRewriteEngine On \nRewriteRule ^/cvmfs/([^./]*)/(.*)$ /cvmfs/$1.opensciencegrid.org/$2 \nRewriteRule ^/cvmfs/([^/]+)/api/(.*)$ /cvmfs/$1/api/$2 [PT] \nRewriteRule ^/cvmfs/(.*)$ /srv/cvmfs/$1 \n\nDirectory\n \n/srv/cvmfs\n \n  Options -MultiViews +FollowSymLinks -Indexes \n  AllowOverride All \n  Require all granted\n\n  EnableMMAP Off EnableSendFile Off\n\n  \nFilesMatch\n \n^\\.cvmfs\n\n    ForceType application/x-cvmfs\n  \n/FilesMatch\n\n\n  Header unset Last-Modified \n  FileETag None\n\n  ExpiresActive On \n  ExpiresDefault \naccess plus 3 days\n \n  ExpiresByType text/html \naccess plus 15 minutes\n \n  ExpiresByType application/x-cvmfs \naccess plus 61 seconds\n \n  ExpiresByType application/json \naccess plus 61 seconds\n \n\n/Directory\n\n\nWSGIDaemonProcess cvmfs-api processes=2 display-name=%{GROUP} \\\n    python-path=/usr/share/cvmfs-server/webapi\nWSGIProcessGroup cvmfs-api \nWSGISocketPrefix /var/run/wsgi \nWSGIScriptAliasMatch /cvmfs/([^/]+)/api /var/www/wsgi-scripts/cvmfs-api.wsgi/$1 \n\n\n\n\n\nOn EL6-based systems (apache httpd 2.2) replace the \"Require all granted\" with \"Order allow, deny\" and \"Allow from all\".\n\n\nIf you will be serving cern.ch repositories, it has the same problem; replace opensciencegrid.org above with cern.ch. If you need to serve both opensciencegrid.org and cern.ch contact Dave Dykstra to discuss the options.\n\n\nThen enable apache.  On EL6 do\n\n\nroot@host #\n chkconfig httpd on \n\nroot@host #\n service httpd start\n\n\n\n\n\nor on EL7 do\n\n\nroot@host #\n systemctl \nenable\n httpd\n\nroot@host #\n systemctl start httpd\n\n\n\n\n\nConfiguring frontier-squid\n\n\nPut the following in \n/etc/squid/customize.sh\n after the existing comment header:\n\n\nawk\n \n--\nfile\n \n`\ndirname\n \n$\n0\n`\n/\ncustomhelps\n.\nawk\n \n--\nsource\n \n{\n\n\n\n# cache only api calls \n\n\ninsertline(\n^http_access deny all\n, \nacl CVMFSAPI urlpath_regex ^/cvmfs/[^/]*/api/\n)\n\n\ninsertline(\n^http_access deny all\n, \ncache deny CVMFSAPI\n)\n\n\n\n# port 80 is also supported, through an iptables redirect \n\n\nsetoption(\nhttp_port\n, \n8000 accel defaultsite=localhost:8080 no-vhost\n)\n\n\nsetoption(\ncache_peer\n, \nlocalhost parent 8080 0 no-query originserver\n)\n\n\n\n# allow incoming http accesses from anywhere\n\n\n# all requests will be forwarded to the originserver \n\n\ncommentout(\nhttp_access allow NET_LOCAL\n)\n\n\ninsertline(\n^http_access deny all\n, \nhttp_access allow all\n)\n\n\n\n# do not let squid cache DNS entries more than 5 minutes \n\n\nsetoption(\npositive_dns_ttl\n, \n5 minutes\n)\n\n\n\n# set shutdown_lifetime to 0 to avoid giving new connections error\n\n\n# codes, which get cached upstream \n\n\nsetoption(\nshutdown_lifetime\n, \n0 seconds\n)\n\n\n\n# turn off collapsed_forwarding to prevent slow clients from slowing down\n\n\n# faster ones\n\n\nsetoption(\ncollapsed_forwarding\n, \noff\n)\n\n\n\nprint\n\n\n}\n\n\n\n\n\n\nOn an EL7 system, make sure that iptables-services is installed and enabled:\n\n\nroot@host #\n yum -y install iptables-services \n\nroot@host #\n systemctl \nenable\n iptables\n\n\n\n\n\nForward port 80 to port 8000 (first command is for external, second command for localhost):\n\n\nroot@host #\n iptables -t nat -A PREROUTING -p tcp -m tcp --dport \n80\n -j REDIRECT --to-ports \n8000\n \n\nroot@host #\n iptables -t nat -A OUTPUT -o lo -p tcp -m tcp --dport \n80\n -j REDIRECT --to-ports \n8000\n \n\nroot@host #\n service iptables save\n\n\n\n\n\nOn EL7 also set up the the same port forwarding for IPv6 (unfortunately it is not supported on EL6):\n\n\nroot@host #\n ip6tables -t nat -A PREROUTING -p tcp -m tcp --dport \n80\n -j REDIRECT --to-ports \n8000\n\n\nroot@host #\n ip6tables -t nat -A OUTPUT -o lo -p tcp -m tcp --dport \n80\n -j REDIRECT --to-ports \n8000\n\n\nroot@host #\n service ip6tables save\n\n\n\n\n\nEnable frontier-squid.  On EL6 do:\n\n\nroot@host #\n chkconfig frontier-squid on\n\nroot@host #\n service frontier-squid start\n\n\n\n\n\nor on EL7 do:\n\n\nroot@host #\n systemctl \nenable\n frontier-squid\n\nroot@host #\n systemctl start frontier-squid\n\n\n\n\n\n\n\nNote\n\n\nThe above configuration is for a single squid thread, which is fine for 1Gbit/s and possibly 2Gbit/s, but if higher bandwidth is needed, see the \ninstructions for running multiple squid workers\n.\n\n\n\n\nVerifying\n\n\nIn order to verify that everything is installed correctly, create a repository replica. The repository chosen for the instructions below is one from egi.eu because it is very small, but you can use another one if you prefer.\n\n\nAdding an example repository\n\n\nThe OSG GOC Stratum 1 should add a repository replica using the \nadd_osg_repository\n script from the oasis-goc rpm. Instructions for installing that are elsewhere. That script assumes that the oasis.opensciencegrid.org replica repository was first created, so this instruction creates it but does not download the first snapshot because that would take a lot of space and time. Use these commands to create the oasis replica and to create and download the example replica:\n\n\nroot@host #\n cvmfs_server add-replica -o root http://oasis.opensciencegrid.org:8000/cvmfs/oasis.opensciencegrid.org /etc/cvmfs/keys/opensciencegrid.org/opensciencegrid.org.pub \n\nroot@host #\n add_osg_repository http://cvmfs-stratum0.gridpp.rl.ac.uk:8000/cvmfs/config-egi.egi.eu\n\n\n\n\n\nIt's a good idea for other Stratum 1s to make their own scripts for adding repository replicas, because there's always two or three commands to run, and it's easy to forget the commands after the first one. The first command is this:\n\n\nroot@host #\n cvmfs_server add-replica -o root http://cvmfs-stratum0.gridpp.rl.ac.uk:8000/cvmfs/config-egi.egi.eu /etc/cvmfs/keys/egi.eu/egi.eu.pub\n\n\n\n\n\nHowever, non-GOC OSG Stratum 1s (that is, at BNL and FNAL), for the sake of fulfilling an OSG security requirement, need to instead read from the OSG GOC machine with this as their first command:\n\n\nroot@host #\n cvmfs_server add-replica -o root http://oasis-replica.opensciencegrid.org:8000/cvmfs/config-egi.egi.eu /etc/cvmfs/keys/egi.eu/egi.eu.pub:/etc/cvmfs/keys/opensciencegrid.org/opensciencegrid.org.pub\n\n\n\n\n\nThe second command for Stratum 1s that have the httpd configuration as described above in the \nConfiguring apache section\n is this:\n\n\nroot@host #\n rm -f /etc/httpd/conf.d/cvmfs.config-egi.egi.eu.conf\n\n\n\n\n\nThen the next command is this:\n\n\nroot@host #\n cvmfs_server snapshot config-egi.egi.eu\n\n\n\n\n\nWith large repositories that can take a very long time, but with small repositories it should be very quick and not show any errors.\n\n\nVerifying that the replica is being served\n\n\nNow to verify that the replication is working, do the following commands:\n\n\nroot@host #\n wget -qdO- http://localhost:8000/cvmfs/config-egi.egi.eu/.cvmfspublished \n|\n cat -v\n\nroot@host #\n wget -qdO- http://localhost:80/cvmfs/config-egi.egi.eu/.cvmfspublished \n|\n cat -v\n\n\n\n\n\nBoth commands should show a short file including gibberish at the end which is the signature.\n\n\nIt is a good idea to familiarize yourself with the log entries at \n/var/log/httpd/access_log\n and also, if you have installed frontier-squid, at \n/var/log/squid/access.log\n. Also, at least 15 minutes after the snapshot is finished, check the log \n/var/log/cvmfs/snapshots.log\n to see that it tried to get an update and got no errors.\n\n\nSetting up monitoring\n\n\nIf you installed frontier-squid and frontier-awstats, there is a little more to do to configure monitoring.\n\n\nFirst, make sure that your firewall accepts UDP queries from the monitoring server at CERN. Details are in \nthe frontier-squid instructions\n. Next, choose any random password and put it in \n/etc/awstats/password-file\n. Then tell Dave Dykstra the fully qualified domain name of your machine and the password you chose, and he'll set up the monitoring servers.", 
            "title": "Install a CVMFS Stratum 1"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#install-a-cvmfs-stratum-1", 
            "text": "This document describes how to install a CVMFS Stratum 1. There are many different variations on how to do that, but this document focuses on the configuration of the OSG GOC Stratum 1 oasis-replica.opensciencegrid.org. It is applicable to other Stratum 1s as well, very likely with modifications (some of which are suggested in the document below).   Applicable versions  The applicable software versions for this document are cvmfs and cvmfs-server  = 2.4.2.", 
            "title": "Install a CVMFS Stratum 1"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#before-starting", 
            "text": "Before starting the installation process, consider the following points:   User IDs and Group IDs:  If your machine is also going to be a repository server like the OSG GOC, the installation will create the same user and group IDs as the  cvmfs client .  If you are installing frontier-squid, the installation will also create the same user id as  frontier-squid .  Network ports:  This installation will host the stratum 1 on ports 80 and 8000 and, if squid is installed, it will host the uncached apache on port 8080.  Host choice:  -  Make sure there is adequate disk space for the repositories that will be served, at  /srv/cvmfs . Do not use xfs as the filesystem type on operating systems older than EL7, because it has been demonstrated to perform poorly for CVMFS repositories; instead use ext3 or ext4. About 10GB should be reserved for apache and squid logs under /var/log on a production server, although they normally will not get that large. A Stratum 1 that is also a repository server should have at least 5GB available at  /var/cache .  SELinux  - Ensure SELinux is disabled   As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system  Obtain root access to the host  Prepare the  required Yum repositories", 
            "title": "Before Starting"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#installing", 
            "text": "All CVMFS Stratum 1s require cvmfs-server software and apache (httpd). It is highly recommended to also install  frontier-squid  and  frontier-awstats  on the same machine to be able to easily join the WLCG  MRTG  and  awstats  monitoring systems. The recommended configuration for frontier-squid below only caches geo api lookups.  Other than that, it is primarily for monitoring.", 
            "title": "Installing"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#installing-cvmfs-server-and-httpd", 
            "text": "The OSG GOC Stratum 1 has to function as a repository server in addition to serving repository replications; most Stratum 1s serve only replications. Instructions are also provided for how to install cvmfs-server on Stratum 1s that do not have to be repository servers.  Choose the appropriate subsection.", 
            "title": "Installing cvmfs-server and httpd"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#installing-a-cvmfs-stratum-1-that-is-also-a-repository-server", 
            "text": "EL6 does not support a CVMFS repository server with the standard kernel, so use EL7.  EL7.2 cannot be reliably used as a repository server, because of bugs in the union filesystem OverlayFS. The bugs are fixed in EL7.3, so use EL7.3 or later.  root@host #  yum -y install cvmfs-server cvmfs mod_wsgi", 
            "title": "Installing a CVMFS stratum 1 that is also a repository server"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#installing-cvmfs-stratum-1-that-is-not-a-repository-server", 
            "text": "If you're not installing for the OSG GOC or otherwise want to support serving repositories on the same machine as a Stratum 1, use this command on either EL6 or EL7:  root@host #  yum -y install cvmfs-server cvmfs-config mod_wsgi", 
            "title": "Installing CVMFS stratum 1 that is not a repository server"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#installing-frontier-squid-and-frontier-awstats", 
            "text": "frontier-awstats  is not distributed by OSG so these instructions get it from its original source.  Do these commands to install frontier-squid and frontier-awstats:  root@host #  rpm -i http://frontier.cern.ch/dist/rpms/RPMS/noarch/frontier-release-1.1-1.noarch.rpm root@host #  yum -y install frontier-awstats", 
            "title": "Installing frontier-squid and frontier-awstats"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#configuring", 
            "text": "", 
            "title": "Configuring"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#configuring-the-system", 
            "text": "Increase the default number of open file descriptors:  root@host #   echo  -e  *\\t\\t-\\tnofile\\t\\t16384   /etc/security/limits.conf  root@host #   ulimit  -n  16384   In order for this to apply also interactively when logging in over ssh, the option  UsePAM  has to be set to  yes  in  /etc/ssh/sshd_config .", 
            "title": "Configuring the system"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#configuring-cron", 
            "text": "First, create the log directory:   root@host #  mkdir -p /var/log/cvmfs  Put the following in  /etc/cron.d/cvmfs :  0,15,30,45 * * * * root test -d /srv/cvmfs || exit;cvmfs_server snapshot -ai \n6 1 * * * root cvmfs_server gc -af 2 /dev/null || true\n0 9 * * * root find /srv/cvmfs/*.*/data/txn -name  *.*  -mtime +2 2 /dev/null|xargs rm -f  Also, put the following in  /etc/logrotate.d/cvmfs :  /var/log/cvmfs/*.log {\n    weekly\n    missingok\n    notifempty\n}", 
            "title": "Configuring cron"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#configuring-apache", 
            "text": "If you are installing frontier-squid, create  /etc/httpd/conf.d/cvmfs.conf  and put the following lines into it:  Listen 8080 KeepAlive On  If you are not installing frontier-squid, instead put the following lines into that file:  Listen 8000 KeepAlive On  If you will be serving opensciencegrid.org repositories, you have to allow for old client configurations that access repositories without the domain name added. For that reason, you will need to remove each  /etc/httpd/conf.d/cvmfs. repositoryname .conf  that adding a replica creates (this is included in the  add_osg_repository script ), and instead add the following to  /etc/httpd/conf.d/cvmfs.conf :  RewriteEngine On \nRewriteRule ^/cvmfs/([^./]*)/(.*)$ /cvmfs/$1.opensciencegrid.org/$2 \nRewriteRule ^/cvmfs/([^/]+)/api/(.*)$ /cvmfs/$1/api/$2 [PT] \nRewriteRule ^/cvmfs/(.*)$ /srv/cvmfs/$1  Directory   /srv/cvmfs  \n  Options -MultiViews +FollowSymLinks -Indexes \n  AllowOverride All \n  Require all granted\n\n  EnableMMAP Off EnableSendFile Off\n\n   FilesMatch   ^\\.cvmfs \n    ForceType application/x-cvmfs\n   /FilesMatch \n\n  Header unset Last-Modified \n  FileETag None\n\n  ExpiresActive On \n  ExpiresDefault  access plus 3 days  \n  ExpiresByType text/html  access plus 15 minutes  \n  ExpiresByType application/x-cvmfs  access plus 61 seconds  \n  ExpiresByType application/json  access plus 61 seconds   /Directory \n\nWSGIDaemonProcess cvmfs-api processes=2 display-name=%{GROUP} \\\n    python-path=/usr/share/cvmfs-server/webapi\nWSGIProcessGroup cvmfs-api \nWSGISocketPrefix /var/run/wsgi \nWSGIScriptAliasMatch /cvmfs/([^/]+)/api /var/www/wsgi-scripts/cvmfs-api.wsgi/$1   On EL6-based systems (apache httpd 2.2) replace the \"Require all granted\" with \"Order allow, deny\" and \"Allow from all\".  If you will be serving cern.ch repositories, it has the same problem; replace opensciencegrid.org above with cern.ch. If you need to serve both opensciencegrid.org and cern.ch contact Dave Dykstra to discuss the options.  Then enable apache.  On EL6 do  root@host #  chkconfig httpd on  root@host #  service httpd start  or on EL7 do  root@host #  systemctl  enable  httpd root@host #  systemctl start httpd", 
            "title": "Configuring apache"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#configuring-frontier-squid", 
            "text": "Put the following in  /etc/squid/customize.sh  after the existing comment header:  awk   -- file   ` dirname   $ 0 ` / customhelps . awk   -- source   {  # cache only api calls   insertline( ^http_access deny all ,  acl CVMFSAPI urlpath_regex ^/cvmfs/[^/]*/api/ )  insertline( ^http_access deny all ,  cache deny CVMFSAPI )  # port 80 is also supported, through an iptables redirect   setoption( http_port ,  8000 accel defaultsite=localhost:8080 no-vhost )  setoption( cache_peer ,  localhost parent 8080 0 no-query originserver )  # allow incoming http accesses from anywhere  # all requests will be forwarded to the originserver   commentout( http_access allow NET_LOCAL )  insertline( ^http_access deny all ,  http_access allow all )  # do not let squid cache DNS entries more than 5 minutes   setoption( positive_dns_ttl ,  5 minutes )  # set shutdown_lifetime to 0 to avoid giving new connections error  # codes, which get cached upstream   setoption( shutdown_lifetime ,  0 seconds )  # turn off collapsed_forwarding to prevent slow clients from slowing down  # faster ones  setoption( collapsed_forwarding ,  off )  print  }   On an EL7 system, make sure that iptables-services is installed and enabled:  root@host #  yum -y install iptables-services  root@host #  systemctl  enable  iptables  Forward port 80 to port 8000 (first command is for external, second command for localhost):  root@host #  iptables -t nat -A PREROUTING -p tcp -m tcp --dport  80  -j REDIRECT --to-ports  8000   root@host #  iptables -t nat -A OUTPUT -o lo -p tcp -m tcp --dport  80  -j REDIRECT --to-ports  8000   root@host #  service iptables save  On EL7 also set up the the same port forwarding for IPv6 (unfortunately it is not supported on EL6):  root@host #  ip6tables -t nat -A PREROUTING -p tcp -m tcp --dport  80  -j REDIRECT --to-ports  8000  root@host #  ip6tables -t nat -A OUTPUT -o lo -p tcp -m tcp --dport  80  -j REDIRECT --to-ports  8000  root@host #  service ip6tables save  Enable frontier-squid.  On EL6 do:  root@host #  chkconfig frontier-squid on root@host #  service frontier-squid start  or on EL7 do:  root@host #  systemctl  enable  frontier-squid root@host #  systemctl start frontier-squid   Note  The above configuration is for a single squid thread, which is fine for 1Gbit/s and possibly 2Gbit/s, but if higher bandwidth is needed, see the  instructions for running multiple squid workers .", 
            "title": "Configuring frontier-squid"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#verifying", 
            "text": "In order to verify that everything is installed correctly, create a repository replica. The repository chosen for the instructions below is one from egi.eu because it is very small, but you can use another one if you prefer.", 
            "title": "Verifying"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#adding-an-example-repository", 
            "text": "The OSG GOC Stratum 1 should add a repository replica using the  add_osg_repository  script from the oasis-goc rpm. Instructions for installing that are elsewhere. That script assumes that the oasis.opensciencegrid.org replica repository was first created, so this instruction creates it but does not download the first snapshot because that would take a lot of space and time. Use these commands to create the oasis replica and to create and download the example replica:  root@host #  cvmfs_server add-replica -o root http://oasis.opensciencegrid.org:8000/cvmfs/oasis.opensciencegrid.org /etc/cvmfs/keys/opensciencegrid.org/opensciencegrid.org.pub  root@host #  add_osg_repository http://cvmfs-stratum0.gridpp.rl.ac.uk:8000/cvmfs/config-egi.egi.eu  It's a good idea for other Stratum 1s to make their own scripts for adding repository replicas, because there's always two or three commands to run, and it's easy to forget the commands after the first one. The first command is this:  root@host #  cvmfs_server add-replica -o root http://cvmfs-stratum0.gridpp.rl.ac.uk:8000/cvmfs/config-egi.egi.eu /etc/cvmfs/keys/egi.eu/egi.eu.pub  However, non-GOC OSG Stratum 1s (that is, at BNL and FNAL), for the sake of fulfilling an OSG security requirement, need to instead read from the OSG GOC machine with this as their first command:  root@host #  cvmfs_server add-replica -o root http://oasis-replica.opensciencegrid.org:8000/cvmfs/config-egi.egi.eu /etc/cvmfs/keys/egi.eu/egi.eu.pub:/etc/cvmfs/keys/opensciencegrid.org/opensciencegrid.org.pub  The second command for Stratum 1s that have the httpd configuration as described above in the  Configuring apache section  is this:  root@host #  rm -f /etc/httpd/conf.d/cvmfs.config-egi.egi.eu.conf  Then the next command is this:  root@host #  cvmfs_server snapshot config-egi.egi.eu  With large repositories that can take a very long time, but with small repositories it should be very quick and not show any errors.", 
            "title": "Adding an example repository"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#verifying-that-the-replica-is-being-served", 
            "text": "Now to verify that the replication is working, do the following commands:  root@host #  wget -qdO- http://localhost:8000/cvmfs/config-egi.egi.eu/.cvmfspublished  |  cat -v root@host #  wget -qdO- http://localhost:80/cvmfs/config-egi.egi.eu/.cvmfspublished  |  cat -v  Both commands should show a short file including gibberish at the end which is the signature.  It is a good idea to familiarize yourself with the log entries at  /var/log/httpd/access_log  and also, if you have installed frontier-squid, at  /var/log/squid/access.log . Also, at least 15 minutes after the snapshot is finished, check the log  /var/log/cvmfs/snapshots.log  to see that it tried to get an update and got no errors.", 
            "title": "Verifying that the replica is being served"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#setting-up-monitoring", 
            "text": "If you installed frontier-squid and frontier-awstats, there is a little more to do to configure monitoring.  First, make sure that your firewall accepts UDP queries from the monitoring server at CERN. Details are in  the frontier-squid instructions . Next, choose any random password and put it in  /etc/awstats/password-file . Then tell Dave Dykstra the fully qualified domain name of your machine and the password you chose, and he'll set up the monitoring servers.", 
            "title": "Setting up monitoring"
        }, 
        {
            "location": "/other/network-performance-toolkit/", 
            "text": "Network Performance Toolkit\n\n\nThis document is for System Administrators and advanced Grid Users. It describes the usage of tools provided by the \nVirtual Data Toolkit\n to evaluate the network performance between resources.\n\n\nIntroduction\n\n\nThe Network Performance Toolkit is a collection of applications provided by the \nperfSONAR project\n and distributed by the \nOpen Science Grid\n. The \nserver\n components of the Network Performance Toolkit have been installed on dedicated resources of the \nOpen Science Grid\n. Following \nclient\n tools are described in this document:\n\n\n\n\nNetwork Diagnostic Tool (NDT)\n\n\nOne Way Active Measurement Protocol (OWAMP)\n\n\nBandwidth Control tool (BWCTL)\n\n\nNetwork Path and Application Diagnosis (NPAD)\n\n\n\n\nInstallation\n\n\nClient Site Installation\n\n\nThe Network Performance Toolkit is installed with the OSG Client. Specifically, the tools included are: BWCTL, NDT and OWAMP (bwctl-client, bwctl-server, bwctl, ndt, owamp-client). NPAD is currently not in OSG client.\n\n\nIf you just want to install the OSG command line clients you can do the following::\n\n\nyum install bwctl-client\n\n\nyum install owamp-client\n\n\nyum install ndt-client\n\n\nyum install npad-client\n\n\n\n\n\n\nYou may install these utilities separately as RPM using yum by following the \nperfSONAR\n instructions. The packages are in the OSG repository, some of them with a separate client or server version, available for the OSG supported platforms:\n\n\n\n\nNDT: ndt\n\n\nOWAMP: owamp, owamp-client, owamp-server\n\n\nBWCTL: bwctl, bwctl-client, bwctl-server\n\n\nNPAD: npad\n\n\n\n\nServer Site Installation\n\n\nThe \nperfSONAR\n-based tools and services support the following tasks for OSG VO's:\n\n\n\n\nmonitor site-to-site network paths and ensure that these paths remain operational\n\n\ntroubleshoot performance problems quickly and efficiently\n\n\n\n\nThe \nserver site components\n can be brought-up \non demand\n using the netinstall provided by \nperfSONAR project downloads\n. Source packages are provided on the \nperfSONAR home page\n.\n\n\nOnce the Toolkit server has booted you may begin on-demand testing. The server tools will use a generic set of configuration files. The intent is to make it easy to stand up a temporary server when and where it is needed. However, it is expected that a permanently installed server will be customized/configured, allowing it to support both on-demand testing and regularly scheduled monitoring. See the \nperfSONAR home page\n for step-by-step instructions on how to complete this customization process.\n\n\nFinding Target Servers\n\n\nFinding servers against which to run on-demand tests can be a major impediment to effectively using these tools. The \nperfSONAR\n project tackles this problem by running a registration service for participating tools. The Performance Node ISO automatically uses this Lookup Service to advertise the tools' existence. You can also create custom views by making web-service calls to retrieve the data of interest. \n\n\nWe also have requested ALL OSG sites register their perfSONAR Toolkit installations in OIM (See \nhttps://www.opensciencegrid.org/bin/view/Documentation/RegisterPSinOIM\n ). You can use the MyOSG -\n Resource Group -\n Resource Group Summary page to see a list of perfSONAR Toolkit hosts that are installed at \nhttp://tinyurl.com/mxfmutg\n. Using this list you can select a \"closest\" relevant instance to use for running on-demand tests. Alternately if you have a perfSONAR toolkit install, the web interface has a \"Global Services\" link you can visit to see ALL perfSONAR instances that have updated the perfSONAR lookup service.\n\n\nUsing the Client Tools\n\n\nThese tools support delay measurements (OWAMP), throughput measurements (BWCTL), and advanced diagnostics (NDT and NPAD). The command syntax for each tool is described in the following sub-sections. Each of the client tools listed above communicates with a companion server process to perform a measurement/test.\n\n\nNetwork Diagnostic Tool (NDT)\n\n\nThe Network Diagnostic Tool (NDT) runs a series of short tests to determine what the current performance is and what, if anything, is limiting that performance. It can distinguish between host configuration and network infrastructure problems. To diagnose the CE/SE configuration and network connection run the \nweb100clt\n command:\n\n\n[user@client /opt/npt]$\n web100clt \u2013n \nTarget Server \nfor\n Measurement\n\n\n\n\n\n\nMore details can be obtained by using the \n-l\n command line option to \nweb100ctl\n:\n\n\n[user@client /opt/npt]$\n web100clt \u2013n \nTarget Server \nfor\n Measurement\n -l\n\n\n\n\n\nTo increase the output further use:\n\n\n[user@client /opt/npt]$\n web100clt \u2013n \nTarget Server \nfor\n Measurement\n -ll\n\n\n\n\n\nOne Way Active Measurement Protocol (OWAMP)\n\n\nThe One Way Active Measurement Protocol (OWAMP) is an advanced version of the common \nping\n program. The OWAMP client \nowping\n communicates with an OWAMP server and measures the delay in each direction using NTP based time stamps. OWAMP can be used to identify delay, loss, and packet reordering problems inside the network. To measure the delay between the CE/SE and the remote server use the \nowping\n command:\n\n\n[user@client /opt/npt]$\n owping \nTarget Server \nfor\n Measurement\n\n\n\n\n\n\nBandwidth Control tool (BWCTL)\n\n\nThe Bandwidth Control tool (BWCTL) is a wrapper for the \niperf\n command, its policy and a daemon. BWCTL improves the usability of \niperf\n by avoiding following problems:\n\n\n\n\nneed for remote access to the target host used for measurement\n\n\nsecurity concerns about leaving an \niperf\n daemon running on the target host\n\n\n\n\nBWCTL supports testing in either direction, or between 2 remote BWCTL servers from a third location. To measure the current throughput from your SE/CE to the remote server use the \nbwctl\n command:\n\n\n[user@client /opt/npt]$\n bwctl \u2013s \nTarget Server \nfor\n Measurement\n\n\n\n\n\n\nThird party tests, between 2 remote BWCTL servers, will let you measure various sections of the end-2-end path, running\n\n\n[user@client /opt/npt]$\n bwctl \u2013c \n1st Server \nfor\n Measurement\n \u2013s \n2nd Server \nfor\n Measurement\n\n\n\n\n\n\nOther useful options are \n-f\n, format, \n-t\n, length of test, and \n-i\n, test interval.\n\n\nNetwork Path and Application Diagnosis (NPAD)\n\n\nNOTE: Network Path and Application Diagnosis (NPAD) is deprecated and won't be in future versions of the OSG distribution\n\n\nThe Network Path and Application Diagnosis (NPAD) tool examines a host and its local network infrastructure to determine what problems, if any, would hinder wide area performance. Issues such as small TCP buffers in switches and routers are detected as well as common host configuration errors. To determine if the CE/SE will achieve maximum performance over a WAN path run the command \ndiag-client\n:\n\n\n[user@client /opt/npt]$\n diag-client \nTarget Server \nfor\n Measurement\n \n8001\n \n10\n \n50\n\n\n\n\n\n\n\n\nNote\n\n\nThe last 2 numeric parameters, after host and port, are the rtt time (in ms) and speed/rate values (in Mbps) you need to achieve. The reason it works this way is that its meant to 'test' local infrastructure. The idea is that if you were testing to an NPAD server that was 5ms away on a 1G network, you would get close to that speed even with network flaws. If you were to supply 80ms and 1G to the server and there truly was a flaw, the NPAD test would tell you it wasn't possible, thus enabling you to fix the problem\n\n\n\n\n\n\nNote\n\n\nThe \ndiag-client\n commands return a partial URL, enabling easy sharing of results between users and site administrators. To view the results, prepend the Toolkit servers name/port to the returned string. The example above would result in this URL: \nhttp://server.this.osg.domain:8002/ServerData/Reports-2011-07/vtbv-ce.uchicago.edu:2011-07-12-18:50:07.html\n.\n\n\n\n\nAdvanced Topic: Scheduled Monitoring\n\n\n(See \nhttp://docs.perfsonar.net/install_quick_start.html\n for more details.)\n\n\nIn addition to the above on-demand tests, the Performance Toolkit server can be configured to continuously monitor the throughput or delay between your site and peer sites of interest. To begin this monitoring, enter the GUI and ensure that your server is a member of the community or communities of interest. Once that is complete, continue on by selecting either the \nperfSONAR-BUOY\n throughput or delay configuration menu item.\n\n\npSB-throughput: This utility will run regularly scheduled BWCTL tests between your Toolkit server and the selected peer servers. Results are stored in a database and displayed on the server's web page. You may also use standard web-service calls to retrieve this data for display on remote web servers. This would allow monitoring of a common core infrastructure at a central site, while each site could keep local/customized views.\n\n\npSB-delay: This utility will run regularly scheduled OWAMP tests between your Toolkit server and the selected peers. Results are stored in a database and displayed on the server\u2019s web page. You may also use standard web-service calls to retrieve this data for display on remote web servers. This would allow monitoring of a common core infrastructure at a central site, while each site could keep local/customized views as required.\n\n\nReferences\n\n\n\n\nOne Way Active Measurement Protocol (OWAMP)\n\n\nBandwidth Control tool (BWCTL)\n\n\n\n\nSee also the OSG/WLCG pages on perfSONAR at \nhttps://twiki.opensciencegrid.org/bin/view/Documentation/DeployperfSONAR", 
            "title": "Network Performance Toolkit"
        }, 
        {
            "location": "/other/network-performance-toolkit/#network-performance-toolkit", 
            "text": "This document is for System Administrators and advanced Grid Users. It describes the usage of tools provided by the  Virtual Data Toolkit  to evaluate the network performance between resources.", 
            "title": "Network Performance Toolkit"
        }, 
        {
            "location": "/other/network-performance-toolkit/#introduction", 
            "text": "The Network Performance Toolkit is a collection of applications provided by the  perfSONAR project  and distributed by the  Open Science Grid . The  server  components of the Network Performance Toolkit have been installed on dedicated resources of the  Open Science Grid . Following  client  tools are described in this document:   Network Diagnostic Tool (NDT)  One Way Active Measurement Protocol (OWAMP)  Bandwidth Control tool (BWCTL)  Network Path and Application Diagnosis (NPAD)", 
            "title": "Introduction"
        }, 
        {
            "location": "/other/network-performance-toolkit/#installation", 
            "text": "", 
            "title": "Installation"
        }, 
        {
            "location": "/other/network-performance-toolkit/#client-site-installation", 
            "text": "The Network Performance Toolkit is installed with the OSG Client. Specifically, the tools included are: BWCTL, NDT and OWAMP (bwctl-client, bwctl-server, bwctl, ndt, owamp-client). NPAD is currently not in OSG client.  If you just want to install the OSG command line clients you can do the following::  yum install bwctl-client  yum install owamp-client  yum install ndt-client  yum install npad-client   You may install these utilities separately as RPM using yum by following the  perfSONAR  instructions. The packages are in the OSG repository, some of them with a separate client or server version, available for the OSG supported platforms:   NDT: ndt  OWAMP: owamp, owamp-client, owamp-server  BWCTL: bwctl, bwctl-client, bwctl-server  NPAD: npad", 
            "title": "Client Site Installation"
        }, 
        {
            "location": "/other/network-performance-toolkit/#server-site-installation", 
            "text": "The  perfSONAR -based tools and services support the following tasks for OSG VO's:   monitor site-to-site network paths and ensure that these paths remain operational  troubleshoot performance problems quickly and efficiently   The  server site components  can be brought-up  on demand  using the netinstall provided by  perfSONAR project downloads . Source packages are provided on the  perfSONAR home page .  Once the Toolkit server has booted you may begin on-demand testing. The server tools will use a generic set of configuration files. The intent is to make it easy to stand up a temporary server when and where it is needed. However, it is expected that a permanently installed server will be customized/configured, allowing it to support both on-demand testing and regularly scheduled monitoring. See the  perfSONAR home page  for step-by-step instructions on how to complete this customization process.", 
            "title": "Server Site Installation"
        }, 
        {
            "location": "/other/network-performance-toolkit/#finding-target-servers", 
            "text": "Finding servers against which to run on-demand tests can be a major impediment to effectively using these tools. The  perfSONAR  project tackles this problem by running a registration service for participating tools. The Performance Node ISO automatically uses this Lookup Service to advertise the tools' existence. You can also create custom views by making web-service calls to retrieve the data of interest.   We also have requested ALL OSG sites register their perfSONAR Toolkit installations in OIM (See  https://www.opensciencegrid.org/bin/view/Documentation/RegisterPSinOIM  ). You can use the MyOSG -  Resource Group -  Resource Group Summary page to see a list of perfSONAR Toolkit hosts that are installed at  http://tinyurl.com/mxfmutg . Using this list you can select a \"closest\" relevant instance to use for running on-demand tests. Alternately if you have a perfSONAR toolkit install, the web interface has a \"Global Services\" link you can visit to see ALL perfSONAR instances that have updated the perfSONAR lookup service.", 
            "title": "Finding Target Servers"
        }, 
        {
            "location": "/other/network-performance-toolkit/#using-the-client-tools", 
            "text": "These tools support delay measurements (OWAMP), throughput measurements (BWCTL), and advanced diagnostics (NDT and NPAD). The command syntax for each tool is described in the following sub-sections. Each of the client tools listed above communicates with a companion server process to perform a measurement/test.", 
            "title": "Using the Client Tools"
        }, 
        {
            "location": "/other/network-performance-toolkit/#network-diagnostic-tool-ndt", 
            "text": "The Network Diagnostic Tool (NDT) runs a series of short tests to determine what the current performance is and what, if anything, is limiting that performance. It can distinguish between host configuration and network infrastructure problems. To diagnose the CE/SE configuration and network connection run the  web100clt  command:  [user@client /opt/npt]$  web100clt \u2013n  Target Server  for  Measurement   More details can be obtained by using the  -l  command line option to  web100ctl :  [user@client /opt/npt]$  web100clt \u2013n  Target Server  for  Measurement  -l  To increase the output further use:  [user@client /opt/npt]$  web100clt \u2013n  Target Server  for  Measurement  -ll", 
            "title": "Network Diagnostic Tool (NDT)"
        }, 
        {
            "location": "/other/network-performance-toolkit/#one-way-active-measurement-protocol-owamp", 
            "text": "The One Way Active Measurement Protocol (OWAMP) is an advanced version of the common  ping  program. The OWAMP client  owping  communicates with an OWAMP server and measures the delay in each direction using NTP based time stamps. OWAMP can be used to identify delay, loss, and packet reordering problems inside the network. To measure the delay between the CE/SE and the remote server use the  owping  command:  [user@client /opt/npt]$  owping  Target Server  for  Measurement", 
            "title": "One Way Active Measurement Protocol (OWAMP)"
        }, 
        {
            "location": "/other/network-performance-toolkit/#bandwidth-control-tool-bwctl", 
            "text": "The Bandwidth Control tool (BWCTL) is a wrapper for the  iperf  command, its policy and a daemon. BWCTL improves the usability of  iperf  by avoiding following problems:   need for remote access to the target host used for measurement  security concerns about leaving an  iperf  daemon running on the target host   BWCTL supports testing in either direction, or between 2 remote BWCTL servers from a third location. To measure the current throughput from your SE/CE to the remote server use the  bwctl  command:  [user@client /opt/npt]$  bwctl \u2013s  Target Server  for  Measurement   Third party tests, between 2 remote BWCTL servers, will let you measure various sections of the end-2-end path, running  [user@client /opt/npt]$  bwctl \u2013c  1st Server  for  Measurement  \u2013s  2nd Server  for  Measurement   Other useful options are  -f , format,  -t , length of test, and  -i , test interval.", 
            "title": "Bandwidth Control tool (BWCTL)"
        }, 
        {
            "location": "/other/network-performance-toolkit/#network-path-and-application-diagnosis-npad", 
            "text": "NOTE: Network Path and Application Diagnosis (NPAD) is deprecated and won't be in future versions of the OSG distribution  The Network Path and Application Diagnosis (NPAD) tool examines a host and its local network infrastructure to determine what problems, if any, would hinder wide area performance. Issues such as small TCP buffers in switches and routers are detected as well as common host configuration errors. To determine if the CE/SE will achieve maximum performance over a WAN path run the command  diag-client :  [user@client /opt/npt]$  diag-client  Target Server  for  Measurement   8001   10   50    Note  The last 2 numeric parameters, after host and port, are the rtt time (in ms) and speed/rate values (in Mbps) you need to achieve. The reason it works this way is that its meant to 'test' local infrastructure. The idea is that if you were testing to an NPAD server that was 5ms away on a 1G network, you would get close to that speed even with network flaws. If you were to supply 80ms and 1G to the server and there truly was a flaw, the NPAD test would tell you it wasn't possible, thus enabling you to fix the problem    Note  The  diag-client  commands return a partial URL, enabling easy sharing of results between users and site administrators. To view the results, prepend the Toolkit servers name/port to the returned string. The example above would result in this URL:  http://server.this.osg.domain:8002/ServerData/Reports-2011-07/vtbv-ce.uchicago.edu:2011-07-12-18:50:07.html .", 
            "title": "Network Path and Application Diagnosis (NPAD)"
        }, 
        {
            "location": "/other/network-performance-toolkit/#advanced-topic-scheduled-monitoring", 
            "text": "(See  http://docs.perfsonar.net/install_quick_start.html  for more details.)  In addition to the above on-demand tests, the Performance Toolkit server can be configured to continuously monitor the throughput or delay between your site and peer sites of interest. To begin this monitoring, enter the GUI and ensure that your server is a member of the community or communities of interest. Once that is complete, continue on by selecting either the  perfSONAR-BUOY  throughput or delay configuration menu item.  pSB-throughput: This utility will run regularly scheduled BWCTL tests between your Toolkit server and the selected peer servers. Results are stored in a database and displayed on the server's web page. You may also use standard web-service calls to retrieve this data for display on remote web servers. This would allow monitoring of a common core infrastructure at a central site, while each site could keep local/customized views.  pSB-delay: This utility will run regularly scheduled OWAMP tests between your Toolkit server and the selected peers. Results are stored in a database and displayed on the server\u2019s web page. You may also use standard web-service calls to retrieve this data for display on remote web servers. This would allow monitoring of a common core infrastructure at a central site, while each site could keep local/customized views as required.", 
            "title": "Advanced Topic: Scheduled Monitoring"
        }, 
        {
            "location": "/other/network-performance-toolkit/#references", 
            "text": "One Way Active Measurement Protocol (OWAMP)  Bandwidth Control tool (BWCTL)   See also the OSG/WLCG pages on perfSONAR at  https://twiki.opensciencegrid.org/bin/view/Documentation/DeployperfSONAR", 
            "title": "References"
        }, 
        {
            "location": "/release/notes/", 
            "text": "Release Notes\n\n\n\n\nNote\n\n\nSince August 9, 2016, release series 3.2.x are not supported anymore. For release notes older than 3.3, please see the \narchives\n.\n\n\n\n\nOSG 3.4\n\n\n\n\n\n\n\n\nVersion\n\n\nDate\n\n\nDetails\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\n3.4.4-2\n\n\n11-Oct-2017\n\n\nDetailed Notes\n\n\nIGTF 1.86, VO Package v75\n\n\n\n\n\n\n3.4.4\n\n\n10-Oct-2017\n\n\nDetailed Notes\n\n\nTBA\n\n\n\n\n\n\n3.4.3\n\n\n12-Sep-2017\n\n\nDetailed Notes\n\n\nTBA\n\n\n\n\n\n\n3.4.2-2\n\n\n14-Aug-2017\n\n\nDetailed Notes\n\n\nigtf-ca-certs-1.85, osg-ca-certs-1.65\n\n\n\n\n\n\n3.4.2\n\n\n08-Aug-2017\n\n\nDetailed Notes\n\n\nTBA\n\n\n\n\n\n\n3.4.1-2\n\n\n13-Jul-2017\n\n\nDetailed Notes\n\n\nigtf-ca-certs-1.84, osg-ca-certs-1.64\n\n\n\n\n\n\n3.4.1\n\n\n12-Jul-2017\n\n\nDetailed Notes\n\n\nblahp-1.18.30.bosco condor-8.6.4 condor-cron-1.1.2 cvmfs-2.3.5-1.1 globus-gridftp-server-11.8-1.3 gratia-probe-1.18.1 gridftp-dsi-posix-1.4-2 htcondor-ce-2.2.1 lcmaps-plugins-verify-proxy-1.5.9-1.2 osg-build-1.10.1 osg-configure-2.1.0-2 osg-gridftp-3.4-3 osg-test-1.11.0 osg-version-3.4.1 rsv-3.14.2, Upcoming: blahp-1.18.30.bosco condor-8.7.2 glite-ce-cream-client-api-c-1.15.4-2.4\n\n\n\n\n\n\n3.4.0-2\n\n\n15-Jun-2017\n\n\nDetailed Notes\n\n\nIGTF 1.83, VO Package v74\n\n\n\n\n\n\n3.4.0\n\n\n14-Jun-2017\n\n\nDetailed Notes\n\n\nautopyfactory-2.4.6-4 blahp-1.18.29.bosco-3 bwctl-1.4-7 cctools-4.4.3 condor-8.6.3-1.1 condor-cron-1.1.1-2 cvmfs-2.3.5 cvmfs-config-osg-2.0-2 cvmfs-x509-helper-1.0 frontier-squid-3.5.24-3.1 glideinwms-3.2.19-2 glite-build-common-cpp-3.3.0.2 glite-ce-cream-client-api-c-1.15.4-2.3 glite-ce-wsdl-1.15.1-1.1 glite-lbjp-common-gsoap-plugin-3.2.12-1.1 globus-ftp-client-8.29-1.1 globus-gridftp-osg-extensions-0.3-2 globus-gridftp-server-11.8-1.1 globus-gridftp-server-control-4.1-1.3 gratia-probe-1.17.5 gsi-openssh-7.1p2f-1.2 htcondor-ce-2.2.0 igtf-ca-certs-1.82 javascriptrrd-1.1.1 koji-1.11.0-1.5 lcas-lcmaps-gt4-interface-0.3.1-1.2 lcmaps-1.6.6-1.6 lcmaps-plugins-basic-1.7.0-2 lcmaps-plugins-scas-client-0.5.6 lcmaps-plugins-verify-proxy-1.5.9-1.1 lcmaps-plugins-voms-1.7.1-1.4 llrun-0.1.3-1.3 mash-0.5.22-3 myproxy-6.1.18-1.4 nuttcp-6.1.2 osg-build-1.10.0 osg-ca-certs-1.62 osg-ca-certs-updater-1.4 osg-ca-generator-1.2.0 osg-ca-scripts-1.1.6 osg-ce-3.4-2 osg-configure-2.0.0-3 osg-control-1.1.0 osg-gridftp-3.4-2 osg-gridftp-xrootd-3.4 osg-oasis-7-9 osg-pki-tools-1.2.20 osg-system-profiler-1.4.0 osg-test-1.10.1 osg-tested-internal-3.4-2 osg-update-vos-1.4.0 osg-version-3.4.0 osg-vo-map-0.0.2 osg-wn-client-3.4 owamp-3.2rc4-2 pegasus-4.7.4-1.1 rsv-3.14.0-2 rsv-gwms-tester-1.1.2 stashcache-0.7-2 uberftp-2.8-2.1 vo-client-73 voms-2.0.14-1.3 xacml-1.5.0 xrootd-4.6.1 xrootd-dsi-3.0.4-22 xrootd-lcmaps-1.3.3-3 xrootd-voms-plugin-0.4.0, Upcoming: glideinwms-3.3.2-2\n\n\n\n\n\n\n\n\nOSG 3.3\n\n\n\n\n\n\n\n\nVersion\n\n\nDate\n\n\nDetails\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\n3.3.29-2\n\n\n11-Oct-2017\n\n\nDetailed Notes\n\n\nIGTF 1.86, VO Package v75\n\n\n\n\n\n\n3.3.29\n\n\n10-Oct-2017\n\n\nDetailed Notes\n\n\nTBA\n\n\n\n\n\n\n3.3.28\n\n\n12-Sep-2017\n\n\nDetailed Notes\n\n\nTBA\n\n\n\n\n\n\n3.3.27\n\n\n08-AUg-2017\n\n\nDetailed Notes\n\n\nTBA\n\n\n\n\n\n\n3.4.1-2, 3.3.26-2\n\n\n13-Jul-2017\n\n\nDetailed Notes\n\n\nigtf-ca-certs-1.84, osg-ca-certs-1.64\n\n\n\n\n\n\n3.3.26\n\n\n12-Jul-2017\n\n\nDetailed Notes\n\n\nblahp-1.18.30.bosco condor-cron-1.1.2 cvmfs-2.3.5-1.1 globus-gridftp-server-11.8-1.2 gratia-probe-1.18.1 gridftp-dsi-posix-1.4-2 hadoop-2.0.0+1612-1.cdh4.7.1.p0.12.7 htcondor-ce-2.2.1 lcmaps-plugins-verify-proxy-1.5.9-1.2 osg-build-1.10.1 osg-configure-1.9.0 osg-gridftp-3.3-5 osg-test-1.11.0 osg-version-3.3.26 rsv-3.14.2\n\n\n\n\n\n\n3.3.25\n\n\n14-Jun-2017\n\n\nDetailed Notes\n\n\nemi-trustmanager-tomcat-3.0.0-15 glideinwms-3.2.19 globus-xio-5.12-1.2 htcondor-ce-2.2.0 lcmaps-plugins-voms-1.7.1-1.4 osg-build-1.10.0 osg-ca-scripts-1.1.6 osg-ce-3.3-13 osg-configure-1.8.1-2 osg-gridftp-3.3-4 osg-update-vos-1.4.0 stashcache-0.7-2 xrootd-4.6.1 xrootd-lcmaps-1.3.3-3\n\n\n\n\n\n\n3.3.24-2\n\n\n16-May-2017\n\n\nDetailed Notes\n\n\nVO Package v73\n\n\n\n\n\n\n3.3.24\n\n\n09-May-2017\n\n\nDetailed Notes\n\n\ncvmfs-x509-helper 1.0, gratia-probe 1.17.5, lcmaps 1.6.6-1.3, osg-build 1.9.0, osg-configure 1.7.0, osg-wn-client 3.3-7; Upcoming: HTCondor 8.6.2, GlideinWMS 3.3.2\n\n\n\n\n\n\n3.3.23\n\n\n11-Apr-2017\n\n\nDetailed Notes\n\n\nlcmaps 1.6.6-1.2, HTCondor-CE 2.1.5, CVMFS 2.3.5, Pegasus 4.7.4, OSG-CE 3.3-12; Upcoming: Frontier squid 3.5.24-3.1\n\n\n\n\n\n\n3.3.22-3\n\n\n10-Apr-2017\n\n\nDetailed Notes\n\n\nIGTF 1.82\n\n\n\n\n\n\n3.3.22-2\n\n\n28-Mar-2017\n\n\nDetailed Notes\n\n\nVO Package v72\n\n\n\n\n\n\n3.3.22\n\n\n14-Mar-2017\n\n\nDetailed Notes\n\n\nBLAHP 1.18.29, CVMFS 2.3.3, emi-trustmanager 3.0.3-14, GlideinWMS 3.2.18, gratia-probe 1.17.4, HTCondor 8.6.1, HTCondor-CE 2.1.4, VOMS 2.0.14-1.3, \nXRootD 4.6.0\n; Upcoming: frontier-squid 3.5.24, HTCondor 8.6.1\n\n\n\n\n\n\n3.3.21-3\n\n\n02-Mar-2017\n\n\nDetailed Notes\n\n\nIGTF 1.81, VO Package v71\n\n\n\n\n\n\n3.3.21-2\n\n\n16-Feb-2017\n\n\nDetailed Notes\n\n\nIGTF 1.80\n\n\n\n\n\n\n3.3.21\n\n\n14-Feb-2017\n\n\nDetailed Notes\n\n\nCVMFS 2.3.2 patched, GlideinWMS 3.2.17, HTCondor 8.4.11, HTCondor-CE 2.1.2, osg-configure 1.6.1, osg-update-vos 1.3.0, RSV-PerfSONAR 1.2.1; Upcoming: Singularity 2.2.1\n\n\n\n\n\n\n3.3.20-2\n\n\n26-Jan-2017\n\n\nDetailed Notes\n\n\nIGTF 1.79, VO Package v70\n\n\n\n\n\n\n3.3.20\n\n\n10-Jan-2017\n\n\nDetailed Notes\n\n\nHTCondor 8.4.10, gratia-probe 1.17.2, XRootD 4.5.0, osg-configure 1.5.4, VOMS 2.0.14; Upcoming: HTCondor 8.5.8, Singularity 2.2, frontier-squid 3.5.23-3.1\n\n\n\n\n\n\n3.3.19\n\n\n13-Dec-2016\n\n\nDetailed Notes\n\n\nHTCondor-CE 2.1.1, osg-configure 1.5.2, gratia-probe 1.17.0-2.6, Upcoming: frontier-squid 3.5.22-2.1\n\n\n\n\n\n\n3.3.18\n\n\n08-Nov-2016\n\n\nDetailed Notes\n\n\nGlideinWMS 3.2.16, edg-mkgridmap 4.0.4, osg-pki-tools 1.2.20, BLAHP 1.18.28, HTCondor-CE 2.0.11, gratia-probe 1.17.0-2.5\n\n\n\n\n\n\n3.3.17-2\n\n\n19-Oct-2016\n\n\nDetailed Notes\n\n\nIGTF 1.78\n\n\n\n\n\n\n3.3.17\n\n\n13-Oct-2016\n\n\nDetailed Notes\n\n\nHTCondor 8.4.9, HTCondor-CE 2.0.10, CVMFS 2.3.2, XRootD 4.4.0, frontier-squid 2.7.STABLE9-27, gratia-probe 1.17.0, osg-control 1.1.0, VO Package v69, HTCondor 8.5.7 in upcoming\n\n\n\n\n\n\n3.3.16\n\n\n13-Sep-2016\n\n\nDetailed Notes\n\n\nGlobus update, BLAHP 1.18.25, GlideinWMS 3.2.15, HTCondor-CE 2.0.8, EL7 support complete, RSV GlideinWMS Tester, VO Package v68, osg-pki-tools 1.2.19, lcas-lcmaps-gt4-interface 0.3.1\n\n\n\n\n\n\n3.3.15\n\n\n09-Aug-2016\n\n\nDetailed Notes\n\n\nIGTF 1.76, VO Package v67, BLAHP 1.18.23, XRootD-HDFS 1.8.8, gridFTP 7.30-1.3, gums 1.5.2-4, RSV 3.13.1, GSI-OpenSSH 7.1p2f, VOMS 2.0.12-3.3, XRootD-DSI 3.0.4-20, EL7: voms-admin-client 2.0.17-1.1, Upcoming: HTCondor 8.5.6\n\n\n\n\n\n\n3.3.14\n\n\n12-Jul-2016\n\n\nDetailed Notes\n\n\nIGTF 1.75, HTCondor 8.4.8, GlideinWMS 3.2.14.1, HTCondor-CE 2.0.7, BLAHP 1.18.21, gridFTP 7.30-1.2, osg-configure 1.4.1, xrootd-voms-plugin 0.4.0, osg-system-profiler 1.4.0, gridFTP-HDFS 0.5.4, cvmfs-config-osg 1.2.5, bigtop-utils, osg-voms 3.3-3\n\n\n\n\n\n\n3.3.13\n\n\n14-Jun-2016\n\n\nDetailed Notes\n\n\nIGTF 1.74, CVMFS 2.2.3, HTCondor 8.4.7, blahp 1.18.20, RSV 3.13.0-3, GUMS on EL7, BeStMan2 2.3.0, Upcoming: HTCondor 8.5.5, CVMFS 2.3.0\n\n\n\n\n\n\n3.3.12\n\n\n10-May-2016\n\n\nDetailed Notes\n\n\nVO Package v66, BeStMan 2.3.0 on EL7, CVMFS 2.2.2, Pegasus 4.6.1, HTCondor 8.4.6, HTCondor CE 2.0.5, HTCondor CE BOSCO Support, osg-configure 1.4.0, blahp 1.18.19, Dropped GRAM packages from OSG CE, Upcoming: HTCondor 8.5.4\n\n\n\n\n\n\n3.3.11\n\n\n12-Apr-2016\n\n\nDetailed Notes\n\n\nVO Package v65, IGTF 1.73, XRootD 4.3.0, HDFS 2.0.0+1612, GlideinWMS 3.2.13, HTCondor CE 2.0.4, BLAHP 1.18.18, osg-pki-tools 1.2.15, HTCondor 8.4.5, Upcoming: HTCondor 8.5.3\n\n\n\n\n\n\n3.3.10\n\n\n08-Mar-2016\n\n\nDetailed Notes\n\n\nVO Package v64, IGTF 1.72, osg-ca-certs-updater 1.4, BLAHP 1.18.17, osg-configure 1.2.6, HTCondor 8.4.4, GUMS 1.5.2, Gratia probes 1.15.0, StashCache 0.6, AutoPyFactory 2.4.6, XRootD-HDFS 1.8.7, GridFTP-HDFS 3.3, HTCondor CE 2.0.2, Upcoming: HTCondor 8.5.2\n\n\n\n\n\n\n3.3.9\n\n\n09-Feb-2016\n\n\nDetailed Notes\n\n\nGSI-OpenSSH 5.7, glideinWMS 3.2.12, XRootD LCMAPS 1.2.1, XRootD HDFS plugin 1.8.6, lcmaps-plugins-scas-client 0.5.6, IGTF 1.71, VO Package v63, VOMS admin 2.7.0, RSV-perfsonar 1.1.2; Upcoming: CVMFS 2.2.0, CVMFS over StashCache\n\n\n\n\n\n\n3.3.8\n\n\n12-Jan-2016\n\n\nDetailed Notes\n\n\nHTCondor CE 2.0, HTCondor 8.4.3, cctools 4.4.3, VO Package v62, MyProxy 6.1.15, gratia 1.16.2, osg-pki-tools 1.2.14, condor-cron 1.0.11, HTCondor 8.5.1 in upcoming\n\n\n\n\n\n\n3.3.7\n\n\n15-Dec-2015\n\n\nDetailed Notes\n\n\nosg-pki-tools-1.2.13\n\n\n\n\n\n\n3.3.6\n\n\n08-Dec-2015\n\n\nDetailed Notes\n\n\nIGTF 1.70, voms-admin-server-2.7.0-1.17, condor-8.4.2-1.2\n\n\n\n\n\n\n3.3.5\n\n\n19-Nov-2015\n\n\nDetailed Notes\n\n\nHTCondor CE 1.20, HTCondor 8.4.2, lcmaps-plugins-scas-client 0.5.5, osg-configure 1.2.4, RSV 3.12.5\n\n\n\n\n\n\n3.3.4\n\n\n10-Nov-2015\n\n\nDetailed Notes\n\n\nCILogon CA certs, CVMFS support approved repos, lcmaps process tracking, osg-ca-cert-updater 1.3, RSV perfSONAR probe, RSV 3.12.0, EL7: Frontier Squid and MyProxy\n\n\n\n\n\n\n3.3.3\n\n\n03-Nov-2015\n\n\nDetailed Notes\n\n\nIGTF 1.69\n\n\n\n\n\n\n3.3.2\n\n\n13-Oct-2015\n\n\nDetailed Notes\n\n\nGlideinWMS 3.2.11.2, XRootD 4.2.3, HTCondor 8.4.0, StashCache 0.6, GUMS 1.5.1, HTCondor-CE 1.16, GIP 1.3.11, osg-configure 1.2.2, RSV 3.10.4, lcmaps-plugins-mount-under-scratch, edg-mkgridmap 4.0.3, IGTF 1.68\n\n\n\n\n\n\n3.3.1\n\n\n08-Sep-2015\n\n\nDetailed Notes\n\n\nGUMS 1.5.0, HTCondor 8.3.8, HTCondor CE 1.15, gridFTP-HDFS checksum verification, IGTF 1.67, StashCache 0.4\n\n\n\n\n\n\n3.3.0\n\n\n11-Aug-2015\n\n\nDetailed Notes\n\n\nEL7 support for worker nodes, XRootD 4.2.2, 2 patches to HTCondor 8.3.6, lcmaps-plugins-verify-proxy 1.5.7, lcmaps-plugins-scas-client 0.5.5, VO Package v61", 
            "title": "Release Notes"
        }, 
        {
            "location": "/release/notes/#release-notes", 
            "text": "Note  Since August 9, 2016, release series 3.2.x are not supported anymore. For release notes older than 3.3, please see the  archives .", 
            "title": "Release Notes"
        }, 
        {
            "location": "/release/notes/#osg-34", 
            "text": "Version  Date  Details  Summary      3.4.4-2  11-Oct-2017  Detailed Notes  IGTF 1.86, VO Package v75    3.4.4  10-Oct-2017  Detailed Notes  TBA    3.4.3  12-Sep-2017  Detailed Notes  TBA    3.4.2-2  14-Aug-2017  Detailed Notes  igtf-ca-certs-1.85, osg-ca-certs-1.65    3.4.2  08-Aug-2017  Detailed Notes  TBA    3.4.1-2  13-Jul-2017  Detailed Notes  igtf-ca-certs-1.84, osg-ca-certs-1.64    3.4.1  12-Jul-2017  Detailed Notes  blahp-1.18.30.bosco condor-8.6.4 condor-cron-1.1.2 cvmfs-2.3.5-1.1 globus-gridftp-server-11.8-1.3 gratia-probe-1.18.1 gridftp-dsi-posix-1.4-2 htcondor-ce-2.2.1 lcmaps-plugins-verify-proxy-1.5.9-1.2 osg-build-1.10.1 osg-configure-2.1.0-2 osg-gridftp-3.4-3 osg-test-1.11.0 osg-version-3.4.1 rsv-3.14.2, Upcoming: blahp-1.18.30.bosco condor-8.7.2 glite-ce-cream-client-api-c-1.15.4-2.4    3.4.0-2  15-Jun-2017  Detailed Notes  IGTF 1.83, VO Package v74    3.4.0  14-Jun-2017  Detailed Notes  autopyfactory-2.4.6-4 blahp-1.18.29.bosco-3 bwctl-1.4-7 cctools-4.4.3 condor-8.6.3-1.1 condor-cron-1.1.1-2 cvmfs-2.3.5 cvmfs-config-osg-2.0-2 cvmfs-x509-helper-1.0 frontier-squid-3.5.24-3.1 glideinwms-3.2.19-2 glite-build-common-cpp-3.3.0.2 glite-ce-cream-client-api-c-1.15.4-2.3 glite-ce-wsdl-1.15.1-1.1 glite-lbjp-common-gsoap-plugin-3.2.12-1.1 globus-ftp-client-8.29-1.1 globus-gridftp-osg-extensions-0.3-2 globus-gridftp-server-11.8-1.1 globus-gridftp-server-control-4.1-1.3 gratia-probe-1.17.5 gsi-openssh-7.1p2f-1.2 htcondor-ce-2.2.0 igtf-ca-certs-1.82 javascriptrrd-1.1.1 koji-1.11.0-1.5 lcas-lcmaps-gt4-interface-0.3.1-1.2 lcmaps-1.6.6-1.6 lcmaps-plugins-basic-1.7.0-2 lcmaps-plugins-scas-client-0.5.6 lcmaps-plugins-verify-proxy-1.5.9-1.1 lcmaps-plugins-voms-1.7.1-1.4 llrun-0.1.3-1.3 mash-0.5.22-3 myproxy-6.1.18-1.4 nuttcp-6.1.2 osg-build-1.10.0 osg-ca-certs-1.62 osg-ca-certs-updater-1.4 osg-ca-generator-1.2.0 osg-ca-scripts-1.1.6 osg-ce-3.4-2 osg-configure-2.0.0-3 osg-control-1.1.0 osg-gridftp-3.4-2 osg-gridftp-xrootd-3.4 osg-oasis-7-9 osg-pki-tools-1.2.20 osg-system-profiler-1.4.0 osg-test-1.10.1 osg-tested-internal-3.4-2 osg-update-vos-1.4.0 osg-version-3.4.0 osg-vo-map-0.0.2 osg-wn-client-3.4 owamp-3.2rc4-2 pegasus-4.7.4-1.1 rsv-3.14.0-2 rsv-gwms-tester-1.1.2 stashcache-0.7-2 uberftp-2.8-2.1 vo-client-73 voms-2.0.14-1.3 xacml-1.5.0 xrootd-4.6.1 xrootd-dsi-3.0.4-22 xrootd-lcmaps-1.3.3-3 xrootd-voms-plugin-0.4.0, Upcoming: glideinwms-3.3.2-2", 
            "title": "OSG 3.4"
        }, 
        {
            "location": "/release/notes/#osg-33", 
            "text": "Version  Date  Details  Summary      3.3.29-2  11-Oct-2017  Detailed Notes  IGTF 1.86, VO Package v75    3.3.29  10-Oct-2017  Detailed Notes  TBA    3.3.28  12-Sep-2017  Detailed Notes  TBA    3.3.27  08-AUg-2017  Detailed Notes  TBA    3.4.1-2, 3.3.26-2  13-Jul-2017  Detailed Notes  igtf-ca-certs-1.84, osg-ca-certs-1.64    3.3.26  12-Jul-2017  Detailed Notes  blahp-1.18.30.bosco condor-cron-1.1.2 cvmfs-2.3.5-1.1 globus-gridftp-server-11.8-1.2 gratia-probe-1.18.1 gridftp-dsi-posix-1.4-2 hadoop-2.0.0+1612-1.cdh4.7.1.p0.12.7 htcondor-ce-2.2.1 lcmaps-plugins-verify-proxy-1.5.9-1.2 osg-build-1.10.1 osg-configure-1.9.0 osg-gridftp-3.3-5 osg-test-1.11.0 osg-version-3.3.26 rsv-3.14.2    3.3.25  14-Jun-2017  Detailed Notes  emi-trustmanager-tomcat-3.0.0-15 glideinwms-3.2.19 globus-xio-5.12-1.2 htcondor-ce-2.2.0 lcmaps-plugins-voms-1.7.1-1.4 osg-build-1.10.0 osg-ca-scripts-1.1.6 osg-ce-3.3-13 osg-configure-1.8.1-2 osg-gridftp-3.3-4 osg-update-vos-1.4.0 stashcache-0.7-2 xrootd-4.6.1 xrootd-lcmaps-1.3.3-3    3.3.24-2  16-May-2017  Detailed Notes  VO Package v73    3.3.24  09-May-2017  Detailed Notes  cvmfs-x509-helper 1.0, gratia-probe 1.17.5, lcmaps 1.6.6-1.3, osg-build 1.9.0, osg-configure 1.7.0, osg-wn-client 3.3-7; Upcoming: HTCondor 8.6.2, GlideinWMS 3.3.2    3.3.23  11-Apr-2017  Detailed Notes  lcmaps 1.6.6-1.2, HTCondor-CE 2.1.5, CVMFS 2.3.5, Pegasus 4.7.4, OSG-CE 3.3-12; Upcoming: Frontier squid 3.5.24-3.1    3.3.22-3  10-Apr-2017  Detailed Notes  IGTF 1.82    3.3.22-2  28-Mar-2017  Detailed Notes  VO Package v72    3.3.22  14-Mar-2017  Detailed Notes  BLAHP 1.18.29, CVMFS 2.3.3, emi-trustmanager 3.0.3-14, GlideinWMS 3.2.18, gratia-probe 1.17.4, HTCondor 8.6.1, HTCondor-CE 2.1.4, VOMS 2.0.14-1.3,  XRootD 4.6.0 ; Upcoming: frontier-squid 3.5.24, HTCondor 8.6.1    3.3.21-3  02-Mar-2017  Detailed Notes  IGTF 1.81, VO Package v71    3.3.21-2  16-Feb-2017  Detailed Notes  IGTF 1.80    3.3.21  14-Feb-2017  Detailed Notes  CVMFS 2.3.2 patched, GlideinWMS 3.2.17, HTCondor 8.4.11, HTCondor-CE 2.1.2, osg-configure 1.6.1, osg-update-vos 1.3.0, RSV-PerfSONAR 1.2.1; Upcoming: Singularity 2.2.1    3.3.20-2  26-Jan-2017  Detailed Notes  IGTF 1.79, VO Package v70    3.3.20  10-Jan-2017  Detailed Notes  HTCondor 8.4.10, gratia-probe 1.17.2, XRootD 4.5.0, osg-configure 1.5.4, VOMS 2.0.14; Upcoming: HTCondor 8.5.8, Singularity 2.2, frontier-squid 3.5.23-3.1    3.3.19  13-Dec-2016  Detailed Notes  HTCondor-CE 2.1.1, osg-configure 1.5.2, gratia-probe 1.17.0-2.6, Upcoming: frontier-squid 3.5.22-2.1    3.3.18  08-Nov-2016  Detailed Notes  GlideinWMS 3.2.16, edg-mkgridmap 4.0.4, osg-pki-tools 1.2.20, BLAHP 1.18.28, HTCondor-CE 2.0.11, gratia-probe 1.17.0-2.5    3.3.17-2  19-Oct-2016  Detailed Notes  IGTF 1.78    3.3.17  13-Oct-2016  Detailed Notes  HTCondor 8.4.9, HTCondor-CE 2.0.10, CVMFS 2.3.2, XRootD 4.4.0, frontier-squid 2.7.STABLE9-27, gratia-probe 1.17.0, osg-control 1.1.0, VO Package v69, HTCondor 8.5.7 in upcoming    3.3.16  13-Sep-2016  Detailed Notes  Globus update, BLAHP 1.18.25, GlideinWMS 3.2.15, HTCondor-CE 2.0.8, EL7 support complete, RSV GlideinWMS Tester, VO Package v68, osg-pki-tools 1.2.19, lcas-lcmaps-gt4-interface 0.3.1    3.3.15  09-Aug-2016  Detailed Notes  IGTF 1.76, VO Package v67, BLAHP 1.18.23, XRootD-HDFS 1.8.8, gridFTP 7.30-1.3, gums 1.5.2-4, RSV 3.13.1, GSI-OpenSSH 7.1p2f, VOMS 2.0.12-3.3, XRootD-DSI 3.0.4-20, EL7: voms-admin-client 2.0.17-1.1, Upcoming: HTCondor 8.5.6    3.3.14  12-Jul-2016  Detailed Notes  IGTF 1.75, HTCondor 8.4.8, GlideinWMS 3.2.14.1, HTCondor-CE 2.0.7, BLAHP 1.18.21, gridFTP 7.30-1.2, osg-configure 1.4.1, xrootd-voms-plugin 0.4.0, osg-system-profiler 1.4.0, gridFTP-HDFS 0.5.4, cvmfs-config-osg 1.2.5, bigtop-utils, osg-voms 3.3-3    3.3.13  14-Jun-2016  Detailed Notes  IGTF 1.74, CVMFS 2.2.3, HTCondor 8.4.7, blahp 1.18.20, RSV 3.13.0-3, GUMS on EL7, BeStMan2 2.3.0, Upcoming: HTCondor 8.5.5, CVMFS 2.3.0    3.3.12  10-May-2016  Detailed Notes  VO Package v66, BeStMan 2.3.0 on EL7, CVMFS 2.2.2, Pegasus 4.6.1, HTCondor 8.4.6, HTCondor CE 2.0.5, HTCondor CE BOSCO Support, osg-configure 1.4.0, blahp 1.18.19, Dropped GRAM packages from OSG CE, Upcoming: HTCondor 8.5.4    3.3.11  12-Apr-2016  Detailed Notes  VO Package v65, IGTF 1.73, XRootD 4.3.0, HDFS 2.0.0+1612, GlideinWMS 3.2.13, HTCondor CE 2.0.4, BLAHP 1.18.18, osg-pki-tools 1.2.15, HTCondor 8.4.5, Upcoming: HTCondor 8.5.3    3.3.10  08-Mar-2016  Detailed Notes  VO Package v64, IGTF 1.72, osg-ca-certs-updater 1.4, BLAHP 1.18.17, osg-configure 1.2.6, HTCondor 8.4.4, GUMS 1.5.2, Gratia probes 1.15.0, StashCache 0.6, AutoPyFactory 2.4.6, XRootD-HDFS 1.8.7, GridFTP-HDFS 3.3, HTCondor CE 2.0.2, Upcoming: HTCondor 8.5.2    3.3.9  09-Feb-2016  Detailed Notes  GSI-OpenSSH 5.7, glideinWMS 3.2.12, XRootD LCMAPS 1.2.1, XRootD HDFS plugin 1.8.6, lcmaps-plugins-scas-client 0.5.6, IGTF 1.71, VO Package v63, VOMS admin 2.7.0, RSV-perfsonar 1.1.2; Upcoming: CVMFS 2.2.0, CVMFS over StashCache    3.3.8  12-Jan-2016  Detailed Notes  HTCondor CE 2.0, HTCondor 8.4.3, cctools 4.4.3, VO Package v62, MyProxy 6.1.15, gratia 1.16.2, osg-pki-tools 1.2.14, condor-cron 1.0.11, HTCondor 8.5.1 in upcoming    3.3.7  15-Dec-2015  Detailed Notes  osg-pki-tools-1.2.13    3.3.6  08-Dec-2015  Detailed Notes  IGTF 1.70, voms-admin-server-2.7.0-1.17, condor-8.4.2-1.2    3.3.5  19-Nov-2015  Detailed Notes  HTCondor CE 1.20, HTCondor 8.4.2, lcmaps-plugins-scas-client 0.5.5, osg-configure 1.2.4, RSV 3.12.5    3.3.4  10-Nov-2015  Detailed Notes  CILogon CA certs, CVMFS support approved repos, lcmaps process tracking, osg-ca-cert-updater 1.3, RSV perfSONAR probe, RSV 3.12.0, EL7: Frontier Squid and MyProxy    3.3.3  03-Nov-2015  Detailed Notes  IGTF 1.69    3.3.2  13-Oct-2015  Detailed Notes  GlideinWMS 3.2.11.2, XRootD 4.2.3, HTCondor 8.4.0, StashCache 0.6, GUMS 1.5.1, HTCondor-CE 1.16, GIP 1.3.11, osg-configure 1.2.2, RSV 3.10.4, lcmaps-plugins-mount-under-scratch, edg-mkgridmap 4.0.3, IGTF 1.68    3.3.1  08-Sep-2015  Detailed Notes  GUMS 1.5.0, HTCondor 8.3.8, HTCondor CE 1.15, gridFTP-HDFS checksum verification, IGTF 1.67, StashCache 0.4    3.3.0  11-Aug-2015  Detailed Notes  EL7 support for worker nodes, XRootD 4.2.2, 2 patches to HTCondor 8.3.6, lcmaps-plugins-verify-proxy 1.5.7, lcmaps-plugins-scas-client 0.5.5, VO Package v61", 
            "title": "OSG 3.3"
        }, 
        {
            "location": "/release/release_series/", 
            "text": "OSG Release Series\n\n\nAn OSG release series is a sequence of OSG software releases that are intended to provide a painless upgrade path. For example, the 3.2 release series contains OSG software 3.2.0, 3.2.1, 3.2.2, and so forth. A release series corresponds to a set of Yum software repositories, including ones for development, testing, and production use. The Yum repositories for one release series are completely distinct from the repositories for a different release series, even though they share many common packages.  A particular release within a series is a snapshot of packages and their exact versions at one point in time. When you install software from a release series, say 3.2, you get the most current versions of software packages within that series, regardless of the current release version.\n\n\nWhen a new series is released, it is an opportunity for the OSG Technology area to add major new software packages, make substantial updates to existing packages, and remove obsolete packages. When a new series is initially released, most packages are identical to the previous release, but two adjacent series will diverge over time.\n\n\nOur goal is, within a series, that one may upgrade their OSG services via \nyum update\n cleanly and without any necessary config file changes or excessive downtime.\n\n\nOSG Release Series\n\n\nSince the start of the RPM-based OSG software stack, we have offered the following release series:\n\n\n\n\n\n\nOSG 3.1\n started in April 2012, and was end-of-lifed in April 2015. While the files have not been removed, it is strongly recommended that it not be installed anymore. Historically, there were 3.0.x releases as well, but there was no separate release series for 3.0 and 3.1; we simply went from 3.0.10 to 3.1.0 in the same repositories.\n\n\n\n\n\n\nOSG 3.2\n started in November 2013, and was end-of-lifed in August 2016. While the files have not been removed, it is strongly recommended that it not be installed anymore. The main differences between it and 3.1 were the introduction of glideinWMS 3.2, HTCondor 8.0, and Hadoop/HDFS 2.0; also the gLite CE Monitor system was dropped in favor of osg-info-services.\n\n\n\n\n\n\nOSG 3.3\n started in August 2015 and is still supported today.  End-of-support is scheduled for May 2018; sites are encouraged to investigate the upgrade to OSG 3.4. The main differences between 3.3 and 3.2 are the dropping of EL5 support, the addition of EL7 support, and the dropping of Globus GRAM support.\n\n\n\n\n\n\nOSG 3.4\n stared June 2017. The main differences between it and 3.3 are the removal of edg-mkgridmap, GUMS, BeStMan, and VOMS Admin Server packages.\n\n\n\n\n\n\nOSG Upcoming\n\n\nThere is one more OSG Series called \"upcoming\" which contains major updates planned for a future release series. The yum repositories for upcoming (\nosg-upcoming\n and \nosg-upcoming-testing\n) are available from all OSG 3.x series, and individual packages can be installed from Upcoming without needing to update entirely to a new series. Note, however, that packages in the \"upcoming\" repositories are tested against the most recent OSG series.  As of the time of writing, \nosg-upcoming\n is meant to work with OSG 3.4.\n\n\nInstalling an OSG Release Series\n\n\nSee the \nyum repositories document\n for instructions on installing the OSG repositories.\n\n\nUpdating from OSG 3.1, 3.2, 3.3 to 3.3 or 3.4\n\n\n\n\n\n\nIf you have an existing installation based on OSG 3.1, 3.2, or 3.3 (which will be referred to as the \nold series\n), and want to upgrade to 3.3 or 3.4 (the \nnew series\n), we recommend the following procedure:\n\n\nFirst, remove the old series yum repositories:\n\n\nroot@host #\n rpm -e osg-release\n\n\n\n\n\nThis step ensures that any local modifications to \n*.repo\n files will not prevent installing the new series repos. Any modified \n*.repo\n files should appear under \n/etc/yum.repos.d/\n with the \n*.rpmsave\n extension. After installing the new OSG repositories (the next step) you may want to apply any changes made in the \n*.rpmsave\n files to the new \n*.repo\n files.\n\n\n\n\n\n\nInstall the OSG repositories:\n\n\nroot@host #\n rpm -Uvh \nURL\n\n\n\n\n\n\nwhere \nURL\n is one of the following:\n\n\n\n\n\n\n\n\nSeries\n\n\nEL5 URL (for RHEL5, CentOS5, or SL5)\n\n\nEL6 URL (for RHEL6, CentOS6, or SL6)\n\n\nEL7 URL (for RHEL7, CentOS7, or SL7)\n\n\n\n\n\n\n\n\n\n\nOSG 3.1\n (unsupported)\n\n\nhttp://repo.opensciencegrid.org/osg/3.1/osg-3.1-el5-release-latest.rpm\n\n\nhttp://repo.opensciencegrid.org/osg/3.1/osg-3.1-el6-release-latest.rpm\n\n\nN/A\n\n\n\n\n\n\nOSG 3.2\n (unsupported)\n\n\nhttp://repo.opensciencegrid.org/osg/3.2/osg-3.2-el5-release-latest.rpm\n\n\nhttp://repo.opensciencegrid.org/osg/3.2/osg-3.2-el6-release-latest.rpm\n\n\nN/A\n\n\n\n\n\n\nOSG 3.3\n\n\nN/A\n\n\nhttp://repo.opensciencegrid.org/osg/3.3/osg-3.3-el6-release-latest.rpm\n\n\nhttp://repo.opensciencegrid.org/osg/3.3/osg-3.3-el7-release-latest.rpm\n\n\n\n\n\n\nOSG 3.4\n\n\nN/A\n\n\nhttp://repo.opensciencegrid.org/osg/3.4/osg-3.4-el6-release-latest.rpm\n\n\nhttp://repo.opensciencegrid.org/osg/3.4/osg-3.4-el7-release-latest.rpm\n\n\n\n\n\n\n\n\n\n\n\n\nClean yum cache:\n\n\nroot@host #\n yum clean all --enablerepo\n=\n*\n\n\n\n\n\n\n\n\n\nUpdate software:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\n\n\nThis command will update \nall\n packages on your system.\n\n\nTroubleshooting\n If you are not having the expected result or having problems with Yum please see the \nYum troubleshooting guide\n\n\nMigrating from edg-mkgridmap to LCMAPS VOMS Plugin\n\n\nAfter following the update instructions above, perform the migration process documented \nhere\n.\n\n\nUpdating from Frontier Squid 2.7 to Frontier Squid 3.5 (upgrading from OSG 3.3)\n\n\nThe program \nfrontier-squid\n received a major version upgrade (versions 2.7 to 3.5) between OSG 3.3 and OSG 3.4. Follow the \nupstream upgrade documentation\n when transitioning your squid server to OSG 3.4.\n\n\nUninstalling BeStMan2 from the Storage Element (upgrading to OSG 3.4)\n\n\nThe program BeStMan2 is no longer available in OSG 3.4 and its functionality has been replaced by \nload-balanced GridFTP\n. To update your storage element to OSG 3.4, you must perform the following procedure:\n\n\n\n\n\n\nEnsure that OSG BeStMan packages are installed:\n\n\nroot@host #\n rpm -q osg-se-bestman\n\n\n\n\n\n\n\n\n\nStop the \nbestman2\n service:\n\n\nroot@host #\n service bestman2 stop\n\n\n\n\n\n\n\n\n\nUninstall the software:\n\n\nroot@host #\n yum erase bestman2-tester-libs bestman2-common-libs \n\\\n\n                            bestman2-server-libs bestman2-server-dep-libs \n\\\n\n                            bestman2-client-libs bestman2-tester bestman2-client \n\\\n\n                            bestman2-server osg-se-bestman\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nIn the output from this command, yum should \nnot\n list other packages than those nine. If it lists other packages, cancel the erase operation, make sure the other packages are updated to their latest OSG 3.3 versions, and try again.\n\n\n\n\nAfter successfully removing BeStMan2, continue updating your host to OSG 3.4 by following the instructions above.\n\n\nUninstalling OSG Info Services from the Compute Element (upgrading from OSG 3.3 or 3.2)\n\n\nThe program OSG Info Services is no longer required on OSG 3.3, and is no longer available starting in OSG 3.4. This is because the service that OSG Info Services reported to, named BDII, has been retired and is no longer functional.\n\n\nTo cleanly uninstall OSG Info Services from your CE, perform the following procedure (after following the main update instructions above):\n\n\n\n\n\n\nEnsure that you are using a sufficiently new version of the \nosg-ce\n metapackages:\n\n\nroot@host #\n rpm -q osg-ce\n\n\n\n\n\nshould be at least 3.3-12 (OSG 3.3) or 3.4-1 (OSG 3.4).  If not, update them:\n\n\nroot@host #\n yum update osg-ce\n\n\n\n\n\n\n\n\n\nStop the \nosg-info-services\n service:\n\n\nroot@host #\n service osg-info-services stop\n\n\n\n\n\n\n\n\n\nUninstall the software:\n\n\nroot@host #\n yum erase gip osg-info-services\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nIn the output from this command, yum should \nnot\n list other packages than those two. If it lists other packages, cancel the erase operation, make sure the other packages are updated to their latest OSG 3.3 (or 3.4) versions, and try again.\n\n\n\n\nUninstalling CEMon from the Compute Element (upgrading from OSG 3.1)\n\n\nThe program CEMon (found in the package \nglite-ce-monitor\n) is no longer available starting in OSG 3.2. Its functionality is no longer required because the service that CEMon reported to has been retired and is no longer functional.\n\n\nTo cleanly uninstall CEMon from your CE, perform the following procedure (after following the main update instructions above):\n\n\n\n\n\n\nEnsure that you are using a sufficiently new version of the \nosg-ce\n metapackages:\n\n\nroot@host #\n rpm -q osg-ce\n\n\n\n\n\nshould be at least 3.3-12 (OSG 3.3) or 3.4-1 (OSG 3.4). If not, update them:\n\n\nroot@host #\n yum update osg-ce\n\n\n\n\n\n\n\n\n\nIf there is a CEMon configuration file at \n/etc/osg/config.d/30-cemon.ini\n, remove it.\n\n\n\n\nRemove CEMon and related packages:\nroot@host #\n yum erase glite-ce-monitor glite-ce-osg-ce-plugin osg-configure-cemon\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nIn the output from this command, yum should \nnot\n list other packages than those three. If it lists other packages, cancel the erase operation, make sure the other packages are updated to their OSG 3.3 (or 3.4) versions (they should have \n.osg33\n or \n.osg34\n in their versions), and try again.\n\n\n\n\nReferences\n\n\n\n\nYUM Repositories\n\n\nBasic use of Yum\n\n\nBest practices in using Yum", 
            "title": "OSG Release Series"
        }, 
        {
            "location": "/release/release_series/#osg-release-series", 
            "text": "An OSG release series is a sequence of OSG software releases that are intended to provide a painless upgrade path. For example, the 3.2 release series contains OSG software 3.2.0, 3.2.1, 3.2.2, and so forth. A release series corresponds to a set of Yum software repositories, including ones for development, testing, and production use. The Yum repositories for one release series are completely distinct from the repositories for a different release series, even though they share many common packages.  A particular release within a series is a snapshot of packages and their exact versions at one point in time. When you install software from a release series, say 3.2, you get the most current versions of software packages within that series, regardless of the current release version.  When a new series is released, it is an opportunity for the OSG Technology area to add major new software packages, make substantial updates to existing packages, and remove obsolete packages. When a new series is initially released, most packages are identical to the previous release, but two adjacent series will diverge over time.  Our goal is, within a series, that one may upgrade their OSG services via  yum update  cleanly and without any necessary config file changes or excessive downtime.", 
            "title": "OSG Release Series"
        }, 
        {
            "location": "/release/release_series/#osg-release-series_1", 
            "text": "Since the start of the RPM-based OSG software stack, we have offered the following release series:    OSG 3.1  started in April 2012, and was end-of-lifed in April 2015. While the files have not been removed, it is strongly recommended that it not be installed anymore. Historically, there were 3.0.x releases as well, but there was no separate release series for 3.0 and 3.1; we simply went from 3.0.10 to 3.1.0 in the same repositories.    OSG 3.2  started in November 2013, and was end-of-lifed in August 2016. While the files have not been removed, it is strongly recommended that it not be installed anymore. The main differences between it and 3.1 were the introduction of glideinWMS 3.2, HTCondor 8.0, and Hadoop/HDFS 2.0; also the gLite CE Monitor system was dropped in favor of osg-info-services.    OSG 3.3  started in August 2015 and is still supported today.  End-of-support is scheduled for May 2018; sites are encouraged to investigate the upgrade to OSG 3.4. The main differences between 3.3 and 3.2 are the dropping of EL5 support, the addition of EL7 support, and the dropping of Globus GRAM support.    OSG 3.4  stared June 2017. The main differences between it and 3.3 are the removal of edg-mkgridmap, GUMS, BeStMan, and VOMS Admin Server packages.", 
            "title": "OSG Release Series"
        }, 
        {
            "location": "/release/release_series/#osg-upcoming", 
            "text": "There is one more OSG Series called \"upcoming\" which contains major updates planned for a future release series. The yum repositories for upcoming ( osg-upcoming  and  osg-upcoming-testing ) are available from all OSG 3.x series, and individual packages can be installed from Upcoming without needing to update entirely to a new series. Note, however, that packages in the \"upcoming\" repositories are tested against the most recent OSG series.  As of the time of writing,  osg-upcoming  is meant to work with OSG 3.4.", 
            "title": "OSG Upcoming"
        }, 
        {
            "location": "/release/release_series/#installing-an-osg-release-series", 
            "text": "See the  yum repositories document  for instructions on installing the OSG repositories.", 
            "title": "Installing an OSG Release Series"
        }, 
        {
            "location": "/release/release_series/#updating-from-osg-31-32-33-to-33-or-34", 
            "text": "If you have an existing installation based on OSG 3.1, 3.2, or 3.3 (which will be referred to as the  old series ), and want to upgrade to 3.3 or 3.4 (the  new series ), we recommend the following procedure:  First, remove the old series yum repositories:  root@host #  rpm -e osg-release  This step ensures that any local modifications to  *.repo  files will not prevent installing the new series repos. Any modified  *.repo  files should appear under  /etc/yum.repos.d/  with the  *.rpmsave  extension. After installing the new OSG repositories (the next step) you may want to apply any changes made in the  *.rpmsave  files to the new  *.repo  files.    Install the OSG repositories:  root@host #  rpm -Uvh  URL   where  URL  is one of the following:     Series  EL5 URL (for RHEL5, CentOS5, or SL5)  EL6 URL (for RHEL6, CentOS6, or SL6)  EL7 URL (for RHEL7, CentOS7, or SL7)      OSG 3.1  (unsupported)  http://repo.opensciencegrid.org/osg/3.1/osg-3.1-el5-release-latest.rpm  http://repo.opensciencegrid.org/osg/3.1/osg-3.1-el6-release-latest.rpm  N/A    OSG 3.2  (unsupported)  http://repo.opensciencegrid.org/osg/3.2/osg-3.2-el5-release-latest.rpm  http://repo.opensciencegrid.org/osg/3.2/osg-3.2-el6-release-latest.rpm  N/A    OSG 3.3  N/A  http://repo.opensciencegrid.org/osg/3.3/osg-3.3-el6-release-latest.rpm  http://repo.opensciencegrid.org/osg/3.3/osg-3.3-el7-release-latest.rpm    OSG 3.4  N/A  http://repo.opensciencegrid.org/osg/3.4/osg-3.4-el6-release-latest.rpm  http://repo.opensciencegrid.org/osg/3.4/osg-3.4-el7-release-latest.rpm       Clean yum cache:  root@host #  yum clean all --enablerepo = *    Update software:  root@host #  yum update    This command will update  all  packages on your system.  Troubleshooting  If you are not having the expected result or having problems with Yum please see the  Yum troubleshooting guide", 
            "title": "Updating from OSG 3.1, 3.2, 3.3 to 3.3 or 3.4"
        }, 
        {
            "location": "/release/release_series/#migrating-from-edg-mkgridmap-to-lcmaps-voms-plugin", 
            "text": "After following the update instructions above, perform the migration process documented  here .", 
            "title": "Migrating from edg-mkgridmap to LCMAPS VOMS Plugin"
        }, 
        {
            "location": "/release/release_series/#updating-from-frontier-squid-27-to-frontier-squid-35-upgrading-from-osg-33", 
            "text": "The program  frontier-squid  received a major version upgrade (versions 2.7 to 3.5) between OSG 3.3 and OSG 3.4. Follow the  upstream upgrade documentation  when transitioning your squid server to OSG 3.4.", 
            "title": "Updating from Frontier Squid 2.7 to Frontier Squid 3.5 (upgrading from OSG 3.3)"
        }, 
        {
            "location": "/release/release_series/#uninstalling-bestman2-from-the-storage-element-upgrading-to-osg-34", 
            "text": "The program BeStMan2 is no longer available in OSG 3.4 and its functionality has been replaced by  load-balanced GridFTP . To update your storage element to OSG 3.4, you must perform the following procedure:    Ensure that OSG BeStMan packages are installed:  root@host #  rpm -q osg-se-bestman    Stop the  bestman2  service:  root@host #  service bestman2 stop    Uninstall the software:  root@host #  yum erase bestman2-tester-libs bestman2-common-libs  \\ \n                            bestman2-server-libs bestman2-server-dep-libs  \\ \n                            bestman2-client-libs bestman2-tester bestman2-client  \\ \n                            bestman2-server osg-se-bestman     Note  In the output from this command, yum should  not  list other packages than those nine. If it lists other packages, cancel the erase operation, make sure the other packages are updated to their latest OSG 3.3 versions, and try again.   After successfully removing BeStMan2, continue updating your host to OSG 3.4 by following the instructions above.", 
            "title": "Uninstalling BeStMan2 from the Storage Element (upgrading to OSG 3.4)"
        }, 
        {
            "location": "/release/release_series/#uninstalling-osg-info-services-from-the-compute-element-upgrading-from-osg-33-or-32", 
            "text": "The program OSG Info Services is no longer required on OSG 3.3, and is no longer available starting in OSG 3.4. This is because the service that OSG Info Services reported to, named BDII, has been retired and is no longer functional.  To cleanly uninstall OSG Info Services from your CE, perform the following procedure (after following the main update instructions above):    Ensure that you are using a sufficiently new version of the  osg-ce  metapackages:  root@host #  rpm -q osg-ce  should be at least 3.3-12 (OSG 3.3) or 3.4-1 (OSG 3.4).  If not, update them:  root@host #  yum update osg-ce    Stop the  osg-info-services  service:  root@host #  service osg-info-services stop    Uninstall the software:  root@host #  yum erase gip osg-info-services     Note  In the output from this command, yum should  not  list other packages than those two. If it lists other packages, cancel the erase operation, make sure the other packages are updated to their latest OSG 3.3 (or 3.4) versions, and try again.", 
            "title": "Uninstalling OSG Info Services from the Compute Element (upgrading from OSG 3.3 or 3.2)"
        }, 
        {
            "location": "/release/release_series/#uninstalling-cemon-from-the-compute-element-upgrading-from-osg-31", 
            "text": "The program CEMon (found in the package  glite-ce-monitor ) is no longer available starting in OSG 3.2. Its functionality is no longer required because the service that CEMon reported to has been retired and is no longer functional.  To cleanly uninstall CEMon from your CE, perform the following procedure (after following the main update instructions above):    Ensure that you are using a sufficiently new version of the  osg-ce  metapackages:  root@host #  rpm -q osg-ce  should be at least 3.3-12 (OSG 3.3) or 3.4-1 (OSG 3.4). If not, update them:  root@host #  yum update osg-ce    If there is a CEMon configuration file at  /etc/osg/config.d/30-cemon.ini , remove it.   Remove CEMon and related packages: root@host #  yum erase glite-ce-monitor glite-ce-osg-ce-plugin osg-configure-cemon     Note  In the output from this command, yum should  not  list other packages than those three. If it lists other packages, cancel the erase operation, make sure the other packages are updated to their OSG 3.3 (or 3.4) versions (they should have  .osg33  or  .osg34  in their versions), and try again.", 
            "title": "Uninstalling CEMon from the Compute Element (upgrading from OSG 3.1)"
        }, 
        {
            "location": "/release/release_series/#references", 
            "text": "YUM Repositories  Basic use of Yum  Best practices in using Yum", 
            "title": "References"
        }, 
        {
            "location": "/release/supported_platforms/", 
            "text": "OSG Software Supported Operating Systems\n\n\nThe OSG Software 3.3 and 3.4 release series are supported on Red Hat Enterprise 6 and 7 for 64-bit (RHEL 6 and RHEL 7) and 32-bit (RHEL 6 only) Intel architectures.\n\n\nOSG also supports select rebuilds of RHEL.  Specifically:\n\n\n\n\nCentOS 6\n\n\nCentOS 7\n\n\nRed Hat Enterprise Linux 6\n\n\nRed Hat Enterprise Linux 7\n\n\nScientific Linux 6\n\n\nScientific Linux 7\n\n\n\n\nThe OSG Software 3.1 and 3.2 release series also supported Red Hat Enterprise Linux 5, but neither 3.1 nor 3.2 are actively supported.", 
            "title": "Supported Platforms"
        }, 
        {
            "location": "/release/supported_platforms/#osg-software-supported-operating-systems", 
            "text": "The OSG Software 3.3 and 3.4 release series are supported on Red Hat Enterprise 6 and 7 for 64-bit (RHEL 6 and RHEL 7) and 32-bit (RHEL 6 only) Intel architectures.  OSG also supports select rebuilds of RHEL.  Specifically:   CentOS 6  CentOS 7  Red Hat Enterprise Linux 6  Red Hat Enterprise Linux 7  Scientific Linux 6  Scientific Linux 7   The OSG Software 3.1 and 3.2 release series also supported Red Hat Enterprise Linux 5, but neither 3.1 nor 3.2 are actively supported.", 
            "title": "OSG Software Supported Operating Systems"
        }, 
        {
            "location": "/other/install-osg-upcoming-software/", 
            "text": "OSG Upcoming Software Repositories Installation Guide\n\n\nHere we describe the OSG Upcoming Software Repositories, their purpose, and how to use them.\n\n\nThis document is intended for site administrators.\n\n\nOverview and Purpose\n\n\nCertain sites have requested new versions of software that would be considered \"disruptive\" or \"experimental\" -- upgrading to them would likely require manual intervention before the site would come back up. We do not want sites to unwittingly upgrade to these versions. For the benefit of the sites that do want to upgrade, we want to provide the same assurance of quality and production-readiness that we provide to the software we currently ship now.\n\n\nDue to the relatively small number of such packages, a full fork of the OSG 3 distribution was not warranted. Instead, we have created a separate set of repositories that contain only the \"disruptive\" versions of the software.\n\n\nThese repositories have the same structure as our standard repositories. For example, there is an \nosg-upcoming-testing\n repository and an \nosg-upcoming\n repository.\n\n\nA full installation of our software stack is \nnot\n be possible using only the \nosg-upcoming\n repositories, since they contain a small subset of the software we ship. Both the main \nosg\n and the \nosg-upcoming\n repositories will need to be enabled for the installation to work. Because of this, interoperability will be maintained between the main \nosg\n and \nosg-upcoming\n.\n\n\nDepending on test results from sites, some packages in \nosg-upcoming\n may eventually end up in the main \nosg\n branch. The rest of the packages will eventually form the nucleus of the next fork of the software stack (e.g. \"OSG 3.5\").\n\n\nInstallation and Usage\n\n\nThe following directions must be followed to install software from the Upcoming repositories.\n\n\nFirst, install the standard OSG YUM repositories as per \nthe YUM repositories guide\n.\n\n\nEnabling Upcoming repositories\n\n\nYou should have a file called \nosg-upcoming.repo\n (el7) or \nosg-el6-upcoming.repo\n (el6) located in \n/etc/yum.repos.d\n. At the top, it should have a section looking like this (el7):\n\n\n[osg-upcoming]\n\n\nname\n=\nOSG Software for Enterprise Linux 7 - Upcoming - $basearch\n\n\n#baseurl=http://repo.grid.iu.edu/upcoming/el7/$basearch/release\n\n\nmirrorlist\n=\nhttp://repo.grid.iu.edu/mirror/upcoming/el7/$basearch/release\n\n\nfailovermethod\n=\npriority\n\n\npriority\n=\n98\n\n\nenabled\n=\n0\n\n\ngpgcheck\n=\n1\n\n\ngpgkey\n=\nfile:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\n\n\nconsider_as_osg\n=\nyes\n\n\n\n\n\n\nEdit this file, and under the section \n[osg-upcoming]\n, change \nenabled=0\n to \nenabled=1\n. This will enable the production upcoming software repositories. Future YUM installs will bring in software from the upcoming repositories.\n\n\nTo get help, please use \nthis page\n.", 
            "title": "OSG upcoming software"
        }, 
        {
            "location": "/other/install-osg-upcoming-software/#osg-upcoming-software-repositories-installation-guide", 
            "text": "Here we describe the OSG Upcoming Software Repositories, their purpose, and how to use them.  This document is intended for site administrators.", 
            "title": "OSG Upcoming Software Repositories Installation Guide"
        }, 
        {
            "location": "/other/install-osg-upcoming-software/#overview-and-purpose", 
            "text": "Certain sites have requested new versions of software that would be considered \"disruptive\" or \"experimental\" -- upgrading to them would likely require manual intervention before the site would come back up. We do not want sites to unwittingly upgrade to these versions. For the benefit of the sites that do want to upgrade, we want to provide the same assurance of quality and production-readiness that we provide to the software we currently ship now.  Due to the relatively small number of such packages, a full fork of the OSG 3 distribution was not warranted. Instead, we have created a separate set of repositories that contain only the \"disruptive\" versions of the software.  These repositories have the same structure as our standard repositories. For example, there is an  osg-upcoming-testing  repository and an  osg-upcoming  repository.  A full installation of our software stack is  not  be possible using only the  osg-upcoming  repositories, since they contain a small subset of the software we ship. Both the main  osg  and the  osg-upcoming  repositories will need to be enabled for the installation to work. Because of this, interoperability will be maintained between the main  osg  and  osg-upcoming .  Depending on test results from sites, some packages in  osg-upcoming  may eventually end up in the main  osg  branch. The rest of the packages will eventually form the nucleus of the next fork of the software stack (e.g. \"OSG 3.5\").", 
            "title": "Overview and Purpose"
        }, 
        {
            "location": "/other/install-osg-upcoming-software/#installation-and-usage", 
            "text": "The following directions must be followed to install software from the Upcoming repositories.  First, install the standard OSG YUM repositories as per  the YUM repositories guide .", 
            "title": "Installation and Usage"
        }, 
        {
            "location": "/other/install-osg-upcoming-software/#enabling-upcoming-repositories", 
            "text": "You should have a file called  osg-upcoming.repo  (el7) or  osg-el6-upcoming.repo  (el6) located in  /etc/yum.repos.d . At the top, it should have a section looking like this (el7):  [osg-upcoming]  name = OSG Software for Enterprise Linux 7 - Upcoming - $basearch  #baseurl=http://repo.grid.iu.edu/upcoming/el7/$basearch/release  mirrorlist = http://repo.grid.iu.edu/mirror/upcoming/el7/$basearch/release  failovermethod = priority  priority = 98  enabled = 0  gpgcheck = 1  gpgkey = file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG  consider_as_osg = yes   Edit this file, and under the section  [osg-upcoming] , change  enabled=0  to  enabled=1 . This will enable the production upcoming software repositories. Future YUM installs will bring in software from the upcoming repositories.  To get help, please use  this page .", 
            "title": "Enabling Upcoming repositories"
        }, 
        {
            "location": "/release/yum-basics/", 
            "text": "Basics of using yum and RPM\n\n\nAbout This Document\n\n\nThis document introduces package management tools that help you install, update, and remove packages. OSG uses RPMs (the Red Hat Packaging Manager) to package its software. While RPM is the packaging format, \nyum\n is the command you will use to do the installation. For example, \nyum\n will resolve and download the dependencies for the package you want to install; \nrpm\n will simply complain if you want to install a package that does not have all its dependencies installed.\n\n\nInstallation\n\n\nInstallation is done with the \nyum install\n command. Each of the individual installation guide shows you the correct command to use to do an installation. Here is an example installation with all of the output from yum.\n\n\nroot@host #\n sudo yum install osg-ca-certs\n\nLoaded plugins: kernel-module, priorities\n\n\nepel                                                                                         | 3.7 kB     00:00     \n\n\nepel/primary_db                                                                              | 3.8 MB     00:00     \n\n\nfermi-base                                                                                   | 2.1 kB     00:00     \n\n\nfermi-base/primary_db                                                                        |  48 kB     00:00     \n\n\nfermi-security                                                                               | 1.9 kB     00:00     \n\n\nfermi-security/primary_db                                                                    | 1.7 MB     00:00     \n\n\nosg                                                                                          | 1.9 kB     00:00     \n\n\nosg/primary_db                                                                               |  65 kB     00:00     \n\n\nsl-base                                                                                      | 2.1 kB     00:00     \n\n\nsl-base/primary_db                                                                           | 2.0 MB     00:00     \n\n\n957 packages excluded due to repository priority protections\n\n\nSetting up Install Process\n\n\nResolving Dependencies\n\n\n--\n Running transaction check\n\n\n---\n Package osg-ca-certs.noarch 0:1.23-1 set to be updated\n\n\n--\n Finished Dependency Resolution\n\n\nBeginning Kernel Module Plugin\n\n\nFinished Kernel Module Plugin\n\n\n\nDependencies Resolved\n\n\n\n====================================================================================================================\n\n\n Package                         Arch                      Version                     Repository              Size\n\n\n====================================================================================================================\n\n\nInstalling:\n\n\n osg-ca-certs                    noarch                    1.23-1                      osg                    450 k\n\n\n\nTransaction Summary\n\n\n====================================================================================================================\n\n\nInstall       1 Package(s)\n\n\nUpgrade       0 Package(s)\n\n\n\nTotal download size: 450 k\n\n\nIs this ok [y/N]: y\n\n\nDownloading Packages:\n\n\nosg-ca-certs-1.23-1.noarch.rpm                                                               | 450 kB     00:00     \n\n\nwarning: rpmts_HdrFromFdno: Header V3 DSA signature: NOKEY, key ID 824b8603\n\n\nosg/gpgkey                                                                                   | 1.7 kB     00:00     \n\n\nImporting GPG key 0x824B8603 \nOSG Software Team (RPM Signing Key for Koji Packages) \nvdt-support@opensciencegrid.org\n from /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\n\n\nIs this ok [y/N]: y\n\n\nRunning rpm_check_debug\n\n\nRunning Transaction Test\n\n\nFinished Transaction Test\n\n\nTransaction Test Succeeded\n\n\nRunning Transaction\n\n\n  Installing     : osg-ca-certs                                                                                 1/1 \n\n\n\nInstalled:\n\n\n  osg-ca-certs.noarch 0:1.23-1                                                                                      \n\n\n\nComplete!\n\n\n\n\n\n\nPlease Note\n: When you first install a package from the OSG repository, you will be prompted to import the GPG key. We use this key to sign our RPMs as a security measure. You should double-check the key id (above it is 824B8603) with the \ninformation on our signed RPMs\n. If it doesn't match, there is a problem somewhere and you should report it to the OSG via goc@opensciencegrid.org.\n\n\nVerifying Packages and Installations\n\n\nYou can check if an RPM has been modified. For instance, to check to see if any files have been modified in the \nosg-ca-certs\n RPM you just installed:\n\n\nuser@host $\n rpm --verify osg-ca-certs\n\n\n\n\n\nThe lack of any output means there were no problems. If you would like to see all the files for which there are no problems, you can do:\n\n\nuser@host $\n rpm --verify -v osg-ca-certs\n\n........    /etc/grid-security/certificates\n\n\n........    /etc/grid-security/certificates/0119347c.0\n\n\n........    /etc/grid-security/certificates/0119347c.namespaces\n\n\n........    /etc/grid-security/certificates/0119347c.signing_policy\n\n\n... etc ...\n\n\n\n\n\n\nEach dot indicates a specific check that was made and passed. If someone had modified a file, you might see this:\n\n\nuser@host $\n rpm --verify osg-ca-certs\n\n..5....T    /etc/grid-security/certificates/ffc3d59b.0\n\n\n\n\n\n\nThis means the files MD5 checksum has changed (so the contents have changed) and the timestamp is different. The complete set of changes you might see (copied from the \nrpm\n man page) are:\n\n\n\n\n\n\n\n\nLetter\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nS\n\n\nfile Size differs\n\n\n\n\n\n\nM\n\n\nMode differs (includes permissions and file type)\n\n\n\n\n\n\n5\n\n\nMD5 sum differs\n\n\n\n\n\n\nD\n\n\nDevice major/minor number mismatch\n\n\n\n\n\n\nL\n\n\nreadLink(2) path mismatch\n\n\n\n\n\n\nU\n\n\nUser ownership differs\n\n\n\n\n\n\nG\n\n\nGroup ownership differs\n\n\n\n\n\n\nT\n\n\nmTime differs\n\n\n\n\n\n\n\n\nIf you don't care about some of those changes, you can tell rpm to ignore them. For instance, to ignore changes in the modification time:\n\n\nuser@host $\n rpm --verify --nomtime osg-ca-certs\n\n..5.....    /etc/grid-security/certificates/ffc3d59b.0\n\n\n\n\n\n\nUnderstanding a package\n\n\nIf you want to know what package a file belongs to, you can ask rpm. For instance, if you're curious what package contains the \nsrm-ls\n command, you can do:\n\n\n#\n \n1\n. Find the exact path\n\nuser@host $\n which srm-ls\n\n/usr/bin/srm-ls\n\n\n\n#\n \n2\n. Ask rpm what package it is part of:\n\nuser@host $\n rpm -q --file /usr/bin/srm-ls\n\nbestman2-client-2.2.0-14.osg.el5.noarch\n\n\n\n\n\n\nIf you want to know what other things are in a package--perhaps the other available tools or configuration files--you can do that as well:\n\n\nuser@host $\n rpm -ql bestman2-client\n\n/etc/bestman2/conf/bestman2.rc\n\n\n/etc/bestman2/conf/bestman2.rc.samples\n\n\n/etc/bestman2/conf/srmclient.conf\n\n\n/etc/sysconfig/bestman2\n\n\n/usr/bin/srm-copy\n\n\n/usr/bin/srm-copy-status\n\n\n/usr/bin/srm-extendfilelifetime\n\n\n/usr/bin/srm-ls\n\n\n/usr/bin/srm-ls-status\n\n\n... output trimmed ...\n\n\n\n\n\n\nWhat else does a package install?\n\n\nSometimes you need to understand what other software is installed by a package. This can be particularly useful for understanding \nmeta-packages\n, which are packages such as the \nosg-wn-client\n (worker node client) that contain nothing by themselves but only depend on other RPMs. To do this, use the \n--requires\n option to rpm. For example, you can see that the worker node client (as of OSG 3.1.8 in early September, 2012) will install \ncurl\n, \nuberftp\n, \nlcg-utils\n, and a dozen or so other packages.\n\n\nuser@host $\n rpm -q --requires osg-wn-client\n\n/usr/bin/curl  \n\n\n/usr/bin/dccp  \n\n\n/usr/bin/ldapsearch  \n\n\n/usr/bin/uberftp  \n\n\n/usr/bin/wget  \n\n\nbestman2-client  \n\n\nconfig(osg-wn-client) = 3.0.0-16.osg.el5\n\n\ndcache-srmclient  \n\n\ndcap-tunnel-gsi  \n\n\nedg-gridftp-client  \n\n\nfetch-crl  \n\n\nglite-fts-client  \n\n\nglobus-gass-copy-progs  \n\n\ngrid-certificates  \n\n\njava-1.6.0-sun-compat  \n\n\nlcg-utils  \n\n\nlfc-client  \n\n\nlfc-python  \n\n\nmyproxy  \n\n\nosg-system-profiler  \n\n\nosg-version  \n\n\nrpmlib(CompressedFileNames) \n= 3.0.4-1\n\n\nrpmlib(PayloadFilesHavePrefix) \n= 4.0-1\n\n\nvo-client\n\n\n\n\n\n\nFinding RPM Packages\n\n\nIt is normally best to read the OSG documentation to decide which packages to install because it may not be obvious what the correct packages to install are. That said, you can use yum to find out all sort of things. For instance, you can list packages that begin with \"voms\":\n\n\nuser@host $\n yum list \nvoms*\n\n\nLoaded plugins: kernel-module, priorities\n\n\n957 packages excluded due to repository priority protections\n\n\nAvailable Packages\n\n\nvoms.i386                                                    2.0.6-3.osg                                        osg \n\n\nvoms.x86_64                                                  2.0.6-3.osg                                        osg \n\n\nvoms-admin-client.x86_64                                     2.0.16-1                                           osg \n\n\nvoms-admin-server.noarch                                     2.6.1-9                                            osg \n\n\nvoms-clients.x86_64                                          2.0.6-3.osg                                        osg \n\n\nvoms-compat.i386                                             1.9.19.2-6.osg                                     osg \n\n\nvoms-compat.x86_64                                           1.9.19.2-6.osg                                     osg \n\n\nvoms-devel.i386                                              2.0.6-3.osg                                        osg \n\n\nvoms-devel.x86_64                                            2.0.6-3.osg                                        osg \n\n\nvoms-doc.x86_64                                              2.0.6-3.osg                                        osg \n\n\nvoms-mysql-plugin.x86_64                                     3.1.5.1-1.el5                                      epel\n\n\nvoms-server.x86_64                                           2.0.6-3.osg                                        osg \n\n\nvomsjapi.x86_64                                              2.0.6-3.osg                                        osg \n\n\nvomsjapi-javadoc.x86_64                                      2.0.6-3.osg                                        osg\n\n\n\n\n\n\nIf you want to search for packages that contain VOMS anywhere in the name or description, you can use \nyum search\n:\n\n\nuser@host $\n yum search voms\n\nLoaded plugins: kernel-module, priorities\n\n\n957 packages excluded due to repository priority protections\n\n\n================================================== Matched: voms ===================================================\n\n\nosg-voms.noarch : OSG VOMS\n\n\nperl-VOMS-Lite.noarch : Perl extension for VOMS Attribute certificate creation\n\n\nperl-voms-server.noarch : Perl extension for VOMS Attribute certificate creation\n\n\nphp-voms-admin.noarch : Web based interface to control VOMS parameters written in PHP\n\n\nvoms.i386 : Virtual Organization Membership Service\n\n\nvoms.x86_64 : Virtual Organization Membership Service\n\n\n... etc ...\n\n\n\n\n\n\nOne last example, if you want to know what RPM would give you the \nvoms-proxy-init\n command, you can ask \nyum\n. The \n*\n indicates that you don't know the full pathname of \nvoms-proxy-init\n.\n\n\nuser@host $\n yum whatprovides \n*voms-proxy-init\n\n\nLoaded plugins: kernel-module, priorities\n\n\n957 packages excluded due to repository priority protections\n\n\nvoms-clients-2.0.6-3.osg.x86_64 : Virtual Organization Membership Service Clients\n\n\nRepo        : osg\n\n\nMatched from:\n\n\nFilename    : /usr/bin/voms-proxy-init\n\n\n\n\n\n\nRemoving Packages\n\n\nTo remove a single RPM, you can use \nyum remove\n. Not only will it uninstall the RPM you requested, but it will uninstall anything that depends on it. For example, if I previously installed the \nvoms-clients\n package, I also installed another package it depends on called \nvoms\n. If I remove \nvoms\n, yum will also remove \nvoms-clients\n:\n\n\nuser@host $\n sudo yum remove voms\n\nLoaded plugins: kernel-module, priorities\n\n\nSetting up Remove Process\n\n\nResolving Dependencies\n\n\n--\n Running transaction check\n\n\n---\n Package voms.x86_64 0:2.0.6-3.osg set to be erased\n\n\n--\n Processing Dependency: libvomsapi.so.1()(64bit) for package: voms-clients\n\n\n--\n Processing Dependency: voms = 2.0.6-3.osg for package: voms-clients\n\n\n--\n Running transaction check\n\n\n---\n Package voms-clients.x86_64 0:2.0.6-3.osg set to be erased\n\n\n--\n Finished Dependency Resolution\n\n\nBeginning Kernel Module Plugin\n\n\nFinished Kernel Module Plugin\n\n\n\nDependencies Resolved\n\n\n\n====================================================================================================================\n\n\n Package                      Arch                   Version                        Repository                 Size\n\n\n====================================================================================================================\n\n\nRemoving:\n\n\n voms                         x86_64                 2.0.6-3.osg                    installed                 407 k\n\n\nRemoving for dependencies:\n\n\n voms-clients                 x86_64                 2.0.6-3.osg                    installed                 373 k\n\n\n\nTransaction Summary\n\n\n====================================================================================================================\n\n\nRemove        2 Package(s)\n\n\nReinstall     0 Package(s)\n\n\nDowngrade     0 Package(s)\n\n\n\nIs this ok [y/N]: y\n\n\nDownloading Packages:\n\n\nRunning rpm_check_debug\n\n\nRunning Transaction Test\n\n\nFinished Transaction Test\n\n\nTransaction Test Succeeded\n\n\nRunning Transaction\n\n\n  Erasing        : voms                                                                                         1/2 \n\n\n  Erasing        : voms-clients                                                                                 2/2 \n\n\n\nRemoved:\n\n\n  voms.x86_64 0:2.0.6-3.osg                                                                                         \n\n\n\nDependency Removed:\n\n\n  voms-clients.x86_64 0:2.0.6-3.osg                                                                                 \n\n\n\nComplete!\n\n\n\n\n\n\nUpgrading Packages\n\n\nYou can check for updates with \nyum check-update\n. For example:\n\n\nroot@host #\n yum check-update\n\nLoaded plugins: kernel-module, priorities\n\n\n957 packages excluded due to repository priority protections\n\n\n\nkernel.x86_64                                            2.6.18-274.3.1.el5                           fermi-security\n\n\nObsoleting Packages\n\n\nocsinventory-agent.noarch                                1.1.2.1-1.el5                                epel          \n\n\n    ocsinventory-client.noarch                           0.9.9-10                                     installed     \n\n\n\n\n\n\nYou can do the update with \nyum update\n. Note that in this case we got more than was listed due to dependencies that needed to be resolved:\n\n\nroot@host #\n yum update\n\n957 packages excluded due to repository priority protections\n\n\nSetting up Update Process\n\n\nResolving Dependencies\n\n\n--\n Running transaction check\n\n\n---\n Package kernel.x86_64 0:2.6.18-274.3.1.el5 set to be installed\n\n\n---\n Package ocsinventory-agent.noarch 0:1.1.2.1-1.el5 set to be updated\n\n\n--\n Processing Dependency: perl(Crypt::SSLeay) for package: ocsinventory-agent\n\n\n--\n Processing Dependency: perl(Proc::Daemon) for package: ocsinventory-agent\n\n\n--\n Processing Dependency: monitor-edid for package: ocsinventory-agent\n\n\n--\n Processing Dependency: perl(Net::IP) for package: ocsinventory-agent\n\n\n--\n Processing Dependency: nmap for package: ocsinventory-agent\n\n\n--\n Processing Dependency: perl(Net::SSLeay) for package: ocsinventory-agent\n\n\n--\n Running transaction check\n\n\n---\n Package monitor-edid.x86_64 0:2.5-1.el5.1 set to be updated\n\n\n---\n Package nmap.x86_64 2:4.11-1.1 set to be updated\n\n\n---\n Package perl-Crypt-SSLeay.x86_64 0:0.51-11.el5 set to be updated\n\n\n---\n Package perl-Net-IP.noarch 0:1.25-2.fc6 set to be updated\n\n\n---\n Package perl-Net-SSLeay.x86_64 0:1.30-4.fc6 set to be updated\n\n\n---\n Package perl-Proc-Daemon.noarch 0:0.03-1.el5 set to be updated\n\n\n--\n Finished Dependency Resolution\n\n\nBeginning Kernel Module Plugin\n\n\nFinished Kernel Module Plugin\n\n\n--\n Running transaction check\n\n\n---\n Package kernel.x86_64 0:2.6.18-238.1.1.el5 set to be erased\n\n\n--\n Finished Dependency Resolution\n\n\n\nDependencies Resolved\n\n\n\n====================================================================================================================\n\n\n Package                        Arch               Version                         Repository                  Size\n\n\n====================================================================================================================\n\n\nInstalling:\n\n\n kernel                         x86_64             2.6.18-274.3.1.el5              fermi-security              21 M\n\n\n ocsinventory-agent             noarch             1.1.2.1-1.el5                   epel                       156 k\n\n\n     replacing  ocsinventory-client.noarch 0.9.9-10\n\n\n\nRemoving:\n\n\n kernel                         x86_64             2.6.18-238.1.1.el5              installed                   93 M\n\n\nInstalling for dependencies:\n\n\n monitor-edid                   x86_64             2.5-1.el5.1                     epel                        82 k\n\n\n nmap                           x86_64             2:4.11-1.1                      sl-base                    680 k\n\n\n perl-Crypt-SSLeay              x86_64             0.51-11.el5                     sl-base                     45 k\n\n\n perl-Net-IP                    noarch             1.25-2.fc6                      sl-base                     31 k\n\n\n perl-Net-SSLeay                x86_64             1.30-4.fc6                      sl-base                    192 k\n\n\n perl-Proc-Daemon               noarch             0.03-1.el5                      epel                       9.4 k\n\n\n\nTransaction Summary\n\n\n====================================================================================================================\n\n\nInstall       8 Package(s)\n\n\nUpgrade       0 Package(s)\n\n\nRemove        1 Package(s)\n\n\nReinstall     0 Package(s)\n\n\nDowngrade     0 Package(s)\n\n\n\nTotal download size: 22 M\n\n\nIs this ok [y/N]: y\n\n\nDownloading Packages:\n\n\n(1/8): perl-Proc-Daemon-0.03-1.el5.noarch.rpm                                                | 9.4 kB     00:00     \n\n\n(2/8): perl-Net-IP-1.25-2.fc6.noarch.rpm                                                     |  31 kB     00:00     \n\n\n(3/8): perl-Crypt-SSLeay-0.51-11.el5.x86_64.rpm                                              |  45 kB     00:00     \n\n\n(4/8): monitor-edid-2.5-1.el5.1.x86_64.rpm                                                   |  82 kB     00:00     \n\n\n(5/8): ocsinventory-agent-1.1.2.1-1.el5.noarch.rpm                                           | 156 kB     00:00     \n\n\n(6/8): perl-Net-SSLeay-1.30-4.fc6.x86_64.rpm                                                 | 192 kB     00:00     \n\n\n(7/8): nmap-4.11-1.1.x86_64.rpm                                                              | 680 kB     00:00     \n\n\n(8/8): kernel-2.6.18-274.3.1.el5.x86_64.rpm                                                  |  21 MB     00:00     \n\n\n--------------------------------------------------------------------------------------------------------------------\n\n\nTotal                                                                               3.5 MB/s |  22 MB     00:06     \n\n\nwarning: rpmts_HdrFromFdno: Header V3 DSA signature: NOKEY, key ID 217521f6\n\n\nepel/gpgkey                                                                                  | 1.7 kB     00:00     \n\n\nImporting GPG key 0x217521F6 \nFedora EPEL \nepel@fedoraproject.org\n from /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL\n\n\nIs this ok [y/N]: y\n\n\nRunning rpm_check_debug\n\n\nRunning Transaction Test\n\n\nFinished Transaction Test\n\n\nTransaction Test Succeeded\n\n\nRunning Transaction\n\n\n  Installing     : perl-Net-SSLeay                                                                             1/10 \n\n\n  Installing     : nmap                                                                                        2/10 \n\n\n  Installing     : monitor-edid                                                                                3/10 \n\n\n  Installing     : perl-Crypt-SSLeay                                                                           4/10 \n\n\n  Installing     : perl-Net-IP                                                                                 5/10 \n\n\n  Installing     : perl-Proc-Daemon                                                                            6/10 \n\n\n  Installing     : kernel                                                                                      7/10 \n\n\n  Installing     : ocsinventory-agent                                                                          8/10 \n\n\nule, priorities\n\n\n957 packages excluded due to repository priority protections\n\n\n\nkernel.x86_64                                            2.6.18-274.3.1.el5                           fermi-security\n\n\nObsoleting Packages\n\n\nocsinventory-agent.noarch                                1.1.2.1-1.el5                                epel          \n\n\n    ocsinventory-client.noarch                           0.9.9-10                                     installed     \n\n\n  Erasing        : ocsinventory-client                                                                         9/10 \n\n\nwarning: /etc/ocsinventory-client/ocsinv.conf saved as /etc/ocsinventory-client/ocsinv.conf.rpmsave\n\n\n  Cleanup        : kernel                                                                                     10/10 \n\n\n\nRemoved:\n\n\n  kernel.x86_64 0:2.6.18-238.1.1.el5                                                                                \n\n\n\nInstalled:\n\n\n  kernel.x86_64 0:2.6.18-274.3.1.el5                    ocsinventory-agent.noarch 0:1.1.2.1-1.el5                   \n\n\n\nDependency Installed:\n\n\n  monitor-edid.x86_64 0:2.5-1.el5.1   nmap.x86_64 2:4.11-1.1                perl-Crypt-SSLeay.x86_64 0:0.51-11.el5  \n\n\n  perl-Net-IP.noarch 0:1.25-2.fc6     perl-Net-SSLeay.x86_64 0:1.30-4.fc6   perl-Proc-Daemon.noarch 0:0.03-1.el5    \n\n\n\nReplaced:\n\n\n  ocsinventory-client.noarch 0:0.9.9-10                                                                             \n\n\n\nComplete!\n\n\n\n\n\n\nAdvanced topic: Only geting OSG updates\n\n\nIf you only want to get updates from the OSG repository and \nno other\n repositories, you can tell yum to do that with the following command:\n\n\nroot@host #\n yum --disablerepo\n=\n* --enablerepo\n=\nosg update\n\n\n\n\n\nAdvanced topic: Getting debugging information for installed software\n\n\nIf you run into a problem with our software and have a hankering to debug it directly (or perhaps we need to ask you for some help), you can install so-called \"debuginfo\" packages. These packages will provide debugging symbols and source code so that you can do things like run \ngdb\n or \npstack\n to get information about a program.\n\n\nInstalling the debuginfo package requires three steps.\n\n\n\n\n\n\nEnable the installation of debuginfo packages. This only needs to be done once. Edit the yum repo file, usually \n/etc/yum.repos.d/osg.repo\n to enable the separate debuginfo repository. Near the bottom of the file, you'll see the \nosg-debug\n repo: \n\n\n[osg-debug]\n\n\n\nname\n=\nOSG Software for Enterprise Linux 5 - $basearch - Debug\n\n\nbaseurl\n=\nhttp://repo.grid.iu.edu/osg-release/$basearch/debu\n\n\nfailovermethod\n=\npriority \n\n\npriority\n=\n98 \n\n\nenabled\n=\n1\n\n\ngpgcheck\n=\n1 \n\n\ngpgkey\n=\nfile:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\n\n\n\n\n\n\nMake sure that \"enabled\" is set to 1.\n\n\n\n\n\n\nFigure out which package installed the program you want to debug. One way to figure it out is to ask RPM. For example, if you want to debug grid-proxy-init:\n\n\nuser@host $\n rpm -qf \n`\nwhich grid-proxy-init\n`\n\n\nglobus-proxy-utils-5.0-5.osg.x86_64\n\n\n\n\n\n\n\n\n\n\nInstall the debugging information for that package. Continuing this example: \n\n\nroot@host #\n debuginfo-install globus-proxy-utils\n\n...\n\n\n=================================================================================================================================\n\n\n Package                                      Arch                   Version                     Repository                 Size\n\n\n=================================================================================================================================\n\n\nInstalling:\n\n\n globus-proxy-utils-debuginfo                 x86_64                 5.0-5.osg                   osg-debug                  61 k\n\n\n\nTransaction Summary\n\n\n=================================================================================================================================\n\n\nInstall       1 Package(s)\n\n\nUpgrade       0 Package(s)\n\n\n\nTotal download size: 61 k\n\n\nIs this ok [y/N]: y\n\n\n...\n\n\nInstalled:\n\n\n  globus-proxy-utils-debuginfo.x86_64 0:5.0-5.osg\n\n\n\n\n\n\nThis last step will select the right package name, then use \nyum\n to install it.\n\n\n\n\n\n\nTroubleshooting\n\n\nYum not finding packages\n\n\nIf you is not finding some packages, e.g.:\n\n\nError Downloading Packages:\n  packageXYZ: failure: packageXYZ.rpm from osg: [Errno 256] No more mirrors to try.\n\n\n\n\n\nthen you can try cleaning up Yum's cache: \n\n\nroot@host #\n yum clean all --enablerpeo\n=\n*\n\n\n\n\n\nto make an even more thorough job you can follow also add:\n\n\nroot@host #\n yum clean expire-cache --enablerepo\n=\n*\n\n\n\n\n\n\n\nNote\n\n\nyum clean\n cleans only enabled repositories. If you want to also clean any (temporarily) disabled repositories you need to use \n--enablerepo=\u2019*\u2019\n option.\n\n\n\n\nYum complaining about missing keys\n\n\nIf yum is complaining you can re-import the keys in your distribution: \n\n\nroot@host #\n rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY*\n\n\n\n\n\nReferences\n\n\n\n\nThe main yum web site\n\n\nA good description of the commands for RPM and yum can be found at \nLearn Linux 101: RPM and YUM Package Management\n.", 
            "title": "Yum Basics"
        }, 
        {
            "location": "/release/yum-basics/#basics-of-using-yum-and-rpm", 
            "text": "", 
            "title": "Basics of using yum and RPM"
        }, 
        {
            "location": "/release/yum-basics/#about-this-document", 
            "text": "This document introduces package management tools that help you install, update, and remove packages. OSG uses RPMs (the Red Hat Packaging Manager) to package its software. While RPM is the packaging format,  yum  is the command you will use to do the installation. For example,  yum  will resolve and download the dependencies for the package you want to install;  rpm  will simply complain if you want to install a package that does not have all its dependencies installed.", 
            "title": "About This Document"
        }, 
        {
            "location": "/release/yum-basics/#installation", 
            "text": "Installation is done with the  yum install  command. Each of the individual installation guide shows you the correct command to use to do an installation. Here is an example installation with all of the output from yum.  root@host #  sudo yum install osg-ca-certs Loaded plugins: kernel-module, priorities  epel                                                                                         | 3.7 kB     00:00       epel/primary_db                                                                              | 3.8 MB     00:00       fermi-base                                                                                   | 2.1 kB     00:00       fermi-base/primary_db                                                                        |  48 kB     00:00       fermi-security                                                                               | 1.9 kB     00:00       fermi-security/primary_db                                                                    | 1.7 MB     00:00       osg                                                                                          | 1.9 kB     00:00       osg/primary_db                                                                               |  65 kB     00:00       sl-base                                                                                      | 2.1 kB     00:00       sl-base/primary_db                                                                           | 2.0 MB     00:00       957 packages excluded due to repository priority protections  Setting up Install Process  Resolving Dependencies  --  Running transaction check  ---  Package osg-ca-certs.noarch 0:1.23-1 set to be updated  --  Finished Dependency Resolution  Beginning Kernel Module Plugin  Finished Kernel Module Plugin  Dependencies Resolved  ====================================================================================================================   Package                         Arch                      Version                     Repository              Size  ====================================================================================================================  Installing:   osg-ca-certs                    noarch                    1.23-1                      osg                    450 k  Transaction Summary  ====================================================================================================================  Install       1 Package(s)  Upgrade       0 Package(s)  Total download size: 450 k  Is this ok [y/N]: y  Downloading Packages:  osg-ca-certs-1.23-1.noarch.rpm                                                               | 450 kB     00:00       warning: rpmts_HdrFromFdno: Header V3 DSA signature: NOKEY, key ID 824b8603  osg/gpgkey                                                                                   | 1.7 kB     00:00       Importing GPG key 0x824B8603  OSG Software Team (RPM Signing Key for Koji Packages)  vdt-support@opensciencegrid.org  from /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG  Is this ok [y/N]: y  Running rpm_check_debug  Running Transaction Test  Finished Transaction Test  Transaction Test Succeeded  Running Transaction    Installing     : osg-ca-certs                                                                                 1/1   Installed:    osg-ca-certs.noarch 0:1.23-1                                                                                        Complete!   Please Note : When you first install a package from the OSG repository, you will be prompted to import the GPG key. We use this key to sign our RPMs as a security measure. You should double-check the key id (above it is 824B8603) with the  information on our signed RPMs . If it doesn't match, there is a problem somewhere and you should report it to the OSG via goc@opensciencegrid.org.", 
            "title": "Installation"
        }, 
        {
            "location": "/release/yum-basics/#verifying-packages-and-installations", 
            "text": "You can check if an RPM has been modified. For instance, to check to see if any files have been modified in the  osg-ca-certs  RPM you just installed:  user@host $  rpm --verify osg-ca-certs  The lack of any output means there were no problems. If you would like to see all the files for which there are no problems, you can do:  user@host $  rpm --verify -v osg-ca-certs ........    /etc/grid-security/certificates  ........    /etc/grid-security/certificates/0119347c.0  ........    /etc/grid-security/certificates/0119347c.namespaces  ........    /etc/grid-security/certificates/0119347c.signing_policy  ... etc ...   Each dot indicates a specific check that was made and passed. If someone had modified a file, you might see this:  user@host $  rpm --verify osg-ca-certs ..5....T    /etc/grid-security/certificates/ffc3d59b.0   This means the files MD5 checksum has changed (so the contents have changed) and the timestamp is different. The complete set of changes you might see (copied from the  rpm  man page) are:     Letter  Meaning      S  file Size differs    M  Mode differs (includes permissions and file type)    5  MD5 sum differs    D  Device major/minor number mismatch    L  readLink(2) path mismatch    U  User ownership differs    G  Group ownership differs    T  mTime differs     If you don't care about some of those changes, you can tell rpm to ignore them. For instance, to ignore changes in the modification time:  user@host $  rpm --verify --nomtime osg-ca-certs ..5.....    /etc/grid-security/certificates/ffc3d59b.0", 
            "title": "Verifying Packages and Installations"
        }, 
        {
            "location": "/release/yum-basics/#understanding-a-package", 
            "text": "If you want to know what package a file belongs to, you can ask rpm. For instance, if you're curious what package contains the  srm-ls  command, you can do:  #   1 . Find the exact path user@host $  which srm-ls /usr/bin/srm-ls  #   2 . Ask rpm what package it is part of: user@host $  rpm -q --file /usr/bin/srm-ls bestman2-client-2.2.0-14.osg.el5.noarch   If you want to know what other things are in a package--perhaps the other available tools or configuration files--you can do that as well:  user@host $  rpm -ql bestman2-client /etc/bestman2/conf/bestman2.rc  /etc/bestman2/conf/bestman2.rc.samples  /etc/bestman2/conf/srmclient.conf  /etc/sysconfig/bestman2  /usr/bin/srm-copy  /usr/bin/srm-copy-status  /usr/bin/srm-extendfilelifetime  /usr/bin/srm-ls  /usr/bin/srm-ls-status  ... output trimmed ...", 
            "title": "Understanding a package"
        }, 
        {
            "location": "/release/yum-basics/#what-else-does-a-package-install", 
            "text": "Sometimes you need to understand what other software is installed by a package. This can be particularly useful for understanding  meta-packages , which are packages such as the  osg-wn-client  (worker node client) that contain nothing by themselves but only depend on other RPMs. To do this, use the  --requires  option to rpm. For example, you can see that the worker node client (as of OSG 3.1.8 in early September, 2012) will install  curl ,  uberftp ,  lcg-utils , and a dozen or so other packages.  user@host $  rpm -q --requires osg-wn-client /usr/bin/curl    /usr/bin/dccp    /usr/bin/ldapsearch    /usr/bin/uberftp    /usr/bin/wget    bestman2-client    config(osg-wn-client) = 3.0.0-16.osg.el5  dcache-srmclient    dcap-tunnel-gsi    edg-gridftp-client    fetch-crl    glite-fts-client    globus-gass-copy-progs    grid-certificates    java-1.6.0-sun-compat    lcg-utils    lfc-client    lfc-python    myproxy    osg-system-profiler    osg-version    rpmlib(CompressedFileNames)  = 3.0.4-1  rpmlib(PayloadFilesHavePrefix)  = 4.0-1  vo-client", 
            "title": "What else does a package install?"
        }, 
        {
            "location": "/release/yum-basics/#finding-rpm-packages", 
            "text": "It is normally best to read the OSG documentation to decide which packages to install because it may not be obvious what the correct packages to install are. That said, you can use yum to find out all sort of things. For instance, you can list packages that begin with \"voms\":  user@host $  yum list  voms*  Loaded plugins: kernel-module, priorities  957 packages excluded due to repository priority protections  Available Packages  voms.i386                                                    2.0.6-3.osg                                        osg   voms.x86_64                                                  2.0.6-3.osg                                        osg   voms-admin-client.x86_64                                     2.0.16-1                                           osg   voms-admin-server.noarch                                     2.6.1-9                                            osg   voms-clients.x86_64                                          2.0.6-3.osg                                        osg   voms-compat.i386                                             1.9.19.2-6.osg                                     osg   voms-compat.x86_64                                           1.9.19.2-6.osg                                     osg   voms-devel.i386                                              2.0.6-3.osg                                        osg   voms-devel.x86_64                                            2.0.6-3.osg                                        osg   voms-doc.x86_64                                              2.0.6-3.osg                                        osg   voms-mysql-plugin.x86_64                                     3.1.5.1-1.el5                                      epel  voms-server.x86_64                                           2.0.6-3.osg                                        osg   vomsjapi.x86_64                                              2.0.6-3.osg                                        osg   vomsjapi-javadoc.x86_64                                      2.0.6-3.osg                                        osg   If you want to search for packages that contain VOMS anywhere in the name or description, you can use  yum search :  user@host $  yum search voms Loaded plugins: kernel-module, priorities  957 packages excluded due to repository priority protections  ================================================== Matched: voms ===================================================  osg-voms.noarch : OSG VOMS  perl-VOMS-Lite.noarch : Perl extension for VOMS Attribute certificate creation  perl-voms-server.noarch : Perl extension for VOMS Attribute certificate creation  php-voms-admin.noarch : Web based interface to control VOMS parameters written in PHP  voms.i386 : Virtual Organization Membership Service  voms.x86_64 : Virtual Organization Membership Service  ... etc ...   One last example, if you want to know what RPM would give you the  voms-proxy-init  command, you can ask  yum . The  *  indicates that you don't know the full pathname of  voms-proxy-init .  user@host $  yum whatprovides  *voms-proxy-init  Loaded plugins: kernel-module, priorities  957 packages excluded due to repository priority protections  voms-clients-2.0.6-3.osg.x86_64 : Virtual Organization Membership Service Clients  Repo        : osg  Matched from:  Filename    : /usr/bin/voms-proxy-init", 
            "title": "Finding RPM Packages"
        }, 
        {
            "location": "/release/yum-basics/#removing-packages", 
            "text": "To remove a single RPM, you can use  yum remove . Not only will it uninstall the RPM you requested, but it will uninstall anything that depends on it. For example, if I previously installed the  voms-clients  package, I also installed another package it depends on called  voms . If I remove  voms , yum will also remove  voms-clients :  user@host $  sudo yum remove voms Loaded plugins: kernel-module, priorities  Setting up Remove Process  Resolving Dependencies  --  Running transaction check  ---  Package voms.x86_64 0:2.0.6-3.osg set to be erased  --  Processing Dependency: libvomsapi.so.1()(64bit) for package: voms-clients  --  Processing Dependency: voms = 2.0.6-3.osg for package: voms-clients  --  Running transaction check  ---  Package voms-clients.x86_64 0:2.0.6-3.osg set to be erased  --  Finished Dependency Resolution  Beginning Kernel Module Plugin  Finished Kernel Module Plugin  Dependencies Resolved  ====================================================================================================================   Package                      Arch                   Version                        Repository                 Size  ====================================================================================================================  Removing:   voms                         x86_64                 2.0.6-3.osg                    installed                 407 k  Removing for dependencies:   voms-clients                 x86_64                 2.0.6-3.osg                    installed                 373 k  Transaction Summary  ====================================================================================================================  Remove        2 Package(s)  Reinstall     0 Package(s)  Downgrade     0 Package(s)  Is this ok [y/N]: y  Downloading Packages:  Running rpm_check_debug  Running Transaction Test  Finished Transaction Test  Transaction Test Succeeded  Running Transaction    Erasing        : voms                                                                                         1/2     Erasing        : voms-clients                                                                                 2/2   Removed:    voms.x86_64 0:2.0.6-3.osg                                                                                           Dependency Removed:    voms-clients.x86_64 0:2.0.6-3.osg                                                                                   Complete!", 
            "title": "Removing Packages"
        }, 
        {
            "location": "/release/yum-basics/#upgrading-packages", 
            "text": "You can check for updates with  yum check-update . For example:  root@host #  yum check-update Loaded plugins: kernel-module, priorities  957 packages excluded due to repository priority protections  kernel.x86_64                                            2.6.18-274.3.1.el5                           fermi-security  Obsoleting Packages  ocsinventory-agent.noarch                                1.1.2.1-1.el5                                epel                ocsinventory-client.noarch                           0.9.9-10                                     installed        You can do the update with  yum update . Note that in this case we got more than was listed due to dependencies that needed to be resolved:  root@host #  yum update 957 packages excluded due to repository priority protections  Setting up Update Process  Resolving Dependencies  --  Running transaction check  ---  Package kernel.x86_64 0:2.6.18-274.3.1.el5 set to be installed  ---  Package ocsinventory-agent.noarch 0:1.1.2.1-1.el5 set to be updated  --  Processing Dependency: perl(Crypt::SSLeay) for package: ocsinventory-agent  --  Processing Dependency: perl(Proc::Daemon) for package: ocsinventory-agent  --  Processing Dependency: monitor-edid for package: ocsinventory-agent  --  Processing Dependency: perl(Net::IP) for package: ocsinventory-agent  --  Processing Dependency: nmap for package: ocsinventory-agent  --  Processing Dependency: perl(Net::SSLeay) for package: ocsinventory-agent  --  Running transaction check  ---  Package monitor-edid.x86_64 0:2.5-1.el5.1 set to be updated  ---  Package nmap.x86_64 2:4.11-1.1 set to be updated  ---  Package perl-Crypt-SSLeay.x86_64 0:0.51-11.el5 set to be updated  ---  Package perl-Net-IP.noarch 0:1.25-2.fc6 set to be updated  ---  Package perl-Net-SSLeay.x86_64 0:1.30-4.fc6 set to be updated  ---  Package perl-Proc-Daemon.noarch 0:0.03-1.el5 set to be updated  --  Finished Dependency Resolution  Beginning Kernel Module Plugin  Finished Kernel Module Plugin  --  Running transaction check  ---  Package kernel.x86_64 0:2.6.18-238.1.1.el5 set to be erased  --  Finished Dependency Resolution  Dependencies Resolved  ====================================================================================================================   Package                        Arch               Version                         Repository                  Size  ====================================================================================================================  Installing:   kernel                         x86_64             2.6.18-274.3.1.el5              fermi-security              21 M   ocsinventory-agent             noarch             1.1.2.1-1.el5                   epel                       156 k       replacing  ocsinventory-client.noarch 0.9.9-10  Removing:   kernel                         x86_64             2.6.18-238.1.1.el5              installed                   93 M  Installing for dependencies:   monitor-edid                   x86_64             2.5-1.el5.1                     epel                        82 k   nmap                           x86_64             2:4.11-1.1                      sl-base                    680 k   perl-Crypt-SSLeay              x86_64             0.51-11.el5                     sl-base                     45 k   perl-Net-IP                    noarch             1.25-2.fc6                      sl-base                     31 k   perl-Net-SSLeay                x86_64             1.30-4.fc6                      sl-base                    192 k   perl-Proc-Daemon               noarch             0.03-1.el5                      epel                       9.4 k  Transaction Summary  ====================================================================================================================  Install       8 Package(s)  Upgrade       0 Package(s)  Remove        1 Package(s)  Reinstall     0 Package(s)  Downgrade     0 Package(s)  Total download size: 22 M  Is this ok [y/N]: y  Downloading Packages:  (1/8): perl-Proc-Daemon-0.03-1.el5.noarch.rpm                                                | 9.4 kB     00:00       (2/8): perl-Net-IP-1.25-2.fc6.noarch.rpm                                                     |  31 kB     00:00       (3/8): perl-Crypt-SSLeay-0.51-11.el5.x86_64.rpm                                              |  45 kB     00:00       (4/8): monitor-edid-2.5-1.el5.1.x86_64.rpm                                                   |  82 kB     00:00       (5/8): ocsinventory-agent-1.1.2.1-1.el5.noarch.rpm                                           | 156 kB     00:00       (6/8): perl-Net-SSLeay-1.30-4.fc6.x86_64.rpm                                                 | 192 kB     00:00       (7/8): nmap-4.11-1.1.x86_64.rpm                                                              | 680 kB     00:00       (8/8): kernel-2.6.18-274.3.1.el5.x86_64.rpm                                                  |  21 MB     00:00       --------------------------------------------------------------------------------------------------------------------  Total                                                                               3.5 MB/s |  22 MB     00:06       warning: rpmts_HdrFromFdno: Header V3 DSA signature: NOKEY, key ID 217521f6  epel/gpgkey                                                                                  | 1.7 kB     00:00       Importing GPG key 0x217521F6  Fedora EPEL  epel@fedoraproject.org  from /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL  Is this ok [y/N]: y  Running rpm_check_debug  Running Transaction Test  Finished Transaction Test  Transaction Test Succeeded  Running Transaction    Installing     : perl-Net-SSLeay                                                                             1/10     Installing     : nmap                                                                                        2/10     Installing     : monitor-edid                                                                                3/10     Installing     : perl-Crypt-SSLeay                                                                           4/10     Installing     : perl-Net-IP                                                                                 5/10     Installing     : perl-Proc-Daemon                                                                            6/10     Installing     : kernel                                                                                      7/10     Installing     : ocsinventory-agent                                                                          8/10   ule, priorities  957 packages excluded due to repository priority protections  kernel.x86_64                                            2.6.18-274.3.1.el5                           fermi-security  Obsoleting Packages  ocsinventory-agent.noarch                                1.1.2.1-1.el5                                epel                ocsinventory-client.noarch                           0.9.9-10                                     installed         Erasing        : ocsinventory-client                                                                         9/10   warning: /etc/ocsinventory-client/ocsinv.conf saved as /etc/ocsinventory-client/ocsinv.conf.rpmsave    Cleanup        : kernel                                                                                     10/10   Removed:    kernel.x86_64 0:2.6.18-238.1.1.el5                                                                                  Installed:    kernel.x86_64 0:2.6.18-274.3.1.el5                    ocsinventory-agent.noarch 0:1.1.2.1-1.el5                     Dependency Installed:    monitor-edid.x86_64 0:2.5-1.el5.1   nmap.x86_64 2:4.11-1.1                perl-Crypt-SSLeay.x86_64 0:0.51-11.el5      perl-Net-IP.noarch 0:1.25-2.fc6     perl-Net-SSLeay.x86_64 0:1.30-4.fc6   perl-Proc-Daemon.noarch 0:0.03-1.el5      Replaced:    ocsinventory-client.noarch 0:0.9.9-10                                                                               Complete!", 
            "title": "Upgrading Packages"
        }, 
        {
            "location": "/release/yum-basics/#advanced-topic-only-geting-osg-updates", 
            "text": "If you only want to get updates from the OSG repository and  no other  repositories, you can tell yum to do that with the following command:  root@host #  yum --disablerepo = * --enablerepo = osg update", 
            "title": "Advanced topic: Only geting OSG updates"
        }, 
        {
            "location": "/release/yum-basics/#advanced-topic-getting-debugging-information-for-installed-software", 
            "text": "If you run into a problem with our software and have a hankering to debug it directly (or perhaps we need to ask you for some help), you can install so-called \"debuginfo\" packages. These packages will provide debugging symbols and source code so that you can do things like run  gdb  or  pstack  to get information about a program.  Installing the debuginfo package requires three steps.    Enable the installation of debuginfo packages. This only needs to be done once. Edit the yum repo file, usually  /etc/yum.repos.d/osg.repo  to enable the separate debuginfo repository. Near the bottom of the file, you'll see the  osg-debug  repo:   [osg-debug]  name = OSG Software for Enterprise Linux 5 - $basearch - Debug  baseurl = http://repo.grid.iu.edu/osg-release/$basearch/debu  failovermethod = priority   priority = 98   enabled = 1  gpgcheck = 1   gpgkey = file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG   Make sure that \"enabled\" is set to 1.    Figure out which package installed the program you want to debug. One way to figure it out is to ask RPM. For example, if you want to debug grid-proxy-init:  user@host $  rpm -qf  ` which grid-proxy-init `  globus-proxy-utils-5.0-5.osg.x86_64     Install the debugging information for that package. Continuing this example:   root@host #  debuginfo-install globus-proxy-utils ...  =================================================================================================================================   Package                                      Arch                   Version                     Repository                 Size  =================================================================================================================================  Installing:   globus-proxy-utils-debuginfo                 x86_64                 5.0-5.osg                   osg-debug                  61 k  Transaction Summary  =================================================================================================================================  Install       1 Package(s)  Upgrade       0 Package(s)  Total download size: 61 k  Is this ok [y/N]: y  ...  Installed:    globus-proxy-utils-debuginfo.x86_64 0:5.0-5.osg   This last step will select the right package name, then use  yum  to install it.", 
            "title": "Advanced topic: Getting debugging information for installed software"
        }, 
        {
            "location": "/release/yum-basics/#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/release/yum-basics/#yum-not-finding-packages", 
            "text": "If you is not finding some packages, e.g.:  Error Downloading Packages:\n  packageXYZ: failure: packageXYZ.rpm from osg: [Errno 256] No more mirrors to try.  then you can try cleaning up Yum's cache:   root@host #  yum clean all --enablerpeo = *  to make an even more thorough job you can follow also add:  root@host #  yum clean expire-cache --enablerepo = *   Note  yum clean  cleans only enabled repositories. If you want to also clean any (temporarily) disabled repositories you need to use  --enablerepo=\u2019*\u2019  option.", 
            "title": "Yum not finding packages"
        }, 
        {
            "location": "/release/yum-basics/#yum-complaining-about-missing-keys", 
            "text": "If yum is complaining you can re-import the keys in your distribution:   root@host #  rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY*", 
            "title": "Yum complaining about missing keys"
        }, 
        {
            "location": "/release/yum-basics/#references", 
            "text": "The main yum web site  A good description of the commands for RPM and yum can be found at  Learn Linux 101: RPM and YUM Package Management .", 
            "title": "References"
        }, 
        {
            "location": "/release/signing/", 
            "text": "OSG Release Signing Information\n\n\nVerifying OSG's RPMs\n\n\nWe use a GPG key to sign our software packages. Normally \nyum\n and \nrpm\n transparently use the GPG signatures to verify the packages have not been corrupted and were created by us. You get our GPG public key when you install the \nosg-release\n RPM.\n\n\nIf you wish to verify one of our RPMs manually, you can run:\n\n\n$\n rpm --checksig -v \nNAME.RPM\n\n\n\n\n\n\nFor example:\n\n\n$\n rpm --checksig -v globus-core-8.0-2.osg.x86_64.rpm \n\nglobus-core-8.0-2.osg.x86_64.rpm:\n\n\n    Header V3 DSA signature: OK, key ID 824b8603\n\n\n    Header SHA1 digest: OK (2b5af4348c548c27f10e2e47e1ec80500c4f85d7)\n\n\n    MD5 digest: OK (d11503a229a1a0e02262034efe0f7e46)\n\n\n    V3 DSA signature: OK, key ID 824b8603\n\n\n\n\n\n\nThe OSG Packaging Signing Key\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLocation\n\n\n/etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\n\n\n\n\n\n\nDownload\n\n\nGitHub\n\n\n\n\n\n\nDownload\n\n\nUW-Madison\n\n\n\n\n\n\nFingerprint\n\n\n6459 !D9D2 AAA9 AB67 A251  FB44 2110 !B1C8 824B 8603\n\n\n\n\n\n\nKey ID\n\n\n824b8603\n\n\n\n\n\n\n\n\nYou can see the fingerprint for yourself:\n\n\n$\n gpg --with-fingerprint /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\n\npub  1024D/824B8603 2011-09-15 OSG Software Team (RPM Signing Key for Koji Packages) \nvdt-support@opensciencegrid.org\n\n\n      Key fingerprint = 6459 D9D2 AAA9 AB67 A251  FB44 2110 B1C8 824B 8603\n\n\nsub  2048g/28E5857C 2011-09-15", 
            "title": "RPM Signing"
        }, 
        {
            "location": "/release/signing/#osg-release-signing-information", 
            "text": "", 
            "title": "OSG Release Signing Information"
        }, 
        {
            "location": "/release/signing/#verifying-osgs-rpms", 
            "text": "We use a GPG key to sign our software packages. Normally  yum  and  rpm  transparently use the GPG signatures to verify the packages have not been corrupted and were created by us. You get our GPG public key when you install the  osg-release  RPM.  If you wish to verify one of our RPMs manually, you can run:  $  rpm --checksig -v  NAME.RPM   For example:  $  rpm --checksig -v globus-core-8.0-2.osg.x86_64.rpm  globus-core-8.0-2.osg.x86_64.rpm:      Header V3 DSA signature: OK, key ID 824b8603      Header SHA1 digest: OK (2b5af4348c548c27f10e2e47e1ec80500c4f85d7)      MD5 digest: OK (d11503a229a1a0e02262034efe0f7e46)      V3 DSA signature: OK, key ID 824b8603", 
            "title": "Verifying OSG's RPMs"
        }, 
        {
            "location": "/release/signing/#the-osg-packaging-signing-key", 
            "text": "Location  /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG    Download  GitHub    Download  UW-Madison    Fingerprint  6459 !D9D2 AAA9 AB67 A251  FB44 2110 !B1C8 824B 8603    Key ID  824b8603     You can see the fingerprint for yourself:  $  gpg --with-fingerprint /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG pub  1024D/824B8603 2011-09-15 OSG Software Team (RPM Signing Key for Koji Packages)  vdt-support@opensciencegrid.org        Key fingerprint = 6459 D9D2 AAA9 AB67 A251  FB44 2110 B1C8 824B 8603  sub  2048g/28E5857C 2011-09-15", 
            "title": "The OSG Packaging Signing Key"
        }, 
        {
            "location": "/release/contrib/", 
            "text": "Contrib software\n\n\nWhat is the contrib software repository?\n\n\nIn addition to our regular software repository, we also have a \ncontrib\n (short for \"contributed\") software repository. This is software that is does not go through the same software testing and release processes as the official OSG Software release, but may be useful to you.\n\n\nCaveats\n\n\nYou should be aware:\n\n\n\n\nThe software in the contrib repository may have been more lightly tested than software in our official release. In particular, we do not test all of the software in the contrib repository together, and there is no guarantee all the software in compatible or even that it works.\n\n\nIf you have problems with the software in the contrib repository, you can ask us for help, but you may be on your own. This software is not officially supported and we may not have time to help you with it.\n\n\n\n\nDo you want more software in the repository?\n\n\nWe're happy to work with you to add more software to the contrib repository. If you provide the RPM packaging (preferably a spec file, so we can build it), it will get into the repository than if we have to do the packaging ourselves.\n\n\nWhat software is in the contrib repository?\n\n\nThe definitive list of software in the contrib repository is the repository itself. You can browse:\n\n\n\n\nThe OSG 3.4 EL6 contrib software repository\n\n\nThe OSG 3.4 EL7 contrib software repository\n\n\n\n\nThe software includes:\n\n\n\n\ngsh:\n Gsh is the \"grid shell\". It acts like a simple shell, something like bash, but with fewer features. All of the commands are run with globus-job-run against a remote Compute Element. This can be a very convenient way to debug a Compute Element.\n\n\nglideinwms\n: The \nosg-contrib\n repository contains the development series for the GlideinWMS workload management system. This system submits condor-based pilots to grid sites to create a dynamic worker node pool for user jobs. Contained in the RPMs are packages for frontend and factory installations. This repository contains the development series that has newer features still in wider testing. At the time of writing, this series is the version 3 with new features for submission to clouds and to TeraGrid sites using corral frontends. For more stable installations, use the \nosg-release\n or \nosg-testing\n repositories.\n\n\nxrootd-cmstfc\n: This package is for a plugin for xrootd storage system to provide a static rule-based logical to physical mapping. This is primarily used by the CMS experiment.\n\n\nxrootd-status-probe\n: This package is a simple nagios probe for xrootd that verifies the first kilobyte of a multi-GB file.\n\n\ncms-xrootd\n: Meta-packages containing sample configuration files for joining the CMS xrootd infrastructure.\n\n\n\n\nInstalling from the contrib repository\n\n\nTo install software, you need to enable the osg-cotrib repository. The easiest way to do that is on case-by-case basis in the \nyum\n command-line. For example, to install \ngsh\n:\n\n\nroot@host #\n yum install --enablerepo\n=\nosg-contrib gsh", 
            "title": "Contrib Software"
        }, 
        {
            "location": "/release/contrib/#contrib-software", 
            "text": "", 
            "title": "Contrib software"
        }, 
        {
            "location": "/release/contrib/#what-is-the-contrib-software-repository", 
            "text": "In addition to our regular software repository, we also have a  contrib  (short for \"contributed\") software repository. This is software that is does not go through the same software testing and release processes as the official OSG Software release, but may be useful to you.", 
            "title": "What is the contrib software repository?"
        }, 
        {
            "location": "/release/contrib/#caveats", 
            "text": "You should be aware:   The software in the contrib repository may have been more lightly tested than software in our official release. In particular, we do not test all of the software in the contrib repository together, and there is no guarantee all the software in compatible or even that it works.  If you have problems with the software in the contrib repository, you can ask us for help, but you may be on your own. This software is not officially supported and we may not have time to help you with it.", 
            "title": "Caveats"
        }, 
        {
            "location": "/release/contrib/#do-you-want-more-software-in-the-repository", 
            "text": "We're happy to work with you to add more software to the contrib repository. If you provide the RPM packaging (preferably a spec file, so we can build it), it will get into the repository than if we have to do the packaging ourselves.", 
            "title": "Do you want more software in the repository?"
        }, 
        {
            "location": "/release/contrib/#what-software-is-in-the-contrib-repository", 
            "text": "The definitive list of software in the contrib repository is the repository itself. You can browse:   The OSG 3.4 EL6 contrib software repository  The OSG 3.4 EL7 contrib software repository   The software includes:   gsh:  Gsh is the \"grid shell\". It acts like a simple shell, something like bash, but with fewer features. All of the commands are run with globus-job-run against a remote Compute Element. This can be a very convenient way to debug a Compute Element.  glideinwms : The  osg-contrib  repository contains the development series for the GlideinWMS workload management system. This system submits condor-based pilots to grid sites to create a dynamic worker node pool for user jobs. Contained in the RPMs are packages for frontend and factory installations. This repository contains the development series that has newer features still in wider testing. At the time of writing, this series is the version 3 with new features for submission to clouds and to TeraGrid sites using corral frontends. For more stable installations, use the  osg-release  or  osg-testing  repositories.  xrootd-cmstfc : This package is for a plugin for xrootd storage system to provide a static rule-based logical to physical mapping. This is primarily used by the CMS experiment.  xrootd-status-probe : This package is a simple nagios probe for xrootd that verifies the first kilobyte of a multi-GB file.  cms-xrootd : Meta-packages containing sample configuration files for joining the CMS xrootd infrastructure.", 
            "title": "What software is in the contrib repository?"
        }, 
        {
            "location": "/release/contrib/#installing-from-the-contrib-repository", 
            "text": "To install software, you need to enable the osg-cotrib repository. The easiest way to do that is on case-by-case basis in the  yum  command-line. For example, to install  gsh :  root@host #  yum install --enablerepo = osg-contrib gsh", 
            "title": "Installing from the contrib repository"
        }, 
        {
            "location": "/monitoring/install-rsv/", 
            "text": "Installing, Configuring, Using, and Troubleshooting RSV\n\n\nAbout This Guide\n\n\nThe Resource and Service Validation (RSV) software helps a site administrator verify that certain site resources and services are working as expected. OSG recommends that sites install and run RSV, but it is optional; further, each site selects which specific tests (called \nprobes\n) to run.\n\n\nUse this page to learn more about RSV in general, and how to install, configure, run, test, and troubleshoot RSV from the OSG software repositories. For documentation on specific probes or on how to write your own probes, please check the \nReference section\n.\n\n\nIntroduction to RSV\n\n\nThe Resource and Service Validation (RSV) software provides OSG site administrators a scalable and easy-to-maintain resource and service monitoring infrastructure. The components of RSV are:\n\n\n\n\nRSV Client.\n The client tools allow a site administrator to run tests against their site by providing a set of tests (which can run on the same or other hosts within a site), HTCondor-Cron for scheduling, and tools for collecting and storing the results (using Gratia). The client package is not installed by default and may be installed on a CE or other host. Generally, you configure the RSV client to run tests at scheduled time intervals and then it makes results available on a local website. Also, the client can upload test results to a central collector (see next item).\n\n\nRSV Collector/Server.\n The central OSG RSV Collector accepts and stores results from RSV clients throughout OSG, which can be viewed in \nMyOSG\n, on the \u201cCurrent RSV Status\u201d page and under the \u201cResource Group\u201d menu.\n\n\nPeriodic Availability Reports.\n The availability of all active registered OSG resources and the services running on each of those resources is calculated using the results received for \ncritical metrics\n. Once a day, these availability numbers are \npublished online\n (More information: \nOutline of reports\n).\n\n\nRSV-SAM Transport.\n The WLCG RSV-SAM Transport infrastructure pushes out RSV results, for resources that are flagged to be part of the WLCG Interoperability agreement, from the GOC collector to WLCG's Service Availability Monitoring (SAM) system. More information on viewing these results is \navailable here\n.\n\n\nMyOSG and OIM Links.\n RSV picks up resource information, WLCG interoperability information, etc., from a MyOSG resource group summary listing, which is in turn based on the \nOSG Information Management (OIM) (topology) system\n (Requires registration). Resource \nmaintenance scheduled on OIM\n are forwarded to WLCG SAM, if applicable.\n\n\n\n\nBefore Starting\n\n\nBefore starting the installation process, consider the following points (consulting \nthe Reference section below\n as needed):\n\n\n\n\nUser IDs:\n If they do not exist already, the installation will create the Linux user IDs \nrsv\n and \ncndrcron\n\n\nService certificate:\n The RSV service requires a service certificate (\n/etc/grid-security/rsv/rsvcert.pem\n) and matching key (\n/etc/grid-security/rsv/rsvkey.pem\n)\n\n\nNetwork ports:\n To view results, port 80 must accept incoming requests; outbound connectivity to tested services must work, too\n\n\nHost choice:\n Install RSV on your site CE unless you have specific reasons (e.g., performance) for installing on a separate host\n\n\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the RSV host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare \nthe required Yum repositories\n\n\nInstall \nCA certificates\n\n\n\n\nInstalling RSV\n\n\nAn installation of RSV at a site consists of the RSV client software, the Apache web server, parts of HTCondor (for its cron-like scheduling capabilities), and various other small tools. To simplify installation, OSG provides a convenience RPM that installs all required software with a single command.\n\n\n\n\n\n\nConsider updating your local cache of Yum repository data and your existing RPM packages:\n\n\nroot@host #\n yum clean all --enablerepo\n=\n\\*\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNote\n\n\nThe \nupdate\n command will update \nall\n packages on your system.\n\n\n\n\n\n\n\n\nIf you have installed HTCondor already but not by RPM, install a special empty RPM to make RSV happy:\n\n\nroot@host #\n yum install empty-condor --enablerepo\n=\nosg-empty\n\n\n\n\n\n\n\n\n\nInstall RSV and related software:\n\n\nroot@host #\n yum install rsv\n\n\n\n\n\n\n\n\n\nConfiguring RSV\n\n\nAfter installation, there are some one-time configuration steps to tell RSV how to operate at your site.\n\n\n\n\n\n\nEdit \n/etc/osg/config.d/30-rsv.ini\n and follow the instructions in the file. There are detailed comments for each setting. In the simplest case \u2014 to monitor only your CE \u2014 set the \nhtcondor_ce_hosts\n variable to the fully qualified hostname of your CE.\n\n\n\n\n\n\nIf you have installed HTCondor already but not by RPM, specify the location of the Condor installation in \n30-rsv.ini\n in the \ncondor_location\n setting. If an HTCondor RPM is installed, you do not need to set \ncondor_location\n.\n\n\n\n\n\n\nComplete the configuration using the \nosg-configure\n tool:\n\n\nroot@host #\n osg-configure -v\n\nroot@host #\n osg-configure -c\n\n\n\n\n\n\n\n\n\nOptional configuration\n\n\nThe following configuration steps are optional and will likely not be required for setting up a small or typical site. If you do not need any of the following special configurations, skip to \nthe section on using RSV\n.\n\n\nGenerally speaking, read the \nConfigureRsv\n page for more advanced configuration options.\n\n\nConfiguring RSV to run probes using a remote server\n\n\nRSV monitors systems by running probes, which can run on the RSV host itself (the default case), via a separate batch system like HTCondor, or via a remote batch system using a Globus gatekeeper and its job manager. The last two options both can count those jobs and report them to, for example, Gratia.\n\n\nIn this case, remember to:\n\n\n\n\nAdd the RSV user \nrsv\n on all the systems where the probes may run, and\n\n\nMap the RSV service certificate to the user you intend to use for RSV. This should be a local user used exclusively for RSV and not belonging to an institutional VO to avoid for the RSV probes to be accounted as regular VO jobs in Gratia. This can be done in \nGUMS\n or \nusing a grid-mapfile-local\n (if you use a grid-mapfile). \nMapServiceCertToRsvUser\n explains how to configure GUMS or the grid-mapfile. Also see the \nCE installation document\n for more information.\n\n\n\n\nConfiguring the RSV web server to use HTTPS instead of HTTP\n\n\nIf you would like your local RSV web server to use HTTPS instead of the default HTTP (for compatibility or security reasons), complete the steps below. This procedure assumes that you already have an HTTP service certificate (or a copy of the host certificate) in \n/etc/grid-security/http/\n. If not, omit the \nSSLCertificate*\n modifications below, and your web server will start with its own, self-signed certificate.\n\n\n\n\n\n\nInstall \nmod_ssl\n:\n\n\nroot@host #\n yum install mod_ssl\n\n\n\n\n\n\n\n\n\nMake an alternate set of HTTP service certificate files:\n\n\nroot@host #\n cp -p /etc/grid-security/http/httpcert.pem /etc/grid-security/http/httpcert2.pem\n\nroot@host #\n cp -p /etc/grid-security/http/httpkey.pem /etc/grid-security/http/httpkey2.pem\n\nroot@host #\n chown apache:apache /etc/grid-security/http/http*2.pem\n\n\n\n\n\n\n\n\n\nBack up existing Apache configuration files:\n\n\nroot@host #\n cp -p /etc/httpd/conf/httpd.conf /etc/httpd/conf/httpd.conf.orig\n\nroot@host #\n cp -p /etc/httpd/conf.d/ssl.conf /etc/httpd/conf.d/ssl.conf.orig\n\n\n\n\n\n\n\n\n\nChange the default port for HTTP connections to 8000 by editing \n/etc/httpd/conf/httpd.conf\n\n\nListen 8000\n\n\n\n\n\n\n\n\n\nSet up HTTPS access by editing \n/etc/httpd/conf.d/ssl.conf\n:\n\n\nListen 8443\n\nVirtualHost _default_:8443\n\nSSLCertificateFile /etc/grid-security/http/httpcert2.pem\nSSLCertificateKeyFile /etc/grid-security/http/httpkey2.pem\n\n\n\n\n\nAfter these changes, when you start the Apache service, it will listening on ports \n8000\n (for HTTP) and \n8443\n (for HTTPS), rather than the default port \n80\n (for HTTP only).\n\n\n\n\nWarning\n\n\nif you make the changes above, you must restart the Apache server after each CA certificate update to pick up the changes.\n\n\n\n\n\n\n\n\nUsing RSV\n\n\nManaging RSV and associated services\n\n\nIn addition to the RSV service itself, there are a number of supporting services in your installation. The specific services are:\n\n\n\n\n\n\n\n\nSoftware\n\n\nService name\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nFetch CRL\n\n\nfetch-crl-boot\n and \nfetch-crl-cron\n\n\nSee \nCA documentation\n\n\n\n\n\n\nApache\n\n\nhttpd\n\n\n\n\n\n\n\n\nHTCondor-Cron\n\n\ncondor-cron\n\n\n\n\n\n\n\n\nRSV\n\n\nrsv\n\n\n\n\n\n\n\n\n\n\nStart the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as \nroot\n):\n\n\n\n\n\n\n\n\nTo \u2026\n\n\nRun the command \u2026\n\n\n\n\n\n\n\n\n\n\nStart a service\n\n\nservice \nSERVICE-NAME\n start\n\n\n\n\n\n\nStop a service\n\n\nservice \nSERVICE-NAME\n stop\n\n\n\n\n\n\nEnable a service to start during boot\n\n\nchkconfig \nSERVICE-NAME\n on\n\n\n\n\n\n\nDisable a service from starting during boot\n\n\nchkconfig \nSERVICE-NAME\n off\n\n\n\n\n\n\n\n\nRunning RSV manually\n\n\nNormally, the HTCondor-Cron scheduler runs RSV periodically. However, you can run RSV probes manually at any time:\n\n\nroot@host #\n rsv-control --run --all-enabled\n\n\n\n\n\nIf successful, results will be available from your local RSV web server (e.g., \nhttp://localhost/rsv\n) and, if enabled (which is the default) on \nMyOSG\n.\n\n\nYou can also run the metrics individually or pass special parameters as explained in the \nrsv-control document\n.\n\n\nTroubleshooting RSV\n\n\nTo get assistance, use the \nhelp procedure\n.\n\n\nRSV has a tool to collect information useful for troubleshooting into a tarball that can be shared with the developers and support staff.\nTo use it:\n\n\nroot@host#\n rsv-control --profile\n\nRunning the rsv-profiler...\n\n\nOSG-RSV Profiler\n\n\nAnalyzing...\n\n\nMaking tarball (rsv-profiler.tar.gz)\n\n\n\n\n\n\nYou can find more information on troubleshooting RSV in the \nrsv-control documentation\n.\n\n\n\n\nNote\n\n\nIf you are getting assistance via the trouble ticket system, you must add a \n.txt\n extension to the tarball so it can be uploaded:\n\n\n\n\nResending failed Gratia records\n\n\nIf RSV fails to send Gratia records, it will save a copy of the output into \n/var/spool/rsv/failed-gratia-scripts\n.\nYou will be notified if files are in this directory on your HTML status page.\n\n\nIf files appear here, you can attempt to determine why by looking at this log file: \n/var/log/rsv/consumers/gratia-consumer.output\n.\n(This file is rotated, so the error message may no longer be present.)\n\n\nUsually this error is spurious - there may have been a problem with the central collector being unavailable, or there may have been a network problem.\nThe first step to fix this problem is to try to resend these files.\nTo do so, move them back into the \ngratia\n directory and they will be resent the next time the gratia-script-consumer runs (about every 5 minutes):\n\n\nroot@host#\n mv /var/spool/rsv/failed-gratia-records/* /var/spool/rsv/gratia-consumer/\n\n\n\n\n\nImportant file locations\n\n\nLogs and configuration:\n\n\n\n\n\n\n\n\nFile Description\n\n\nLocation\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nMetric log files\n\n\n/var/log/rsv/metrics\n\n\n\n\n\n\n\n\nConsumer log files\n\n\n/var/log/rsv/consumers\n\n\n\n\n\n\n\n\nHTML files\n\n\n/usr/share/rsv/www/\n\n\nAvailable at \nhttp://your.host.example.com/rsv\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFile Description\n\n\nLocation\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nInitial configuration\n\n\n/etc/osg/config.d/30-rsv.ini\n\n\nRead by \nosg-configure\n\n\n\n\n\n\nRSV configuration\n\n\n/etc/rsv\n\n\nGenerally files in this directory should not be edited directly. Use \nosg-configure\n instead.\n\n\n\n\n\n\nMetric configuration\n\n\n/etc/rsv/metrics/HOSTNAME/METRICNAME.conf\n\n\nTo change arguments and environment\n\n\n\n\n\n\n\n\nTo find the metrics and the other files in RSV you can use also the RPM commands: \nrpm -ql rsv-metrics\n and \nrpm -ql rsv\n.\n\n\nGetting more information from rsv-control\n\n\nThe first step to getting more information is to run rsv-control with more verbosity. Use the \n--verbose\n (\n-v\n) flag. This flag can be used with any of rsv-control's abilities (run, enable, list, etc). The verbosity levels are:\n\n\n\n\n0 = print nothing\n\n\n1 = print warnings and errors along with usual output of command being run (1 is the default level)\n\n\n2 = adds informational messages\n\n\n3 = full debugging output\n\n\n\n\nFor example, here is the output when running a metric with -v2.\n\n\n\n  \nShow detailed ouput\n\n\n   [root@fermicloud016 condor]# rsv-control -r org.osg.general.osg-version -v 2 -u osg-edu.cs.wisc.edu\n\n\n   INFO: Reading configuration file /etc/rsv/rsv.conf\n\n\n   INFO: Reading configuration file /etc/rsv/consumers.conf\n\n\n   INFO: Validating configuration:\n\n\n   INFO: Validating user:\n\n\nINFO:     Invoked as root.  Switching to \nrsv\n user (uid: 100 - gid: 102)\n\n\nINFO: Registered consumers: html-consumer, gratia-consumer\n\n\nINFO: Loading config file \n/etc/rsv/meta/metrics/org.osg.general.osg-version.meta\n\n\nINFO: Loading config file \n/etc/rsv/metrics/org.osg.general.osg-version.conf\n\n\nINFO: Optional config file \n/etc/rsv/metrics/osg-edu.cs.wisc.edu/org.osg.general.osg-version.conf\n does not exist\n\n\nINFO: Checking proxy:\n\n\nINFO:     Using service certificate proxy\n\n\nINFO: Running command with timeout (1200 seconds):\n\n\n        /usr/bin/openssl x509 -in /tmp/rsvproxy -noout -enddate -checkend 21600\n\n\nINFO: Exit code of job: 0\n\n\nINFO:     Service certificate valid for at least 6 hours.\n\n\nINFO: Pinging host osg-edu.cs.wisc.edu:\n\n\nINFO: Running command with timeout (1200 seconds):\n\n\n        /bin/ping -W 3 -c 1 osg-edu.cs.wisc.edu\n\n\nINFO: Exit code of job: 0\n\n\nINFO:     Ping successful\n\n\n\nRunning metric org.osg.general.osg-version:\n\n\n\nINFO: Executing job remotely using Condor-G\n\n\nINFO: Setting up job environment:\n\n\nINFO:     No environment setup declared\n\n\nINFO: Condor-G working directory: /var/tmp/rsv/condor_g-JiQthF\n\n\nINFO: Forming arguments:\n\n\nINFO:     Arguments: \n\n\nINFO: List of files to transfer: /usr/libexec/rsv/probes/RSVMetric.pm\n\n\nINFO: Condor submission: Submitting job(s).\n\n\n1 job(s) submitted to cluster 2.\n\n\nINFO: Trimming data to 10000 bytes because details-data-trim-length is set\n\n\nINFO: Creating record for html-consumer consumer at \n/var/spool/rsv/html-consumer/org.osg.general.osg-version.7rgLfn\n\n\nINFO: Creating record for gratia-consumer consumer at \n/var/spool/rsv/gratia-consumer/org.osg.general.osg-version.-qelnL\n\n\nINFO: Result:\n\n\n\nmetricName: org.osg.general.osg-version\n\n\nmetricType: status\n\n\ntimestamp: 2012-01-25 16:12:40 CST\n\n\nmetricStatus: OK\n\n\nserviceType: OSG-CE\n\n\nserviceURI: osg-edu.cs.wisc.edu\n\n\ngatheredAt: fermicloud016.fnal.gov\n\n\nsummaryData: OK\n\n\ndetailsData: OSG 1.2.26\n\n\n\nEOT\n\n\n\n\n\n\n\n\nGetting Help\n\n\nTo get assistance, please use \nthis page\n and attach the output of \nrsv-control --profile\n:\n\n\nroot@host #\n rsv-control --profile\n\nRunning the rsv-profiler...\n\n\nOSG-RSV Profiler\n\n\nAnalyzing...\n\n\nMaking tarball (rsv-profiler.tar.gz)\n\n\n\n\n\n\nReference\n\n\nHere are some other RSV documents that might be helpful:\n\n\n\n\nThe RSV architecture\n\n\nRSV storage probes\n\n\n\n\nUsers\n\n\nThe RSV installation will create two users unless they are already created. The users are created when the \nrsv\n and \ncondor-cron\n packages are installed.\n\n\n\n\n\n\n\n\nUser\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nrsv\n\n\nRuns the RSV tests; the RSV certificate (below) will need to be owned by this user\n\n\n\n\n\n\ncndrcron\n\n\nRuns the Condor Cron processes to schedule the running of the tests\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nif you pre-create the RSV user, it should have a working shell. That is, it shouldn't have a default shell of \n/sbin/nologin\n.\n\n\n\n\n\n\nWarning\n\n\nIf you manage your \n/etc/passwd\n file with configuration management software such as Puppet, CFEngine or 411, make sure the UID and GID in \n/etc/condor-cron/config.d/condor_ids\n matches the UID and GID of the \ncndrcron\n user and group in \n/etc/passwd\n. If it does not, create a file named \n/etc/condor-cron/config.d/condor_ids_override\n with the contents:\n\n\n\n\nCONDOR_IDS=UID.GID\n\n\n\n\n\nwhere \nUID\n and \nGID\n are the UID and GID of the \ncndrcron\n user and group.\n\n\nCertificates\n\n\n\n\n\n\n\n\nCertificate\n\n\nUser that owns certificate\n\n\nPath to certificate\n\n\n\n\n\n\n\n\n\n\nRSV service certificate\n\n\nrsv\n\n\n/etc/grid-security/rsv/rsvcert.pem\n\n\n\n\n\n\n\n\n\n\n/etc/grid-security/rsv/rsvkey.pem\n\n\n\n\n\n\n\n\nEnsure an RSV service certificate is installed in \n/etc/grid-security/rsv/\n and the certificate files are owned by the \nrsv\n user. Adjust the permissions if necessary (cert needs to be readable by all, key needs to be readable by nobody but owner).\n\n\nYou may need another certificate owned by \napache\n if you'd like an authenticated web server; see \nConfiguring the RSV web server to use HTTPS instead of HTTP\n above.\n\n\nSee \ninstructions\n to request a service certificate.\n\n\nNetworking\n\n\n\n\n\n\n\n\nService Name\n\n\nProtocol\n\n\nPort Number\n\n\nInbound\n\n\nOutbound\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nHTTP\n\n\ntcp\n\n\n80\n\n\nYES\n\n\n\n\nRSV runs an HTTP server (Apache) that publishes a page with the RSV testing results\n\n\n\n\n\n\nHTTP\n\n\ntcp\n\n\n80\n\n\n\n\nYES\n\n\nRSV pushes testing results to the OSG Gratia Collectors at opensciencegrid.org\n\n\n\n\n\n\nvarious\n\n\nvarious\n\n\nvarious\n\n\n\n\nYES\n\n\nAllow outbound network connection to all services that you want to test\n\n\n\n\n\n\n\n\nOr, if you'd rather have your RSV web page appear as \nhttps\n://...:8443/rsv/\n like it used to in OSG 1.2, the first column above would be \nHTTPS\n / \ntcp\n / \n8443\n. See \nabove\n for how to configure this.", 
            "title": "Install RSV"
        }, 
        {
            "location": "/monitoring/install-rsv/#installing-configuring-using-and-troubleshooting-rsv", 
            "text": "", 
            "title": "Installing, Configuring, Using, and Troubleshooting RSV"
        }, 
        {
            "location": "/monitoring/install-rsv/#about-this-guide", 
            "text": "The Resource and Service Validation (RSV) software helps a site administrator verify that certain site resources and services are working as expected. OSG recommends that sites install and run RSV, but it is optional; further, each site selects which specific tests (called  probes ) to run.  Use this page to learn more about RSV in general, and how to install, configure, run, test, and troubleshoot RSV from the OSG software repositories. For documentation on specific probes or on how to write your own probes, please check the  Reference section .", 
            "title": "About This Guide"
        }, 
        {
            "location": "/monitoring/install-rsv/#introduction-to-rsv", 
            "text": "The Resource and Service Validation (RSV) software provides OSG site administrators a scalable and easy-to-maintain resource and service monitoring infrastructure. The components of RSV are:   RSV Client.  The client tools allow a site administrator to run tests against their site by providing a set of tests (which can run on the same or other hosts within a site), HTCondor-Cron for scheduling, and tools for collecting and storing the results (using Gratia). The client package is not installed by default and may be installed on a CE or other host. Generally, you configure the RSV client to run tests at scheduled time intervals and then it makes results available on a local website. Also, the client can upload test results to a central collector (see next item).  RSV Collector/Server.  The central OSG RSV Collector accepts and stores results from RSV clients throughout OSG, which can be viewed in  MyOSG , on the \u201cCurrent RSV Status\u201d page and under the \u201cResource Group\u201d menu.  Periodic Availability Reports.  The availability of all active registered OSG resources and the services running on each of those resources is calculated using the results received for  critical metrics . Once a day, these availability numbers are  published online  (More information:  Outline of reports ).  RSV-SAM Transport.  The WLCG RSV-SAM Transport infrastructure pushes out RSV results, for resources that are flagged to be part of the WLCG Interoperability agreement, from the GOC collector to WLCG's Service Availability Monitoring (SAM) system. More information on viewing these results is  available here .  MyOSG and OIM Links.  RSV picks up resource information, WLCG interoperability information, etc., from a MyOSG resource group summary listing, which is in turn based on the  OSG Information Management (OIM) (topology) system  (Requires registration). Resource  maintenance scheduled on OIM  are forwarded to WLCG SAM, if applicable.", 
            "title": "Introduction to RSV"
        }, 
        {
            "location": "/monitoring/install-rsv/#before-starting", 
            "text": "Before starting the installation process, consider the following points (consulting  the Reference section below  as needed):   User IDs:  If they do not exist already, the installation will create the Linux user IDs  rsv  and  cndrcron  Service certificate:  The RSV service requires a service certificate ( /etc/grid-security/rsv/rsvcert.pem ) and matching key ( /etc/grid-security/rsv/rsvkey.pem )  Network ports:  To view results, port 80 must accept incoming requests; outbound connectivity to tested services must work, too  Host choice:  Install RSV on your site CE unless you have specific reasons (e.g., performance) for installing on a separate host   As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the RSV host has  a supported operating system  Obtain root access to the host  Prepare  the required Yum repositories  Install  CA certificates", 
            "title": "Before Starting"
        }, 
        {
            "location": "/monitoring/install-rsv/#installing-rsv", 
            "text": "An installation of RSV at a site consists of the RSV client software, the Apache web server, parts of HTCondor (for its cron-like scheduling capabilities), and various other small tools. To simplify installation, OSG provides a convenience RPM that installs all required software with a single command.    Consider updating your local cache of Yum repository data and your existing RPM packages:  root@host #  yum clean all --enablerepo = \\*  root@host #  yum update   Note  The  update  command will update  all  packages on your system.     If you have installed HTCondor already but not by RPM, install a special empty RPM to make RSV happy:  root@host #  yum install empty-condor --enablerepo = osg-empty    Install RSV and related software:  root@host #  yum install rsv", 
            "title": "Installing RSV"
        }, 
        {
            "location": "/monitoring/install-rsv/#configuring-rsv", 
            "text": "After installation, there are some one-time configuration steps to tell RSV how to operate at your site.    Edit  /etc/osg/config.d/30-rsv.ini  and follow the instructions in the file. There are detailed comments for each setting. In the simplest case \u2014 to monitor only your CE \u2014 set the  htcondor_ce_hosts  variable to the fully qualified hostname of your CE.    If you have installed HTCondor already but not by RPM, specify the location of the Condor installation in  30-rsv.ini  in the  condor_location  setting. If an HTCondor RPM is installed, you do not need to set  condor_location .    Complete the configuration using the  osg-configure  tool:  root@host #  osg-configure -v root@host #  osg-configure -c", 
            "title": "Configuring RSV"
        }, 
        {
            "location": "/monitoring/install-rsv/#optional-configuration", 
            "text": "The following configuration steps are optional and will likely not be required for setting up a small or typical site. If you do not need any of the following special configurations, skip to  the section on using RSV .  Generally speaking, read the  ConfigureRsv  page for more advanced configuration options.", 
            "title": "Optional configuration"
        }, 
        {
            "location": "/monitoring/install-rsv/#configuring-rsv-to-run-probes-using-a-remote-server", 
            "text": "RSV monitors systems by running probes, which can run on the RSV host itself (the default case), via a separate batch system like HTCondor, or via a remote batch system using a Globus gatekeeper and its job manager. The last two options both can count those jobs and report them to, for example, Gratia.  In this case, remember to:   Add the RSV user  rsv  on all the systems where the probes may run, and  Map the RSV service certificate to the user you intend to use for RSV. This should be a local user used exclusively for RSV and not belonging to an institutional VO to avoid for the RSV probes to be accounted as regular VO jobs in Gratia. This can be done in  GUMS  or  using a grid-mapfile-local  (if you use a grid-mapfile).  MapServiceCertToRsvUser  explains how to configure GUMS or the grid-mapfile. Also see the  CE installation document  for more information.", 
            "title": "Configuring RSV to run probes using a remote server"
        }, 
        {
            "location": "/monitoring/install-rsv/#configuring-the-rsv-web-server-to-use-https-instead-of-http", 
            "text": "If you would like your local RSV web server to use HTTPS instead of the default HTTP (for compatibility or security reasons), complete the steps below. This procedure assumes that you already have an HTTP service certificate (or a copy of the host certificate) in  /etc/grid-security/http/ . If not, omit the  SSLCertificate*  modifications below, and your web server will start with its own, self-signed certificate.    Install  mod_ssl :  root@host #  yum install mod_ssl    Make an alternate set of HTTP service certificate files:  root@host #  cp -p /etc/grid-security/http/httpcert.pem /etc/grid-security/http/httpcert2.pem root@host #  cp -p /etc/grid-security/http/httpkey.pem /etc/grid-security/http/httpkey2.pem root@host #  chown apache:apache /etc/grid-security/http/http*2.pem    Back up existing Apache configuration files:  root@host #  cp -p /etc/httpd/conf/httpd.conf /etc/httpd/conf/httpd.conf.orig root@host #  cp -p /etc/httpd/conf.d/ssl.conf /etc/httpd/conf.d/ssl.conf.orig    Change the default port for HTTP connections to 8000 by editing  /etc/httpd/conf/httpd.conf  Listen 8000    Set up HTTPS access by editing  /etc/httpd/conf.d/ssl.conf :  Listen 8443 VirtualHost _default_:8443 \nSSLCertificateFile /etc/grid-security/http/httpcert2.pem\nSSLCertificateKeyFile /etc/grid-security/http/httpkey2.pem  After these changes, when you start the Apache service, it will listening on ports  8000  (for HTTP) and  8443  (for HTTPS), rather than the default port  80  (for HTTP only).   Warning  if you make the changes above, you must restart the Apache server after each CA certificate update to pick up the changes.", 
            "title": "Configuring the RSV web server to use HTTPS instead of HTTP"
        }, 
        {
            "location": "/monitoring/install-rsv/#using-rsv", 
            "text": "", 
            "title": "Using RSV"
        }, 
        {
            "location": "/monitoring/install-rsv/#managing-rsv-and-associated-services", 
            "text": "In addition to the RSV service itself, there are a number of supporting services in your installation. The specific services are:     Software  Service name  Notes      Fetch CRL  fetch-crl-boot  and  fetch-crl-cron  See  CA documentation    Apache  httpd     HTCondor-Cron  condor-cron     RSV  rsv      Start the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as  root ):     To \u2026  Run the command \u2026      Start a service  service  SERVICE-NAME  start    Stop a service  service  SERVICE-NAME  stop    Enable a service to start during boot  chkconfig  SERVICE-NAME  on    Disable a service from starting during boot  chkconfig  SERVICE-NAME  off", 
            "title": "Managing RSV and associated services"
        }, 
        {
            "location": "/monitoring/install-rsv/#running-rsv-manually", 
            "text": "Normally, the HTCondor-Cron scheduler runs RSV periodically. However, you can run RSV probes manually at any time:  root@host #  rsv-control --run --all-enabled  If successful, results will be available from your local RSV web server (e.g.,  http://localhost/rsv ) and, if enabled (which is the default) on  MyOSG .  You can also run the metrics individually or pass special parameters as explained in the  rsv-control document .", 
            "title": "Running RSV manually"
        }, 
        {
            "location": "/monitoring/install-rsv/#troubleshooting-rsv", 
            "text": "To get assistance, use the  help procedure .  RSV has a tool to collect information useful for troubleshooting into a tarball that can be shared with the developers and support staff.\nTo use it:  root@host#  rsv-control --profile Running the rsv-profiler...  OSG-RSV Profiler  Analyzing...  Making tarball (rsv-profiler.tar.gz)   You can find more information on troubleshooting RSV in the  rsv-control documentation .   Note  If you are getting assistance via the trouble ticket system, you must add a  .txt  extension to the tarball so it can be uploaded:", 
            "title": "Troubleshooting RSV"
        }, 
        {
            "location": "/monitoring/install-rsv/#resending-failed-gratia-records", 
            "text": "If RSV fails to send Gratia records, it will save a copy of the output into  /var/spool/rsv/failed-gratia-scripts .\nYou will be notified if files are in this directory on your HTML status page.  If files appear here, you can attempt to determine why by looking at this log file:  /var/log/rsv/consumers/gratia-consumer.output .\n(This file is rotated, so the error message may no longer be present.)  Usually this error is spurious - there may have been a problem with the central collector being unavailable, or there may have been a network problem.\nThe first step to fix this problem is to try to resend these files.\nTo do so, move them back into the  gratia  directory and they will be resent the next time the gratia-script-consumer runs (about every 5 minutes):  root@host#  mv /var/spool/rsv/failed-gratia-records/* /var/spool/rsv/gratia-consumer/", 
            "title": "Resending failed Gratia records"
        }, 
        {
            "location": "/monitoring/install-rsv/#important-file-locations", 
            "text": "Logs and configuration:     File Description  Location  Comment      Metric log files  /var/log/rsv/metrics     Consumer log files  /var/log/rsv/consumers     HTML files  /usr/share/rsv/www/  Available at  http://your.host.example.com/rsv        File Description  Location  Comment      Initial configuration  /etc/osg/config.d/30-rsv.ini  Read by  osg-configure    RSV configuration  /etc/rsv  Generally files in this directory should not be edited directly. Use  osg-configure  instead.    Metric configuration  /etc/rsv/metrics/HOSTNAME/METRICNAME.conf  To change arguments and environment     To find the metrics and the other files in RSV you can use also the RPM commands:  rpm -ql rsv-metrics  and  rpm -ql rsv .", 
            "title": "Important file locations"
        }, 
        {
            "location": "/monitoring/install-rsv/#getting-more-information-from-rsv-control", 
            "text": "The first step to getting more information is to run rsv-control with more verbosity. Use the  --verbose  ( -v ) flag. This flag can be used with any of rsv-control's abilities (run, enable, list, etc). The verbosity levels are:   0 = print nothing  1 = print warnings and errors along with usual output of command being run (1 is the default level)  2 = adds informational messages  3 = full debugging output   For example, here is the output when running a metric with -v2.  \n   Show detailed ouput     [root@fermicloud016 condor]# rsv-control -r org.osg.general.osg-version -v 2 -u osg-edu.cs.wisc.edu     INFO: Reading configuration file /etc/rsv/rsv.conf     INFO: Reading configuration file /etc/rsv/consumers.conf     INFO: Validating configuration:     INFO: Validating user:  INFO:     Invoked as root.  Switching to  rsv  user (uid: 100 - gid: 102)  INFO: Registered consumers: html-consumer, gratia-consumer  INFO: Loading config file  /etc/rsv/meta/metrics/org.osg.general.osg-version.meta  INFO: Loading config file  /etc/rsv/metrics/org.osg.general.osg-version.conf  INFO: Optional config file  /etc/rsv/metrics/osg-edu.cs.wisc.edu/org.osg.general.osg-version.conf  does not exist  INFO: Checking proxy:  INFO:     Using service certificate proxy  INFO: Running command with timeout (1200 seconds):          /usr/bin/openssl x509 -in /tmp/rsvproxy -noout -enddate -checkend 21600  INFO: Exit code of job: 0  INFO:     Service certificate valid for at least 6 hours.  INFO: Pinging host osg-edu.cs.wisc.edu:  INFO: Running command with timeout (1200 seconds):          /bin/ping -W 3 -c 1 osg-edu.cs.wisc.edu  INFO: Exit code of job: 0  INFO:     Ping successful  Running metric org.osg.general.osg-version:  INFO: Executing job remotely using Condor-G  INFO: Setting up job environment:  INFO:     No environment setup declared  INFO: Condor-G working directory: /var/tmp/rsv/condor_g-JiQthF  INFO: Forming arguments:  INFO:     Arguments:   INFO: List of files to transfer: /usr/libexec/rsv/probes/RSVMetric.pm  INFO: Condor submission: Submitting job(s).  1 job(s) submitted to cluster 2.  INFO: Trimming data to 10000 bytes because details-data-trim-length is set  INFO: Creating record for html-consumer consumer at  /var/spool/rsv/html-consumer/org.osg.general.osg-version.7rgLfn  INFO: Creating record for gratia-consumer consumer at  /var/spool/rsv/gratia-consumer/org.osg.general.osg-version.-qelnL  INFO: Result:  metricName: org.osg.general.osg-version  metricType: status  timestamp: 2012-01-25 16:12:40 CST  metricStatus: OK  serviceType: OSG-CE  serviceURI: osg-edu.cs.wisc.edu  gatheredAt: fermicloud016.fnal.gov  summaryData: OK  detailsData: OSG 1.2.26  EOT", 
            "title": "Getting more information from rsv-control"
        }, 
        {
            "location": "/monitoring/install-rsv/#getting-help", 
            "text": "To get assistance, please use  this page  and attach the output of  rsv-control --profile :  root@host #  rsv-control --profile Running the rsv-profiler...  OSG-RSV Profiler  Analyzing...  Making tarball (rsv-profiler.tar.gz)", 
            "title": "Getting Help"
        }, 
        {
            "location": "/monitoring/install-rsv/#reference", 
            "text": "Here are some other RSV documents that might be helpful:   The RSV architecture  RSV storage probes", 
            "title": "Reference"
        }, 
        {
            "location": "/monitoring/install-rsv/#users", 
            "text": "The RSV installation will create two users unless they are already created. The users are created when the  rsv  and  condor-cron  packages are installed.     User  Comment      rsv  Runs the RSV tests; the RSV certificate (below) will need to be owned by this user    cndrcron  Runs the Condor Cron processes to schedule the running of the tests      Note  if you pre-create the RSV user, it should have a working shell. That is, it shouldn't have a default shell of  /sbin/nologin .    Warning  If you manage your  /etc/passwd  file with configuration management software such as Puppet, CFEngine or 411, make sure the UID and GID in  /etc/condor-cron/config.d/condor_ids  matches the UID and GID of the  cndrcron  user and group in  /etc/passwd . If it does not, create a file named  /etc/condor-cron/config.d/condor_ids_override  with the contents:   CONDOR_IDS=UID.GID  where  UID  and  GID  are the UID and GID of the  cndrcron  user and group.", 
            "title": "Users"
        }, 
        {
            "location": "/monitoring/install-rsv/#certificates", 
            "text": "Certificate  User that owns certificate  Path to certificate      RSV service certificate  rsv  /etc/grid-security/rsv/rsvcert.pem      /etc/grid-security/rsv/rsvkey.pem     Ensure an RSV service certificate is installed in  /etc/grid-security/rsv/  and the certificate files are owned by the  rsv  user. Adjust the permissions if necessary (cert needs to be readable by all, key needs to be readable by nobody but owner).  You may need another certificate owned by  apache  if you'd like an authenticated web server; see  Configuring the RSV web server to use HTTPS instead of HTTP  above.  See  instructions  to request a service certificate.", 
            "title": "Certificates"
        }, 
        {
            "location": "/monitoring/install-rsv/#networking", 
            "text": "Service Name  Protocol  Port Number  Inbound  Outbound  Comment      HTTP  tcp  80  YES   RSV runs an HTTP server (Apache) that publishes a page with the RSV testing results    HTTP  tcp  80   YES  RSV pushes testing results to the OSG Gratia Collectors at opensciencegrid.org    various  various  various   YES  Allow outbound network connection to all services that you want to test     Or, if you'd rather have your RSV web page appear as  https ://...:8443/rsv/  like it used to in OSG 1.2, the first column above would be  HTTPS  /  tcp  /  8443 . See  above  for how to configure this.", 
            "title": "Networking"
        }, 
        {
            "location": "/monitoring/advanced-rsv-configuration/", 
            "text": "Advanced RSV Configuration\n\n\nAbout This Document\n\n\nMost site administrators will be able to configure RSV by editing \n/etc/osg/config.d/30-rsv.ini\n and running osg-configure as described in \nthe RSV installation document\n.  This document provides instructions for configuration beyond what osg-configure is able to do.\n\n\nConfiguring metrics\n\n\nIf you need to change the behavior of a metric you can edit the metric configuration files. These replace the spec files from previous versions of RSV.\n\n\n\n\n/etc/rsv/metrics\n - changes made to conf file in this directory named after a metric will affect the metric when run against all hosts\n\n\n/etc/rsv/metrics/\nHOST\n - changes made to conf files in this directory (named as the host FQDN) will affect the metric when run against the specific host\n\n\n\n\nThe configuration files are in INI format and have two sections:\n\n\n\n\na first one named after the metric with execution options\n\n\na second one with the name including the \"args\" keyword, including parameters sent to the probe at invokation\n\n\n\n\nChanging the times a metric runs\n\n\nTo change the time a metric runs set the \ncron-interval\n setting in the metric's conf file. Use \nman 5 crontab\n for a description of the format. For example, to change the \norg.osg.general.ping-host\n to run at a different time:\n\n\n[org.osg.general.ping-host]\n\n\ncron-interval\n \n=\n \n45 * * * *\n\n\n\n[org.osg.general.ping-host args]\n\n\n#ping-count =\n\n\n#ping-timeout =\n\n\n\n\n\n\n\n\nNote\n\n\nBe sure to put the \ncron-interval\n setting in the \n[org.osg.general.ping-host]\n section, and not the \n[org.osg.general.ping-host args]\n section! The purpose of the \"args\" section is described in the \"passing extra parameters to a metric\" section below.\n\n\n\n\nAfter modifying the cron time of a metric you must restart RSV for the change to take effect.\n\n\nTo see what times each of the metrics is running you can use \nrsv-control\n as follows:\n\n\nroot@host#\n rsv-control -l --cron-times\n\n\nMetrics enabled for host: osg-edu.cs.wisc.edu:10443 | Cron times\n\n\n----------------------------------------------------+--------------------\n\n\norg.osg.srm.srmcp-readwrite                         | 28 * * * *\n\n\norg.osg.srm.srmping                                 | 13,33,53 * * * *\n\n\n...\n\n\n\n\n\n\nPassing extra parameters to a metric\n\n\nAny \nkey=value\n pairs in the \"args\" section of the metric's \nconf\n file will be turned into command line parameters to the probe. For example, for this file:\n\n\n[org.osg.certificates.cacert-expiry args]\n\n\nwarning-hours\n \n=\n \n6\n\n\nerror-hours\n \n=\n \n12\n\n\n\n\n\n\nThis would lead to the probe getting called with the command-line parameters \n--warning-hours 6 --error-hours 12\n.\n\n\nConfigure consumers\n\n\nThere is a configuration file common to all consumers: \n/etc/rsv/consumers.conf\n. It is a file in INI format and possible entries are:\n\n\n\n\n\n\n\n\nSetting\n\n\nValues\n\n\nDetails\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nconsumers\n\n\nComma-separated list of consumers to be enabled\n\n\n\n\n\n\ntimestamp\n\n\nlocal\n\n\nIf this is set to local, a record with a local timestamp will be supplied to the consumer. If this is set to any other value, or is not set, a record with the GMT will be created.\n\n\n\n\n\n\n\n\nEach consumer has a configuration file in \n/etc/rsv/consumers\n named after it. This allows to specify command lines and environment for the consumers. Some consumers may have also their own configuration file, usually in \n/etc/rsv/\n. Below is an example for the Nagios consumer.\n\n\nSending RSV records to Nagios\n\n\n\n\nEdit your \n/etc/rsv/rsv-nagios.conf\n file and fill in the appropriate information. The path of the configuration file is specified in \n/etc/rsv/consumers/nagios-consumer.conf\n.\n\n\n\n\nIf your Nagios config file contains password information you will want to lock down the permissions. Here is a suggested way to do this (replace \nrsvuser\n with the group of your RSV user (\nrsvuser\n by default)):\n\n\nroot@host#\n chown root:\nrsvuser\n /etc/rsv/rsv-nagios.conf\n\nroot@host#\n chmod \n0440\n /etc/rsv/rsv-nagios.conf\n\n\n\n\n\n\n\n\n\nIn the configuration file at \n/etc/rsv/consumers/nagios-consumer.conf\n, check the following two settings:\n\n\n\n\nMake sure that the path to your config file is correct. It may be referencing a directory \nconfig\n instead of \netc\n\n\nIf you want to use \nrsv2nsca\n add the string \"--send-nsca\" to the \nargs\n line.\n\n\n\n\n\n\n\n\nEnable and start the Nagios consumer by editing \nconsumers.conf\n or by using \nrsv-control\n as follows:\n\n\nroot@host#\n rsv-control --enable nagios-consumer\n\n\n\n\n\nThe Nagios consumer will be started the next time that you start RSV. If you are already running RSV you can turn on the Nagios consumer immediately by running:\n\n\nroot@host#\n rsv-control --on nagios-consumer\n\n\n\n\n\n\n\n\n\nTo verify that the Nagios consumer is running you can run \nrsv-control -j\n.\n\n\n\n\nThe log information for the Nagios consumer can be found in these files:\n\n\n/var/log/rsv/consumers/nagios-consumer.log\n\n\n/var/log/rsv/consumers/nagios-consumer.out\n\n\n/var/log/rsv/consumers/nagios-consumer.err\n\n\n\n\n\n\n\n\nGeneral RSV configuration options\n\n\nYou can configure the RSV framework using \n/etc/rsv/rsv.conf\n. It is a file in INI format and possible entries are:\n\n\n\n\n\n\n\n\nSetting\n\n\nValues\n\n\nDetails\n\n\n\n\n\n\n\n\n\n\nuser\n\n\nusername\n\n\nThe UNIX username that owns RSV. This is mandatory\n\n\n\n\n\n\nservice-cert\n\n\npath\n\n\nAbsolute path to the service certificate file. If this is set service-key and service-proxy must also be set.\n\n\n\n\n\n\nservice-key\n\n\npath\n\n\nAbsolute path to the service key file. This must be used with service-cert.\n\n\n\n\n\n\nservice-proxy\n\n\npath\n\n\nAbsolute path where the service proxy will be generated. This must be used with service-cert.\n\n\n\n\n\n\nproxy-file\n\n\npath\n\n\nAlternative to service-cert. The absolute path where the user proxy file is located. This will not be auto-regenerated.\n\n\n\n\n\n\ndetails-data-trim-length\n\n\ninteger\n\n\nThe number of bytes to trim the detailsData section to. If set to 0 no trimming will occur.\n\n\n\n\n\n\njob-timeout\n\n\ninteger\n\n\nTime in seconds before a metric is killed. A metric that times out will return a CRITICAL status.\n\n\n\n\n\n\n\n\n\nTroubleshooting\n\n\nImportant files locations\n\n\nConfiguration files:\n\n\n\n\n\n\n\n\nFile Description\n\n\nLocation\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nRSV configuration directory\n\n\n/etc/rsv\n\n\n\n\n\n\n\n\nRSV configuration\n\n\n/etc/rsv/rsv.conf\n\n\nRSV framework configuration\n\n\n\n\n\n\nConsumers configuration in RSV\n\n\n/etc/rsv/consumers.conf\n\n\nSelect the consumers and change generic options\n\n\n\n\n\n\nConsumers configuration\n\n\n/etc/rsv/consumers/\nCONSUMERNAME\n\n\nTo change arguments and environment\n\n\n\n\n\n\nGeneric metrics configuration\n\n\n/etc/rsv/metrics/\nMETRICNAME\n.conf\n\n\nTo change arguments and environment\n\n\n\n\n\n\nHost specific metrics configuration\n\n\n/etc/rsv/metrics/\nHOSTNAME\n/\nMETRICNAME\n.conf\n\n\nTo change arguments and environment when running on HOSTNAME\n\n\n\n\n\n\n\n\nOther files:\n\n\n\n\n\n\n\n\nFile Description\n\n\nLocation\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nMetric log files\n\n\n/var/log/rsv/metrics\n\n\n\n\n\n\n\n\nConsumer log files\n\n\n/var/log/rsv/consumers\n\n\n\n\n\n\n\n\nInitial configuration\n\n\n/etc/osg/config.d/30-rsv.ini\n\n\nRead by \nosg-configure\n\n\n\n\n\n\nWeb files output\n\n\n/usr/share/rsv/www/\n\n\n\n\n\n\n\n\n\n\nTo find the metrics and the other files in RSV you can use also the RPM commands: \nrpm -ql rsv-metrics\n and \nrpm -ql rsv\n.", 
            "title": "Advanced RSV Configuration"
        }, 
        {
            "location": "/monitoring/advanced-rsv-configuration/#advanced-rsv-configuration", 
            "text": "", 
            "title": "Advanced RSV Configuration"
        }, 
        {
            "location": "/monitoring/advanced-rsv-configuration/#about-this-document", 
            "text": "Most site administrators will be able to configure RSV by editing  /etc/osg/config.d/30-rsv.ini  and running osg-configure as described in  the RSV installation document .  This document provides instructions for configuration beyond what osg-configure is able to do.", 
            "title": "About This Document"
        }, 
        {
            "location": "/monitoring/advanced-rsv-configuration/#configuring-metrics", 
            "text": "If you need to change the behavior of a metric you can edit the metric configuration files. These replace the spec files from previous versions of RSV.   /etc/rsv/metrics  - changes made to conf file in this directory named after a metric will affect the metric when run against all hosts  /etc/rsv/metrics/ HOST  - changes made to conf files in this directory (named as the host FQDN) will affect the metric when run against the specific host   The configuration files are in INI format and have two sections:   a first one named after the metric with execution options  a second one with the name including the \"args\" keyword, including parameters sent to the probe at invokation", 
            "title": "Configuring metrics"
        }, 
        {
            "location": "/monitoring/advanced-rsv-configuration/#changing-the-times-a-metric-runs", 
            "text": "To change the time a metric runs set the  cron-interval  setting in the metric's conf file. Use  man 5 crontab  for a description of the format. For example, to change the  org.osg.general.ping-host  to run at a different time:  [org.osg.general.ping-host]  cron-interval   =   45 * * * *  [org.osg.general.ping-host args]  #ping-count =  #ping-timeout =    Note  Be sure to put the  cron-interval  setting in the  [org.osg.general.ping-host]  section, and not the  [org.osg.general.ping-host args]  section! The purpose of the \"args\" section is described in the \"passing extra parameters to a metric\" section below.   After modifying the cron time of a metric you must restart RSV for the change to take effect.  To see what times each of the metrics is running you can use  rsv-control  as follows:  root@host#  rsv-control -l --cron-times Metrics enabled for host: osg-edu.cs.wisc.edu:10443 | Cron times  ----------------------------------------------------+--------------------  org.osg.srm.srmcp-readwrite                         | 28 * * * *  org.osg.srm.srmping                                 | 13,33,53 * * * *  ...", 
            "title": "Changing the times a metric runs"
        }, 
        {
            "location": "/monitoring/advanced-rsv-configuration/#passing-extra-parameters-to-a-metric", 
            "text": "Any  key=value  pairs in the \"args\" section of the metric's  conf  file will be turned into command line parameters to the probe. For example, for this file:  [org.osg.certificates.cacert-expiry args]  warning-hours   =   6  error-hours   =   12   This would lead to the probe getting called with the command-line parameters  --warning-hours 6 --error-hours 12 .", 
            "title": "Passing extra parameters to a metric"
        }, 
        {
            "location": "/monitoring/advanced-rsv-configuration/#configure-consumers", 
            "text": "There is a configuration file common to all consumers:  /etc/rsv/consumers.conf . It is a file in INI format and possible entries are:     Setting  Values  Details      enabled  consumers  Comma-separated list of consumers to be enabled    timestamp  local  If this is set to local, a record with a local timestamp will be supplied to the consumer. If this is set to any other value, or is not set, a record with the GMT will be created.     Each consumer has a configuration file in  /etc/rsv/consumers  named after it. This allows to specify command lines and environment for the consumers. Some consumers may have also their own configuration file, usually in  /etc/rsv/ . Below is an example for the Nagios consumer.", 
            "title": "Configure consumers"
        }, 
        {
            "location": "/monitoring/advanced-rsv-configuration/#sending-rsv-records-to-nagios", 
            "text": "Edit your  /etc/rsv/rsv-nagios.conf  file and fill in the appropriate information. The path of the configuration file is specified in  /etc/rsv/consumers/nagios-consumer.conf .   If your Nagios config file contains password information you will want to lock down the permissions. Here is a suggested way to do this (replace  rsvuser  with the group of your RSV user ( rsvuser  by default)):  root@host#  chown root: rsvuser  /etc/rsv/rsv-nagios.conf root@host#  chmod  0440  /etc/rsv/rsv-nagios.conf    In the configuration file at  /etc/rsv/consumers/nagios-consumer.conf , check the following two settings:   Make sure that the path to your config file is correct. It may be referencing a directory  config  instead of  etc  If you want to use  rsv2nsca  add the string \"--send-nsca\" to the  args  line.     Enable and start the Nagios consumer by editing  consumers.conf  or by using  rsv-control  as follows:  root@host#  rsv-control --enable nagios-consumer  The Nagios consumer will be started the next time that you start RSV. If you are already running RSV you can turn on the Nagios consumer immediately by running:  root@host#  rsv-control --on nagios-consumer    To verify that the Nagios consumer is running you can run  rsv-control -j .   The log information for the Nagios consumer can be found in these files:  /var/log/rsv/consumers/nagios-consumer.log  /var/log/rsv/consumers/nagios-consumer.out  /var/log/rsv/consumers/nagios-consumer.err", 
            "title": "Sending RSV records to Nagios"
        }, 
        {
            "location": "/monitoring/advanced-rsv-configuration/#general-rsv-configuration-options", 
            "text": "You can configure the RSV framework using  /etc/rsv/rsv.conf . It is a file in INI format and possible entries are:     Setting  Values  Details      user  username  The UNIX username that owns RSV. This is mandatory    service-cert  path  Absolute path to the service certificate file. If this is set service-key and service-proxy must also be set.    service-key  path  Absolute path to the service key file. This must be used with service-cert.    service-proxy  path  Absolute path where the service proxy will be generated. This must be used with service-cert.    proxy-file  path  Alternative to service-cert. The absolute path where the user proxy file is located. This will not be auto-regenerated.    details-data-trim-length  integer  The number of bytes to trim the detailsData section to. If set to 0 no trimming will occur.    job-timeout  integer  Time in seconds before a metric is killed. A metric that times out will return a CRITICAL status.", 
            "title": "General RSV configuration options"
        }, 
        {
            "location": "/monitoring/advanced-rsv-configuration/#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/monitoring/advanced-rsv-configuration/#important-files-locations", 
            "text": "Configuration files:     File Description  Location  Comment      RSV configuration directory  /etc/rsv     RSV configuration  /etc/rsv/rsv.conf  RSV framework configuration    Consumers configuration in RSV  /etc/rsv/consumers.conf  Select the consumers and change generic options    Consumers configuration  /etc/rsv/consumers/ CONSUMERNAME  To change arguments and environment    Generic metrics configuration  /etc/rsv/metrics/ METRICNAME .conf  To change arguments and environment    Host specific metrics configuration  /etc/rsv/metrics/ HOSTNAME / METRICNAME .conf  To change arguments and environment when running on HOSTNAME     Other files:     File Description  Location  Comment      Metric log files  /var/log/rsv/metrics     Consumer log files  /var/log/rsv/consumers     Initial configuration  /etc/osg/config.d/30-rsv.ini  Read by  osg-configure    Web files output  /usr/share/rsv/www/      To find the metrics and the other files in RSV you can use also the RPM commands:  rpm -ql rsv-metrics  and  rpm -ql rsv .", 
            "title": "Important files locations"
        }, 
        {
            "location": "/monitoring/rsv-control/", 
            "text": "Using rsv-control\n\n\nOverview\n\n\nThis document is for System Administrators. It details the usage of the \nrsv-control\n command for enabling, disabling, testing and running RSV probes.\n\n\nrsv-control\n provides an interface to many RSV tasks. \nrsv-control\n can view RSV jobs, run metrics, enable or disable metrics and consumers, and allow advanced configuration.\n\n\n\n\nWarning\n\n\nrsv-control\n can be used to configure RSV as described here and in \nthe advanced configuration document\n. Most site admins will be able to configure RSV by editing \n/etc/osg/config.d/30-rsv.ini\n and running \nosg-configure\n as described in the \ninstallation doc\n.\n\n\n\n\nUsing \nrsv-control\n to configure is for advanced RSV use including enabling non-default metrics. Admins who don't use \nrsv-control\n for configuration can still use it to view their RSV jobs, run RSV tests, and help debug RSV problems. Anyone can view the jobs, but you must be root or the RSV user (\nrsv\n by default) to execute other commands, e.g. run, enable and disable probes, or to turn RSV on and off.\n\n\nViewing RSV jobs\n\n\nrsv-control provides two different views: viewing the \ndesired\n state and viewing the current \nactual\n state.\n\n\n\n\nDesired = what metrics and consumers will start the next time RSV is started\n\n\nActual = what metrics and consumers are currently running\n\n\n\n\nDesired state\n\n\nTo view the desired state, use the \n--list\n (\n-l\n for short) flag. This will create one table for each host showing the metrics that are enabled to run against that host.\n\n\nroot@host#\n rsv-control --list\n\n\nMetrics enabled for host: osgitb1.nhn.ou.edu              | Service\n\n\n----------------------------------------------------------+--------------------\n\n\norg.osg.batch.jobmanager-default-status                   | OSG-CE\n\n\norg.osg.batch.jobmanagers-available                       | OSG-CE\n\n\norg.osg.certificates.cacert-expiry                        | OSG-CE\n\n\norg.osg.certificates.crl-expiry                           | OSG-CE\n\n\norg.osg.general.osg-directories-CE-permissions            | OSG-CE\n\n\norg.osg.general.osg-version                               | OSG-CE\n\n\norg.osg.general.ping-host                                 | OSG-CE\n\n\norg.osg.general.vdt-version                               | OSG-CE\n\n\norg.osg.general.vo-supported                              | OSG-CE\n\n\norg.osg.globus.gram-authentication                        | OSG-CE\n\n\norg.osg.globus.gridftp-simple                             | OSG-GridFTP\n\n\norg.osg.gratia.condor                                     | OSG-CE\n\n\norg.osg.gratia.metric                                     | OSG-CE\n\n\n\n\nMetrics enabled for host: osg-edu.cs.wisc.edu:10443       | Service\n\n\n----------------------------------------------------------+--------------------\n\n\norg.osg.srm.srmcp-readwrite                               | OSG-SRM\n\n\norg.osg.srm.srmping                                       | OSG-SRM\n\n\n\n\n\n\nOther options:\n\n\n\n\nTo view all installed metrics use the \n--all\n (\n-a\n) flag along with \n--list\n. This will print an extra table showing metrics that are disabled on all hosts.\n\n\nIf you are having problems with the output being truncated, try the \n--wide\n (\n-w\n) flag.\n\n\n\n\nActual state\n\n\nTo view the current, running state of RSV jobs, use the \n--job-list\n flag (\n-j\n for short). This will show all metrics and consumers running in RSV. (It queries the underlying Condor Cron system that we use to run the metrics).\n\n\nroot@host#\n rsv-control --job-list\n\n\nHostname: osg-edu.cs.wisc.edu\n\n\n     ID OWNER      ST NEXT RUN TIME   METRIC\n\n\n  154.0 rsvuser    I  11-19 12:15     org.osg.certificates.cacert-expiry\n\n\n  155.0 rsvuser    R  11-19 11:23     org.osg.gratia.metric\n\n\n  156.0 rsvuser    I  11-19 18:47     org.osg.general.vdt-version\n\n\n  157.0 rsvuser    I  11-19 12:30     org.osg.certificates.crl-expiry\n\n\n  158.0 rsvuser    I  11-19 11:31     org.osg.globus.gram-authentication\n\n\n  159.0 rsvuser    I  11-19 11:41     org.osg.general.osg-version\n\n\n  160.0 rsvuser    R  11-19 11:25     org.osg.batch.jobmanager-default-status\n\n\n  161.0 rsvuser    I  11-20 04:59     org.osg.batch.jobmanagers-available\n\n\n  162.0 rsvuser    I  11-19 11:37     org.osg.general.osg-directories-CE-permissions\n\n\n  163.0 rsvuser    I  11-19 12:08     org.osg.globus.gridftp-simple\n\n\n  164.0 rsvuser    I  11-19 12:09     org.osg.gratia.condor\n\n\n  165.0 rsvuser    R  11-19 11:27     org.osg.general.ping-host\n\n\n  166.0 rsvuser    I  11-19 18:47     org.osg.general.vo-supported\n\n\n\nHostname: osg-edu.cs.wisc.edu:10443\n\n\n     ID OWNER      ST NEXT RUN TIME   METRIC\n\n\n  113.0 rsvuser    I  11-19 11:33     org.osg.srm.srmping\n\n\n  114.0 rsvuser    R  11-19 11:28     org.osg.srm.srmcp-readwrite\n\n\n\n     ID OWNER      ST CONSUMER\n\n\n  198.0 rsvuser    R  html-consumer\n\n\n  199.0 rsvuser    R  gratia-consumer\n\n\n\n\n\n\nThe ST field indicates the current job status:\n\n\n\n\nR = the metric is currently running\n\n\nI = the metric is idle and will be run at the next scheduled interval\n\n\nAny other letter may indicate a problem\n\n\nConsumers will always appear to be running even though they will only run once every five minutes.\n\n\n\n\nRunning a metric\n\n\nrsv-control\n can be used to run metrics one time against a host. This can be useful for:\n\n\n\n\nupdating the status of a metric that had a problem instead of waiting until the next scheduled run time\n\n\ntesting a metric against a host before deciding whether to enable it\n\n\n\n\nNote that \nthe record for each run will be published to all active consumers\n. That is, it will be published to Gratia or will show up on your local web page, if you have those enabled.\n\n\nSimplest test\n\n\nUse the \n--run\n (\n-r\n) flag. You must also provide the \n--host\n flag. The syntax is:\n\n\nrsv-control --run --host \nHOST\n \nMETRIC\n [ \nMETRIC2\n ...]\n\n\nwhere \nMETRIC\n is the full metric name (e.g. \norg.osg.general.osg-version\n). You can get the metric names from the \n--list\n output.\n\n\nroot@host#\n rsv-control --run \n\\\n\n    --host osg-edu.cs.wisc.edu org.osg.general.osg-version\n\n\nRunning metric org.osg.general.osg-version:\n\n\n\nmetricName: org.osg.general.osg-version\n\n\nmetricType: status\n\n\ntimestamp: 2010-11-19 11:40:19 CST\n\n\nmetricStatus: OK\n\n\nserviceType: OSG-CE\n\n\nserviceURI: osg-edu.cs.wisc.edu\n\n\ngatheredAt: vdt-itb.cs.wisc.edu\n\n\nsummaryData: OK\n\n\ndetailsData: OSG 1.2.15\n\n\nEOT\n\n\n\n\n\n\nNote the \nmetricStatus\n in the example above: that's where you can see if it was successful or not. In this case, it was successful, because it printed OK.\n\n\nYou may run multiple metrics against a single host by specifying multiple metrics to \nrsv-control\n.\n\n\nIn order to run metrics against multiple hosts you must run \nrsv-control\n multiple times, once for each host.\n\n\nRunning all enabled metrics\n\n\nWhen RSV is first installed it can take up to a day for each enabled metric to run once. A new option is provided to force each metric to run immediately, for all hosts. Use the \n--all-enabled\n flag along with \n--run\n. With this option it is not necessary to specify a host - all enabled metrics for all configured hosts will be run (in fact, if you do specify a host it will be ignored).\n\n\nroot@host#\n rsv-control -r --all-enabled\n\n\nRunning metric org.osg.certificates.cacert-expiry (1 of 15)\n\n\n\nmetricName: org.osg.certificates.cacert-expiry\n\n\nmetricType: status\n\n\ntimestamp: 2010-11-19 13:44:08 CST\n\n\nmetricStatus: OK\n\n\nserviceType: OSG-CE\n\n\nserviceURI: osg-edu.cs.wisc.edu\n\n\ngatheredAt: vdt-itb.cs.wisc.edu\n\n\nsummaryData: OK\n\n\ndetailsData: Security Probe Version: 1.1\n\n\nOK: CAs are in sync with OSG distribution\n\n\nEOT\n\n\n\n\n...\n\n\n\n\n\n\nPassing extra configuration\n\n\nIf you want to pass extra configuration when running a metric without editing its configuration file you can make an INI-formatted file and pass it on the command line. For example, you can make a file like this for the \norg.osg.srm.srmclient-ping\n metric (tmp-srm.ini):\n\n\n[org.osg.srm.srmclient-ping args]\n\n\nsrm-destination-dir\n=\n/srmcache/~\n\n\nsrm-webservice-path\n=\nsrm/v2/server\n\n\n\n\n\n\nThen use the \n--extra-config-file\n parameter and pass the path to the INI file:\n\n\nroot@host#\n rsv-control -r --extra-config-file tmp-srm.ini \n\\\n\n    --host osg-edu.cs.wisc.edu:10443 org.osg.srm.srmclient-ping\n\n\nRunning metric org.osg.srm.srmclient-ping:\n\n\n\nmetricName: org.osg.srm.srmclient-ping\n\n\nmetricType: status\n\n\ntimestamp: 2010-11-19 14:12:35 CST\n\n\nmetricStatus: OK\n\n\nserviceType: OSG-SRM\n\n\nserviceURI: osg-edu.cs.wisc.edu:10443\n\n\ngatheredAt: vdt-itb.cs.wisc.edu\n\n\nsummaryData: OK\n\n\ndetailsData: SRM server running on osg-edu.cs.wisc.edu is alive and responding to the srmping command.\n\n\n.  Details: Storage Resource Manager (SRM) Client version 2.1.5-16\n\n\nCopyright (c) 2002-2009 Fermi National Accelerator Laboratory\n\n\n\n...\n\n\n\n\n\n\nEnabling and disabling metrics and consumers\n\n\nMetrics and consumers can be enabled or disabled by \nrsv-control\n using the \n--enable\n and \n--disable\n flags. Note that \"enable\" and \"disable\" are desired states (this is similar to \nosg-control\n). After enabling a metric you should turn it on if you want it to be running immediately. After disabling a metric that is running, you should still turn it off (a message will print after each of these actions to remind you of this behavior).\n\n\nEnabling\n\n\nThe syntax for enabling metrics looks similar to the syntax for running metrics:\n\n\nrsv-control --enable --host \nHOST\n \nMETRIC\n [ \nMETRIC2\n ...]\n\n\nYou must provide a host to enable the metric against (in order to enable a metric on multiple hosts you must run \nrsv-control\n once per host).\n\n\nroot@host#\n rsv-control --enable \n\\\n\n    --host osg-edu.cs.wisc.edu org.osg.gip.consistency\n\nEnabling metric \norg.osg.gip.consistency\n for host \nosg-edu.cs.wisc.edu\n\n\n\nOne or more metrics have been enabled and will be started the next time RSV is started.  To turn them on immediately run \nrsv-control --on\n.\n\n\n\n\n\n\nConsumers do not run against a specific host, they process records for all hosts. When enabling consumers a host is not required (if a host is passed it will be ignored).\n\n\nroot@host#\n rsv-control --enable nagios-consumer\n\nEnabling consumer nagios-consumer\n\n\n\n\n\n\nDisabling\n\n\nThe syntax for disabling metrics looks similar to the syntax for running metrics:\n\n\nrsv-control --disable --host \nHOST\n \nMETRIC\n [ \nMETRIC2\n ...]\n\n\nYou must provide a host to disable the metric against (in order to disable a metric on multiple hosts you must run \nrsv-control\n once per host).\n\n\nroot@host#\n rsv-control --disable \n\\\n\n    --host vdt-itb.cs.wisc.edu org.osg.local.containercert-expiry\n\nDisabling metric \norg.osg.local.containercert-expiry\n for host \nvdt-itb.cs.wisc.edu\n\n\n\nOne or more metrics have been disabled and will not start the next time RSV is started.  You may still need to turn them off if they are currently running.\n\n\n\n\n\n\nConsumers do not run against a specific host, they process records for all hosts. When disabling consumers a host is not required (if a host is passed it will be ignored).\n\n\nroot@host#\n rsv-control --disable html-consumer gratia-consumer\n\nDisabling consumer html-consumer\n\n\nDisabling consumer gratia-consumer\n\n\n   Consumer already disabled\n\n\n\n\n\n\nMetrics and consumers can both be listed in the same disable command.\n\n\nTroubleshooting\n\n\nGetting more information from rsv-control\n\n\nThe first step to getting more information is to run \nrsv-control\n with more verbosity. Use the \n--verbose\n (\n-v\n) flag. This flag can be used with any of rsv-control's abilities (run, enable, list, etc). The verbosity levels are:\n\n\n\n\n0 = print nothing\n\n\n1 = print warnings and errors along with usual output of command being run (1 is the default level)\n\n\n2 = adds informational messages\n\n\n3 = full debugging output\n\n\n\n\nUsing the RSV verify tool\n\n\nThe \n--verify\n flag will run some basic checks for your RSV installation:\n\n\nroot@host#\n rsv-control --verify\n\nTesting if Condor-Cron is running...\n\n\nOK\n\n\n\nTesting if metrics are running...\n\n\nOK (98 running metrics)\n\n\n\nTesting if consumers are running...\n\n\nOK (1 running consumers)\n\n\n\nChecking which consumers are configured...\n\n\nThe following consumers are enabled: html-consumer\n\n\nWARNING: The gratia-consumer is not enabled.  This indicates that your\n\n\n         resource is not reporting to OSG.\n\n\n\n\n\n\nThis tool is still under development and it does only basic checks, but it is a good first step when debugging issues.\n\n\nRunning the RSV profiler\n\n\nRSV has a tool to collect information useful for troubleshooting into a tarball that can be shared with the developers and support staff.\nTo use it:\n\n\nroot@host#\n rsv-control --profile\n\nRunning the rsv-profiler...\n\n\nOSG-RSV Profiler\n\n\nAnalyzing...\n\n\nMaking tarball (rsv-profiler.tar.gz)\n\n\n\n\n\n\n\n\nNote\n\n\nIf you are getting assistance via the trouble ticket system, you must add a \n.txt\n extension to the tarball so it can be uploaded.\n\n\nroot@host#\n mv rsv-profiler.tar.gz rsv-profiler.tar.gz.txt", 
            "title": "Manage RSV via rsv-control"
        }, 
        {
            "location": "/monitoring/rsv-control/#using-rsv-control", 
            "text": "", 
            "title": "Using rsv-control"
        }, 
        {
            "location": "/monitoring/rsv-control/#overview", 
            "text": "This document is for System Administrators. It details the usage of the  rsv-control  command for enabling, disabling, testing and running RSV probes.  rsv-control  provides an interface to many RSV tasks.  rsv-control  can view RSV jobs, run metrics, enable or disable metrics and consumers, and allow advanced configuration.   Warning  rsv-control  can be used to configure RSV as described here and in  the advanced configuration document . Most site admins will be able to configure RSV by editing  /etc/osg/config.d/30-rsv.ini  and running  osg-configure  as described in the  installation doc .   Using  rsv-control  to configure is for advanced RSV use including enabling non-default metrics. Admins who don't use  rsv-control  for configuration can still use it to view their RSV jobs, run RSV tests, and help debug RSV problems. Anyone can view the jobs, but you must be root or the RSV user ( rsv  by default) to execute other commands, e.g. run, enable and disable probes, or to turn RSV on and off.", 
            "title": "Overview"
        }, 
        {
            "location": "/monitoring/rsv-control/#viewing-rsv-jobs", 
            "text": "rsv-control provides two different views: viewing the  desired  state and viewing the current  actual  state.   Desired = what metrics and consumers will start the next time RSV is started  Actual = what metrics and consumers are currently running", 
            "title": "Viewing RSV jobs"
        }, 
        {
            "location": "/monitoring/rsv-control/#desired-state", 
            "text": "To view the desired state, use the  --list  ( -l  for short) flag. This will create one table for each host showing the metrics that are enabled to run against that host.  root@host#  rsv-control --list Metrics enabled for host: osgitb1.nhn.ou.edu              | Service  ----------------------------------------------------------+--------------------  org.osg.batch.jobmanager-default-status                   | OSG-CE  org.osg.batch.jobmanagers-available                       | OSG-CE  org.osg.certificates.cacert-expiry                        | OSG-CE  org.osg.certificates.crl-expiry                           | OSG-CE  org.osg.general.osg-directories-CE-permissions            | OSG-CE  org.osg.general.osg-version                               | OSG-CE  org.osg.general.ping-host                                 | OSG-CE  org.osg.general.vdt-version                               | OSG-CE  org.osg.general.vo-supported                              | OSG-CE  org.osg.globus.gram-authentication                        | OSG-CE  org.osg.globus.gridftp-simple                             | OSG-GridFTP  org.osg.gratia.condor                                     | OSG-CE  org.osg.gratia.metric                                     | OSG-CE  Metrics enabled for host: osg-edu.cs.wisc.edu:10443       | Service  ----------------------------------------------------------+--------------------  org.osg.srm.srmcp-readwrite                               | OSG-SRM  org.osg.srm.srmping                                       | OSG-SRM   Other options:   To view all installed metrics use the  --all  ( -a ) flag along with  --list . This will print an extra table showing metrics that are disabled on all hosts.  If you are having problems with the output being truncated, try the  --wide  ( -w ) flag.", 
            "title": "Desired state"
        }, 
        {
            "location": "/monitoring/rsv-control/#actual-state", 
            "text": "To view the current, running state of RSV jobs, use the  --job-list  flag ( -j  for short). This will show all metrics and consumers running in RSV. (It queries the underlying Condor Cron system that we use to run the metrics).  root@host#  rsv-control --job-list Hostname: osg-edu.cs.wisc.edu       ID OWNER      ST NEXT RUN TIME   METRIC    154.0 rsvuser    I  11-19 12:15     org.osg.certificates.cacert-expiry    155.0 rsvuser    R  11-19 11:23     org.osg.gratia.metric    156.0 rsvuser    I  11-19 18:47     org.osg.general.vdt-version    157.0 rsvuser    I  11-19 12:30     org.osg.certificates.crl-expiry    158.0 rsvuser    I  11-19 11:31     org.osg.globus.gram-authentication    159.0 rsvuser    I  11-19 11:41     org.osg.general.osg-version    160.0 rsvuser    R  11-19 11:25     org.osg.batch.jobmanager-default-status    161.0 rsvuser    I  11-20 04:59     org.osg.batch.jobmanagers-available    162.0 rsvuser    I  11-19 11:37     org.osg.general.osg-directories-CE-permissions    163.0 rsvuser    I  11-19 12:08     org.osg.globus.gridftp-simple    164.0 rsvuser    I  11-19 12:09     org.osg.gratia.condor    165.0 rsvuser    R  11-19 11:27     org.osg.general.ping-host    166.0 rsvuser    I  11-19 18:47     org.osg.general.vo-supported  Hostname: osg-edu.cs.wisc.edu:10443       ID OWNER      ST NEXT RUN TIME   METRIC    113.0 rsvuser    I  11-19 11:33     org.osg.srm.srmping    114.0 rsvuser    R  11-19 11:28     org.osg.srm.srmcp-readwrite       ID OWNER      ST CONSUMER    198.0 rsvuser    R  html-consumer    199.0 rsvuser    R  gratia-consumer   The ST field indicates the current job status:   R = the metric is currently running  I = the metric is idle and will be run at the next scheduled interval  Any other letter may indicate a problem  Consumers will always appear to be running even though they will only run once every five minutes.", 
            "title": "Actual state"
        }, 
        {
            "location": "/monitoring/rsv-control/#running-a-metric", 
            "text": "rsv-control  can be used to run metrics one time against a host. This can be useful for:   updating the status of a metric that had a problem instead of waiting until the next scheduled run time  testing a metric against a host before deciding whether to enable it   Note that  the record for each run will be published to all active consumers . That is, it will be published to Gratia or will show up on your local web page, if you have those enabled.", 
            "title": "Running a metric"
        }, 
        {
            "location": "/monitoring/rsv-control/#simplest-test", 
            "text": "Use the  --run  ( -r ) flag. You must also provide the  --host  flag. The syntax is:  rsv-control --run --host  HOST   METRIC  [  METRIC2  ...]  where  METRIC  is the full metric name (e.g.  org.osg.general.osg-version ). You can get the metric names from the  --list  output.  root@host#  rsv-control --run  \\ \n    --host osg-edu.cs.wisc.edu org.osg.general.osg-version Running metric org.osg.general.osg-version:  metricName: org.osg.general.osg-version  metricType: status  timestamp: 2010-11-19 11:40:19 CST  metricStatus: OK  serviceType: OSG-CE  serviceURI: osg-edu.cs.wisc.edu  gatheredAt: vdt-itb.cs.wisc.edu  summaryData: OK  detailsData: OSG 1.2.15  EOT   Note the  metricStatus  in the example above: that's where you can see if it was successful or not. In this case, it was successful, because it printed OK.  You may run multiple metrics against a single host by specifying multiple metrics to  rsv-control .  In order to run metrics against multiple hosts you must run  rsv-control  multiple times, once for each host.", 
            "title": "Simplest test"
        }, 
        {
            "location": "/monitoring/rsv-control/#running-all-enabled-metrics", 
            "text": "When RSV is first installed it can take up to a day for each enabled metric to run once. A new option is provided to force each metric to run immediately, for all hosts. Use the  --all-enabled  flag along with  --run . With this option it is not necessary to specify a host - all enabled metrics for all configured hosts will be run (in fact, if you do specify a host it will be ignored).  root@host#  rsv-control -r --all-enabled Running metric org.osg.certificates.cacert-expiry (1 of 15)  metricName: org.osg.certificates.cacert-expiry  metricType: status  timestamp: 2010-11-19 13:44:08 CST  metricStatus: OK  serviceType: OSG-CE  serviceURI: osg-edu.cs.wisc.edu  gatheredAt: vdt-itb.cs.wisc.edu  summaryData: OK  detailsData: Security Probe Version: 1.1  OK: CAs are in sync with OSG distribution  EOT  ...", 
            "title": "Running all enabled metrics"
        }, 
        {
            "location": "/monitoring/rsv-control/#passing-extra-configuration", 
            "text": "If you want to pass extra configuration when running a metric without editing its configuration file you can make an INI-formatted file and pass it on the command line. For example, you can make a file like this for the  org.osg.srm.srmclient-ping  metric (tmp-srm.ini):  [org.osg.srm.srmclient-ping args]  srm-destination-dir = /srmcache/~  srm-webservice-path = srm/v2/server   Then use the  --extra-config-file  parameter and pass the path to the INI file:  root@host#  rsv-control -r --extra-config-file tmp-srm.ini  \\ \n    --host osg-edu.cs.wisc.edu:10443 org.osg.srm.srmclient-ping Running metric org.osg.srm.srmclient-ping:  metricName: org.osg.srm.srmclient-ping  metricType: status  timestamp: 2010-11-19 14:12:35 CST  metricStatus: OK  serviceType: OSG-SRM  serviceURI: osg-edu.cs.wisc.edu:10443  gatheredAt: vdt-itb.cs.wisc.edu  summaryData: OK  detailsData: SRM server running on osg-edu.cs.wisc.edu is alive and responding to the srmping command.  .  Details: Storage Resource Manager (SRM) Client version 2.1.5-16  Copyright (c) 2002-2009 Fermi National Accelerator Laboratory  ...", 
            "title": "Passing extra configuration"
        }, 
        {
            "location": "/monitoring/rsv-control/#enabling-and-disabling-metrics-and-consumers", 
            "text": "Metrics and consumers can be enabled or disabled by  rsv-control  using the  --enable  and  --disable  flags. Note that \"enable\" and \"disable\" are desired states (this is similar to  osg-control ). After enabling a metric you should turn it on if you want it to be running immediately. After disabling a metric that is running, you should still turn it off (a message will print after each of these actions to remind you of this behavior).", 
            "title": "Enabling and disabling metrics and consumers"
        }, 
        {
            "location": "/monitoring/rsv-control/#enabling", 
            "text": "The syntax for enabling metrics looks similar to the syntax for running metrics:  rsv-control --enable --host  HOST   METRIC  [  METRIC2  ...]  You must provide a host to enable the metric against (in order to enable a metric on multiple hosts you must run  rsv-control  once per host).  root@host#  rsv-control --enable  \\ \n    --host osg-edu.cs.wisc.edu org.osg.gip.consistency Enabling metric  org.osg.gip.consistency  for host  osg-edu.cs.wisc.edu  One or more metrics have been enabled and will be started the next time RSV is started.  To turn them on immediately run  rsv-control --on .   Consumers do not run against a specific host, they process records for all hosts. When enabling consumers a host is not required (if a host is passed it will be ignored).  root@host#  rsv-control --enable nagios-consumer Enabling consumer nagios-consumer", 
            "title": "Enabling"
        }, 
        {
            "location": "/monitoring/rsv-control/#disabling", 
            "text": "The syntax for disabling metrics looks similar to the syntax for running metrics:  rsv-control --disable --host  HOST   METRIC  [  METRIC2  ...]  You must provide a host to disable the metric against (in order to disable a metric on multiple hosts you must run  rsv-control  once per host).  root@host#  rsv-control --disable  \\ \n    --host vdt-itb.cs.wisc.edu org.osg.local.containercert-expiry Disabling metric  org.osg.local.containercert-expiry  for host  vdt-itb.cs.wisc.edu  One or more metrics have been disabled and will not start the next time RSV is started.  You may still need to turn them off if they are currently running.   Consumers do not run against a specific host, they process records for all hosts. When disabling consumers a host is not required (if a host is passed it will be ignored).  root@host#  rsv-control --disable html-consumer gratia-consumer Disabling consumer html-consumer  Disabling consumer gratia-consumer     Consumer already disabled   Metrics and consumers can both be listed in the same disable command.", 
            "title": "Disabling"
        }, 
        {
            "location": "/monitoring/rsv-control/#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/monitoring/rsv-control/#getting-more-information-from-rsv-control", 
            "text": "The first step to getting more information is to run  rsv-control  with more verbosity. Use the  --verbose  ( -v ) flag. This flag can be used with any of rsv-control's abilities (run, enable, list, etc). The verbosity levels are:   0 = print nothing  1 = print warnings and errors along with usual output of command being run (1 is the default level)  2 = adds informational messages  3 = full debugging output", 
            "title": "Getting more information from rsv-control"
        }, 
        {
            "location": "/monitoring/rsv-control/#using-the-rsv-verify-tool", 
            "text": "The  --verify  flag will run some basic checks for your RSV installation:  root@host#  rsv-control --verify Testing if Condor-Cron is running...  OK  Testing if metrics are running...  OK (98 running metrics)  Testing if consumers are running...  OK (1 running consumers)  Checking which consumers are configured...  The following consumers are enabled: html-consumer  WARNING: The gratia-consumer is not enabled.  This indicates that your           resource is not reporting to OSG.   This tool is still under development and it does only basic checks, but it is a good first step when debugging issues.", 
            "title": "Using the RSV verify tool"
        }, 
        {
            "location": "/monitoring/rsv-control/#running-the-rsv-profiler", 
            "text": "RSV has a tool to collect information useful for troubleshooting into a tarball that can be shared with the developers and support staff.\nTo use it:  root@host#  rsv-control --profile Running the rsv-profiler...  OSG-RSV Profiler  Analyzing...  Making tarball (rsv-profiler.tar.gz)    Note  If you are getting assistance via the trouble ticket system, you must add a  .txt  extension to the tarball so it can be uploaded.  root@host#  mv rsv-profiler.tar.gz rsv-profiler.tar.gz.txt", 
            "title": "Running the RSV profiler"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/", 
            "text": "Installing and Using the RSV GlideinWMS Tester\n\n\nAbout This Guide\n\n\nThe RSV GlideinWMS Tester (or \nTester\n, in this document) is a tool that a VO front-end administrator can use to test remote sites for the ability to run the VO\u2019s jobs. It is particularly useful when setting up a VO for the first time or when changing the sites at which a VO\u2019s jobs can run. For a site to pass the test, it must successfully run a simple test job via the normal GlideinWMS mechanisms, in much the same way as a real VO job.\n\n\nUse this page to learn how to install, configure, and use the Tester for your VO front-end.\n\n\nBefore Starting\n\n\nBefore starting the installation process, consider the following points (consulting \nthe Reference section below\n as needed):\n\n\n\n\nSoftware:\n You must have \na GlideinWMS Front-end\n installed\n\n\nConfiguration:\n The GlideinWMS Front-end must be configured (a) \nto have at least one group that matches pilots to sites using DESIRED_SITES\n, and (b) \nto support the is_itb user job attribute\n\n\nHost choice:\n The Tester should be installed on its own host; a small Virtual Machine (VM) is ideal\n\n\nService certificate:\n The Tester requires a host certificate at \n/etc/grid-security/hostcert.pem\n and an accompanying key at \n/etc/grid-security/hostkey.pem\n\n\nNetwork ports:\n Test jobs must be able to contact the tester using the HTCondor Shared Port on port 9615 (TCP), and you must be able to contact a web server on port 80 (TCP) to view test results.\n\n\n\n\nInstalling the Tester\n\n\nThe Tester software takes advantage of several other OSG software components, so the installation will also include OSG\u2019s site validation system (RSV), HTCondor, and the GlideinWMS pilot submission software.\n\n\nroot@host #\n yum install rsv-gwms-tester\n\n\n\n\n\nConfiguring the Tester\n\n\nBefore you use the Tester, there are some one-time configuration steps to complete, one set on your GlideinWMS Front-end Central Manager host and one set on the Tester host.\n\n\nConfiguring the GlideinWMS Front-end Central Manager\n\n\nComplete these steps \non your GlideinWMS Front-end Central Manager host\n:\n\n\n\n\n\n\nAuthorize the Tester host to connect to your Central Manager:\n\n\nroot@host #\n glidecondor_addDN -allow-others -daemon \nCOMMENT\n \nTESTER_DN\n condor\n\n\n\n\n\nWhere \nCOMMENT\n is a human-readable label for the Tester host (e.g., \u201cRSV GWMS Tester at myhost\u201d), and \nTESTER_DN\n is the Distinguished Name (DN) of the host certificate of your Tester host. Most likely, you will need to quote both of these values to protect them from the shell. For example:\n\n\nroot@host #\n glidecondor_addDN -allow-others -daemon \nRSV GWMS Tester on Fermicloud\n \n/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=fermicloud357.fnal.gov\n condor\n\n\n\n\n\n\n\n\n\nRestart HTCondor to apply the changes\n\n\nOn \nEL\u00a06\n systems:\n\n\nroot@host #\n service condor restart\n\n\n\n\n\nOn \nEL\u00a07\n systems:\n\n\nroot@host #\n systemctl restart condor\n\n\n\n\n\n\n\n\n\nAdd the new Tester to your GlideinWMS front-end configuration.\n   Edit the file \n/etc/gwms-frontend/frontend.xml\n and add a line as follows within the \nschedds\n element\n\n\nschedd DN=\nTESTER_DN\n fullname=\nTESTER_HOSTNAME\n\n\n\n\n\n\nWhere \nTESTER_DN\n is the Distinguished Name (DN) of the host certificate of your Tester host (as above), and \nTESTER_HOSTNAME\n is the fully qualified hostname of the Tester host. For example:\n\n\nschedd DN=\n/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=fermicloud357.fnal.gov\n fullname=\nfermicloud357.fnal.gov\n\n\n\n\n\n\nReconfigure your GlideinWMS front-end to apply the changes:\n\n\nroot@host #\n service gwms-frontend reconfig\n\n\n\n\n\n\n\n\n\nConfiguring the Tester host\n\n\nComplete the following steps \non your Tester host\n:\n\n\n\n\n\n\nConfigure the Tester for the VOs that your Front-end supports\n\n\nEdit the file \n/etc/rsv/metrics/org.osg.local-gfactory-site-querying-local.conf\n. The \nconstraint\n line is an HTCondor ClassAd expression containing one \nstringListMember\n function per VO that your Front-end supports. If there is more than one VO, the function invocations are joined by the \u201clogical or\u201d operator, \n||\n. Edit the \nconstraint\n line for your Front-end.\n\n\nFor example, for a single VO named \nFoo\n, the line would be:\n\n\nconstraint = stringListMember(\nFoo\n, GLIDEIN_Supported_VOs)\n\n\n\n\n\nFor two VOs named \nFoo\n and \nBar\n, the line would be:\n\n\nconstraint = stringListMember(\nFoo\n, GLIDEIN_Supported_VOs) || stringListMember(\nBar\n, GLIDEIN_Supported_VOs)\n\n\n\n\n\nDo not change the other settings in this file, unless you have clear and specific reasons to do so.\n\n\n\n\n\n\nAuthorize the central manager of your Front-end to connect to the tester host:\n\n\nroot@host #\n glidecondor_addDN -allow-others -daemon \nCOMMENT\n \nCENTRAL_MGR\n condor\n\n\n\n\n\nWhere \nCOMMENT\n is a human-readable identifier for the Central Manager, and \nCENTRAL_MGR\n is the Distinguished Name (DN) of the host certificate of your GlideinWMS Front-end\u2019s Central Manager host. Most likely, you will need to quote both of these values to protect them from the shell. For example:\n\n\nroot@host #\n glidecondor_addDN -allow-others -daemon \nUCSD central manager DN\n \n/DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=osg-ligo-1.t2.ucsd.edu\n condor\n\n\n\n\n\n\n\n\n\nConfigure the special HTCondor-RSV instance with your host IP address.\n\n\nCreate the file \n/etc/condor/config.d/98_public_interface.config\n with this content:\n\n\nNETWORK_INTERFACE = \nADDRESS\n\nCONDOR_HOST = \nCENTRAL_MGR\n\n\n\n\n\n\nWhere \nADDRESS\n is the IP address of your Tester host, and \nCENTRAL_MGR\n is the hostname of your GlideinWMS Front-end Central Manager.\n\n\n\n\n\n\nEnable the Tester\u2019s RSV probe:\n\n\nroot@host #\n rsv-control --enable org.osg.local-gfactory-site-querying-local --host localhost\n\n\n\n\n\n\n\n\n\nUsing the Tester\n\n\nThere are at least two aspects of using the Tester:\n\n\n\n\nManaging the services that are associated with the Tester software\n\n\nViewing results from the Tester\n\n\n\n\nManaging Tester services\n\n\nBecause the Tester is built on other OSG software, there are a number of services in your installation. The specific services are:\n\n\n\n\n\n\n\n\nSoftware\n\n\nService name\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nApache HTTP Server\n\n\nhttpd\n\n\nWeb server for results\n\n\n\n\n\n\nHTCondor-Cron\n\n\ncondor-cron\n\n\ncron-like jobs in HTCondor\n\n\n\n\n\n\nRSV\n\n\nrsv\n\n\nOSG site validator\n\n\n\n\n\n\n\n\nViewing Tester results\n\n\nOnce the Tester RSV probe is enabled and active, and the services listed above have been started, there are two kinds of RSV probes that run periodically:\n\n\n\n\nOne probe asks the GlideinWMS factory for the up-to-date list of sites supported by your VO(s)\u00a0\u2014 runs every 30 minutes\n\n\nOne probe submits and monitors one test job to each site supported by your VO(s)\u00a0\u2014 run every 60 minutes\n\n\n\n\nYou can view the latest results of both probe types on an RSV results web page, or you can manually run the first probe to see the full list of sites.\n\n\nViewing RSV results online\n\n\nTo see the latest results, access \nhttps://\nHOSTNAME\n/rsv/\n (where \nHOSTNAME\n is the name of your Tester host).\n\n\n\n\nThere should be one result row per site supported by your VO(s), using the \u201corg.osg.general.dummy-vanilla-probe\u201d probe (aka \nmetric\n)\n\n\nThere should be exactly one result row for the probe that fetches the list of sites, which is the \u201corg.osg.local-gfactory-site-querying-local\u201d probe (aka \nmetric\n)\n\n\nThere is a legend for the background colors at the end of the page\n\n\n\n\nIdeally, each site supported by your VO(s) should be shown with a green background, which indicates that a Tester job ran at that site recently and successfully. There may be transient failures but if you notice a site in the failed state over multiple days, contact OSG Factory Operations (\n) about the failing site, including a link to your Tester RSV results page.\n\n\nTo see detailed information from each probe, click on the probe name in the Metric column.\n\n\nTo see the list of sites that are supported by your VO(s) and are being tested, click the \u201corg.osg.local-gfactory-site-querying-local\u201d link at the bottom of the list of probes. You can also run the probe manually, as described next.\n\n\nListing supported sites manually\n\n\nTo manually run the probe that fetches the list of sites supported by your VO(s), run the following command on your Tester host:\n\n\nroot@host #\n rsv-control --run org.osg.local-gfactory-site-querying-local --host localhost\n\n\n\n\n\nThe probe produces many lines of output, some of which are just about the probe execution itself. But look for lines like this:\n\n\nMSG: Updating configuration for host \nUCSD\n\n\n\n\n\n\nThe highlighted name is the site name, and there should be one such line per site supported by your VO(s).\n\n\nTroubleshooting RSV-GWMS-Tester\n\n\nYou can find more information on troubleshooting in the \nRSV troubleshooting section\n\n\nLogs and configuration:\n\n\n\n\n\n\n\n\nFile Description\n\n\nLocation\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nCondor Cron log files\n\n\n/var/log/condor-cron\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFile Description\n\n\nLocation\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nMetric configuration\n\n\n/etc/rsv/metrics/org.osg.local-gfactory-site-querying-local.conf\n\n\nTo change arguments and environment\n\n\n\n\n\n\n\n\nGetting Help\n\n\nTo get assistance, please use the \nthis page\n.\n\n\nReference\n\n\nCertificates\n\n\n\n\n\n\n\n\nCertificate\n\n\nUser that owns certificate\n\n\nPath to certificate\n\n\n\n\n\n\n\n\n\n\nHost certificate\n\n\nroot\n\n\n/etc/grid-security/hostcert.pem\n\n\n\n\n\n\nHost key\n\n\nroot\n\n\n/etc/grid-security/hostkey.pem\n\n\n\n\n\n\n\n\nFind instructions to request a host certificate \nhere\n.", 
            "title": "RSV GlideinWMS Tester"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#installing-and-using-the-rsv-glideinwms-tester", 
            "text": "", 
            "title": "Installing and Using the RSV GlideinWMS Tester"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#about-this-guide", 
            "text": "The RSV GlideinWMS Tester (or  Tester , in this document) is a tool that a VO front-end administrator can use to test remote sites for the ability to run the VO\u2019s jobs. It is particularly useful when setting up a VO for the first time or when changing the sites at which a VO\u2019s jobs can run. For a site to pass the test, it must successfully run a simple test job via the normal GlideinWMS mechanisms, in much the same way as a real VO job.  Use this page to learn how to install, configure, and use the Tester for your VO front-end.", 
            "title": "About This Guide"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#before-starting", 
            "text": "Before starting the installation process, consider the following points (consulting  the Reference section below  as needed):   Software:  You must have  a GlideinWMS Front-end  installed  Configuration:  The GlideinWMS Front-end must be configured (a)  to have at least one group that matches pilots to sites using DESIRED_SITES , and (b)  to support the is_itb user job attribute  Host choice:  The Tester should be installed on its own host; a small Virtual Machine (VM) is ideal  Service certificate:  The Tester requires a host certificate at  /etc/grid-security/hostcert.pem  and an accompanying key at  /etc/grid-security/hostkey.pem  Network ports:  Test jobs must be able to contact the tester using the HTCondor Shared Port on port 9615 (TCP), and you must be able to contact a web server on port 80 (TCP) to view test results.", 
            "title": "Before Starting"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#installing-the-tester", 
            "text": "The Tester software takes advantage of several other OSG software components, so the installation will also include OSG\u2019s site validation system (RSV), HTCondor, and the GlideinWMS pilot submission software.  root@host #  yum install rsv-gwms-tester", 
            "title": "Installing the Tester"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#configuring-the-tester", 
            "text": "Before you use the Tester, there are some one-time configuration steps to complete, one set on your GlideinWMS Front-end Central Manager host and one set on the Tester host.", 
            "title": "Configuring the Tester"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#configuring-the-glideinwms-front-end-central-manager", 
            "text": "Complete these steps  on your GlideinWMS Front-end Central Manager host :    Authorize the Tester host to connect to your Central Manager:  root@host #  glidecondor_addDN -allow-others -daemon  COMMENT   TESTER_DN  condor  Where  COMMENT  is a human-readable label for the Tester host (e.g., \u201cRSV GWMS Tester at myhost\u201d), and  TESTER_DN  is the Distinguished Name (DN) of the host certificate of your Tester host. Most likely, you will need to quote both of these values to protect them from the shell. For example:  root@host #  glidecondor_addDN -allow-others -daemon  RSV GWMS Tester on Fermicloud   /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=fermicloud357.fnal.gov  condor    Restart HTCondor to apply the changes  On  EL\u00a06  systems:  root@host #  service condor restart  On  EL\u00a07  systems:  root@host #  systemctl restart condor    Add the new Tester to your GlideinWMS front-end configuration.\n   Edit the file  /etc/gwms-frontend/frontend.xml  and add a line as follows within the  schedds  element  schedd DN= TESTER_DN  fullname= TESTER_HOSTNAME   Where  TESTER_DN  is the Distinguished Name (DN) of the host certificate of your Tester host (as above), and  TESTER_HOSTNAME  is the fully qualified hostname of the Tester host. For example:  schedd DN= /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=fermicloud357.fnal.gov  fullname= fermicloud357.fnal.gov   Reconfigure your GlideinWMS front-end to apply the changes:  root@host #  service gwms-frontend reconfig", 
            "title": "Configuring the GlideinWMS Front-end Central Manager"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#configuring-the-tester-host", 
            "text": "Complete the following steps  on your Tester host :    Configure the Tester for the VOs that your Front-end supports  Edit the file  /etc/rsv/metrics/org.osg.local-gfactory-site-querying-local.conf . The  constraint  line is an HTCondor ClassAd expression containing one  stringListMember  function per VO that your Front-end supports. If there is more than one VO, the function invocations are joined by the \u201clogical or\u201d operator,  || . Edit the  constraint  line for your Front-end.  For example, for a single VO named  Foo , the line would be:  constraint = stringListMember( Foo , GLIDEIN_Supported_VOs)  For two VOs named  Foo  and  Bar , the line would be:  constraint = stringListMember( Foo , GLIDEIN_Supported_VOs) || stringListMember( Bar , GLIDEIN_Supported_VOs)  Do not change the other settings in this file, unless you have clear and specific reasons to do so.    Authorize the central manager of your Front-end to connect to the tester host:  root@host #  glidecondor_addDN -allow-others -daemon  COMMENT   CENTRAL_MGR  condor  Where  COMMENT  is a human-readable identifier for the Central Manager, and  CENTRAL_MGR  is the Distinguished Name (DN) of the host certificate of your GlideinWMS Front-end\u2019s Central Manager host. Most likely, you will need to quote both of these values to protect them from the shell. For example:  root@host #  glidecondor_addDN -allow-others -daemon  UCSD central manager DN   /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=osg-ligo-1.t2.ucsd.edu  condor    Configure the special HTCondor-RSV instance with your host IP address.  Create the file  /etc/condor/config.d/98_public_interface.config  with this content:  NETWORK_INTERFACE =  ADDRESS \nCONDOR_HOST =  CENTRAL_MGR   Where  ADDRESS  is the IP address of your Tester host, and  CENTRAL_MGR  is the hostname of your GlideinWMS Front-end Central Manager.    Enable the Tester\u2019s RSV probe:  root@host #  rsv-control --enable org.osg.local-gfactory-site-querying-local --host localhost", 
            "title": "Configuring the Tester host"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#using-the-tester", 
            "text": "There are at least two aspects of using the Tester:   Managing the services that are associated with the Tester software  Viewing results from the Tester", 
            "title": "Using the Tester"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#managing-tester-services", 
            "text": "Because the Tester is built on other OSG software, there are a number of services in your installation. The specific services are:     Software  Service name  Notes      Apache HTTP Server  httpd  Web server for results    HTCondor-Cron  condor-cron  cron-like jobs in HTCondor    RSV  rsv  OSG site validator", 
            "title": "Managing Tester services"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#viewing-tester-results", 
            "text": "Once the Tester RSV probe is enabled and active, and the services listed above have been started, there are two kinds of RSV probes that run periodically:   One probe asks the GlideinWMS factory for the up-to-date list of sites supported by your VO(s)\u00a0\u2014 runs every 30 minutes  One probe submits and monitors one test job to each site supported by your VO(s)\u00a0\u2014 run every 60 minutes   You can view the latest results of both probe types on an RSV results web page, or you can manually run the first probe to see the full list of sites.", 
            "title": "Viewing Tester results"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#viewing-rsv-results-online", 
            "text": "To see the latest results, access  https:// HOSTNAME /rsv/  (where  HOSTNAME  is the name of your Tester host).   There should be one result row per site supported by your VO(s), using the \u201corg.osg.general.dummy-vanilla-probe\u201d probe (aka  metric )  There should be exactly one result row for the probe that fetches the list of sites, which is the \u201corg.osg.local-gfactory-site-querying-local\u201d probe (aka  metric )  There is a legend for the background colors at the end of the page   Ideally, each site supported by your VO(s) should be shown with a green background, which indicates that a Tester job ran at that site recently and successfully. There may be transient failures but if you notice a site in the failed state over multiple days, contact OSG Factory Operations ( ) about the failing site, including a link to your Tester RSV results page.  To see detailed information from each probe, click on the probe name in the Metric column.  To see the list of sites that are supported by your VO(s) and are being tested, click the \u201corg.osg.local-gfactory-site-querying-local\u201d link at the bottom of the list of probes. You can also run the probe manually, as described next.", 
            "title": "Viewing RSV results online"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#listing-supported-sites-manually", 
            "text": "To manually run the probe that fetches the list of sites supported by your VO(s), run the following command on your Tester host:  root@host #  rsv-control --run org.osg.local-gfactory-site-querying-local --host localhost  The probe produces many lines of output, some of which are just about the probe execution itself. But look for lines like this:  MSG: Updating configuration for host  UCSD   The highlighted name is the site name, and there should be one such line per site supported by your VO(s).", 
            "title": "Listing supported sites manually"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#troubleshooting-rsv-gwms-tester", 
            "text": "You can find more information on troubleshooting in the  RSV troubleshooting section  Logs and configuration:     File Description  Location  Comment      Condor Cron log files  /var/log/condor-cron         File Description  Location  Comment      Metric configuration  /etc/rsv/metrics/org.osg.local-gfactory-site-querying-local.conf  To change arguments and environment", 
            "title": "Troubleshooting RSV-GWMS-Tester"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#getting-help", 
            "text": "To get assistance, please use the  this page .", 
            "title": "Getting Help"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#reference", 
            "text": "", 
            "title": "Reference"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#certificates", 
            "text": "Certificate  User that owns certificate  Path to certificate      Host certificate  root  /etc/grid-security/hostcert.pem    Host key  root  /etc/grid-security/hostkey.pem     Find instructions to request a host certificate  here .", 
            "title": "Certificates"
        }, 
        {
            "location": "/data/bestman-install/", 
            "text": "Installing BeStMan\n\n\n\n\nWarning\n\n\nAs of the June 2017 release of OSG 3.4.0, this software is officially deprecated.  Support is scheduled to end as of May 2018.\n\n\n\n\nAbout this Document\n\n\nThis document explains how to install a BeStMan SRMv2 service. This procedure will guide one through the installation and configuration of a basic \nbestman2\n host with an underlying GridFTP server. This will allow the service to service requests via the SRM (Storage Resource Manager) protocol or the GridFTP protocol.\n\n\nInstalling BeStMan Storage Element\n\n\nThis procedure explains how to install the stand-alone BeStMan Storage Element server; \nsee below\n for notes on upgrading.  The service has the following components:\n\n\n\n\nBeStMan - provides load-balancing across GridFTP servers.\n\n\nGridFTP server - provides file transfer services using the GridFTP protocol.\n\n\nGratia gridftp transfer probe\n (optional) - provides transfer accounting information to the OSG.\n\n\n\n\nRequirements\n\n\nHost and OS\n\n\n\n\nYou need at least one node in order to install this service.\n\n\nThe OS must be in the \nsupported platforms\n list.\n\n\nThe \nOSG software repositories\n must be configured correctly.\n\n\nAll procedures in this document require \nroot\n privileges.\n\n\n\n\nUsers\n\n\nThis installation will create following users unless they are already created:\n\n\n\n\n\n\n\n\nUser\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nbestman\n\n\nUsed by Bestman SRM server\n\n\n\n\n\n\n\n\nFor full functionality, the \nbestman\n account will need limited \nsudo\n access to a few commands, described below.\n\n\nFor this package to function correctly, you will have to create the users needed for grid operation. Any user that can be authenticated should be created.  For grid-mapfile users, each line of the grid-mapfile is a certificate/user pair. Each user in this file should be created on the server. For GUMS sites, this means that each user that can be authenticated by GUMS should be created on the server.\n\n\nNote that these users must be kept in sync with the authentication method. For instance, if new users or rules are added in GUMS, then new users should also be added here.\n\n\nCertificates\n\n\nTwo certificates are needed for operation of this service.\n\n\n\n\n\n\n\n\nCertificate\n\n\nUser that owns certificate\n\n\nPath to certificate\n\n\n\n\n\n\n\n\n\n\nHost certificate\n\n\nroot\n\n\n/etc/grid-security/hostcert.pem\n and \n/etc/grid-security/hostkey.pem\n\n\n\n\n\n\nBestman service certificate\n\n\nbestman\n\n\n/etc/grid-security/bestman/bestmancert.pem\n and \n/etc/grid-security/bestman/bestmankey.pem\n\n\n\n\n\n\n\n\nFollowing the \ninstructions\n to request a service certificate.\n\n\nYou will also need a copy of CA certificates. Note that the \nosg-se-bestman\n package will automatically install a certificate package but will not necessarily pick the cert package you expect; see \nthe CA certificates\n documentation for more information.\n\n\nNetworking\n\n\nFor more details on overall firewall configuration, please see our \nfirewall documentation\n.\n\n\n\n\n\n\n\n\nService Name\n\n\nProtocol\n\n\nPort Number\n\n\nInbound\n\n\nOutbound\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nGridFTP data channels\n\n\ntcp\n\n\nGLOBUS_TCP_PORT_RANGE\n\n\nX\n\n\n\n\ncontiguous range of ports is necessary.\n\n\n\n\n\n\nGridFTP data channels\n\n\ntcp\n\n\nGLOBUS_TCP_SOURCE_RANGE\n\n\n\n\nX\n\n\ncontiguous range of ports is necessary.\n\n\n\n\n\n\nGridFTP control channel\n\n\ntcp\n\n\n2811\n\n\nX\n\n\n\n\n\n\n\n\n\n\nSRM\n\n\ntcp\n\n\n8443\n\n\nX\n\n\n\n\n\n\n\n\n\n\n\n\nEngineering Considerations\n\n\nPlease answer following questions before you proceed with installation and configuration of BeStMan storage element:\n\n\nQ. \nWhat authorization mechanism should I use?\n\n\nDecide between a \ngrid-mapfile\n or a \nGUMS\n server for authorization.  Both mechanisms are deprecated with a planned removal by May 2018.  The replacement mechanism, however, does not work with \nbestman2\n.\n\n\nQ. \nHow many GridFTP servers will I need?\n\n\nChoose to run multiple GridFTP servers for load balancing and better performance. We recommend to install additional GridFTP servers if your Storage Element:\n\n\n\n\nIs serving data to more than 1000 cores for VOs that use storage heavily (e.g. CMS, ATLAS, CDF, and D0),\n\n\nIs managing more than 500 TB of disk space, OR\n\n\nHas more than 10Gbps bandwidth\n\n\n\n\nWe recommend approximately one GridFTP server for each 8Gbps of desired utilized bandwidth.\n\n\nQ. \nDo I need to change default configuration of Gridftp server?\n\n\nYes, you may want to do this if the node on which GridFTP server will be installed has multiple network interfaces. Read \nthis section\n for more details.\n\n\nQ. \nDo you need to enable Gratia gridftp-transfer probes?\n \n\nThe Gratia gridftp-transfer probes provide OSG storage statistics for accounting purposes. The reports include the source and destination of transfers, certificate subject of transfer initiator, as well as the size and status of the transferred file. The probe needs to be installed on every GridFTP server.\n\n\nInstall Instructions\n\n\nInstalling BeStMan2\n\n\n\n\nInstall Java using \nthese instructions\n.\n\n\nInstall the BeStMan Storage element meta-package:\n\n\n\n\nroot@host #\n yum install osg-se-bestman\n\n\n\n\n\nAuthorization\n\n\nThere are two authorization options:\n\n\n\n\nGridmap file\n\n\nGUMS authentication server\n\n\n\n\nPlease choose one of these and follow the instructions in one of the two following sections.\n\n\nConfiguring Gridmap Support\n\n\nBy default, GridFTP uses a gridmap file, found in \n/etc/grid-security/grid-mapfile\n. This file is not generated by default. can generate this file. You can generate this file manually, by including DN/username combinations (this is most useful for debugging). Otherwise, you can use \nedg-mkgridmap\n, which will periodically contact a list of VOMS servers that you specify.\n\n\nOnce \nedg-mkgridmap\n is configured, you will have to modify \n/etc/bestman2/conf/bestman2.rc\n and change \nGridMapFileName\n from \n/etc/bestman2/conf/grid-mapfile.empty\n to:\n\n\nGridMapFileName=/etc/grid-security/grid-mapfile\n\n\n\n\n\nIn \n/etc/sysconfig/bestman2\n, change\n\n\nBESTMAN_GUMS_ENABLED\n=\nno\n\n\n\n\n\nConfiguring GUMS support\n\n\nBy default, GridFTP uses a gridmap file, found in \n/etc/grid-security/gridmap-file\n. If you want to use GUMS security (recommended), you will need to enable it using the following steps.\n\n\nFirst, edit \n/etc/grid-security/gsi-authz.conf\n and uncomment the authorization callout:\n\n\nglobus_mapping liblcas_lcmaps_gt4_mapping.so lcmaps_callout\n\n\n\n\n\nNext edit \n/etc/lcmaps.db\n to enter the correct GUMS hostname:\n\n\ngumsclient = \nlcmaps_gums_client.mod\n\n             \n-resourcetype ce\n\n             \n-actiontype execute-now\n\n             \n-capath /etc/grid-security/certificates\n\n             \n-cert   /etc/grid-security/hostcert.pem\n\n             \n-key    /etc/grid-security/hostkey.pem\n\n             \n--cert-owner root\n\n# Change this URL to your GUMS server\n             \n--endpoint https://\nGUMS_HOSTNAME\n:8443/gums/services/GUMSXACMLAuthorizationServicePort\n\n\n\n\n\n\nYou will need to modify the following settings in \n/etc/sysconfig/bestman2\n\n\nBESTMAN_GUMSCERTPATH\n=\n/etc/grid-security/bestman/bestmancert.pem\n\nBESTMAN_GUMSKEYPATH\n=\n/etc/grid-security/bestman/bestmankey.pem\n\n\n\n\n\nYou will need to modify the following settings in \n/etc/bestman2/conf/bestman2.rc\n\n\nGUMSserviceURL=https://\nGUMS_HOST\n:8443/gums/services/GUMSXACMLAuthorizationServicePort\n\n\n\n\n\nEdit Bestman Settings\n\n\nBestman settings are split into three files:\n\n\n\n\nEnvironment variables (except those that represent server and client libraries) are stored in \n/etc/sysconfig/bestman2\n.\n\n\nThe server and client library environment variables are stored in \n/etc/sysconfig/bestman2lib\n.\n\n\nConfiguration is stored in \n/etc/bestman2/conf/bestman2.rc\n.\n\n\n\n\nYou should review these settings to make sure all of them comply with your environment. You are not expected to edit \n/etc/sysconfig/bestman2lib\n .\n\n\n\n\nNote\n\n\nIf you are upgrading from a version prior to 2.3.0-9, you will need to remove \nall\n entries for \nBESTMAN2_SERVER_LIB\n and \nBESTMAN2_CLIENT_LIB\n in file \n/etc/sysconfig/bestman2.\n These settings are now present in file \n/etc/sysconfig/bestman2lib\n\n\n\n\nYou will likely need to modify the following settings in \n/etc/bestman2/conf/bestman2.rc\n:\n\n\nlocalPathListAllowed=/tmp\nCertFileName=/etc/grid-security/bestman/bestmancert.pem\nKeyFileName=/etc/grid-security/bestman/bestmankey.pem\nsupportedProtocolList=gsiftp://\nGRIDFTP_HOSTNAME\n;gsiftp://\nGRIDFTP_HOSTNAME2\n\n\n\n\n\n\n\n\nNote\n\n\nMake sure the value for \nlocalPathListAllowed\n is correctly entered - i.e. each path separated by a \n;\n. If it is not, this parameter may not be effective.\nMake sure the permissions for the \nlocalPathListAllowed\n directory(ies) are set to 1777, which is the default for \n/tmp\n. Further, note that on many systems, \n/tmp\n gets cleared out automatically, so you may want to use a different location to ensure that the files persist.\n\n\n\n\nBeStMan requires up to two sets of certificate pairs. One is for host services; when clients connect to BeStMan, they will receive this certificate (\nCertFileName\n, \nKeyFileName\n) as proof of the server identity. The second certificate pair (\nBESTMAN_GUMSCERTPATH\n, \nBESTMAN_GUMSKEYPATH\n) is used to communicate with GUMS when verifying identity information (this only applicable for GUMS-enabled sites). These two can (and usually will be) the same files, but can be split if your GUMS setup requires a specific identity.\n\n\nlocalPathListAllowed\n determines which paths users will be able to access via SRM.\n\n\nsupportedProtocolList\n is a semi-colon list of GridFTP servers that the BeStMan will use as transfer agents. If you are using anything but the standard GridFTP port 2811, you will also have to add the port (ie \ngsiftp://\nHOSTNAME\n:port\n).\n\n\nFinally, modify \nGUMSserviceURL\n to use your local GUMS installation if you are using GUMS.\n\n\nModify \n/etc/sudoers\n\n\nBeStman requires the \nsudo\n command in order to write information as the proper user. You will need to give the \nbestman\n user the proper permissions to run these commands.\n\n\nModify \n/etc/sudoers\n and comment the following line.\n\n\n#Defaults    requiretty\n\n\n\n\n\nThen add the following lines at the end of the \n/etc/sudoers\n file.\n\n\nCmnd_Alias SRM_CMD = /bin/rm, /bin/mkdir, /bin/rmdir, /bin/mv, /bin/cp, /bin/ls\nRunas_Alias SRM_USR = ALL, !root\nbestman   ALL=(SRM_USR) NOPASSWD: SRM_CMD\n\n\n\n\n\nCopying certificates to an alternate location\n\n\nBeStMan requires a certificate pair to function; this must be readable by the \nbestman\n user.\n\n\nroot@host #\n cp /etc/grid-security/hostkey.pem /etc/grid-security/bestman/bestmankey.pem\n\nroot@host #\n cp /etc/grid-security/hostcert.pem /etc/grid-security/bestman/bestmancert.pem\n\nroot@host #\n chown -R bestman:bestman /etc/grid-security/bestman/\n\n\n\n\n\nVerify \nCertFileName\n and \nKeyFileName\n in \n/etc/bestman2/conf/bestman2.rc\n are set appropriately.\n\n\n(Optional) Using a different bestman user\n\n\nIf you would like to use a different user than the default \nbestman\n user (\nnot recommended\n), you will need to change the following:\n\n\n\n\nOwnership of bestman certs in \n/etc/grid-security/bestman\n.\n\n\nSRM_OWNER\n in \n/etc/sysconfig/bestman2\n to the new user.\n\n\nUser in \n/etc/sudoers\n. The last line (\nbestman ALL(SRM_USR) NOPASSWD: SRM_CMD\n) should be changed from \nbestman\n to the new user.\n\n\nOwnership of \n/var/log/bestman2\n\n\n\n\n\n\nWarning\n\n\nCurrently the RPM packaging will change the ownership of the \n/var/log/bestman2\n directory back to \nbestman\n on upgrades.\n\n\n\n\n(Optional) Modifying default logging for \nevent.srm.log\n\n\nThe logging directory (\n/var/log/bestman2\n) has two types of logs - \nbestman2.log\n and \nevent.srm.log\n.\n\n\nLog-rotation of \nbestman2.log\n file is controlled by \n/etc/logrotate.d/bestman2\n file.\n\n\nBy default, the size of \nevent.srm.log\n log file is set to 50MB within the Bestman code itself.\n\n\nLeft unchanged, \nevent.srm.log\n file counts will keep increasing indefinitely.  Depending on the usage, the number of these files can become high enough to fill up the partition that holds these logs.\n\n\nThere are 3 ways to avoid this -\n\n\n\n\n\n\nModify following parameters (commented by default) in the \n/etc/sysconfig/bestman2\n file\n\n\n# Number of files to keep\n\n\nBESTMAN_EVENT_LOG_COUNT\n=\n10\n\n\n# Size of each file in bytes\n\n\nBESTMAN_EVENT_LOG_SIZE\n=\n20971520\n\n\n\n\n\n\nThe optimal value for these depends on usage of the service.\n\n\n\n\n\n\nCreate a directory under a much bigger partition and have a symlink from \n/var/log/bestman2\n to that directory.\n\n\n\n\nLeave the default settings, but have your own custom script that cleans these files according to your needs.\n\n\n\n\nStarting Services\n\n\n\n\n\n\nfetch-crl\n\n\nFor RHEL 6:\n\n\nroot@host #\n /usr/sbin/fetch-crl   \n# This fetches the CRLs \n\n\nroot@host #\n /sbin/service fetch-crl-boot start\n\nroot@host #\n /sbin/service fetch-crl-cron start\n\n\n\n\n\nFor RHEL 7:\n\n\nroot@host #\n /usr/sbin/fetch-crl   \n# This fetches the CRLs \n\n\nroot@host #\n systemctl start fetch-crl-boot\n\nroot@host #\n systemctl start fetch-crl-cron\n\n\n\n\n\n\n\n\n\nGridFTP\n\n\nroot@host #\n service globus-gridftp-server start\n\n\n\n\n\n\n\n\n\nBestman\n\n\nroot@host #\n service bestman2 start\n\n\n\n\n\nTo start Bestman automatically at boot time\n\n\nroot@host #\n chkconfig bestman2 on\n\n\n\n\n\n\n\n\n\nGratia transfer probe:\n\n\nroot@host #\n service gratia-gridftp-transfer start\n\n\n\n\n\n\n\n\n\nStopping Services\n\n\n\n\n\n\nfetch-crl\n\n\nFor RHEL 6:\n\n\nroot@host #\n /usr/sbin/fetch-crl   \n# This fetches the CRLs\n\n\nroot@host #\n /sbin/service fetch-crl-boot stop\n\nroot@host #\n /sbin/service fetch-crl-cron stop\n\n\n\n\n\nFor RHEL 7:\n\n\nroot@host #\n /usr/sbin/fetch-crl   \n# This fetches the CRLs\n\n\nroot@host #\n systemctl stop fetch-crl-boot\n\nroot@host #\n systemctl stop fetch-crl-cron\n\n\n\n\n\n\n\n\n\nGridFTP\n\n\nroot@host #\n service globus-gridftp-server stop\n\n\n\n\n\n\n\n\n\nBestman\n\n\nroot@host #\n service bestman2 stop\n\n\n\n\n\n\n\n\n\nGratia transfer probe\n\n\nroot@host #\n service gratia-gridftp-transfer stop\n\n\n\n\n\n\n\n\n\nValidation of Service Operation\n\n\nOnce you have your SE setup and configured, there are several ways to monitor your installation. Refer to the following pages for more information:\n\n\n\n\nBeStMan SRM Tester\n.\n\n\nRSV\n which includes SRM probes as well.\n\n\n\n\nYou can also self-test to verify your installation with an SRM client such as \ngfal-copy\n.\n\n\nTroubleshooting\n\n\n\n\n\n\n\n\nService/Process\n\n\nLog File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nBeStMan2\n\n\n/var/log/bestman2/bestman2.log\n\n\nBeStMan2 server log and errors\n\n\n\n\n\n\n\n\n/var/log/bestman2/event.srm.log\n\n\nRecords all SRM transactions\n\n\n\n\n\n\nGridFTP\n\n\n/var/log/gridftp.log\n\n\nTransfer log\n\n\n\n\n\n\n\n\n/var/log/gridftp-auth.log\n\n\nAuthentication log\n\n\n\n\n\n\n\n\n/var/log/messages\n\n\nMain system log (look here for LCMAPS errors)\n\n\n\n\n\n\n\n\nDebugging Procedure\n\n\nIf system validation failed, you would probably need to check each component in order to verify your installation. In order to do so, you should check all of them in the following order\n\n\n\n\nGUMS (if in use)\n\n\nGridFTP\n\n\nBeStMan\n\n\n\n\nVerifying GUMS\n\n\nMake sure that the service certificate you specified for BeStMan configuration with \nGUMSHOSTCERT\n, \nGUMSHOSTKEY\n options and GridFTP service certificate are accepted by GUMS.\n\n\nTest GUMS by running:\n\n\nroot@host #\n srm-ping srm://\nBESTMAN_HOST\n:8443/srm/v2/server\n\n\n\n\n\nIn the output, check that your \ngumsIDMapped\n is not \nnull\n. It returns the \nuid\n that GUMS will map you to. This can be obtained from your GUMS administrator. Verify that this \nuid\n exists on BeStMan and GridFTP node.\n\n\nVerifying GridFTP\n\n\nLogin on the node where your certificate and \nOSG Worker Node Client\n is installed You will need to generate your proxy credentials using \ngrid-proxy-init\n or \nvoms-proxy-init\n.\n\n\nThen test GridFTP using \nglobus-url-copy\n:\n\n\nuser@host $\n \necho\n \nThis is a test\n \n/tmp/test \n\nuser@host $\n globus-url-copy -dbg file:///tmp/test gsiftp://\nGRIDFTP_HOST\n/tmp/test \n\n\n\n\n\nCheck the GridFTP logs to see if you have encountered any errors.\n\n\nVerifying BeStMan\n\n\nMake sure that the BeStMan process is running\n\n\nroot@host #\n ps -ef \n|\n grep bestman\n\nbestman   5121     1 99 19:59 ?        00:00:01 /usr/java/latest/bin/java -server -Xmx1024m -XX:MaxDirectMemorySize=1024m -DX509_CERT_DIR=/etc/grid-security/certificates -DCADIR=/etc/grid-security/certificates -Daxis.socketSecureFactory=org.glite.security.trustmanager.axis.AXISSocketFactory -DsslCAFiles=/etc/grid-security/certificates/*.0 -DsslCertfile=/etc/grid-security/bestman/bestmancert.pem -DsslKey=/etc/grid-security/bestman/bestmankey.pem -DJettyConfiguration=/etc/bestman2/conf/WEB-INF/jetty.xml -DJettyDescriptor=/etc/bestman2/conf/WEB-INF/web.xml -DJettyResource=/etc/bestman2/conf/ -Dorg.eclipse.jetty.util.log.IGNORE=true gov.lbl.srm.server.Server /etc/bestman2/conf/bestman2.rc\n\n\n\n\n\n\nIf \nbestman2\n is not running, check information in the log file \n/var/log/bestman2/bestman2.log\n.\n\n\nUseful Configuration and Log Files\n\n\n\n\n\n\n\n\nService/Process\n\n\nConfiguration File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nBeStMan2\n\n\n/etc/bestman2/conf/bestman2.rc\n\n\nMain BeStMan2 configuration file\n\n\n\n\n\n\n\n\n/etc/sysconfig/bestman2\n\n\nEnvironment variables used by BeStMan2\n\n\n\n\n\n\n\n\n/etc/sysconfig/bestman2lib\n\n\nEnvironment variables that store values of various client and server libraries used by BeStMan2\n\n\n\n\n\n\n\n\n/etc/bestman2/conf/*\n\n\nOther runtime configuration files\n\n\n\n\n\n\n\n\n/etc/init.d/bestman2\n\n\ninit.d startup script\n\n\n\n\n\n\n\n\n/etc/gridftp.conf\n\n\nStartup parameters\n\n\n\n\n\n\nGridFTP\n\n\n/etc/sysconfig/globus-gridftp-server\n\n\nEnvironment variables for GridFTP\n\n\n\n\n\n\nGratia Probe\n\n\n/etc/gratia/gridftp-transfer/ProbeConfig\n\n\nGridFTP Gratia Probe configuration\n\n\n\n\n\n\n\n\n/etc/cron.d/gratia-probe-gridftp-transfer.cron\n\n\nCron tab file\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nService/Process\n\n\nLog File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nBeStMan2\n\n\n/var/log/bestman2/bestman2.log\n\n\nBeStMan2 container log\n\n\n\n\n\n\n\n\n/var/log/bestman2/event.srm.log\n\n\nRecords all SRM transactions\n\n\n\n\n\n\nGridFTP\n\n\n/var/log/gridftp.log\n\n\nTransfer log\n\n\n\n\n\n\n\n\n/var/log/gridftp-auth.log\n\n\nGridFTP authorization log\n\n\n\n\n\n\n\n\n/var/log/messages\n\n\nMain system log (look here for LCMAPS errors)\n\n\n\n\n\n\nGratia probe\n\n\n/var/log/gratia\n\n\n\n\n\n\n\n\n\n\nUpgrading BeStMan\n\n\nUpgrading BeStMan can be done by\n\n\nroot@host #\n yum upgrade bestman2-server\n\n\n\n\n\nThere are a few notes to be aware of when upgrading BeStMan.\n\n\n\n\nFrom many of the versions of the BeStMan, configuration changes have taken place. Do not ignore any warnings about rpmsave or rpmnew files. You will need to especially be careful about and \n/etc/bestman2/conf/bestman2.rc\n.\n\n\nBeginning with BeStMan 2.3.0-9, many dependency locations changed. Be sure that \n/etc/sysconfig/bestman2lib\n contains the \nbuild-classpath\n directives in the \nBESTMAN2_SERVER_LIB\n and \nBESTMAN2_CLIENT_LIB\n. Otherwise, you may get java class loading errors on startup or on run-time. Be sure to remove these entries from the \n/etc/sysconfig/bestman2\n file.\n\n\nFor BeStMan 2.1.3, certain versions had a combined sysconfig and configuration file. You may need to split these files apart if this is the case.\n\n\n\n\nFor more help, please contact the GOC to create a support ticket.\n\n\nHow to get Help?\n\n\nIf you cannot resolve the problem, there are several ways to receive help:\n\n\n\n\nFor bug support and issues, submit a ticket to the \nGrid Operations Center\n.\n\n\nFor community support and best-effort software team support contact \n.\n\n\n\n\nFor a full set of help options, see the \nHelp Procedure\n.\n\n\nReferences\n\n\n\n\nStorage infrastructure software\n\n\nInformation on planning, installing and validating storage software\n\n\nTips and FAQ\n\n\nOSG Gratia Transfer Probe page\n\n\nSRM v2.2 LBNL client command line examples\n\n\nSRM-Tester\n\n\nBeStMan official website\n\n\nBeStMan User guides\n\n\nBeStMan FAQ\n\n\n\n\n\n\nSLAC Gateway mode Instruction\n - SLAC guide on gateway mode\n\n\nUS ATLAS instruction page\n\n\nSRM specifications and collaboration\n - from SRM collaboration working group\n\n\nS2\n - A SRM v2.2 test suite from CERN. It provides basic functionality tests based on use cases, and cross-copy tests, as part of the certification process and supports file access/transfer protocols: rfio, dcap, gsidcap, gsiftp\n\n\nSHA-2 compliance page\n\n\nBestman scalability tuning\n\n\n\n\nKnown Issues\n\n\nRequesting host certificates in RHEL6\n\n\nBestman may not start if the certificates were requested on slc6. This is be caused by a bug in JGlobus (see \nJGlobusIssue118\n), a \nbestman2\n dependency. A known workaround is to run this command\n\n\nuser@host $\n openssl rsa -in mykey.pem -out mykey.pem.old\n\n\n\n\n\nThis command on converts \nmykey.pem\n to \nmykey.pem.old\n; the latter format is supported.", 
            "title": "BeStMan Install"
        }, 
        {
            "location": "/data/bestman-install/#installing-bestman", 
            "text": "Warning  As of the June 2017 release of OSG 3.4.0, this software is officially deprecated.  Support is scheduled to end as of May 2018.", 
            "title": "Installing BeStMan"
        }, 
        {
            "location": "/data/bestman-install/#about-this-document", 
            "text": "This document explains how to install a BeStMan SRMv2 service. This procedure will guide one through the installation and configuration of a basic  bestman2  host with an underlying GridFTP server. This will allow the service to service requests via the SRM (Storage Resource Manager) protocol or the GridFTP protocol.", 
            "title": "About this Document"
        }, 
        {
            "location": "/data/bestman-install/#installing-bestman-storage-element", 
            "text": "This procedure explains how to install the stand-alone BeStMan Storage Element server;  see below  for notes on upgrading.  The service has the following components:   BeStMan - provides load-balancing across GridFTP servers.  GridFTP server - provides file transfer services using the GridFTP protocol.  Gratia gridftp transfer probe  (optional) - provides transfer accounting information to the OSG.", 
            "title": "Installing BeStMan Storage Element"
        }, 
        {
            "location": "/data/bestman-install/#requirements", 
            "text": "", 
            "title": "Requirements"
        }, 
        {
            "location": "/data/bestman-install/#host-and-os", 
            "text": "You need at least one node in order to install this service.  The OS must be in the  supported platforms  list.  The  OSG software repositories  must be configured correctly.  All procedures in this document require  root  privileges.", 
            "title": "Host and OS"
        }, 
        {
            "location": "/data/bestman-install/#users", 
            "text": "This installation will create following users unless they are already created:     User  Comment      bestman  Used by Bestman SRM server     For full functionality, the  bestman  account will need limited  sudo  access to a few commands, described below.  For this package to function correctly, you will have to create the users needed for grid operation. Any user that can be authenticated should be created.  For grid-mapfile users, each line of the grid-mapfile is a certificate/user pair. Each user in this file should be created on the server. For GUMS sites, this means that each user that can be authenticated by GUMS should be created on the server.  Note that these users must be kept in sync with the authentication method. For instance, if new users or rules are added in GUMS, then new users should also be added here.", 
            "title": "Users"
        }, 
        {
            "location": "/data/bestman-install/#certificates", 
            "text": "Two certificates are needed for operation of this service.     Certificate  User that owns certificate  Path to certificate      Host certificate  root  /etc/grid-security/hostcert.pem  and  /etc/grid-security/hostkey.pem    Bestman service certificate  bestman  /etc/grid-security/bestman/bestmancert.pem  and  /etc/grid-security/bestman/bestmankey.pem     Following the  instructions  to request a service certificate.  You will also need a copy of CA certificates. Note that the  osg-se-bestman  package will automatically install a certificate package but will not necessarily pick the cert package you expect; see  the CA certificates  documentation for more information.", 
            "title": "Certificates"
        }, 
        {
            "location": "/data/bestman-install/#networking", 
            "text": "For more details on overall firewall configuration, please see our  firewall documentation .     Service Name  Protocol  Port Number  Inbound  Outbound  Comment      GridFTP data channels  tcp  GLOBUS_TCP_PORT_RANGE  X   contiguous range of ports is necessary.    GridFTP data channels  tcp  GLOBUS_TCP_SOURCE_RANGE   X  contiguous range of ports is necessary.    GridFTP control channel  tcp  2811  X      SRM  tcp  8443  X", 
            "title": "Networking"
        }, 
        {
            "location": "/data/bestman-install/#engineering-considerations", 
            "text": "Please answer following questions before you proceed with installation and configuration of BeStMan storage element:  Q.  What authorization mechanism should I use?  Decide between a  grid-mapfile  or a  GUMS  server for authorization.  Both mechanisms are deprecated with a planned removal by May 2018.  The replacement mechanism, however, does not work with  bestman2 .  Q.  How many GridFTP servers will I need?  Choose to run multiple GridFTP servers for load balancing and better performance. We recommend to install additional GridFTP servers if your Storage Element:   Is serving data to more than 1000 cores for VOs that use storage heavily (e.g. CMS, ATLAS, CDF, and D0),  Is managing more than 500 TB of disk space, OR  Has more than 10Gbps bandwidth   We recommend approximately one GridFTP server for each 8Gbps of desired utilized bandwidth.  Q.  Do I need to change default configuration of Gridftp server?  Yes, you may want to do this if the node on which GridFTP server will be installed has multiple network interfaces. Read  this section  for more details.  Q.  Do you need to enable Gratia gridftp-transfer probes?   \nThe Gratia gridftp-transfer probes provide OSG storage statistics for accounting purposes. The reports include the source and destination of transfers, certificate subject of transfer initiator, as well as the size and status of the transferred file. The probe needs to be installed on every GridFTP server.", 
            "title": "Engineering Considerations"
        }, 
        {
            "location": "/data/bestman-install/#install-instructions", 
            "text": "", 
            "title": "Install Instructions"
        }, 
        {
            "location": "/data/bestman-install/#installing-bestman2", 
            "text": "Install Java using  these instructions .  Install the BeStMan Storage element meta-package:   root@host #  yum install osg-se-bestman", 
            "title": "Installing BeStMan2"
        }, 
        {
            "location": "/data/bestman-install/#authorization", 
            "text": "There are two authorization options:   Gridmap file  GUMS authentication server   Please choose one of these and follow the instructions in one of the two following sections.", 
            "title": "Authorization"
        }, 
        {
            "location": "/data/bestman-install/#configuring-gridmap-support", 
            "text": "By default, GridFTP uses a gridmap file, found in  /etc/grid-security/grid-mapfile . This file is not generated by default. can generate this file. You can generate this file manually, by including DN/username combinations (this is most useful for debugging). Otherwise, you can use  edg-mkgridmap , which will periodically contact a list of VOMS servers that you specify.  Once  edg-mkgridmap  is configured, you will have to modify  /etc/bestman2/conf/bestman2.rc  and change  GridMapFileName  from  /etc/bestman2/conf/grid-mapfile.empty  to:  GridMapFileName=/etc/grid-security/grid-mapfile  In  /etc/sysconfig/bestman2 , change  BESTMAN_GUMS_ENABLED = no", 
            "title": "Configuring Gridmap Support"
        }, 
        {
            "location": "/data/bestman-install/#configuring-gums-support", 
            "text": "By default, GridFTP uses a gridmap file, found in  /etc/grid-security/gridmap-file . If you want to use GUMS security (recommended), you will need to enable it using the following steps.  First, edit  /etc/grid-security/gsi-authz.conf  and uncomment the authorization callout:  globus_mapping liblcas_lcmaps_gt4_mapping.so lcmaps_callout  Next edit  /etc/lcmaps.db  to enter the correct GUMS hostname:  gumsclient =  lcmaps_gums_client.mod \n              -resourcetype ce \n              -actiontype execute-now \n              -capath /etc/grid-security/certificates \n              -cert   /etc/grid-security/hostcert.pem \n              -key    /etc/grid-security/hostkey.pem \n              --cert-owner root \n# Change this URL to your GUMS server\n              --endpoint https:// GUMS_HOSTNAME :8443/gums/services/GUMSXACMLAuthorizationServicePort   You will need to modify the following settings in  /etc/sysconfig/bestman2  BESTMAN_GUMSCERTPATH = /etc/grid-security/bestman/bestmancert.pem BESTMAN_GUMSKEYPATH = /etc/grid-security/bestman/bestmankey.pem  You will need to modify the following settings in  /etc/bestman2/conf/bestman2.rc  GUMSserviceURL=https:// GUMS_HOST :8443/gums/services/GUMSXACMLAuthorizationServicePort", 
            "title": "Configuring GUMS support"
        }, 
        {
            "location": "/data/bestman-install/#edit-bestman-settings", 
            "text": "Bestman settings are split into three files:   Environment variables (except those that represent server and client libraries) are stored in  /etc/sysconfig/bestman2 .  The server and client library environment variables are stored in  /etc/sysconfig/bestman2lib .  Configuration is stored in  /etc/bestman2/conf/bestman2.rc .   You should review these settings to make sure all of them comply with your environment. You are not expected to edit  /etc/sysconfig/bestman2lib  .   Note  If you are upgrading from a version prior to 2.3.0-9, you will need to remove  all  entries for  BESTMAN2_SERVER_LIB  and  BESTMAN2_CLIENT_LIB  in file  /etc/sysconfig/bestman2.  These settings are now present in file  /etc/sysconfig/bestman2lib   You will likely need to modify the following settings in  /etc/bestman2/conf/bestman2.rc :  localPathListAllowed=/tmp\nCertFileName=/etc/grid-security/bestman/bestmancert.pem\nKeyFileName=/etc/grid-security/bestman/bestmankey.pem\nsupportedProtocolList=gsiftp:// GRIDFTP_HOSTNAME ;gsiftp:// GRIDFTP_HOSTNAME2    Note  Make sure the value for  localPathListAllowed  is correctly entered - i.e. each path separated by a  ; . If it is not, this parameter may not be effective.\nMake sure the permissions for the  localPathListAllowed  directory(ies) are set to 1777, which is the default for  /tmp . Further, note that on many systems,  /tmp  gets cleared out automatically, so you may want to use a different location to ensure that the files persist.   BeStMan requires up to two sets of certificate pairs. One is for host services; when clients connect to BeStMan, they will receive this certificate ( CertFileName ,  KeyFileName ) as proof of the server identity. The second certificate pair ( BESTMAN_GUMSCERTPATH ,  BESTMAN_GUMSKEYPATH ) is used to communicate with GUMS when verifying identity information (this only applicable for GUMS-enabled sites). These two can (and usually will be) the same files, but can be split if your GUMS setup requires a specific identity.  localPathListAllowed  determines which paths users will be able to access via SRM.  supportedProtocolList  is a semi-colon list of GridFTP servers that the BeStMan will use as transfer agents. If you are using anything but the standard GridFTP port 2811, you will also have to add the port (ie  gsiftp:// HOSTNAME :port ).  Finally, modify  GUMSserviceURL  to use your local GUMS installation if you are using GUMS.", 
            "title": "Edit Bestman Settings"
        }, 
        {
            "location": "/data/bestman-install/#modify-etcsudoers", 
            "text": "BeStman requires the  sudo  command in order to write information as the proper user. You will need to give the  bestman  user the proper permissions to run these commands.  Modify  /etc/sudoers  and comment the following line.  #Defaults    requiretty  Then add the following lines at the end of the  /etc/sudoers  file.  Cmnd_Alias SRM_CMD = /bin/rm, /bin/mkdir, /bin/rmdir, /bin/mv, /bin/cp, /bin/ls\nRunas_Alias SRM_USR = ALL, !root\nbestman   ALL=(SRM_USR) NOPASSWD: SRM_CMD", 
            "title": "Modify /etc/sudoers"
        }, 
        {
            "location": "/data/bestman-install/#copying-certificates-to-an-alternate-location", 
            "text": "BeStMan requires a certificate pair to function; this must be readable by the  bestman  user.  root@host #  cp /etc/grid-security/hostkey.pem /etc/grid-security/bestman/bestmankey.pem root@host #  cp /etc/grid-security/hostcert.pem /etc/grid-security/bestman/bestmancert.pem root@host #  chown -R bestman:bestman /etc/grid-security/bestman/  Verify  CertFileName  and  KeyFileName  in  /etc/bestman2/conf/bestman2.rc  are set appropriately.", 
            "title": "Copying certificates to an alternate location"
        }, 
        {
            "location": "/data/bestman-install/#optional-using-a-different-bestman-user", 
            "text": "If you would like to use a different user than the default  bestman  user ( not recommended ), you will need to change the following:   Ownership of bestman certs in  /etc/grid-security/bestman .  SRM_OWNER  in  /etc/sysconfig/bestman2  to the new user.  User in  /etc/sudoers . The last line ( bestman ALL(SRM_USR) NOPASSWD: SRM_CMD ) should be changed from  bestman  to the new user.  Ownership of  /var/log/bestman2    Warning  Currently the RPM packaging will change the ownership of the  /var/log/bestman2  directory back to  bestman  on upgrades.", 
            "title": "(Optional) Using a different bestman user"
        }, 
        {
            "location": "/data/bestman-install/#optional-modifying-default-logging-for-eventsrmlog", 
            "text": "The logging directory ( /var/log/bestman2 ) has two types of logs -  bestman2.log  and  event.srm.log .  Log-rotation of  bestman2.log  file is controlled by  /etc/logrotate.d/bestman2  file.  By default, the size of  event.srm.log  log file is set to 50MB within the Bestman code itself.  Left unchanged,  event.srm.log  file counts will keep increasing indefinitely.  Depending on the usage, the number of these files can become high enough to fill up the partition that holds these logs.  There are 3 ways to avoid this -    Modify following parameters (commented by default) in the  /etc/sysconfig/bestman2  file  # Number of files to keep  BESTMAN_EVENT_LOG_COUNT = 10  # Size of each file in bytes  BESTMAN_EVENT_LOG_SIZE = 20971520   The optimal value for these depends on usage of the service.    Create a directory under a much bigger partition and have a symlink from  /var/log/bestman2  to that directory.   Leave the default settings, but have your own custom script that cleans these files according to your needs.", 
            "title": "(Optional) Modifying default logging for event.srm.log"
        }, 
        {
            "location": "/data/bestman-install/#starting-services", 
            "text": "fetch-crl  For RHEL 6:  root@host #  /usr/sbin/fetch-crl    # This fetches the CRLs   root@host #  /sbin/service fetch-crl-boot start root@host #  /sbin/service fetch-crl-cron start  For RHEL 7:  root@host #  /usr/sbin/fetch-crl    # This fetches the CRLs   root@host #  systemctl start fetch-crl-boot root@host #  systemctl start fetch-crl-cron    GridFTP  root@host #  service globus-gridftp-server start    Bestman  root@host #  service bestman2 start  To start Bestman automatically at boot time  root@host #  chkconfig bestman2 on    Gratia transfer probe:  root@host #  service gratia-gridftp-transfer start", 
            "title": "Starting Services"
        }, 
        {
            "location": "/data/bestman-install/#stopping-services", 
            "text": "fetch-crl  For RHEL 6:  root@host #  /usr/sbin/fetch-crl    # This fetches the CRLs  root@host #  /sbin/service fetch-crl-boot stop root@host #  /sbin/service fetch-crl-cron stop  For RHEL 7:  root@host #  /usr/sbin/fetch-crl    # This fetches the CRLs  root@host #  systemctl stop fetch-crl-boot root@host #  systemctl stop fetch-crl-cron    GridFTP  root@host #  service globus-gridftp-server stop    Bestman  root@host #  service bestman2 stop    Gratia transfer probe  root@host #  service gratia-gridftp-transfer stop", 
            "title": "Stopping Services"
        }, 
        {
            "location": "/data/bestman-install/#validation-of-service-operation", 
            "text": "Once you have your SE setup and configured, there are several ways to monitor your installation. Refer to the following pages for more information:   BeStMan SRM Tester .  RSV  which includes SRM probes as well.   You can also self-test to verify your installation with an SRM client such as  gfal-copy .", 
            "title": "Validation of Service Operation"
        }, 
        {
            "location": "/data/bestman-install/#troubleshooting", 
            "text": "Service/Process  Log File  Description      BeStMan2  /var/log/bestman2/bestman2.log  BeStMan2 server log and errors     /var/log/bestman2/event.srm.log  Records all SRM transactions    GridFTP  /var/log/gridftp.log  Transfer log     /var/log/gridftp-auth.log  Authentication log     /var/log/messages  Main system log (look here for LCMAPS errors)", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/data/bestman-install/#debugging-procedure", 
            "text": "If system validation failed, you would probably need to check each component in order to verify your installation. In order to do so, you should check all of them in the following order   GUMS (if in use)  GridFTP  BeStMan", 
            "title": "Debugging Procedure"
        }, 
        {
            "location": "/data/bestman-install/#verifying-gums", 
            "text": "Make sure that the service certificate you specified for BeStMan configuration with  GUMSHOSTCERT ,  GUMSHOSTKEY  options and GridFTP service certificate are accepted by GUMS.  Test GUMS by running:  root@host #  srm-ping srm:// BESTMAN_HOST :8443/srm/v2/server  In the output, check that your  gumsIDMapped  is not  null . It returns the  uid  that GUMS will map you to. This can be obtained from your GUMS administrator. Verify that this  uid  exists on BeStMan and GridFTP node.", 
            "title": "Verifying GUMS"
        }, 
        {
            "location": "/data/bestman-install/#verifying-gridftp", 
            "text": "Login on the node where your certificate and  OSG Worker Node Client  is installed You will need to generate your proxy credentials using  grid-proxy-init  or  voms-proxy-init .  Then test GridFTP using  globus-url-copy :  user@host $   echo   This is a test   /tmp/test  user@host $  globus-url-copy -dbg file:///tmp/test gsiftp:// GRIDFTP_HOST /tmp/test   Check the GridFTP logs to see if you have encountered any errors.", 
            "title": "Verifying GridFTP"
        }, 
        {
            "location": "/data/bestman-install/#verifying-bestman", 
            "text": "Make sure that the BeStMan process is running  root@host #  ps -ef  |  grep bestman bestman   5121     1 99 19:59 ?        00:00:01 /usr/java/latest/bin/java -server -Xmx1024m -XX:MaxDirectMemorySize=1024m -DX509_CERT_DIR=/etc/grid-security/certificates -DCADIR=/etc/grid-security/certificates -Daxis.socketSecureFactory=org.glite.security.trustmanager.axis.AXISSocketFactory -DsslCAFiles=/etc/grid-security/certificates/*.0 -DsslCertfile=/etc/grid-security/bestman/bestmancert.pem -DsslKey=/etc/grid-security/bestman/bestmankey.pem -DJettyConfiguration=/etc/bestman2/conf/WEB-INF/jetty.xml -DJettyDescriptor=/etc/bestman2/conf/WEB-INF/web.xml -DJettyResource=/etc/bestman2/conf/ -Dorg.eclipse.jetty.util.log.IGNORE=true gov.lbl.srm.server.Server /etc/bestman2/conf/bestman2.rc   If  bestman2  is not running, check information in the log file  /var/log/bestman2/bestman2.log .", 
            "title": "Verifying BeStMan"
        }, 
        {
            "location": "/data/bestman-install/#useful-configuration-and-log-files", 
            "text": "Service/Process  Configuration File  Description      BeStMan2  /etc/bestman2/conf/bestman2.rc  Main BeStMan2 configuration file     /etc/sysconfig/bestman2  Environment variables used by BeStMan2     /etc/sysconfig/bestman2lib  Environment variables that store values of various client and server libraries used by BeStMan2     /etc/bestman2/conf/*  Other runtime configuration files     /etc/init.d/bestman2  init.d startup script     /etc/gridftp.conf  Startup parameters    GridFTP  /etc/sysconfig/globus-gridftp-server  Environment variables for GridFTP    Gratia Probe  /etc/gratia/gridftp-transfer/ProbeConfig  GridFTP Gratia Probe configuration     /etc/cron.d/gratia-probe-gridftp-transfer.cron  Cron tab file        Service/Process  Log File  Description      BeStMan2  /var/log/bestman2/bestman2.log  BeStMan2 container log     /var/log/bestman2/event.srm.log  Records all SRM transactions    GridFTP  /var/log/gridftp.log  Transfer log     /var/log/gridftp-auth.log  GridFTP authorization log     /var/log/messages  Main system log (look here for LCMAPS errors)    Gratia probe  /var/log/gratia", 
            "title": "Useful Configuration and Log Files"
        }, 
        {
            "location": "/data/bestman-install/#upgrading-bestman", 
            "text": "Upgrading BeStMan can be done by  root@host #  yum upgrade bestman2-server  There are a few notes to be aware of when upgrading BeStMan.   From many of the versions of the BeStMan, configuration changes have taken place. Do not ignore any warnings about rpmsave or rpmnew files. You will need to especially be careful about and  /etc/bestman2/conf/bestman2.rc .  Beginning with BeStMan 2.3.0-9, many dependency locations changed. Be sure that  /etc/sysconfig/bestman2lib  contains the  build-classpath  directives in the  BESTMAN2_SERVER_LIB  and  BESTMAN2_CLIENT_LIB . Otherwise, you may get java class loading errors on startup or on run-time. Be sure to remove these entries from the  /etc/sysconfig/bestman2  file.  For BeStMan 2.1.3, certain versions had a combined sysconfig and configuration file. You may need to split these files apart if this is the case.   For more help, please contact the GOC to create a support ticket.", 
            "title": "Upgrading BeStMan"
        }, 
        {
            "location": "/data/bestman-install/#how-to-get-help", 
            "text": "If you cannot resolve the problem, there are several ways to receive help:   For bug support and issues, submit a ticket to the  Grid Operations Center .  For community support and best-effort software team support contact  .   For a full set of help options, see the  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/data/bestman-install/#references", 
            "text": "Storage infrastructure software  Information on planning, installing and validating storage software  Tips and FAQ  OSG Gratia Transfer Probe page  SRM v2.2 LBNL client command line examples  SRM-Tester  BeStMan official website  BeStMan User guides  BeStMan FAQ    SLAC Gateway mode Instruction  - SLAC guide on gateway mode  US ATLAS instruction page  SRM specifications and collaboration  - from SRM collaboration working group  S2  - A SRM v2.2 test suite from CERN. It provides basic functionality tests based on use cases, and cross-copy tests, as part of the certification process and supports file access/transfer protocols: rfio, dcap, gsidcap, gsiftp  SHA-2 compliance page  Bestman scalability tuning", 
            "title": "References"
        }, 
        {
            "location": "/data/bestman-install/#known-issues", 
            "text": "", 
            "title": "Known Issues"
        }, 
        {
            "location": "/data/bestman-install/#requesting-host-certificates-in-rhel6", 
            "text": "Bestman may not start if the certificates were requested on slc6. This is be caused by a bug in JGlobus (see  JGlobusIssue118 ), a  bestman2  dependency. A known workaround is to run this command  user@host $  openssl rsa -in mykey.pem -out mykey.pem.old  This command on converts  mykey.pem  to  mykey.pem.old ; the latter format is supported.", 
            "title": "Requesting host certificates in RHEL6"
        }, 
        {
            "location": "/data/install-bestman-xrootd/", 
            "text": "Install Bestman Xrootd SE\n\n\n\n\nWarning\n\n\nAs of the June 2017 release of OSG 3.4.0, this software is officially deprecated.  Support is scheduled to end as of May 2018.\n\n\n\n\nAbout this Document\n\n\nThis page explains how to install the BeStMan Storage Element with underlying XRootD storage.\n\n\nRequirements\n\n\nHost and OS\n\n\n\n\nOS is Red Hat Enterprise Linux 6, 7, and variants (see \ndetails...\n)\n\n\nEPEL\n repos enabled.\n\n\nA working XRootD Server. See \nInstallXrootd\n for details.\n\n\nRoot access\n\n\n\n\nUsers\n\n\nThis installation will create several users unless they are already created.\n\n\n\n\n\n\n\n\nUser\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nbestman\n\n\nUsed by Bestman SRM server (needs sudo access).\n\n\n\n\n\n\ndaemon\n\n\nUsed by globus-gridftp-server.\n\n\n\n\n\n\nxrootd\n\n\nUsed by the xrootd client to contact xrootd redirector.\n\n\n\n\n\n\n\n\nFor this package to function correctly, you will have to create the users needed for grid operation. Any user that can be authenticated should be created.\n\n\nFor grid-mapfile users, each line of the grid-mapfile is a certificate/user pair. Each user in this file should be created on the server.\n\n\nFor gums users, this means that each user that can be authenticated by gums should be created on the server.\n\n\nNote that these users must be kept in sync with the authentication method. For instance, if new users or rules are added in gums, then new users should also be added here.\n\n\nCertificates\n\n\n\n\n\n\n\n\nCertificate\n\n\nUser that owns certificate\n\n\nPath to certificate\n\n\n\n\n\n\n\n\n\n\nHost certificate\n\n\nroot\n\n\n/etc/grid-security/hostcert.pem\n \n/etc/grid-security/hostkey.pem\n\n\n\n\n\n\nBestman service certificate\n\n\nbestman\n\n\n/etc/grid-security/bestman/bestmancert.pem\n \n/etc/grid-security/bestman/bestmankey.pem\n\n\n\n\n\n\n\n\nInstructions\n to request a service certificate.\n\n\nYou will also need a copy of CA certificates (see below).\n\n\nNetworking\n\n\n\n\n\n\n\n\nService Name\n\n\nProtocol\n\n\nPort Number\n\n\nInbound\n\n\nOutbound\n\n\nCommen\n\n\n\n\n\n\n\n\n\n\nGridFTP\n\n\ntcp\n\n\n2811 and \nGLOBUS_TCP_SOURCE_RANGE\n\n\nYES\n\n\n\n\ncontiguous range of ports\n\n\n\n\n\n\nStorage Resource Manager\n\n\ntcp\n\n\n8443\n\n\nYES\n\n\n\n\n\n\n\n\n\n\n\n\nInstall Instructions\n\n\nNote that this package is primarily intended for Bestman-Gateway acting as an endpoint for XRootD server. If you have not installed an XRootD server yet, follow the instructions in \nInstallXrootd\n.\n\n\nCertificates\n\n\nGridFTP, which is a part of this meta-package, requires a certificate package to run. If you require a specific certificate package, follow the \nInstallCertAuth\n instructions to install it. If you do not install a grid certificate package first, the install procedure will install one for you as part of its dependencies. (usually osg-ca-certs).\n\n\nPackage installation instructions\n\n\n\n\nInstall Java using \nthese instructions\n\n\nInstall the BeStMan Gateway XRootD Storage element meta-package:\nroot@host #\n yum install osg-se-bestman-xrootd\n\n\n\n\n\n\n\n\n\nConfiguring GridFTP authentication\n\n\nFor information on how to configure authentication for your GridFTP installation, please refer to the \nconfiguring authentication section of the GridFTP guide\n.\n\n\nConfiguring GridFTP XRootD support\n\n\nIn order to configure GridFTP to work with XRootD, you will need to configure the Data Storage Interface (DSI) module with Xrootd pre-load libraries. This module is used to access Xrootd and POSIX file systems.\n\n\nEdit \n/etc/sysconfig/xrootd-dsi\n (create it if it is missing) and set XROOTD_VMP (XRootD Virtual Mount Point) to use your Xrootd redirector.\n\n\nexport XROOTD_VMP=\nredirector:1094:/local_path=/remote_path\n\n\n\n\n\n\n\n\nNote\n\n\nThe syntax of the above environment variable is a little confusing, so make sure that you adhere to the following directions for XROOTD_VMP (Virtual Mount Point):\n\n\n- Redirector: This is the hostname and domain of the local XRootD redirector server.\n- local_path: This is the path used to access the GridFTP server (ie this server).\n- remote_path: This is the path used to access the XRootD redirector.\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe xrootd-dsi module overloads the \ngridftp.conf\n file and uses the alternate file \n/etc/xrootd-dsi/gridftp-xrootd.conf\n. If you have made local changes to your \ngridftp.conf\n file, then you will need to carry them over to \n/etc/xrootd-dsi/gridftp-xrootd.conf\n.\n\n\n\n\nConfiguring xrootdfs\n\n\nThough the DSI module will work for GridFTP, you will need a FUSE mount in order for BeStMan to work correctly with XRootD. Configure it using the following steps.\n\n\nModify \n/etc/fstab\n by adding the following entries:\n\n\n....\nxrootdfs                \n/mnt/xrootd\n              fuse    rdr=xroot://\nredirector1.domain.com\n:1094/\n/path/\n,uid=xrootd 0 0\n\n\n\n\n\nReplace \n/mnt/xrootd\n with the path that you would like to access with BeStMan. This should also match the GridFTP settings for the \nXROOTD_VMP\n local path. Create \n/mnt/xrootd\n directory. Once you are finished, you can mount it:\n\n\nmount /mnt/xrootd\n\n\n\n\n\nYou should now be able to run UNIX commands such as \nls /mnt/xrootd\n to see the contents of the XRootD server.\n\n\n(Optional) Configuring secured xrootdfs\n\n\nIf you want to enable security for access to xrootd via xrootdfs you will need to modify xrootd configuration and perform several steps to make xrootdfs secured.\n\n\n\n\n\n\nOn the xrootd redirector node, execute the following command:\n\n\nroot@host #\n xrdsssadmin -k \nmy_key_name\n -u anybody -g usrgroup add \nkeyfile\n\n\n\neg:\n\n\n\nroot@host #\n xrdsssadmin -k top_secret -u anybody -g usrgroup add /etc/xrootd/xrootd.key\n\n\n\n\n\n\n\n\n\nSet ownership\n\n\nroot@host #\n chown xrootd.xrootd /etc/xrootd/xrootd.key\n\n\n\n\n\n\n\n\n\nOn the node where xrootdfs is installed modify \n/etc/fstab\n add security information:\n\n\nroot@host #\n xrootdfs \n/mnt/xrootd %ENDCOLOR\n fuse rdr=xroot://\nredirector1.domain.com\n:1094/\n/path/redirector1\n,uid=xrootd,sss=\nkeyfile\n 0 0\n\n\n\n\n\n\n\n\n\n\nOn all xrootd data servers and redirector node, modify xrootd configuration (\n/etc/xrootd/xrootd-clustered.cfg\n) by adding the following segment: \n\n\n# ENABLE_SECURITY_BEGIN \n   xrootd.seclib /usr/lib64/libXrdSec.so \n   #the line below should be before \nsec.protocol ... unix\n\n   \nsec.protocol /usr/lib64 sss -s keyfile \n\n   sec.protocol /usr/lib64 unix \n   # this specify that we use the \nunix\n authentication module, additional one can be specified. \n   # this is the authorization file\n   acc.authdb /etc/xrootd/auth_file\n   ofs.authorize \n   # ENABLE_SECURITY_END\n\n\n\n\n\n\n\n\n\nOn all xrootd data server nodes, edit /etc/xrootd/auth_file to add authorized users of the form \nu \nusername\n \n/directoryname\n lr\n where \"lr\" is the permission set.\n\n\n\n\n\n\nCopy \nkeyfile\n from redirector node to every data server node and the xrootdfs node. Make sure that this file is owned by the \nxrootd\n user.\n\n\n\n\n\n\nRestart xrootd cluster by following \nthese instructions\n\n\n\n\n\n\nOn xroodfs node execute mount:\n\n\nroot@host #\n mount \n/mnt/xrootd\n\n\n\n\n\n\n\n\n\n\nVerify that you can access the mount point (df,ls) and can not write into unauthorized path, e.g:\n\n\nroot@host #\n cp /bin/sh /mnt/xrootd/tlevshin/test1 cp: \n\ncannot create regular file \\`/mnt/xrootd/tlevshin/test1\n: Permission denied\n\n\n\n\n\n\nLogin as yourself and try:\n\n\nroot@host #\n su - tlevshin \n\nuser@host $\n cp /bin/sh /mnt/xrootd/tlevshin/test1\n\n\n\n\n\n\n\n\n\nEdit Bestman Settings\n\n\nSee \nEdit Bestman Settings\n\n\nModify \n/etc/sudoers\n\n\nSee \nBestman Instructions\n\n\n(Optional) Copying certificates to a bestman location\n\n\nSee \nBestman Instructions\n\n\nValidation\n\n\nValidation can be done similar to a stand-alone BeStMan or GridFTP server. For more information, see \nBeStMan Validation\n and \nGridFTP Validation\n.\n\n\nServices\n\n\n\n\n\n\n\n\nSoftware\n\n\nService name\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nfetch-crl\n\n\nfetch-crl-boot\n and \nfetch-crl-cron\n\n\nSee  \nCA documentation\n\n\n\n\n\n\nGridftp\n\n\nglobus-gridftp-server\n\n\n\n\n\n\n\n\nBestMan\n\n\nbestman2\n\n\nSee \nBestman Services\n\n\n\n\n\n\nGratia probes\n\n\ngratia-xrootd-transfer\n and \ngratia-xrootd-storage\n\n\n\n\n\n\n\n\n\n\nAs a reminder, here are common service commands (all run as \nroot\n):\n\n\n\n\n\n\n\n\nTo \u2026\n\n\nRun the command \u2026\n\n\n\n\n\n\n\n\n\n\nStart a service\n\n\nservice \nSERVICE-NAME\n start\n\n\n\n\n\n\nStop a service\n\n\nservice \nSERVICE-NAME\n stop\n\n\n\n\n\n\nEnable a service to start during boot\n\n\nchkconfig \nSERVICE-NAME\n on\n\n\n\n\n\n\nDisable a service from starting during boot\n\n\nchkconfig \nSERVICE-NAME\n off\n\n\n\n\n\n\n\n\nNotes on Upgrading Bestman\n\n\nSee \nUpgrading Bestman\n\n\nHow to get Help?\n\n\nIf you cannot resolve the problem, there are several ways to receive help:\n\n\n\n\nFor bug support and issues, submit a ticket to the \nGrid Operations Center\n.\n\n\n\n\nFor a full set of help options, see \nHelp Procedure\n.", 
            "title": "BeStMan Xrootd SE"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#install-bestman-xrootd-se", 
            "text": "Warning  As of the June 2017 release of OSG 3.4.0, this software is officially deprecated.  Support is scheduled to end as of May 2018.", 
            "title": "Install Bestman Xrootd SE"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#about-this-document", 
            "text": "This page explains how to install the BeStMan Storage Element with underlying XRootD storage.", 
            "title": "About this Document"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#requirements", 
            "text": "", 
            "title": "Requirements"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#host-and-os", 
            "text": "OS is Red Hat Enterprise Linux 6, 7, and variants (see  details... )  EPEL  repos enabled.  A working XRootD Server. See  InstallXrootd  for details.  Root access", 
            "title": "Host and OS"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#users", 
            "text": "This installation will create several users unless they are already created.     User  Comment      bestman  Used by Bestman SRM server (needs sudo access).    daemon  Used by globus-gridftp-server.    xrootd  Used by the xrootd client to contact xrootd redirector.     For this package to function correctly, you will have to create the users needed for grid operation. Any user that can be authenticated should be created.  For grid-mapfile users, each line of the grid-mapfile is a certificate/user pair. Each user in this file should be created on the server.  For gums users, this means that each user that can be authenticated by gums should be created on the server.  Note that these users must be kept in sync with the authentication method. For instance, if new users or rules are added in gums, then new users should also be added here.", 
            "title": "Users"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#certificates", 
            "text": "Certificate  User that owns certificate  Path to certificate      Host certificate  root  /etc/grid-security/hostcert.pem   /etc/grid-security/hostkey.pem    Bestman service certificate  bestman  /etc/grid-security/bestman/bestmancert.pem   /etc/grid-security/bestman/bestmankey.pem     Instructions  to request a service certificate.  You will also need a copy of CA certificates (see below).", 
            "title": "Certificates"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#networking", 
            "text": "Service Name  Protocol  Port Number  Inbound  Outbound  Commen      GridFTP  tcp  2811 and  GLOBUS_TCP_SOURCE_RANGE  YES   contiguous range of ports    Storage Resource Manager  tcp  8443  YES", 
            "title": "Networking"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#install-instructions", 
            "text": "Note that this package is primarily intended for Bestman-Gateway acting as an endpoint for XRootD server. If you have not installed an XRootD server yet, follow the instructions in  InstallXrootd .", 
            "title": "Install Instructions"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#certificates_1", 
            "text": "GridFTP, which is a part of this meta-package, requires a certificate package to run. If you require a specific certificate package, follow the  InstallCertAuth  instructions to install it. If you do not install a grid certificate package first, the install procedure will install one for you as part of its dependencies. (usually osg-ca-certs).", 
            "title": "Certificates"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#package-installation-instructions", 
            "text": "Install Java using  these instructions  Install the BeStMan Gateway XRootD Storage element meta-package: root@host #  yum install osg-se-bestman-xrootd", 
            "title": "Package installation instructions"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#configuring-gridftp-authentication", 
            "text": "For information on how to configure authentication for your GridFTP installation, please refer to the  configuring authentication section of the GridFTP guide .", 
            "title": "Configuring GridFTP authentication"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#configuring-gridftp-xrootd-support", 
            "text": "In order to configure GridFTP to work with XRootD, you will need to configure the Data Storage Interface (DSI) module with Xrootd pre-load libraries. This module is used to access Xrootd and POSIX file systems.  Edit  /etc/sysconfig/xrootd-dsi  (create it if it is missing) and set XROOTD_VMP (XRootD Virtual Mount Point) to use your Xrootd redirector.  export XROOTD_VMP= redirector:1094:/local_path=/remote_path    Note  The syntax of the above environment variable is a little confusing, so make sure that you adhere to the following directions for XROOTD_VMP (Virtual Mount Point):  - Redirector: This is the hostname and domain of the local XRootD redirector server.\n- local_path: This is the path used to access the GridFTP server (ie this server).\n- remote_path: This is the path used to access the XRootD redirector.    Note  The xrootd-dsi module overloads the  gridftp.conf  file and uses the alternate file  /etc/xrootd-dsi/gridftp-xrootd.conf . If you have made local changes to your  gridftp.conf  file, then you will need to carry them over to  /etc/xrootd-dsi/gridftp-xrootd.conf .", 
            "title": "Configuring GridFTP XRootD support"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#configuring-xrootdfs", 
            "text": "Though the DSI module will work for GridFTP, you will need a FUSE mount in order for BeStMan to work correctly with XRootD. Configure it using the following steps.  Modify  /etc/fstab  by adding the following entries:  ....\nxrootdfs                 /mnt/xrootd               fuse    rdr=xroot:// redirector1.domain.com :1094/ /path/ ,uid=xrootd 0 0  Replace  /mnt/xrootd  with the path that you would like to access with BeStMan. This should also match the GridFTP settings for the  XROOTD_VMP  local path. Create  /mnt/xrootd  directory. Once you are finished, you can mount it:  mount /mnt/xrootd  You should now be able to run UNIX commands such as  ls /mnt/xrootd  to see the contents of the XRootD server.", 
            "title": "Configuring xrootdfs"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#optional-configuring-secured-xrootdfs", 
            "text": "If you want to enable security for access to xrootd via xrootdfs you will need to modify xrootd configuration and perform several steps to make xrootdfs secured.    On the xrootd redirector node, execute the following command:  root@host #  xrdsssadmin -k  my_key_name  -u anybody -g usrgroup add  keyfile  eg:  root@host #  xrdsssadmin -k top_secret -u anybody -g usrgroup add /etc/xrootd/xrootd.key    Set ownership  root@host #  chown xrootd.xrootd /etc/xrootd/xrootd.key    On the node where xrootdfs is installed modify  /etc/fstab  add security information:  root@host #  xrootdfs  /mnt/xrootd %ENDCOLOR  fuse rdr=xroot:// redirector1.domain.com :1094/ /path/redirector1 ,uid=xrootd,sss= keyfile  0 0     On all xrootd data servers and redirector node, modify xrootd configuration ( /etc/xrootd/xrootd-clustered.cfg ) by adding the following segment:   # ENABLE_SECURITY_BEGIN \n   xrootd.seclib /usr/lib64/libXrdSec.so \n   #the line below should be before  sec.protocol ... unix \n    sec.protocol /usr/lib64 sss -s keyfile  \n   sec.protocol /usr/lib64 unix \n   # this specify that we use the  unix  authentication module, additional one can be specified. \n   # this is the authorization file\n   acc.authdb /etc/xrootd/auth_file\n   ofs.authorize \n   # ENABLE_SECURITY_END    On all xrootd data server nodes, edit /etc/xrootd/auth_file to add authorized users of the form  u  username   /directoryname  lr  where \"lr\" is the permission set.    Copy  keyfile  from redirector node to every data server node and the xrootdfs node. Make sure that this file is owned by the  xrootd  user.    Restart xrootd cluster by following  these instructions    On xroodfs node execute mount:  root@host #  mount  /mnt/xrootd     Verify that you can access the mount point (df,ls) and can not write into unauthorized path, e.g:  root@host #  cp /bin/sh /mnt/xrootd/tlevshin/test1 cp:  cannot create regular file \\`/mnt/xrootd/tlevshin/test1 : Permission denied   Login as yourself and try:  root@host #  su - tlevshin  user@host $  cp /bin/sh /mnt/xrootd/tlevshin/test1", 
            "title": "(Optional) Configuring secured xrootdfs"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#edit-bestman-settings", 
            "text": "See  Edit Bestman Settings", 
            "title": "Edit Bestman Settings"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#modify-etcsudoers", 
            "text": "See  Bestman Instructions", 
            "title": "Modify /etc/sudoers"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#optional-copying-certificates-to-a-bestman-location", 
            "text": "See  Bestman Instructions", 
            "title": "(Optional) Copying certificates to a bestman location"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#validation", 
            "text": "Validation can be done similar to a stand-alone BeStMan or GridFTP server. For more information, see  BeStMan Validation  and  GridFTP Validation .", 
            "title": "Validation"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#services", 
            "text": "Software  Service name  Notes      fetch-crl  fetch-crl-boot  and  fetch-crl-cron  See   CA documentation    Gridftp  globus-gridftp-server     BestMan  bestman2  See  Bestman Services    Gratia probes  gratia-xrootd-transfer  and  gratia-xrootd-storage      As a reminder, here are common service commands (all run as  root ):     To \u2026  Run the command \u2026      Start a service  service  SERVICE-NAME  start    Stop a service  service  SERVICE-NAME  stop    Enable a service to start during boot  chkconfig  SERVICE-NAME  on    Disable a service from starting during boot  chkconfig  SERVICE-NAME  off", 
            "title": "Services"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#notes-on-upgrading-bestman", 
            "text": "See  Upgrading Bestman", 
            "title": "Notes on Upgrading Bestman"
        }, 
        {
            "location": "/data/install-bestman-xrootd/#how-to-get-help", 
            "text": "If you cannot resolve the problem, there are several ways to receive help:   For bug support and issues, submit a ticket to the  Grid Operations Center .   For a full set of help options, see  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/security/edg-mkgridmap/", 
            "text": "Warning\n\n\nEDG-Mkgridmap is no longer be supported in OSG 3.4. The \nLCMAPS VOMS Plugin\n is the preferred method for site authentication in the OSG and is available in both OSG 3.3 and OSG 3.4.\n\n\n\n\nAbout this Document\n\n\nThis document describes how you can use \nedg-mkgridmap\n to authorize users accessing the resources that you provide.\n\n\nRequirements\n\n\n\n\nThe purpose of using \nedg-mkgridmap\n is to create a grid-mapfile that will contain users from all VOs that are supported on your site. For this to happen, make sure that \nbefore\n you run \nedg-mkgridmap\n, you have created user accounts for all such VOs.\n\n\nThe RPM named \nedg-mkgridmap\n should be installed on your node\n\n\n\n\nCertificates\n\n\nCertificates\n\n\n\n\n\n\n\n\nCertificate\n\n\nOwnership and permissions\n\n\nPath to certificate\n\n\n\n\n\n\n\n\n\n\nHost certificate\n\n\nroot\n, \n0644\n\n\n/etc/grid-security/hostcert.pem\n\n\n\n\n\n\nHost key\n\n\nroot\n, \n0600\n\n\n/etc/grid-security/hostkey.pem\n\n\n\n\n\n\n\n\nConfiguration\n\n\nIf you claim to support a VO, make sure the following is true:\n\n\n\n\nThe VO is enabled in \n/etc/edg-mkgridmap.conf\n\n\nThe local user account to which VO members are mapped to exists on the CE\n\n\nAfter you run \nedg-mkgridmap\n, the VO should exist in \n/var/lib/osg/supported-vo-list\n\n\n\n\nLet's begin with configuration:\n\n\nStep 1\n Make sure for all Virtual Organizations (VOs) that you want to support, you have created a local user account. After that, you need to tell \nedg-mkgridmap\n, which Virtual Organizations (VOs) you want to support. For this, edit \n/etc/edg-mkgridmap.conf\n such that all VOs that you want to support are uncommented and all VOs that you do not want to support are commented out. Here is an example \nedg-mkgridmap.conf\n in which \ncdf\n VO is disabled and \nfermilab\n VO is enabled:\n\n\n/etc/edg-mkgridmap.conf\n:\n\n\n#### GROUP: group URI [lcluser]\n#\n#-------------------\n# USER-VO-MAP cdf CDF -- 1 -- Dennis Box (dbox@fnal.gov)\n#group vomss://voms.fnal.gov:8443/voms/cdf cdf\n#group vomss://voms.cnaf.infn.it:8443/voms/cdf cdf\n#-------------------\n# USER-VO-MAP fermilab FERMILAB -- 2 -- Fermilab Service Desk (servicedesk@fnal.gov)\ngroup vomss://voms.fnal.gov:8443/voms/fermilab fermilab\n[root@server]#\n\n\n\n\n\nStep 2\n For any additional DNs that you want to support (i.e. you want them to be present in the final \ngrid-mapfile\n that gets created), you need to add their \nDN\n \nusername\n mapping to a local grid-mapfile, for example \n/etc/osg/local-user-vo-map\n. Here is an example of how each line of this file should look like.\n\n\n/etc/grid-security/local-grid-mapfile\n:\n\n\n/DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Neha Sharma/CN=UID:neha\n neha\n\n\n\n\n\n\n\nNote\n\n\nThe local gridmap file is used also to add certificates of service accounts like the one used for RSV testing.\n\n\n\n\nStep 3\n You need to tell \nedg-mkgridmap\n about this local grid-mapfile. For this, you need to use the \ngmf_local\n directive. And if the entry in the local gridmap introduces a new user you should add it to a VO by adding a line \nusername\n \nvoname\n in the local vo-map file \n. And if you use a new VO you must add a VO naming line in \nedg-mkgridmap.conf\n as well (one starting with \n# USER-VO-MAP ...\n ). Here is an example of how your \nedg-mkgridmap.conf\n would now look like:\n\n\n/etc/edg-mkgridmap.conf\n:\n\n\n#### GROUP: group URI [lcluser]\n\n\n#\n\n\n#-------------------\n\n\n# USER-VO-MAP cdf CDF -- 1 -- Dennis Box (dbox@fnal.gov)     \n\n\n#group vomss://voms.fnal.gov:8443/voms/cdf cdf\n\n\n#group vomss://voms.cnaf.infn.it:8443/voms/cdf cdf\n\n\n#-------------------\n\n\n# USER-VO-MAP fermilab FERMILAB -- 2 -- Fermilab Service Desk (servicedesk@fnal.gov)  \n\n\ngroup\n \nvomss:\n//\nvoms\n.\nfnal\n.\ngov:8443\n/\nvoms\n/\nfermilab\n \nfermilab\n\n\n# USER-VO-MAP rsv RSV -- 101 -- Your Name (your@email)\n\n\ngmf_local\n /\netc\n/\ngrid-security\n/\nlocal-grid-mapfile\n\n\n\n\n\n\nStep 4\n If you have additional \nuseraccount vo\n mappings that you want to be included in final \n/var/lib/osg/user-vo-map\n file, add those mappings to \n/etc/osg/local-user-vo-map\n file.\n\n\n\n\nNote\n\n\nMore information on \nedg-mkgridmap.conf\n file can be found \nhere\n\n\n\n\nEnable/Disable\n\n\nEnable\n\n\nYou need to configure \nedg-mkgridmap\n to run automatically via a cron job. To do this, you need to run the following command.\n\n\n[root@server]#\n service edg-mkgridmap start\n\nEnabling periodic edg-mkgridmap:                           [  OK  ]\n\n\n\n\n\n\nThe entry in cron file (\n/etc/cron.d/edg-mkgridmap-cron\n) looks like\n\n\n/etc/cron.d/edg-mkgridmap-cron\n:\n\n\n# Cron job running by default every 6 hours.\n# The lock file can be enabled or disabled via a\n# service edg-mkgridmap start\n# chkconfig edg-mkgridmap on\n\n# Note the lock file not existing is success hence the the slightly odd logic\n# below.\n# run every 6 hours but sleep for random time every 5 hours\n\n0 */6 * * * root perl -e \nsleep rand 18000\n; [ ! -f /var/lock/subsys/edg-mkgridmap ] || /usr/sbin/edg-mkgridmap\n\n\n\n\n\n\n\nNote\n\n\nAs you can see above, this cron will run every 6 hours. If you do not want to wait that long, you can execute the command \n/usr/sbin/edg-mkgridmap\n directly on command line\n\n\n\n\nDisable\n\n\n[root@server]#\n service edg-mkgridmap stop\n\nDisabling periodic edg-mkgridmap:                          [  OK  ]\n\n\n\n\n\n\n5.0 Chkconfig on/off\n\n\n5.1 On\n\n\n[root@server]#\n chkconfig edg-mkgridmap on\n\n[root@server]#\n chkconfig --list edg-mkgridmap\n\nedg-mkgridmap   0:off   1:off   2:on    3:on    4:on    5:on    6:off\n\n\n\n\n\n\n5.2 Off\n\n\n[root@server]#\n chkconfig edg-mkgridmap off\n\n[root@server]#\n chkconfig --list edg-mkgridmap\n\nedg-mkgridmap   0:off   1:off   2:off   3:off   4:off   5:off   6:off\n\n\n\n\n\n\nWhat happens behind the scenes?\n\n\nUpon execution, \nedg-mkgridmap\n reads the configuration file at location \n/etc/edg-mkgridmap.conf\n and for all VOs for which a local user account exists, it contacts their VOMS server to retrieve list of all users that belong to that VO. Its output is the following files\n\n\n\n\n/var/lib/osg/supported-vo-list\n. This file contains names of all VOs you want to support minus all VOs for whom you do not have a user account. In other words, this is list of VOs that will \nactually\n be supported.\n\n\n/var/lib/osg/user-vo-map\n . This file contains mapping of form 'useraccount VO' for all VOs in \n/var/lib/osg/supported-vo-list\n and also mappings from \n/etc/osg/local-user-vo-map\n file.\n\n\n/var/lib/osg/undefined-accounts\n. This file contains names of all VOs for which it \ncould not\n find user account.\n\n\n/etc/grid-security/grid-mapfile\n. This file contains 'DN username' mappings that are present in local grid-mapfile (if you have one) and mappings for \nall\n users from \nall\n VOs that are present in \n/var/lib/osg/supported-vo-list\n.\n\n\n\n\nWhat happens when you perform an upgrade?\n\n\nThere are two scenerios:\n\n\nYou do not have a custom edg-mkgridmap configuration\n\n\nThis means you are using the same \nedg-mkgridmap.conf\n file that came with prior installation of vo-client-edgmkgridmap package. In this case, during upgrade, the newer \nedg-mkgridmap.conf\n file will replace the old one. It will probably have new VOs in it and if you want to support them, you need to satisfy conditions as mentioned in Configuration section above.\n\n\nYou have a custom edg-mkgridmap configuration\n\n\nThis means you have modified the \nedg-mkgridmap.conf\n file. In this case, during upgrade, the existing file \n/etc/edg-mkgridmap.conf\n will remain untouched and new configuration file will be saved as \n/etc/edg-mkgridmap.conf.rpmnew\n. You can compare the two files and make changes as required.\n\n\nFiles\n\n\n\n\n\n\n\n\nFile Location\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n/etc/edg-mkgridmap.conf\n\n\nMain Edg-mkgridmap configuration file\n\n\n\n\n\n\n/etc/cron.d/edg-mkgridmap-cron\n\n\nEdg-mkgridmap cron file\n\n\n\n\n\n\n/etc/rc.d/init.d/edg-mkgridmap\n\n\nEdg-mkgridmap init file\n\n\n\n\n\n\n/var/log/edg-mkgridmap.log\n\n\nEdg-mkgridmap log file\n\n\n\n\n\n\n\n\nTroubleshooting\n\n\nCommon Problems\n\n\n\n\ngrid-mapfile not created\n\n\nError code 64\n\n\n\n\nTroubleshooting Procedure\n\n\nIn case of problems first of all check the service log file \n/var/log/edg-mkgridmap.log\n. This file lists:\n\n\n\n\nany errors accessing individual voms servers or any general errors\n\n\nany mappings eliminated from the \n/var/lib/osg/user-vo-map\n and \n/var/lib/osg/supported-vo-list\n files where the Unix account a user would be mapped to does not exist. There is an \n/var/lib/osg/undefined-accounts\n file created that lists these accounts.\n\n\nchanges from the last time the script was run\n\n\n\n\nThere is also a \nlast_checked\n file in \n/etc/grid-security\n that tells you the last time the cron process was run successfully (the \ngrid-mapfile\n only gets updated if there is a difference from the previous run).\n\n\n[root@server]#\n ls -al /etc/grid-security\n\n-rw-r--r--  1 root     root     1162888 Jul 12 02:14 grid-mapfile\n\n\n-rw-r--r--  1 root     root           0 Jul 12 08:14 grid-mapfile.last_checked\n\n\n\n\n\n\nGrid-Mapfile was not created\n\n\nFirst make sure that the \nedg-mkgridmap\n service is enabled:\n\n\n[root@server]#\n /sbin/service edg-mkgridmap status\n\nPeriodic edg-mkgridmap is enabled.\n\n\n\n\n\n\nNext, make sure an entry for \nedg-mkgridmap\n is present:\n\n\n[root@server]#\n cat /etc/cron.d/edg-mkgridmap-cron\n\n#\n Cron job running by default every \n6\n hours.\n\n#\n The lock file can be enabled or disabled via a\n\n#\n service edg-mkgridmap start\n\n#\n chkconfig edg-mkgridmap on\n\n\n#\n Note the lock file not existing is success hence the the slightly odd logic\n\n#\n below.\n\n#\n run every \n6\n hours but sleep \nfor\n random \ntime\n every \n5\n hours\n\n\n0 */6 * * * root perl -e \nsleep rand 18000\n; [ ! -f /var/lock/subsys/edg-mkgridmap ] || /usr/sbin/edg-mkgridmap\n\n\n\n\n\n\n\n\nNote\n\n\nIf no entry for \nedg-mkgridmap\n is present, activate the service.\n\n\n\n\nLast, run the \nedg-mkgridmap\n script on the command line and check that \n/etc/grid-security/grid-mapfile\n gets created:\n\n\n[root@server]#\n edg-mkgridmap\n\n\n\n\n\n\n\nNote\n\n\nThe script will not output to the command line. Check the log file at \n/var/log/edg-mkgridmap.log\n instead.\n\n\n\n\nError Code 64\n\n\nThis type of error occurs when the script established contact with the VOMS server but the requested group or subgroup does not exist there. It is really a \n'not found'\n type error. It is usually caused by a bad entry in the \n/etc/edg-mkgridmap.conf\n file.\n\n\nIn the example below the configuration file contains a request for the VOMS server to send a list of all members of the uscms VO belonging to a non-existent \n/uscms/production\n group.\n\n\nConfiguration file \n/etc/edg-mkgridmap.conf\n entry:\n\n\n group vomss://voms.fnal.gov:8443/voms/uscms/production uscms01\n\n\n\n\n\nCommand run on CE node:\n\n\n[root@server]#\n edg-mkgridmap\n\n  voms search(https://voms.fnal.gov:8443/voms/uscms/production/services/VOMSAdmin?method=listMembers): /voms/uscms/production/services/VOMSAdmin\n\n\n\n   Exit with error(s) (code=64)\n\n\n\n\n\n\nOn the VOMS host you are attempting to access, the following error can be seen in the \n/var/log/tomcat6/voms-admin-VO_NAME.log\n or \n/var/log/tomcat/voms-admin-VO_NAME.log\n\n\n2005-05-26 15:51:04,003 INFO  [Ajp13Processor[8892][1]]  Connection from \n131.225.82.73\n by\n        /DC=org/DC=doegrids/OU=Services/CN=ce_host (serial 4511) -  service.InitSecurityContext\n2005-05-26 15:51:04,004 INFO  [Ajp13Processor[8892][1]]  listMembers (\n/uscms/production\n) -  admin.VOMSAdminSoapBindingImpl\n2005-05-26 15:51:04,013 ERROR [Ajp13Processor[8892][1]]  org.edg.security.voms.service.NotInDatabase:\n       Not in database: group \n/uscms/production\n -  connection.Database\n\n\n\n\n\nUseful Options for edg-mkgridmap troubleshooting\n\n\n\n\n--help\n\n\n--version\n\n\n--conf=\nconfig_file\n\n    (Default configuration file is at \n/etc/edg-mkgridmap.conf\n)\n\n\n--output=\noutput_file\n\n\n--verbose\n\n\n\n\nHow to get Help?\n\n\nIf you cannot resolve the problem or have general questions, there are several ways to receive help:\n\n\n\n\nFor bug support and issues, submit a ticket to the \nGrid Operations Center\n.\n\n\nFor community support and best-effort software team support contact \n.\n\n\n\n\nFor a full set of help options, see \nHelp Procedure\n.", 
            "title": "EDG-Mkgridmap Install"
        }, 
        {
            "location": "/security/edg-mkgridmap/#about-this-document", 
            "text": "This document describes how you can use  edg-mkgridmap  to authorize users accessing the resources that you provide.", 
            "title": "About this Document"
        }, 
        {
            "location": "/security/edg-mkgridmap/#requirements", 
            "text": "The purpose of using  edg-mkgridmap  is to create a grid-mapfile that will contain users from all VOs that are supported on your site. For this to happen, make sure that  before  you run  edg-mkgridmap , you have created user accounts for all such VOs.  The RPM named  edg-mkgridmap  should be installed on your node", 
            "title": "Requirements"
        }, 
        {
            "location": "/security/edg-mkgridmap/#certificates", 
            "text": "Certificates     Certificate  Ownership and permissions  Path to certificate      Host certificate  root ,  0644  /etc/grid-security/hostcert.pem    Host key  root ,  0600  /etc/grid-security/hostkey.pem", 
            "title": "Certificates"
        }, 
        {
            "location": "/security/edg-mkgridmap/#configuration", 
            "text": "If you claim to support a VO, make sure the following is true:   The VO is enabled in  /etc/edg-mkgridmap.conf  The local user account to which VO members are mapped to exists on the CE  After you run  edg-mkgridmap , the VO should exist in  /var/lib/osg/supported-vo-list   Let's begin with configuration:  Step 1  Make sure for all Virtual Organizations (VOs) that you want to support, you have created a local user account. After that, you need to tell  edg-mkgridmap , which Virtual Organizations (VOs) you want to support. For this, edit  /etc/edg-mkgridmap.conf  such that all VOs that you want to support are uncommented and all VOs that you do not want to support are commented out. Here is an example  edg-mkgridmap.conf  in which  cdf  VO is disabled and  fermilab  VO is enabled:  /etc/edg-mkgridmap.conf :  #### GROUP: group URI [lcluser]\n#\n#-------------------\n# USER-VO-MAP cdf CDF -- 1 -- Dennis Box (dbox@fnal.gov)\n#group vomss://voms.fnal.gov:8443/voms/cdf cdf\n#group vomss://voms.cnaf.infn.it:8443/voms/cdf cdf\n#-------------------\n# USER-VO-MAP fermilab FERMILAB -- 2 -- Fermilab Service Desk (servicedesk@fnal.gov)\ngroup vomss://voms.fnal.gov:8443/voms/fermilab fermilab\n[root@server]#  Step 2  For any additional DNs that you want to support (i.e. you want them to be present in the final  grid-mapfile  that gets created), you need to add their  DN   username  mapping to a local grid-mapfile, for example  /etc/osg/local-user-vo-map . Here is an example of how each line of this file should look like.  /etc/grid-security/local-grid-mapfile :  /DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Neha Sharma/CN=UID:neha  neha   Note  The local gridmap file is used also to add certificates of service accounts like the one used for RSV testing.   Step 3  You need to tell  edg-mkgridmap  about this local grid-mapfile. For this, you need to use the  gmf_local  directive. And if the entry in the local gridmap introduces a new user you should add it to a VO by adding a line  username   voname  in the local vo-map file  . And if you use a new VO you must add a VO naming line in  edg-mkgridmap.conf  as well (one starting with  # USER-VO-MAP ...  ). Here is an example of how your  edg-mkgridmap.conf  would now look like:  /etc/edg-mkgridmap.conf :  #### GROUP: group URI [lcluser]  #  #-------------------  # USER-VO-MAP cdf CDF -- 1 -- Dennis Box (dbox@fnal.gov)       #group vomss://voms.fnal.gov:8443/voms/cdf cdf  #group vomss://voms.cnaf.infn.it:8443/voms/cdf cdf  #-------------------  # USER-VO-MAP fermilab FERMILAB -- 2 -- Fermilab Service Desk (servicedesk@fnal.gov)    group   vomss: // voms . fnal . gov:8443 / voms / fermilab   fermilab  # USER-VO-MAP rsv RSV -- 101 -- Your Name (your@email)  gmf_local  / etc / grid-security / local-grid-mapfile   Step 4  If you have additional  useraccount vo  mappings that you want to be included in final  /var/lib/osg/user-vo-map  file, add those mappings to  /etc/osg/local-user-vo-map  file.   Note  More information on  edg-mkgridmap.conf  file can be found  here", 
            "title": "Configuration"
        }, 
        {
            "location": "/security/edg-mkgridmap/#enabledisable", 
            "text": "", 
            "title": "Enable/Disable"
        }, 
        {
            "location": "/security/edg-mkgridmap/#enable", 
            "text": "You need to configure  edg-mkgridmap  to run automatically via a cron job. To do this, you need to run the following command.  [root@server]#  service edg-mkgridmap start Enabling periodic edg-mkgridmap:                           [  OK  ]   The entry in cron file ( /etc/cron.d/edg-mkgridmap-cron ) looks like  /etc/cron.d/edg-mkgridmap-cron :  # Cron job running by default every 6 hours.\n# The lock file can be enabled or disabled via a\n# service edg-mkgridmap start\n# chkconfig edg-mkgridmap on\n\n# Note the lock file not existing is success hence the the slightly odd logic\n# below.\n# run every 6 hours but sleep for random time every 5 hours\n\n0 */6 * * * root perl -e  sleep rand 18000 ; [ ! -f /var/lock/subsys/edg-mkgridmap ] || /usr/sbin/edg-mkgridmap   Note  As you can see above, this cron will run every 6 hours. If you do not want to wait that long, you can execute the command  /usr/sbin/edg-mkgridmap  directly on command line", 
            "title": "Enable"
        }, 
        {
            "location": "/security/edg-mkgridmap/#disable", 
            "text": "[root@server]#  service edg-mkgridmap stop Disabling periodic edg-mkgridmap:                          [  OK  ]", 
            "title": "Disable"
        }, 
        {
            "location": "/security/edg-mkgridmap/#50-chkconfig-onoff", 
            "text": "", 
            "title": "5.0 Chkconfig on/off"
        }, 
        {
            "location": "/security/edg-mkgridmap/#51-on", 
            "text": "[root@server]#  chkconfig edg-mkgridmap on [root@server]#  chkconfig --list edg-mkgridmap edg-mkgridmap   0:off   1:off   2:on    3:on    4:on    5:on    6:off", 
            "title": "5.1 On"
        }, 
        {
            "location": "/security/edg-mkgridmap/#52-off", 
            "text": "[root@server]#  chkconfig edg-mkgridmap off [root@server]#  chkconfig --list edg-mkgridmap edg-mkgridmap   0:off   1:off   2:off   3:off   4:off   5:off   6:off", 
            "title": "5.2 Off"
        }, 
        {
            "location": "/security/edg-mkgridmap/#what-happens-behind-the-scenes", 
            "text": "Upon execution,  edg-mkgridmap  reads the configuration file at location  /etc/edg-mkgridmap.conf  and for all VOs for which a local user account exists, it contacts their VOMS server to retrieve list of all users that belong to that VO. Its output is the following files   /var/lib/osg/supported-vo-list . This file contains names of all VOs you want to support minus all VOs for whom you do not have a user account. In other words, this is list of VOs that will  actually  be supported.  /var/lib/osg/user-vo-map  . This file contains mapping of form 'useraccount VO' for all VOs in  /var/lib/osg/supported-vo-list  and also mappings from  /etc/osg/local-user-vo-map  file.  /var/lib/osg/undefined-accounts . This file contains names of all VOs for which it  could not  find user account.  /etc/grid-security/grid-mapfile . This file contains 'DN username' mappings that are present in local grid-mapfile (if you have one) and mappings for  all  users from  all  VOs that are present in  /var/lib/osg/supported-vo-list .", 
            "title": "What happens behind the scenes?"
        }, 
        {
            "location": "/security/edg-mkgridmap/#what-happens-when-you-perform-an-upgrade", 
            "text": "There are two scenerios:", 
            "title": "What happens when you perform an upgrade?"
        }, 
        {
            "location": "/security/edg-mkgridmap/#you-do-not-have-a-custom-edg-mkgridmap-configuration", 
            "text": "This means you are using the same  edg-mkgridmap.conf  file that came with prior installation of vo-client-edgmkgridmap package. In this case, during upgrade, the newer  edg-mkgridmap.conf  file will replace the old one. It will probably have new VOs in it and if you want to support them, you need to satisfy conditions as mentioned in Configuration section above.", 
            "title": "You do not have a custom edg-mkgridmap configuration"
        }, 
        {
            "location": "/security/edg-mkgridmap/#you-have-a-custom-edg-mkgridmap-configuration", 
            "text": "This means you have modified the  edg-mkgridmap.conf  file. In this case, during upgrade, the existing file  /etc/edg-mkgridmap.conf  will remain untouched and new configuration file will be saved as  /etc/edg-mkgridmap.conf.rpmnew . You can compare the two files and make changes as required.", 
            "title": "You have a custom edg-mkgridmap configuration"
        }, 
        {
            "location": "/security/edg-mkgridmap/#files", 
            "text": "File Location  Description      /etc/edg-mkgridmap.conf  Main Edg-mkgridmap configuration file    /etc/cron.d/edg-mkgridmap-cron  Edg-mkgridmap cron file    /etc/rc.d/init.d/edg-mkgridmap  Edg-mkgridmap init file    /var/log/edg-mkgridmap.log  Edg-mkgridmap log file", 
            "title": "Files"
        }, 
        {
            "location": "/security/edg-mkgridmap/#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/security/edg-mkgridmap/#common-problems", 
            "text": "grid-mapfile not created  Error code 64", 
            "title": "Common Problems"
        }, 
        {
            "location": "/security/edg-mkgridmap/#troubleshooting-procedure", 
            "text": "In case of problems first of all check the service log file  /var/log/edg-mkgridmap.log . This file lists:   any errors accessing individual voms servers or any general errors  any mappings eliminated from the  /var/lib/osg/user-vo-map  and  /var/lib/osg/supported-vo-list  files where the Unix account a user would be mapped to does not exist. There is an  /var/lib/osg/undefined-accounts  file created that lists these accounts.  changes from the last time the script was run   There is also a  last_checked  file in  /etc/grid-security  that tells you the last time the cron process was run successfully (the  grid-mapfile  only gets updated if there is a difference from the previous run).  [root@server]#  ls -al /etc/grid-security -rw-r--r--  1 root     root     1162888 Jul 12 02:14 grid-mapfile  -rw-r--r--  1 root     root           0 Jul 12 08:14 grid-mapfile.last_checked", 
            "title": "Troubleshooting Procedure"
        }, 
        {
            "location": "/security/edg-mkgridmap/#grid-mapfile-was-not-created", 
            "text": "First make sure that the  edg-mkgridmap  service is enabled:  [root@server]#  /sbin/service edg-mkgridmap status Periodic edg-mkgridmap is enabled.   Next, make sure an entry for  edg-mkgridmap  is present:  [root@server]#  cat /etc/cron.d/edg-mkgridmap-cron #  Cron job running by default every  6  hours. #  The lock file can be enabled or disabled via a #  service edg-mkgridmap start #  chkconfig edg-mkgridmap on #  Note the lock file not existing is success hence the the slightly odd logic #  below. #  run every  6  hours but sleep  for  random  time  every  5  hours 0 */6 * * * root perl -e  sleep rand 18000 ; [ ! -f /var/lock/subsys/edg-mkgridmap ] || /usr/sbin/edg-mkgridmap    Note  If no entry for  edg-mkgridmap  is present, activate the service.   Last, run the  edg-mkgridmap  script on the command line and check that  /etc/grid-security/grid-mapfile  gets created:  [root@server]#  edg-mkgridmap   Note  The script will not output to the command line. Check the log file at  /var/log/edg-mkgridmap.log  instead.", 
            "title": "Grid-Mapfile was not created"
        }, 
        {
            "location": "/security/edg-mkgridmap/#error-code-64", 
            "text": "This type of error occurs when the script established contact with the VOMS server but the requested group or subgroup does not exist there. It is really a  'not found'  type error. It is usually caused by a bad entry in the  /etc/edg-mkgridmap.conf  file.  In the example below the configuration file contains a request for the VOMS server to send a list of all members of the uscms VO belonging to a non-existent  /uscms/production  group.  Configuration file  /etc/edg-mkgridmap.conf  entry:   group vomss://voms.fnal.gov:8443/voms/uscms/production uscms01  Command run on CE node:  [root@server]#  edg-mkgridmap   voms search(https://voms.fnal.gov:8443/voms/uscms/production/services/VOMSAdmin?method=listMembers): /voms/uscms/production/services/VOMSAdmin     Exit with error(s) (code=64)   On the VOMS host you are attempting to access, the following error can be seen in the  /var/log/tomcat6/voms-admin-VO_NAME.log  or  /var/log/tomcat/voms-admin-VO_NAME.log  2005-05-26 15:51:04,003 INFO  [Ajp13Processor[8892][1]]  Connection from  131.225.82.73  by\n        /DC=org/DC=doegrids/OU=Services/CN=ce_host (serial 4511) -  service.InitSecurityContext\n2005-05-26 15:51:04,004 INFO  [Ajp13Processor[8892][1]]  listMembers ( /uscms/production ) -  admin.VOMSAdminSoapBindingImpl\n2005-05-26 15:51:04,013 ERROR [Ajp13Processor[8892][1]]  org.edg.security.voms.service.NotInDatabase:\n       Not in database: group  /uscms/production  -  connection.Database", 
            "title": "Error Code 64"
        }, 
        {
            "location": "/security/edg-mkgridmap/#useful-options-for-edg-mkgridmap-troubleshooting", 
            "text": "--help  --version  --conf= config_file \n    (Default configuration file is at  /etc/edg-mkgridmap.conf )  --output= output_file  --verbose", 
            "title": "Useful Options for edg-mkgridmap troubleshooting"
        }, 
        {
            "location": "/security/edg-mkgridmap/#how-to-get-help", 
            "text": "If you cannot resolve the problem or have general questions, there are several ways to receive help:   For bug support and issues, submit a ticket to the  Grid Operations Center .  For community support and best-effort software team support contact  .   For a full set of help options, see  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/security/install-gums/", 
            "text": "Warning\n\n\nGUMS is no longer be supported in OSG 3.4. The \nLCMAPS VOMS Plugin\n is the preferred method for site authentication in the OSG and is available in both OSG 3.3 and OSG 3.4.\n\n\n\n\nGUMS Install Guide\n\n\nThis document is intended for site administrators who want to install and configure the GUMS service.\n\n\nGUMS is a service that authorizes and maps users from their global (X.509) identity to a local (Linux user) identity. It is not a required service (some sites use the simple \nedg-mkgridmap\n to construct a \ngrid-mapfile\n instead), but it is commonly used. GUMS is useful when more than one resource (Compute Element, Storage Element, etc.) needs to authorize or map users, because it helps them share data. It is particularly helpful when using gLExec at a site, because gLExec runs on every worker node and needs authorization and mapping information. GUMS is a web application that runs in Tomcat.\n\n\nBefore Starting\n\n\nBefore starting the installation process, consider the following points (consulting \nthe Reference section below\n as needed):\n\n\n\n\nUser IDs:\n If they do not exist already, the installation will create the Linux users \nmysql\n (UID 27) and \ntomcat\n (UID 91)\n\n\nService certificate:\n The GUMS service uses a host certificate at \n/etc/grid-security/http/httpcert.pem\n and an accompanying key at \n/etc/grid-security/http/httpkey.pem\n\n\nNetwork ports:\n Hosts using your GUMS server for authentication (e.g., HTCondor-CE, GridFTP) must be able to contact it on port 8443 (TCP)\n\n\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare the \nrequired Yum repositories\n\n\nInstall \nCA certificates\n\n\n\n\nInstalling GUMS\n\n\n\n\nWhether installing or upgrading GUMS, please make sure to follow \nthese instructions\n for updating/installing Java to work correctly with GUMS\n\n\nIf you have an existing GUMS installation on the same host, shut down Tomcat.\n    The service is named \ntomcat6\n on EL6 and \ntomcat\n on EL7\n\n\n\n\nInstall the GUMS Service:\n\n\n[root@server]#\n yum install osg-gums\n\n\n\n\n\n\n\n\n\nConfigure Tomcat to use GSI:\n\n\n[root@server]#\n /var/lib/trustmanager-tomcat/configure.sh\n\n\n\n\n\n\n\nNote\n\n\nThis step will overwrite your \nserver.xml\n file in your tomcat configuration directory (\n/etc/tomcat6\n on EL6, \n/etc/tomcat\n on EL7).\nIf you are using Tomcat on this host for non-grid purposes, you may want to save the \nserver.xml\n file first, run the script, then merge your own configuration back into the file\n\n\n\n\n\n\n\n\nConfiguring GUMS\n\n\nConfigure GUMS database\n\n\nIn this section, you will configure the GUMS MySQL database, either by creating a new database or by copying an existing GUMS database. Pick the appropriate subsection below for your environment.\n\n\nNew Installation\n\n\n\n\n\n\nStart the database server\n\n\n\n\n\n\nOn EL6, this is MySQL:\n\n\n[root@server]#\n service mysqld start\n\n\n\n\n\n\n\n\n\nOn EL7, this is MariaDB:\n\n\n[root@server]#\n systemctl start mariadb\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreate a new GUMS database:\n\n\n[root@server]#\n /usr/bin/gums-setup-mysql-database \n\\\n\n    --user gums --host localhost:3306 --password \npassword\n \n\\\n\n    --template /etc/gums/gums.config.template\n\n\n\n\n\n\n\nNote\n\n\nThe password is saved in plaintext in the \n/etc/gums/gums.config\n file, so choose one that is not used elsewhere. The \ngums.config\n file should be readable only by the \ntomcat\n user, but this situation provides light security at best\n\n\n\n\n\n\nNote\n\n\nWhen specifying the database host with the \n--host\n option above, it is also possible to use \n$HOSTNAME\n instead of \nlocalhost\n, though \nlocalhost\n is recommended. This value corresponds to \n@SERVER@\n in a \ngums.config.template\n file, discussed below\n\n\n\n\n\n\nNote\n\n\nAlthough it is possible to specify a different location for the \n--template\n or to omit the option and use the default GUMS template (at \n/usr/lib/gums/config/gums.config.template\n), neither option is typically useful for an OSG installation\n\n\n\n\n\n\n\n\nAdd yourself as a GUMS administrator:\n\n\n[root@server]#\n gums-add-mysql-admin \nYOUR DN\n\n\n\n\n\n\n\n\n\n\n[Optional but recommended:] Apply reasonable MySQL security settings:\n\n\n[root@server]#\n /usr/bin/mysql_secure_installation\n\n\n\n\n\n\n\n\n\nUpgrade from existing GUMS server on another host\n\n\nNote for GUMS 1.4+:\n Since the database schema has not changed between GUMS 1.3 and 1.4, the database name continues to be \nGUMS_1_3\n. \nDo not\n rename \nGUMS_1_3\n database references to \nGUMS_1_4\n. There was a schema change within the GUMS 1.4 series, but this happens automatically when GUMS is started - make sure the GUMS user has permission to perform schema changes. Nevertheless the database name remains \nGUMS_1_3\n in GUMS 1.4 and GUMS 1.5.\n\n\n\n\n\n\nOn the older host, dump the GUMS_1_3 database to a text file:\n\n\n[root@server]#\n mysqldump GUMS_1_3 \n gums_1_3.sql\n\n\n\n\n\n\n\n\n\nCopy the \ngums_1_3.sql\n file from the old host to the new one\n\n\n\n\n\n\nStart MySQL:\n\n\nFor EL 6:\n\n\n[root@server]#\n service mysqld start\n\n\n\n\n\nFor EL 7:\n\n\n[root@server]#\n systemctl start mariadb\n\n\n\n\n\n\n\n\n\nLoad the old GUMS data into the new MySQL database:\n\n\n[root@server]#\n \necho\n \nCREATE DATABASE IF NOT EXISTS GUMS_1_3;\n \n|\n mysql\n\n[root@server]#\n mysql GUMS_1_3 \n gums_1_3.sql\n\n\n\n\n\n\n\n\n\n[Optional but recommended for new MySQL instances:] Apply reasonable MySQL security settings:\n\n\n[root@server]#\n /usr/bin/mysql_secure_installation\n\n\n\n\n\n\n\n\n\nSet Initial GUMS Configuration\n\n\nIn this section, you will set up an initial GUMS configuration file, either by copying in an OSG template or by copying an existing configuration from an old installation. Pick the appropriate subsection below for your environment.\n\n\nNew Installation\n\n\nIf you ran the \ngums-setup-mysql-database\n command above with the \n--template\n option, the OSG GUMS template will be used. This should have created a suitable \n/etc/gums/gums.config\n with the configuration values in this section already filled in. \nIn that case, you can skip this section.\n\n\nIf you ran the \ngums-setup-mysql-database\n command above \nwithout\n a \n--template\n option, it created a default, pre-configured \n/etc/gums/gums.config\n file. It is almost certainly not what you want. Instead, it is recommended that you start with an OSG template for your configuration.\n\n\n\n\n\n\nCopy the OSG template over the default configuration file:\n\n\n[root@server]#\n cp /etc/gums/gums.config.template /etc/gums/gums.config\n\n\n\n\n\n\n\n\n\nEdit the new \n/etc/gums/gums.config\n file and change the following settings (note: each placeholder occurs exactly once in the file):\n\n\n\n\n\n\n\n\nSearch for\n\n\nReplace with\n\n\n\n\n\n\n\n\n\n\n@USER@\n\n\nThe name of the MySQL GUMS user. If you followed the instructions above, this will be \ngums\n\n\n\n\n\n\n@PASSWORD@\n\n\nThe password for the MYSQL GUMS user (see above)\n\n\n\n\n\n\n@SERVER@\n\n\nThe name of your computer and port (e.g. \nlocalhost\n or \nmy.computer:3306\n). See note\n\n\n\n\n\n\n@DOMAINNAME@\n\n\nYour local domain (e.g. \nwisc.edu\n)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nNormally MySQL is running on the same machine as GUMS (as in the instructions above).\n\nWe \nhighly recommend\n using \nlocalhost\n instead of the actual hostname; this will cause MySQL to use a local Unix socket instead of listening on the network, which is more secure.\nIf you use \nlocalhost\n, there is no need to specify a port.\nIn either case, the value for \n@SERVER@\n \nmust\n match the value for \n--host\n used when setting up the GUMS database with the \ngums-setup-mysql-database\n command\n\n\n\n\n\n\n\n\nConfigure Log Rotation\n\n\nBy default, certain output is written to files named \n/var/log/tomcat*/catalina.YYYY-MM-DD.log\n without automatic cleanup of old logs. To configure log rotation and cleanup of \ncatalina.log\n, follow the steps below:\n\n\n\n\n\n\nChoose the Tomcat directory name based on your operating system:\n\n\n\n\n\n\n\n\nIf your operating system is...\n\n\nThen your TOMCAT DIR NAME is...\n\n\n\n\n\n\n\n\n\n\nEL6\n\n\ntomcat6\n\n\n\n\n\n\nEL7\n\n\ntomcat\n\n\n\n\n\n\n\n\n\n\n\n\nEdit \n/etc/tomcat*/logging.properties\n so that Tomcat only produces a single, undated log file:\n\n\n--- /etc/\nTOMCAT DIR NAME\n/logging.properties.orig\n\n\n+++ /etc/\nTOMCAT DIR NAME\n/logging.properties\n\n\n@@ -24,7 +24,8 @@\n\n catalina.org.apache.juli.FileHandler.level = FINE\n catalina.org.apache.juli.FileHandler.directory = ${catalina.base}/logs\n\n-catalina.org.apache.juli.FileHandler.prefix = catalina.\n\n\n+catalina.org.apache.juli.FileHandler.prefix = catalina\n\n\n+catalina.org.apache.juli.FileHandler.rotatable = false\n\n\n localhost.org.apache.juli.FileHandler.level = FINE\n localhost.org.apache.juli.FileHandler.directory = ${catalina.base}/logs\n\n\n\n\n\n\n\n\n\nWrite \n/etc/logrotate.d/tomcat_catalina_logs\n to configure \nlogrotate\n:\n\n\n/var/log/\nTOMCAT DIR NAME\n/catalina.log\n{ copytruncate weekly rotate 52 compress missingok create 0644 tomcat tomcat }\n\n\n\n\n\n\n\n\n\nServices\n\n\nThe GUMS service is actually a web application running within the Tomcat web application server. It also uses the MySQL database server for storage and the Fetch CRL service to maintain each CRL. Choose the list of services based on your host's operating system:\n\n\n\n\n\n\nFor EL6 hosts\n\n\n\n\n\n\n\n\nSoftware\n\n\nService name\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nFetch CRL\n\n\nfetch-crl-boot\n and \nfetch-crl-cron\n\n\nSee \nCA documentation\n for more info\n\n\n\n\n\n\nMySQL\n\n\nmysqld\n\n\n\n\n\n\n\n\nTomcat\n\n\ntomcat6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor EL7 hosts\n\n\n\n\n\n\n\n\nSoftware\n\n\nService name\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nFetch CRL\n\n\nfetch-crl-boot\n and \nfetch-crl-cron\n\n\nSee \nCA documentation\n for more info\n\n\n\n\n\n\nMariaDB\n\n\nmariadb\n\n\n\n\n\n\n\n\nTomcat\n\n\ntomcat\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStart the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as \nroot\n):\n\n\n\n\n\n\n\n\nTo...\n\n\nOn EL6, run the command...\n\n\nOn EL7, run the command...\n\n\n\n\n\n\n\n\n\n\nStart a service\n\n\nservice \nSERVICE-NAME\n start\n\n\nsystemctl start \nSERVICE-NAME\n\n\n\n\n\n\nStop a  service\n\n\nservice \nSERVICE-NAME\n stop\n\n\nsystemctl stop \nSERVICE-NAME\n\n\n\n\n\n\nEnable a service to start on boot\n\n\nchkconfig \nSERVICE-NAME\n on\n\n\nsystemctl enable \nSERVICE-NAME\n\n\n\n\n\n\nDisable a service from starting on boot\n\n\nchkconfig \nSERVICE-NAME\n off\n\n\nsystemctl disable \nSERVICE-NAME\n\n\n\n\n\n\n\n\nValidating GUMS\n\n\nThis section is optional, but if you would like to verify that your GUMS installation and configuration are good, consider using some or all of the sections below.\n\n\nConnect to the GUMS web page\n\n\nConnect to \nhttps://\nHOSTNAME\n:8443/gums/\n to use your GUMS instance. You must have the certificate that you used for \ngums-add-mysql-admin\n above loaded in your browser. You should see the GUMS web page load.\n\n\n\n\nNote\n\n\nJavascript must be enabled in order to make any configuration changes on the web interface.\n\n\n\n\nIf you do not see it load, check a few things:\n\n\nFor EL 6:\n\n\n\n\nLook for errors in \n/var/log/tomcat6/catalina.out\n and \n/var/log/tomcat6/catalina.err\n.\n\n\nLook for errors in \n/var/log/tomcat6/trustmanager.log\n. There are likely to be CRL errors in this file, this can be ignored unless all your CA's get CRL errors in which case you should check to make sure that your CRL updates are running correctly.\n\n\n\n\nFor EL 7:\n\n\n\n\nLook for errors in \n/var/log/tomcat/catalina.*.log\n\n\nLook for errors in \n/var/log/tomcat/trustmanager.log\n. There are likely to be CRL errors in this file, this can be ignored unless all your CA's get CRL errors in which case you should check to make sure that your CRL updates are running correctly.\n\n\n\n\nFor all systems:\n\n\n\n\nEnsure that you have an http certificate in \n/etc/grid-security/http/httpcert.pem\n and \n/etc/grid-security/http/httpkey.pem\n. Make sure it is readable by the \ntomcat\n user. Permissions should be as follows:\n\n\n\n\n[root@server]#\n ls -l /etc/grid-security/http/\n\ntotal 8\n\n\n-r--r--r-- 1 tomcat tomcat 1671 Jul 2 15:54 httpcert.pem\n\n\n-r-------- 1 tomcat tomcat 1675 Jul 2 15:54 httpkey.pem\n\n\n\n\n\n\nIf you change the permissions/ownership, make sure to restart tomcat so that your changes take effect.\n\n\nCheck accounts\n\n\nAfter you connect to the GUMS web page, go to the Summary tab to check the configuration. You should see several dozen OSG VOs listed.\n\n\nIn the Account column on the summary page, you will see the local Unix user accounts that these VO users will be mapped to. It is critical that these accounts exist on the gatekeeper and worker nodes at your site. If they do not, there will be errors when users attempt to access your site.\n\n\nUpdate VO members list\n\n\nGUMS contacts each VOMS server to update its knowledge of VO membership every 6 hours. After installing or updating GUMS, you should trigger the update manually by going to the Update VO Members tab, and clicking update.\n\n\nYou can track the progress of the update process by watching a log file.\n\n\nFor EL 6:\n\n\n[root@server]#\n tail -f /var/log/tomcat6/gums-service-admin.log\n\n\n\n\n\nFor EL 7:\n\n\n[root@server]#\n tail -f /var/log/tomcat/gums-service-admin.log\n\n\n\n\n\nWith so many VOMS servers in the OSG config, several member updates may fail for various reasons (e.g., host down \"connect timed out\", bad or expired host certificates, etc.). Unfortunately, this situation is normal. Typically, you will see about 5 or 6 failed updates, with the rest succeeding. The update will take a while and then should display any errors that occurred during the updates. To get more details or track the update process in real time, look at \n/var/log/gums-service-admin.log\n.\n\n\nMap a known good user DN\n\n\n\n\nGo to Map Grid Identity to Account tab: \nhttps://\nHOSTNAME\n:8443/gums/map_grid_identity_form.jsp\n\n\nFill in the required info. Service DN means the DN of the host certificate of your CE (see above). Use the DN of a user (probably yourself) who you know belongs to a particular VO. Fill in the VO name in the VOMS FQAN field.\n\n\nClick \"map user\". A failed mapping will display \"null\". A successful mapping will display a UNIX account name.\n\n\n\n\nMiscellaneous Procedures\n\n\nForcing GUMS to update the set of users\n\n\nGUMS automatically contacts each VOMS server every 6 hours to update its knowledge of VO membership. To trigger a manual update:\n\n\n\n\nAccess the \u201cUpdate VO Members\u201d tab\n\n\nClick \"Update\"\n\n\n\n\n[Optional:] Monitor update progress via a log file:\n\n\nFor EL 6:\n\n\n[root@server]#\n tail -f /var/log/tomcat6/gums-service-admin.log\n\n\n\n\n\nFor EL 7:\n\n\n[root@server]#\n tail -f /var/log/tomcat/gums-service-admin.log\n\n\n\n\n\n\n\n\n\nWith so many VOMS servers in the OSG config, several member updates may fail for various reasons (e.g., host down \"connect timed out\", bad or expired host certificates, etc.). Unfortunately, this situation is normal.\n\n\nUpdating the GUMS configuration\n\n\nPeriodically, the OSG Grid Operations Center will release an updated template for the GUMS configuration that updates information about an existing VO or adds a new VO. You may get the update as part of a regular update process, or you can force an update by using yum:\n\n\n[root@server]#\n yum update osg-gums-config\n\n\n\n\n\nThis step does \nnot\n update your GUMS configuration (\n/etc/gums/gums.config\n) but will update the template for your configuration (\n/etc/gums/gums.config.template\n), because RPM cannot merge configuration changes. Instead, use GUMS to merge in the new VO configuration information:\n\n\n\n\nGo to the Merge Configuration tab: \nhttps://\nHOSTNAME\n:8443/gums/mergeConfiguration.jsp\n\n\n\n\nCut and paste the URL of the OSG template into the Configuration URI field\n\n\nFor the template provided in the RPM, use: \nfile:///etc/gums/gums.config.template\n\n\nTo fetch it directly from the GOC, use \nhttp://repo.opensciencegrid.org/pacman/tarballs/vo-version/gums.template\n\n\n\n\n\n\nClick Merge\n\n\nYou should get a green success message if it has worked, along with a suggestion that you update the VO members\n\n\n\n\n\n\nCheck the Summary tab to verify the set of VOs you have, as well as their accounts\n\n\n\n\n\n\nTroubleshooting\n\n\nUseful Configuration and Log Files\n\n\nConfiguration Files\n\n\n\n\n\n\n\n\nService or Process\n\n\nConfiguration File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nMySQL\n\n\n/etc/my.cnf\n\n\nMySQL configuration, e.g. server port\n\n\n\n\n\n\ntomcat (EL6)\n\n\n/etc/tomcat6/\n\n\nTomcat configuration files\n\n\n\n\n\n\ntomcat (EL7)\n\n\n/etc/tomcat/\n\n\nTomcat configuration files\n\n\n\n\n\n\n\n\nLog files\n\n\n\n\n\n\n\n\nService or Process\n\n\nLog File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ntomcat (EL6)\n\n\n/var/log/tomcat6/catalina.out\n\n\nThis is the Tomcat log file. Problems (and a lot of noise) are reported here.\n\n\n\n\n\n\n\n\n/var/log/tomcat6/trustmanager.log\n\n\nThe trustmanager handles things related to authentication. Useful errors are sometimes here.\n\n\n\n\n\n\ntomcat (EL6/EL7)\n\n\n/var/log/tomcat*/catalina.*.log\n\n\nThese are the Tomcat log files. Problems (and a lot of noise) are reported here.\n\n\n\n\n\n\ntomcat (EL6/EL7)\n\n\n/var/log/tomcat*/catalina.log\n\n\nAlternate non-rotated location for tomcat log file. Not the same as \ncatalina.out\n. Problems (and a lot of noise) are reported here.\n\n\n\n\n\n\n\n\n/var/log/tomcat/trustmanager.log\n\n\nThe trustmanager handles things related to authentication. Useful errors are sometimes here.\n\n\n\n\n\n\nGUMS\n (EL6)\n\n\n/var/log/tomcat6/gums-service-admin.log\n\n\nGUMS outputs error messages related to its operations here.\n\n\n\n\n\n\n\n\n/var/log/tomcat6/gums-service-cybersecurity.log\n\n\nGUMS outputs security related messages to this file.\n\n\n\n\n\n\nGUMS\n (EL7)\n\n\n/var/log/tomcat/gums-service-admin.log\n\n\nGUMS outputs error messages related to its operations here.\n\n\n\n\n\n\n\n\n/var/log/tomcat/gums-service-cybersecurity.log\n\n\nGUMS outputs security related messages to this file.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGUMS\n (EL6/EL7)\n\n\n/var/log/gums/gums-developer.root.log\n\n\n\n\n\n\n\n\n\n\n/var/log/gums/gums-egee-security.root.log\n\n\nGUMS may also output some security related messages to this file as well.\n\n\n\n\n\n\n\n\n/var/log/gums/gums-privilege.root.log\n\n\nGUMS outputs mapping related errors to this file.\n\n\n\n\n\n\n\n\nHow to get Help?\n\n\nTo get assistance please use the \nHelp Procedure\n.\n\n\nReferences\n\n\n\n\nOfficial Tomcat 6 documentation\n\n\nOfficial Tomcat 7 documentation\n\n\nOfficial Hibernate documentation\n (Hibernate is the GUMS database interface)\n\n\n\n\nHost\n\n\nFor security reasons, it is recommended to install GUMS on a separate host from the CE,\n    but it is not necessary\n\n\nUsers\n\n\nThe GUMS installation will create two users unless they exist already:\n\n\n\n\n\n\n\n\nUser\n\n\nDefault UID\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nmysql\n\n\n27\n\n\nRuns the MySQL database server, which GUMS uses\n\n\n\n\n\n\ntomcat\n\n\n91\n\n\nRuns the Tomcat web application server, which runs GUMS\n\n\n\n\n\n\n\n\nNote that if UIDs 27 and 91 are taken already but not used for the appropriate users, you will experience errors.\n\n\nCertificates\n\n\n\n\n\n\n\n\nCertificate\n\n\nOwner, Permissions\n\n\nPath\n\n\n\n\n\n\n\n\n\n\nHTTP service certificate\n\n\ntomcat:tomcat\n, 0644\n\n\n/etc/grid-security/http/httpcert.pem\n\n\n\n\n\n\nHTTP service key\n\n\ntomcat:tomcat\n, 0600\n\n\n/etc/grid-security/http/httpkey.pem\n\n\n\n\n\n\n\n\nNetworking\n\n\nGUMS communicates on TCP port 8443; this port must be accessible to the Compute Element and any other hosts that need to authenticate via GUMS.", 
            "title": "GUMS"
        }, 
        {
            "location": "/security/install-gums/#gums-install-guide", 
            "text": "This document is intended for site administrators who want to install and configure the GUMS service.  GUMS is a service that authorizes and maps users from their global (X.509) identity to a local (Linux user) identity. It is not a required service (some sites use the simple  edg-mkgridmap  to construct a  grid-mapfile  instead), but it is commonly used. GUMS is useful when more than one resource (Compute Element, Storage Element, etc.) needs to authorize or map users, because it helps them share data. It is particularly helpful when using gLExec at a site, because gLExec runs on every worker node and needs authorization and mapping information. GUMS is a web application that runs in Tomcat.", 
            "title": "GUMS Install Guide"
        }, 
        {
            "location": "/security/install-gums/#before-starting", 
            "text": "Before starting the installation process, consider the following points (consulting  the Reference section below  as needed):   User IDs:  If they do not exist already, the installation will create the Linux users  mysql  (UID 27) and  tomcat  (UID 91)  Service certificate:  The GUMS service uses a host certificate at  /etc/grid-security/http/httpcert.pem  and an accompanying key at  /etc/grid-security/http/httpkey.pem  Network ports:  Hosts using your GUMS server for authentication (e.g., HTCondor-CE, GridFTP) must be able to contact it on port 8443 (TCP)   As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system  Obtain root access to the host  Prepare the  required Yum repositories  Install  CA certificates", 
            "title": "Before Starting"
        }, 
        {
            "location": "/security/install-gums/#installing-gums", 
            "text": "Whether installing or upgrading GUMS, please make sure to follow  these instructions  for updating/installing Java to work correctly with GUMS  If you have an existing GUMS installation on the same host, shut down Tomcat.\n    The service is named  tomcat6  on EL6 and  tomcat  on EL7   Install the GUMS Service:  [root@server]#  yum install osg-gums    Configure Tomcat to use GSI:  [root@server]#  /var/lib/trustmanager-tomcat/configure.sh   Note  This step will overwrite your  server.xml  file in your tomcat configuration directory ( /etc/tomcat6  on EL6,  /etc/tomcat  on EL7).\nIf you are using Tomcat on this host for non-grid purposes, you may want to save the  server.xml  file first, run the script, then merge your own configuration back into the file", 
            "title": "Installing GUMS"
        }, 
        {
            "location": "/security/install-gums/#configuring-gums", 
            "text": "", 
            "title": "Configuring GUMS"
        }, 
        {
            "location": "/security/install-gums/#configure-gums-database", 
            "text": "In this section, you will configure the GUMS MySQL database, either by creating a new database or by copying an existing GUMS database. Pick the appropriate subsection below for your environment.", 
            "title": "Configure GUMS database"
        }, 
        {
            "location": "/security/install-gums/#new-installation", 
            "text": "Start the database server    On EL6, this is MySQL:  [root@server]#  service mysqld start    On EL7, this is MariaDB:  [root@server]#  systemctl start mariadb      Create a new GUMS database:  [root@server]#  /usr/bin/gums-setup-mysql-database  \\ \n    --user gums --host localhost:3306 --password  password   \\ \n    --template /etc/gums/gums.config.template   Note  The password is saved in plaintext in the  /etc/gums/gums.config  file, so choose one that is not used elsewhere. The  gums.config  file should be readable only by the  tomcat  user, but this situation provides light security at best    Note  When specifying the database host with the  --host  option above, it is also possible to use  $HOSTNAME  instead of  localhost , though  localhost  is recommended. This value corresponds to  @SERVER@  in a  gums.config.template  file, discussed below    Note  Although it is possible to specify a different location for the  --template  or to omit the option and use the default GUMS template (at  /usr/lib/gums/config/gums.config.template ), neither option is typically useful for an OSG installation     Add yourself as a GUMS administrator:  [root@server]#  gums-add-mysql-admin  YOUR DN     [Optional but recommended:] Apply reasonable MySQL security settings:  [root@server]#  /usr/bin/mysql_secure_installation", 
            "title": "New Installation"
        }, 
        {
            "location": "/security/install-gums/#upgrade-from-existing-gums-server-on-another-host", 
            "text": "Note for GUMS 1.4+:  Since the database schema has not changed between GUMS 1.3 and 1.4, the database name continues to be  GUMS_1_3 .  Do not  rename  GUMS_1_3  database references to  GUMS_1_4 . There was a schema change within the GUMS 1.4 series, but this happens automatically when GUMS is started - make sure the GUMS user has permission to perform schema changes. Nevertheless the database name remains  GUMS_1_3  in GUMS 1.4 and GUMS 1.5.    On the older host, dump the GUMS_1_3 database to a text file:  [root@server]#  mysqldump GUMS_1_3   gums_1_3.sql    Copy the  gums_1_3.sql  file from the old host to the new one    Start MySQL:  For EL 6:  [root@server]#  service mysqld start  For EL 7:  [root@server]#  systemctl start mariadb    Load the old GUMS data into the new MySQL database:  [root@server]#   echo   CREATE DATABASE IF NOT EXISTS GUMS_1_3;   |  mysql [root@server]#  mysql GUMS_1_3   gums_1_3.sql    [Optional but recommended for new MySQL instances:] Apply reasonable MySQL security settings:  [root@server]#  /usr/bin/mysql_secure_installation", 
            "title": "Upgrade from existing GUMS server on another host"
        }, 
        {
            "location": "/security/install-gums/#set-initial-gums-configuration", 
            "text": "In this section, you will set up an initial GUMS configuration file, either by copying in an OSG template or by copying an existing configuration from an old installation. Pick the appropriate subsection below for your environment.", 
            "title": "Set Initial GUMS Configuration"
        }, 
        {
            "location": "/security/install-gums/#new-installation_1", 
            "text": "If you ran the  gums-setup-mysql-database  command above with the  --template  option, the OSG GUMS template will be used. This should have created a suitable  /etc/gums/gums.config  with the configuration values in this section already filled in.  In that case, you can skip this section.  If you ran the  gums-setup-mysql-database  command above  without  a  --template  option, it created a default, pre-configured  /etc/gums/gums.config  file. It is almost certainly not what you want. Instead, it is recommended that you start with an OSG template for your configuration.    Copy the OSG template over the default configuration file:  [root@server]#  cp /etc/gums/gums.config.template /etc/gums/gums.config    Edit the new  /etc/gums/gums.config  file and change the following settings (note: each placeholder occurs exactly once in the file):     Search for  Replace with      @USER@  The name of the MySQL GUMS user. If you followed the instructions above, this will be  gums    @PASSWORD@  The password for the MYSQL GUMS user (see above)    @SERVER@  The name of your computer and port (e.g.  localhost  or  my.computer:3306 ). See note    @DOMAINNAME@  Your local domain (e.g.  wisc.edu )      Note  Normally MySQL is running on the same machine as GUMS (as in the instructions above). \nWe  highly recommend  using  localhost  instead of the actual hostname; this will cause MySQL to use a local Unix socket instead of listening on the network, which is more secure.\nIf you use  localhost , there is no need to specify a port.\nIn either case, the value for  @SERVER@   must  match the value for  --host  used when setting up the GUMS database with the  gums-setup-mysql-database  command", 
            "title": "New Installation"
        }, 
        {
            "location": "/security/install-gums/#configure-log-rotation", 
            "text": "By default, certain output is written to files named  /var/log/tomcat*/catalina.YYYY-MM-DD.log  without automatic cleanup of old logs. To configure log rotation and cleanup of  catalina.log , follow the steps below:    Choose the Tomcat directory name based on your operating system:     If your operating system is...  Then your TOMCAT DIR NAME is...      EL6  tomcat6    EL7  tomcat       Edit  /etc/tomcat*/logging.properties  so that Tomcat only produces a single, undated log file:  --- /etc/ TOMCAT DIR NAME /logging.properties.orig  +++ /etc/ TOMCAT DIR NAME /logging.properties  @@ -24,7 +24,8 @@ \n catalina.org.apache.juli.FileHandler.level = FINE\n catalina.org.apache.juli.FileHandler.directory = ${catalina.base}/logs -catalina.org.apache.juli.FileHandler.prefix = catalina.  +catalina.org.apache.juli.FileHandler.prefix = catalina  +catalina.org.apache.juli.FileHandler.rotatable = false \n\n localhost.org.apache.juli.FileHandler.level = FINE\n localhost.org.apache.juli.FileHandler.directory = ${catalina.base}/logs    Write  /etc/logrotate.d/tomcat_catalina_logs  to configure  logrotate :  /var/log/ TOMCAT DIR NAME /catalina.log\n{ copytruncate weekly rotate 52 compress missingok create 0644 tomcat tomcat }", 
            "title": "Configure Log Rotation"
        }, 
        {
            "location": "/security/install-gums/#services", 
            "text": "The GUMS service is actually a web application running within the Tomcat web application server. It also uses the MySQL database server for storage and the Fetch CRL service to maintain each CRL. Choose the list of services based on your host's operating system:    For EL6 hosts     Software  Service name  Notes      Fetch CRL  fetch-crl-boot  and  fetch-crl-cron  See  CA documentation  for more info    MySQL  mysqld     Tomcat  tomcat6        For EL7 hosts     Software  Service name  Notes      Fetch CRL  fetch-crl-boot  and  fetch-crl-cron  See  CA documentation  for more info    MariaDB  mariadb     Tomcat  tomcat        Start the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as  root ):     To...  On EL6, run the command...  On EL7, run the command...      Start a service  service  SERVICE-NAME  start  systemctl start  SERVICE-NAME    Stop a  service  service  SERVICE-NAME  stop  systemctl stop  SERVICE-NAME    Enable a service to start on boot  chkconfig  SERVICE-NAME  on  systemctl enable  SERVICE-NAME    Disable a service from starting on boot  chkconfig  SERVICE-NAME  off  systemctl disable  SERVICE-NAME", 
            "title": "Services"
        }, 
        {
            "location": "/security/install-gums/#validating-gums", 
            "text": "This section is optional, but if you would like to verify that your GUMS installation and configuration are good, consider using some or all of the sections below.", 
            "title": "Validating GUMS"
        }, 
        {
            "location": "/security/install-gums/#connect-to-the-gums-web-page", 
            "text": "Connect to  https:// HOSTNAME :8443/gums/  to use your GUMS instance. You must have the certificate that you used for  gums-add-mysql-admin  above loaded in your browser. You should see the GUMS web page load.   Note  Javascript must be enabled in order to make any configuration changes on the web interface.   If you do not see it load, check a few things:  For EL 6:   Look for errors in  /var/log/tomcat6/catalina.out  and  /var/log/tomcat6/catalina.err .  Look for errors in  /var/log/tomcat6/trustmanager.log . There are likely to be CRL errors in this file, this can be ignored unless all your CA's get CRL errors in which case you should check to make sure that your CRL updates are running correctly.   For EL 7:   Look for errors in  /var/log/tomcat/catalina.*.log  Look for errors in  /var/log/tomcat/trustmanager.log . There are likely to be CRL errors in this file, this can be ignored unless all your CA's get CRL errors in which case you should check to make sure that your CRL updates are running correctly.   For all systems:   Ensure that you have an http certificate in  /etc/grid-security/http/httpcert.pem  and  /etc/grid-security/http/httpkey.pem . Make sure it is readable by the  tomcat  user. Permissions should be as follows:   [root@server]#  ls -l /etc/grid-security/http/ total 8  -r--r--r-- 1 tomcat tomcat 1671 Jul 2 15:54 httpcert.pem  -r-------- 1 tomcat tomcat 1675 Jul 2 15:54 httpkey.pem   If you change the permissions/ownership, make sure to restart tomcat so that your changes take effect.", 
            "title": "Connect to the GUMS web page"
        }, 
        {
            "location": "/security/install-gums/#check-accounts", 
            "text": "After you connect to the GUMS web page, go to the Summary tab to check the configuration. You should see several dozen OSG VOs listed.  In the Account column on the summary page, you will see the local Unix user accounts that these VO users will be mapped to. It is critical that these accounts exist on the gatekeeper and worker nodes at your site. If they do not, there will be errors when users attempt to access your site.", 
            "title": "Check accounts"
        }, 
        {
            "location": "/security/install-gums/#update-vo-members-list", 
            "text": "GUMS contacts each VOMS server to update its knowledge of VO membership every 6 hours. After installing or updating GUMS, you should trigger the update manually by going to the Update VO Members tab, and clicking update.  You can track the progress of the update process by watching a log file.  For EL 6:  [root@server]#  tail -f /var/log/tomcat6/gums-service-admin.log  For EL 7:  [root@server]#  tail -f /var/log/tomcat/gums-service-admin.log  With so many VOMS servers in the OSG config, several member updates may fail for various reasons (e.g., host down \"connect timed out\", bad or expired host certificates, etc.). Unfortunately, this situation is normal. Typically, you will see about 5 or 6 failed updates, with the rest succeeding. The update will take a while and then should display any errors that occurred during the updates. To get more details or track the update process in real time, look at  /var/log/gums-service-admin.log .", 
            "title": "Update VO members list"
        }, 
        {
            "location": "/security/install-gums/#map-a-known-good-user-dn", 
            "text": "Go to Map Grid Identity to Account tab:  https:// HOSTNAME :8443/gums/map_grid_identity_form.jsp  Fill in the required info. Service DN means the DN of the host certificate of your CE (see above). Use the DN of a user (probably yourself) who you know belongs to a particular VO. Fill in the VO name in the VOMS FQAN field.  Click \"map user\". A failed mapping will display \"null\". A successful mapping will display a UNIX account name.", 
            "title": "Map a known good user DN"
        }, 
        {
            "location": "/security/install-gums/#miscellaneous-procedures", 
            "text": "", 
            "title": "Miscellaneous Procedures"
        }, 
        {
            "location": "/security/install-gums/#forcing-gums-to-update-the-set-of-users", 
            "text": "GUMS automatically contacts each VOMS server every 6 hours to update its knowledge of VO membership. To trigger a manual update:   Access the \u201cUpdate VO Members\u201d tab  Click \"Update\"   [Optional:] Monitor update progress via a log file:  For EL 6:  [root@server]#  tail -f /var/log/tomcat6/gums-service-admin.log  For EL 7:  [root@server]#  tail -f /var/log/tomcat/gums-service-admin.log    With so many VOMS servers in the OSG config, several member updates may fail for various reasons (e.g., host down \"connect timed out\", bad or expired host certificates, etc.). Unfortunately, this situation is normal.", 
            "title": "Forcing GUMS to update the set of users"
        }, 
        {
            "location": "/security/install-gums/#updating-the-gums-configuration", 
            "text": "Periodically, the OSG Grid Operations Center will release an updated template for the GUMS configuration that updates information about an existing VO or adds a new VO. You may get the update as part of a regular update process, or you can force an update by using yum:  [root@server]#  yum update osg-gums-config  This step does  not  update your GUMS configuration ( /etc/gums/gums.config ) but will update the template for your configuration ( /etc/gums/gums.config.template ), because RPM cannot merge configuration changes. Instead, use GUMS to merge in the new VO configuration information:   Go to the Merge Configuration tab:  https:// HOSTNAME :8443/gums/mergeConfiguration.jsp   Cut and paste the URL of the OSG template into the Configuration URI field  For the template provided in the RPM, use:  file:///etc/gums/gums.config.template  To fetch it directly from the GOC, use  http://repo.opensciencegrid.org/pacman/tarballs/vo-version/gums.template    Click Merge  You should get a green success message if it has worked, along with a suggestion that you update the VO members    Check the Summary tab to verify the set of VOs you have, as well as their accounts", 
            "title": "Updating the GUMS configuration"
        }, 
        {
            "location": "/security/install-gums/#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/security/install-gums/#useful-configuration-and-log-files", 
            "text": "Configuration Files     Service or Process  Configuration File  Description      MySQL  /etc/my.cnf  MySQL configuration, e.g. server port    tomcat (EL6)  /etc/tomcat6/  Tomcat configuration files    tomcat (EL7)  /etc/tomcat/  Tomcat configuration files     Log files     Service or Process  Log File  Description      tomcat (EL6)  /var/log/tomcat6/catalina.out  This is the Tomcat log file. Problems (and a lot of noise) are reported here.     /var/log/tomcat6/trustmanager.log  The trustmanager handles things related to authentication. Useful errors are sometimes here.    tomcat (EL6/EL7)  /var/log/tomcat*/catalina.*.log  These are the Tomcat log files. Problems (and a lot of noise) are reported here.    tomcat (EL6/EL7)  /var/log/tomcat*/catalina.log  Alternate non-rotated location for tomcat log file. Not the same as  catalina.out . Problems (and a lot of noise) are reported here.     /var/log/tomcat/trustmanager.log  The trustmanager handles things related to authentication. Useful errors are sometimes here.    GUMS  (EL6)  /var/log/tomcat6/gums-service-admin.log  GUMS outputs error messages related to its operations here.     /var/log/tomcat6/gums-service-cybersecurity.log  GUMS outputs security related messages to this file.    GUMS  (EL7)  /var/log/tomcat/gums-service-admin.log  GUMS outputs error messages related to its operations here.     /var/log/tomcat/gums-service-cybersecurity.log  GUMS outputs security related messages to this file.               GUMS  (EL6/EL7)  /var/log/gums/gums-developer.root.log      /var/log/gums/gums-egee-security.root.log  GUMS may also output some security related messages to this file as well.     /var/log/gums/gums-privilege.root.log  GUMS outputs mapping related errors to this file.", 
            "title": "Useful Configuration and Log Files"
        }, 
        {
            "location": "/security/install-gums/#how-to-get-help", 
            "text": "To get assistance please use the  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/security/install-gums/#references", 
            "text": "Official Tomcat 6 documentation  Official Tomcat 7 documentation  Official Hibernate documentation  (Hibernate is the GUMS database interface)", 
            "title": "References"
        }, 
        {
            "location": "/security/install-gums/#host", 
            "text": "For security reasons, it is recommended to install GUMS on a separate host from the CE,\n    but it is not necessary", 
            "title": "Host"
        }, 
        {
            "location": "/security/install-gums/#users", 
            "text": "The GUMS installation will create two users unless they exist already:     User  Default UID  Comment      mysql  27  Runs the MySQL database server, which GUMS uses    tomcat  91  Runs the Tomcat web application server, which runs GUMS     Note that if UIDs 27 and 91 are taken already but not used for the appropriate users, you will experience errors.", 
            "title": "Users"
        }, 
        {
            "location": "/security/install-gums/#certificates", 
            "text": "Certificate  Owner, Permissions  Path      HTTP service certificate  tomcat:tomcat , 0644  /etc/grid-security/http/httpcert.pem    HTTP service key  tomcat:tomcat , 0600  /etc/grid-security/http/httpkey.pem", 
            "title": "Certificates"
        }, 
        {
            "location": "/security/install-gums/#networking", 
            "text": "GUMS communicates on TCP port 8443; this port must be accessible to the Compute Element and any other hosts that need to authenticate via GUMS.", 
            "title": "Networking"
        }, 
        {
            "location": "/worker-node/install-glexec/", 
            "text": "Glexec Installation Guide\n\n\n\n\nWarning\n\n\nAs of the June 2017 release of OSG 3.4.0, this software is officially deprecated.  Support is scheduled to end as of May 2018.\n\n\n\n\nThis document is intended for System Administrators that are installing the OSG version of glexec.\n\n\nGlexec is commonly used for what are referred to as \"pilot\" or \"glidein\" jobs.\n\n\nTraditionally, users submitted their jobs directly to a remote site (or compute element gatekeeper). The user job was authenticated/authorized to run at that site based on the user\u2019s proxy credentials and run under the local unix account assigned.\n\n\nIn a pilot-based infrastructure, users submit their jobs to a centralized site (or queue). The pilot/glidein software at the centralized site then recognizes there is a demand for computing resources. It will then submit what is called a pilot/glidein job to a remote site. This pilot job gets authenticated/authorized to run on a worker node in that site\u2019s cluster. It will then \"pull\" down user jobs from the centralized queue and execute them. Both the pilot and the user job are run under the pilot job\u2019s proxy certificate credentials and local unix account. This represents a security problem in pilot-based systems as there is no authentication/authorization of the individual user\u2019s proxy credentials and, thus, the user\u2019s jobs do not run using it\u2019s own local unix account.\n\n\nGlexec is a security tool that can be used to resolve this problem. It is meant to be used by VOs that run these pilot-based jobs. It has a number of authentication plugins and can be used both by European grid and by OSG.\n\n\nThe pilot job will \"pull\" user jobs down from the central queue and invoke glexec which will then\n\n\n\n\nauthenticate the user job\u2019s proxy,\n\n\nperform an authorization callout (to GUMS in the case of OSG, or possibly a gridmapfile) similar to that done by the gatekeeper,\n\n\nand then run the user job under the local account assigned by the authorization service for that user.\n\n\n\n\nIn effect, glexec functions much the same as a compute element gatekeeper, except these functions are now performed on the individual worker node. The pilot jobs authentication/authorization is done by the gatekeeper and the individual user jobs are now done by glexec on the individual worker node.\n\n\nMany worker node clusters use shared file systems like NFS for much of their software and user home accounts. Since glexec is an suid program, it must be installed on every single worker node individually. Most shared file systems do not handle this correctly so it cannot and must not be NFS-exported.\n\n\nFor more information regarding pilot-based systems and glexec:\n\n\n\n\nglideinWMS - The glidein based WMS\n\n\nAddressing the pilot security problem with gLExec (pdf)\n\n\n\n\nEngineering Considerations\n\n\nIf glexec is to be used at a site, it should be installed and configured on every worker node.\n\n\nA large number of batch slots using glexec can occasionally put an enormous strain on GUMS servers and cause overloading and client timeouts. In order to survive peak loads, the sysctl parameter \nnet.core.somaxconn\n on a GUMS server machine should be set at least as high as the maximum number of job slots that might attempt to contact the server at about the same time.  For example, Fermilab set the value to 4096 on each of two servers and tested with a continuous load from 5000 job slots.\n\n\nRequirements\n\n\nThese are the requirements that must be met to install glexec.\n\n\n\n\nNote\n\n\nNormally you will install the \nOSG worker node\n first. Installing the \nosg-wn-client-glexec\n package will also install the worker node, but we do not duplicate instructions specific to the worker node here.\n\n\n\n\n\n\nVerify you are using one of the \nsupported platforms\n.\n\n\nPrior to installing \nglexec\n, verify the \nyum repositories\n are correctly configured.\n\n\nroot\n access to the host is required.\n\n\nThe worker nodes will need the \nCA certificates\n used to sign the proxies.\n\n\nfetch-crl\n should be enabled and working.\n\n\n\n\nThe glexec installation will create two users unless they are already created.\n\n\n\n\n\n\n\n\nUser\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nglexec\n\n\nReduced privilege separate id used to improve security. If creating the account by hand, set the default gid of the \nglexec\n user to be a group that is also called \nglexec\n.\n\n\n\n\n\n\ngratia\n\n\nNeeded for the glexec gratia probe which is also automatically installed.\n\n\n\n\n\n\n\n\nIn addition, OSG glexec requires a range of \ngroup ids\n for tracking purposes. You don\u2018t actually have to create the group entries but it is recommended to do so in order to reserve the gids and so they can be associated with names in the \n/usr/bin/id\n command. The recommended names are \nglexecNN\n where NN is a number starting from 00.\n\n\n\n\nDefine at least 4 group ids per batch slot per worker node. A conservative way to handle this is to multiply the number of batch slots on the largest worker node by 6 and then share the group ids between all the worker nodes.\n\n\nThey must be consecutive and in any range (default range is 65000-65049, configured in the \nconfiguring glexec\n section below).\n\n\nThe same group IDs can be used on every worker node.\n\n\n\n\nInstall Instructions\n\n\n\n\nNote\n\n\nThe glexec tracking function requires a part of HTCondor. There are multiple ways to install HTCondor, for details see \nthese instructions\n. If you want a minimal install, you can run just this command to install the needed piece from the OSG distribution:\n\n\nroot@host #\n yum install condor-procd\n\n\n\n\n\n\n\nAfter meeting all the requirements in the previous section, install glexec with this command:\n\n\nroot@host #\n yum install osg-wn-client-glexec\n\n\n\n\n\nConfiguring glexec\n\n\nThe following steps need to be done after the glexec installation is complete.\n\n\n\n\nFirst, review the contents of \n/etc/glexec.conf\n. All of the defaults should be fine, but if you want to change the behavior, the parameters are described in \nman glexec.conf\n.\n\n\n\n\nNext, review all of the contents of \n/etc/lcmaps.db\n and in particular update the following pieces.\n\n\n\n\n\n\nIf you have GUMS, change \nGUMS_HOST\n in the following line to the fully qualified domain name of your GUMS server:\n\n\n\u2013endpoint https://\nGUMS_HOST\n:8443/gums/services/GUMSXACMLAuthorizationServicePort\n\n\n\n\n\n\n\n\n\n\nIf you want to use a range of tracking group ids other than the default as described in the \nRequirements\n section above, uncomment and change the \n-min-gid\n and \n-max-gid\n lines to your chosen values:\n\n\n-min-gid 65000\n \n-max-gid 65049\n\n\n\n\n\n\n\n\n\n\nUncomment the following two lines:\n\n\nglexectracking = \nlcmaps_glexec_tracking.mod\n\n                 \n-exec /usr/sbin/glexec_monitor\n\n\n\n\n\n\n\n\n\n\nIf you have GUMS, uncomment the following policy toward the end of the file:\n\n\nverifyproxy -\n gumsclient\ngumsclient -\n glexectracking\n\n\n\n\n\nor if you have do not have GUMS and want to use a gridmapfile, uncomment the following policy:\n\n\nverifyproxy -\n gridmapfile\ngridmapfile -\n glexectracking\n\n\n\n\n\n\n\n\n\n\n\n\n\nTesting the Installation of glexec\n\n\nNow, \nas a non-privileged user (not root)\n, do the following (where \n is your VO, and \n is your uid as reported by \n/usr/bin/id\n):\n\n\nuser@host $\n voms-proxy-init -voms \nYOURVO\n:/\nYOURVO\n\n\nuser@host $\n \nexport\n \nGLEXEC_CLIENT_CERT\n=\n/tmp/x509up_u\nUID\n\n\nuser@host $\n \nexport\n \nX509_USER_PROXY\n=\n/tmp/x509up_u\nUID\n\n\nuser@host $\n /usr/sbin/glexec /usr/bin/id\n\nuser@host $\n \nuid\n=\n13160\n(\nfnalgrid\n)\n \ngid\n=\n9767\n(\nfnalgrid\n)\n \ngroups\n=\n65000\n(\nglexec00\n)\n\n\n\n\n\n\nIf \nglexec\n is successful, it will print out the uid and gid that your proxy would normally be mapped to by your GUMS server, plus a supplementary tracking group. (The actual names and numbers will be different from what you see above.)\n\n\nIf you have problems, please read about \ntroubleshooting glexec\n.\n\n\nGlexec log files\n\n\nGlexec sends all its log information by default to syslog. By default they go to \n/var/log/messages\n, but this may differ if you have customized your syslog setup. Here are some sample messages:\n\n\nApr 25 16:36:16 fermicloud053 glexec[2867]: lcmaps: lcmaps.mod-PluginInit(): plugin glexectracking not found (arguments: )\nApr 25 16:36:16 fermicloud053 glexec[2867]: lcmaps: lcmaps.mod-lcmaps_startPluginManager(): error initializing plugin: glexectracking\nApr 25 16:36:16 fermicloud053 glexec[2867]: lcmaps: lcmaps_init() error: could not start plugin manager\nApr 25 16:36:16 fermicloud053 glexec[2867]: Initialisation of LCMAPS failed.\n\n\n\n\n\nThese particular messages are pretty common, caused by forgetting to uncomment the beginning of the \nglexectracking\n rule in \n/etc/lcmaps.db\n.\n\n\nIt is possible to redirect glexec log messages to a different file with standard syslog. To do that, choose one of the \nLOG_LOCAL[0-7]\n log facilities that are unused, for example \nLOG_LOCAL1\n. Then set the following in \n/etc/glexec.conf\n:\n\n\nsyslog_facility = LOG_LOCAL1\n\n\n\n\n\nand add a corresponding parameter to the \nlcmaps_glexec_tracking.mod\n entry in \n/etc/lcmaps.db\n:\n\n\n  \n-log-facility LOG_LOCAL1\n\n\n\n\n\n\nThen in \n/etc/rsyslog.conf\n, add a line like this:\n\n\nlocal1.* /var/log/glexec.log\n\n\n\n\n\nand also exclude those messages from \n/var/log/messages\n by adding \nlocal1.none\n after other wildcards on the existing \n/var/log/messages\n line, for example:\n\n\n*.info;local1.none;mail.none;authpriv.none;cron.none /var/log/messages\n\n\n\n\n\nBe sure to notify the system logger to re-read the configuration file with \nservice rsyslog restart\n.\n\n\nrsyslog\n, by default, limits the rate at which messages may be logged, and if maximum debugging is enabled in glexec this limit is reached. To avoid that, you can add the following to \n/etc/rsyslog.conf\n after the line \"$ModLoad imuxsock.so\":\n\n\n$SystemLogRateLimitInterval 0\n$SystemLogRateLimitBurst 0\n\n\n\n\n\nand do \nservice rsyslog restart\n.\n\n\nAlternatively, \nsyslog-ng\n (available in the EPEL repository) can do the same job by matching all the messages that have the string \"glexec\" in the name.\nThese rules in \n/etc/syslog-ng/syslog-ng.conf\n will separate the glexec messages into \n/var/log/glexec.log\n:\n\n\ndestination d_glexec { file(\n/var/log/glexec.log\n); };\nfilter f_glexec { program(\n^glexec\n); };\nfilter f_notglexec { not program(\n^glexec\n); };\nlog { source(s_sys); filter(f_glexec); destination(d_glexec); };\n\n\n\n\n\nThen later, in the log rule writing sending to \nd_mesg\n, add a \nfilter(f_notglexec);\n before the destination rule to keep glexec messages out of \n/var/log/messages\n:\n\n\nlog { source(s_sys); filter(f_filter1); filter(f_notglexec); destination(d_mesg); };\n\n\n\n\n\nHow to get Help?\n\n\nTo get assistance please use the \nHelp Procedure\n.", 
            "title": "Worker node glexec Install"
        }, 
        {
            "location": "/worker-node/install-glexec/#glexec-installation-guide", 
            "text": "Warning  As of the June 2017 release of OSG 3.4.0, this software is officially deprecated.  Support is scheduled to end as of May 2018.   This document is intended for System Administrators that are installing the OSG version of glexec.  Glexec is commonly used for what are referred to as \"pilot\" or \"glidein\" jobs.  Traditionally, users submitted their jobs directly to a remote site (or compute element gatekeeper). The user job was authenticated/authorized to run at that site based on the user\u2019s proxy credentials and run under the local unix account assigned.  In a pilot-based infrastructure, users submit their jobs to a centralized site (or queue). The pilot/glidein software at the centralized site then recognizes there is a demand for computing resources. It will then submit what is called a pilot/glidein job to a remote site. This pilot job gets authenticated/authorized to run on a worker node in that site\u2019s cluster. It will then \"pull\" down user jobs from the centralized queue and execute them. Both the pilot and the user job are run under the pilot job\u2019s proxy certificate credentials and local unix account. This represents a security problem in pilot-based systems as there is no authentication/authorization of the individual user\u2019s proxy credentials and, thus, the user\u2019s jobs do not run using it\u2019s own local unix account.  Glexec is a security tool that can be used to resolve this problem. It is meant to be used by VOs that run these pilot-based jobs. It has a number of authentication plugins and can be used both by European grid and by OSG.  The pilot job will \"pull\" user jobs down from the central queue and invoke glexec which will then   authenticate the user job\u2019s proxy,  perform an authorization callout (to GUMS in the case of OSG, or possibly a gridmapfile) similar to that done by the gatekeeper,  and then run the user job under the local account assigned by the authorization service for that user.   In effect, glexec functions much the same as a compute element gatekeeper, except these functions are now performed on the individual worker node. The pilot jobs authentication/authorization is done by the gatekeeper and the individual user jobs are now done by glexec on the individual worker node.  Many worker node clusters use shared file systems like NFS for much of their software and user home accounts. Since glexec is an suid program, it must be installed on every single worker node individually. Most shared file systems do not handle this correctly so it cannot and must not be NFS-exported.  For more information regarding pilot-based systems and glexec:   glideinWMS - The glidein based WMS  Addressing the pilot security problem with gLExec (pdf)", 
            "title": "Glexec Installation Guide"
        }, 
        {
            "location": "/worker-node/install-glexec/#engineering-considerations", 
            "text": "If glexec is to be used at a site, it should be installed and configured on every worker node.  A large number of batch slots using glexec can occasionally put an enormous strain on GUMS servers and cause overloading and client timeouts. In order to survive peak loads, the sysctl parameter  net.core.somaxconn  on a GUMS server machine should be set at least as high as the maximum number of job slots that might attempt to contact the server at about the same time.  For example, Fermilab set the value to 4096 on each of two servers and tested with a continuous load from 5000 job slots.", 
            "title": "Engineering Considerations"
        }, 
        {
            "location": "/worker-node/install-glexec/#requirements", 
            "text": "These are the requirements that must be met to install glexec.   Note  Normally you will install the  OSG worker node  first. Installing the  osg-wn-client-glexec  package will also install the worker node, but we do not duplicate instructions specific to the worker node here.    Verify you are using one of the  supported platforms .  Prior to installing  glexec , verify the  yum repositories  are correctly configured.  root  access to the host is required.  The worker nodes will need the  CA certificates  used to sign the proxies.  fetch-crl  should be enabled and working.   The glexec installation will create two users unless they are already created.     User  Comment      glexec  Reduced privilege separate id used to improve security. If creating the account by hand, set the default gid of the  glexec  user to be a group that is also called  glexec .    gratia  Needed for the glexec gratia probe which is also automatically installed.     In addition, OSG glexec requires a range of  group ids  for tracking purposes. You don\u2018t actually have to create the group entries but it is recommended to do so in order to reserve the gids and so they can be associated with names in the  /usr/bin/id  command. The recommended names are  glexecNN  where NN is a number starting from 00.   Define at least 4 group ids per batch slot per worker node. A conservative way to handle this is to multiply the number of batch slots on the largest worker node by 6 and then share the group ids between all the worker nodes.  They must be consecutive and in any range (default range is 65000-65049, configured in the  configuring glexec  section below).  The same group IDs can be used on every worker node.", 
            "title": "Requirements"
        }, 
        {
            "location": "/worker-node/install-glexec/#install-instructions", 
            "text": "Note  The glexec tracking function requires a part of HTCondor. There are multiple ways to install HTCondor, for details see  these instructions . If you want a minimal install, you can run just this command to install the needed piece from the OSG distribution:  root@host #  yum install condor-procd   After meeting all the requirements in the previous section, install glexec with this command:  root@host #  yum install osg-wn-client-glexec", 
            "title": "Install Instructions"
        }, 
        {
            "location": "/worker-node/install-glexec/#configuring-glexec", 
            "text": "The following steps need to be done after the glexec installation is complete.   First, review the contents of  /etc/glexec.conf . All of the defaults should be fine, but if you want to change the behavior, the parameters are described in  man glexec.conf .   Next, review all of the contents of  /etc/lcmaps.db  and in particular update the following pieces.    If you have GUMS, change  GUMS_HOST  in the following line to the fully qualified domain name of your GUMS server:  \u2013endpoint https:// GUMS_HOST :8443/gums/services/GUMSXACMLAuthorizationServicePort     If you want to use a range of tracking group ids other than the default as described in the  Requirements  section above, uncomment and change the  -min-gid  and  -max-gid  lines to your chosen values:  -min-gid 65000   -max-gid 65049     Uncomment the following two lines:  glexectracking =  lcmaps_glexec_tracking.mod \n                  -exec /usr/sbin/glexec_monitor     If you have GUMS, uncomment the following policy toward the end of the file:  verifyproxy -  gumsclient\ngumsclient -  glexectracking  or if you have do not have GUMS and want to use a gridmapfile, uncomment the following policy:  verifyproxy -  gridmapfile\ngridmapfile -  glexectracking", 
            "title": "Configuring glexec"
        }, 
        {
            "location": "/worker-node/install-glexec/#testing-the-installation-of-glexec", 
            "text": "Now,  as a non-privileged user (not root) , do the following (where   is your VO, and   is your uid as reported by  /usr/bin/id ):  user@host $  voms-proxy-init -voms  YOURVO :/ YOURVO  user@host $   export   GLEXEC_CLIENT_CERT = /tmp/x509up_u UID  user@host $   export   X509_USER_PROXY = /tmp/x509up_u UID  user@host $  /usr/sbin/glexec /usr/bin/id user@host $   uid = 13160 ( fnalgrid )   gid = 9767 ( fnalgrid )   groups = 65000 ( glexec00 )   If  glexec  is successful, it will print out the uid and gid that your proxy would normally be mapped to by your GUMS server, plus a supplementary tracking group. (The actual names and numbers will be different from what you see above.)  If you have problems, please read about  troubleshooting glexec .", 
            "title": "Testing the Installation of glexec"
        }, 
        {
            "location": "/worker-node/install-glexec/#glexec-log-files", 
            "text": "Glexec sends all its log information by default to syslog. By default they go to  /var/log/messages , but this may differ if you have customized your syslog setup. Here are some sample messages:  Apr 25 16:36:16 fermicloud053 glexec[2867]: lcmaps: lcmaps.mod-PluginInit(): plugin glexectracking not found (arguments: )\nApr 25 16:36:16 fermicloud053 glexec[2867]: lcmaps: lcmaps.mod-lcmaps_startPluginManager(): error initializing plugin: glexectracking\nApr 25 16:36:16 fermicloud053 glexec[2867]: lcmaps: lcmaps_init() error: could not start plugin manager\nApr 25 16:36:16 fermicloud053 glexec[2867]: Initialisation of LCMAPS failed.  These particular messages are pretty common, caused by forgetting to uncomment the beginning of the  glexectracking  rule in  /etc/lcmaps.db .  It is possible to redirect glexec log messages to a different file with standard syslog. To do that, choose one of the  LOG_LOCAL[0-7]  log facilities that are unused, for example  LOG_LOCAL1 . Then set the following in  /etc/glexec.conf :  syslog_facility = LOG_LOCAL1  and add a corresponding parameter to the  lcmaps_glexec_tracking.mod  entry in  /etc/lcmaps.db :     -log-facility LOG_LOCAL1   Then in  /etc/rsyslog.conf , add a line like this:  local1.* /var/log/glexec.log  and also exclude those messages from  /var/log/messages  by adding  local1.none  after other wildcards on the existing  /var/log/messages  line, for example:  *.info;local1.none;mail.none;authpriv.none;cron.none /var/log/messages  Be sure to notify the system logger to re-read the configuration file with  service rsyslog restart .  rsyslog , by default, limits the rate at which messages may be logged, and if maximum debugging is enabled in glexec this limit is reached. To avoid that, you can add the following to  /etc/rsyslog.conf  after the line \"$ModLoad imuxsock.so\":  $SystemLogRateLimitInterval 0\n$SystemLogRateLimitBurst 0  and do  service rsyslog restart .  Alternatively,  syslog-ng  (available in the EPEL repository) can do the same job by matching all the messages that have the string \"glexec\" in the name.\nThese rules in  /etc/syslog-ng/syslog-ng.conf  will separate the glexec messages into  /var/log/glexec.log :  destination d_glexec { file( /var/log/glexec.log ); };\nfilter f_glexec { program( ^glexec ); };\nfilter f_notglexec { not program( ^glexec ); };\nlog { source(s_sys); filter(f_glexec); destination(d_glexec); };  Then later, in the log rule writing sending to  d_mesg , add a  filter(f_notglexec);  before the destination rule to keep glexec messages out of  /var/log/messages :  log { source(s_sys); filter(f_filter1); filter(f_notglexec); destination(d_mesg); };", 
            "title": "Glexec log files"
        }, 
        {
            "location": "/worker-node/install-glexec/#how-to-get-help", 
            "text": "To get assistance please use the  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/other/install-voms/", 
            "text": "Install VOMS\n\n\n\n\nWarning\n\n\nAs of the June 2017 release of OSG 3.4.0, this software is officially deprecated.  Support is scheduled to end as of May 2018.\n\n\n\n\nThis document is for VO System Administrators.\n\n\nHere we describe how to install and configure VOMS on your Linux machine. When you install VOMS as described here, you get all three parts: MySQL database, VOMS admin and VOMS server. Instructions for creating your VO are also on this page, in the configuration section.\n\n\nWe also describe how to remove a VO from VOMS if the need arises.\n\n\nThe installation section will give you a bare-bones installation. In order to use VOMS you need to go through the configuration: run \nconfigure_voms\n to create your VO, probably enable read-only access to your VO and probably set a VO-admin to add/remove users. The other sections help in troubleshooting and knowing what is running.\n\n\nThis is not a complete reference. If you need to administer a VO, please check the \nVOMS User guide\n linked also in the \nreference\n section.\n\n\nIntroduction\n\n\nThe \nVirtual Organization Membership Service\n allows to manage the members of a VO and their privileges (groups and roles). It has three main parts:\n\n\n\n\nMySQL database which is a persistent repository for VO membership information,\n\n\nVOMS admin which provides the Web UI/services to maintain the VO membership. This requires Apache/Tomcat.\n\n\nVOMS server, a daemon process which services the \nvoms-proxy-init\n requests.\n\n\n\n\nRequirements\n\n\nHost and OS\n\n\n\n\nA host to install the VOMS Service (Pristine node)\n\n\nOS is Red Hat Enterprise Linux 6, 7, and variants (see \ndetails...\n). Currently most of our testing has been done on Scientific Linux 5.\n\n\nTime must be synchronized (e.g. \nntpd\n). \n\n\nRoot access\n\n\n\n\nUsers\n\n\nThe VOMS and voms-admin installation will create two users unless they are already created.\n\n\n\n\n\n\n\n\nUser\n\n\nDefault uid\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nvoms\n\n\nnone\n\n\nRuns the VOMS daemons, one per VO.\n\n\n\n\n\n\ntomcat\n\n\n91\n\n\nRuns tomcat6 and owns voms-admin configuration and certificates.\n\n\n\n\n\n\n\n\nNote that if uid 91 is already taken but not used for the tomcat user, you will experience errors. \nDetails...\n\n\nCertificates\n\n\nYou will need two service certificates. \nHere\n are instructions to request a host certificate. \n\n\n\n\n\n\n\n\nCertificate\n\n\nUser that owns certificate\n\n\nPath to certificate\n\n\n\n\n\n\n\n\n\n\nVOMS service certificate\n\n\nvoms\n\n\n/etc/grid-security/voms/vomscert.pem\n\n\n\n\n\n\n/etc/grid-security/voms/vomskey.pem\n\n\n\n\n\n\n\n\n\n\nTomcat service certificate\n\n\ntomcat\n\n\n/etc/grid-security/http/httpcert.pem\n\n\n\n\n\n\n/etc/grid-security/http/httpkey.pem\n\n\n\n\n\n\n\n\n\n\n\n\nNetworking\n\n\n\n\n\n\n\n\nService Name\n\n\nProtocol\n\n\nPort Number\n\n\nInbound\n\n\nOutbound\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nVOMS\n\n\ntcp\n\n\n15001+\n\n\nY\n\n\n\n\nrange of ports, increment by 1 for each VO supported\n\n\n\n\n\n\nVOMS Admin\n\n\ntcp\n\n\n8443\n\n\nY\n\n\n\n\nVOMS Admin (which runs within Tomcat) web interface, it must be available to VO administrators and users registering online\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n* 15001 is the port specified in the \nconfiguration command\n. You can choose a different one or have multiple ones if you support multiple VOs.\n\n\n\n\nAdditional Requirements\n\n\n\n\nTesting requirements:\n\n\nThe Subject and Issuer of a voms admin certificate (it could be your certificate)\n\n\nAccess to your own certificate if you want to create your voms proxy\n\n\nYour certificate uploaded into a browser if you want to test voms-admin WEB UI\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe configuration and testing require some familiarity with openssl commands, e.g. to extract the subject (DN) and the issuer of a certificate. If you are not familiar please check the guides in the \nreference section\n:\n\n\n\n\nInstallation Procedure\n\n\nInstall VOMS\n\n\n\n\nInstall Java using \nthese instructions\n\n\n\n\nInstall OSG-VOMS:\n\n\nconsole\n[root@voms ~]$ yum install osg-voms\n\n\n\n\n\n\nConfigure\n\n\nThe configuration requires some initial steps plus the addition of one or more VO. You will not be able to start voms-admin if you don't add at least one VO.\n\n\nConfigure MySQL Database\n\n\nBefore you will be able configure VOMS you will need to have access to MySQL database. The MySQL software is installed with the \nosg-voms\n package. By default it will be using port 3306 and will not have password for \nroot\n.\n\n\nIt is not necessary to change the port number if you don't want to. If you want to modify the port you will need to edit \n/etc/my.cnf\n file and add a new port:\n\n\n[mysqld]\n\n\ndatadir\n=\n/var/lib/mysql\n\n\nsocket\n=\n/var/lib/mysql/mysql.sock\n\n\nport\n=\nMYSQL_PORT_NUMBER\n\n\n\n\n\n\nIt is wise to set a password for the \nroot\n user. Run the following command to set the password:\n\n\n[root@voms ~]$\n /etc/init.d/mysqld start\n\n[root@voms ~]$\n mysqladmin -u root password top_secret\n\n\n\n\n\nSetup the Service Certificates\n\n\nMake sure that you have the host, VOMS and Tomcat service certificates/keys in the right directories and with the correct owners and permissions (\n600\n for key and \n644\n the certificate). \nHost certificate\n is required and must be in \n/etc/grid-security/hostcert.pem\n and \nhostkey.pem\n. It is recommended to provide also service certificates for VOMS and Tomcat. If you don't, here are the instructions to copy the host certificate/key and set the correct owner (permissions are preserved). Note that if you did not previously have a voms or tomcat user on your system, they were added during the RPM install:\n\n\n\n\n\n\nVOMS-core daemon is running as user \nvoms\n and requires its service certificate and key in \n/etc/grid-security/voms\n. To copy the host certificate/key for this service, you need to do the following:\n\n\nconsole\n[root@voms ~]$ cd /etc/grid-security\n[root@voms ~]$ cp hostcert.pem voms/vomscert.pem\n[root@voms ~]$ cp hostkey.pem voms/vomskey.pem\n[root@voms ~]$ chown -R voms:voms voms\n\n\n\n\n\n\nTomcat service is running as user \ntomcat\n and requires its service certificate and key in \n/etc/grid-security/http\n. To copy the host certificate/key for this service, you need to do the following:\n\n\nconsole\n[root@voms ~]$ cd /etc/grid-security\n[root@voms ~]$ mkdir /etc/grid-security/http\n[root@voms ~]$ cp hostcert.pem http/httpcert.pem\n[root@voms ~]$ cp hostkey.pem http/httpkey.pem\n[root@voms ~]$ chown -R tomcat:tomcat http\n\n\n\n\n\n\nConfigure Tomcat options\n\n\nEdit \n/etc/tomcat6/tomcat6.conf\n to add the following line to the bottom of the file. This is important for good performance and essential if you run more than one VO.\n\n\nCATALINA_OPTS=\n${\nCATALINA_OPTS\n}\n -XX:MaxPermSize=256m\n\n\n\n\n\n\nIf you run a large number of VOs, you might need to increase the amount of memory used by Tomcat. You can do this by adding one more line to the bottom of \n/etc/tomcat6/tomcat6.conf\n:\n\n\nJAVA_OPTS=\n${\nJAVA_OPTS\n}\n -Xmx2048m\n\n\n\n\n\n\nYou must add the following line to \n/etc/tomcat6/tomcat6.conf\n to use VOMS-Admin successfully:\n\n\nJAVA_ENDORSED_DIRS=\n${\nJAVA_ENDORSED_DIRS\n}\n:/usr/share/voms-admin/endorsed\n\n\n\n\n\n\nDisable SSLv3 in Tomcat\n\n\nThe OSG Security team has concluded that the POODLE SSLv3 vulnerability is not a \ncritical\n concern to OSG software installations. Most services are not affected, and those that are affected are difficult to exploit in a meaningful way. Nonetheless, the recommendation is to disable support for SSLv3 where reasonable.\n\n\nOSG software includes VOMS Admin Server (currently, version 2.7.0), which runs within Tomcat. By default Tomcat allows SSLv3 connections, but that is easy to change. To disable SSLv3 support from a Tomcat instance that contains VOMS Admin Server, set \nprotocols=\"TLSv1.1,TLSv1.2\"\n in \n/etc/tomcat[56]/server.xml\n, as shown below.\n\n\nNote: OSG Software has tested this change only for Tomcat and VOMS Admin Server. But this is a server-level Tomcat configuration change and thus affects all Tomcat web applications running on the same server. For now, we recommend making this change to Tomcat servers that run only VOMS Admin Server. There are known issues with applying this change to a Tomcat instance that runs GUMS, in that dCache clients (at least) fail to work with the changed GUMS server.\n\n\n--- server.xml.old      2014-10-28 11:27:11.000000000 -0500\n\n\n+++ server.xml  2014-10-28 11:29:34.000000000 -0500\n\n\n@@ -20,19 +20,20 @@\n\n     \nConnector port=\n8443\n SSLEnabled=\ntrue\n\n               maxThreads=\n150\n minSpareThreads=\n25\n maxSpareThreads=\n75\n\n               enableLookups=\nfalse\n disableUploadTimeout=\ntrue\n\n               acceptCount=\n100\n debug=\n0\n scheme=\nhttps\n secure=\ntrue\n\n               sSLImplementation=\norg.glite.security.trustmanager.tomcat.TMSSLImplementation\n\n               trustStoreDir=\n/etc/grid-security/certificates\n\n               sslCertFile=\n/etc/grid-security/http/httpcert.pem\n\n               sslKey=\n/etc/grid-security/http/httpkey.pem\n\n               crlUpdateInterval=\n2h\n\n               log4jConfFile=\n/usr/share/tomcat6/conf/log4j-trustmanager.properties\n\n               clientAuth=\ntrue\n sslProtocol=\nTLS\n sslEnabledProtocols=\nTLSv1\n\n\n+              protocols=\nTLSv1.1,TLSv1.2\n\n               crlEnabled=\ntrue\n crlRequired=\ntrue\n/\n\n\n    \nEngine name=\nCatalina\n defaultHost=\nlocalhost\n\n\n      \nHost name=\nlocalhost\n appBase=\nwebapps\n /\n\n    \n/Engine\n\n  \n/Service\n\n\n/Server\n\n\n\n\n\n\nAllow Tomcat to have more open files and processes\n\n\nIf you run several VOs, you may need to extend the limits for Tomcat so that it can open more file and create more processes. To do this, add the following lines to \n/etc/security/limits.conf\n:\n\n\ntomcat          soft    nofile  63536\ntomcat          hard    nofile  63536\n\ntomcat          soft    nproc   16384\ntomcat          hard    nproc   16384\n\n\n\n\n\nNote that this is not needed if you are running just a single VO, this is only needed if you run many VOs.\n\n\nConfigure Trust Manager\n\n\nConfigure Tomcat's trust manager. This must be done once, before starting up Tomcat for the first time:\n\n\n\n\n\n\nYou have to configure Tomcat in order to enable EMI trustmanager:\n\n\nconsole\n[root@voms ~]$ /var/lib/trustmanager-tomcat/configure.sh\nInfo: using default install root: /\nInfo: using default configuration file: //var/lib/trustmanager-tomcat/config.properties\nInfo: using default configuration directory: //var/lib/trustmanager-tomcat\nInfo: you can clean up using the following commands\n      mv -f /etc/tomcat5/server.xml.old-trustmanager /etc/tomcat5/server.xml\n      rm -f /usr/share/tomcat5/server/lib/bcprov*.jar\n      rm -f /usr/share/tomcat5/server/lib/log4j*.jar\n      rm -f /usr/share/tomcat5/server/lib/trustmanager-*.jar\n      rm -f /etc/tomcat5/log4j-trustmanager.properties\n      rm -f //var/lib/trustmanager-tomcat/server.xml\n\n\n\n\n\n\n(the output may look different on EL6)\n\n\nConfigure Email Services for the VOMS Server\n\n\nThe VOMS server may need to send administrative emails. To enable this feature \non EL 6 systems\n, you must complete a one-time initialization of Java email services within Tomcat:\n\n\n\n\n\n\nRun the \nbuild-jar-repository\n command:\n\n\nconsole\n[root@voms ~]$ /usr/bin/build-jar-repository /usr/share/tomcat6/lib javamail\n\n\n\n\n\n\nSet the hostname for reminder e-mails in \n/etc/voms-admin/VO_NAME/voms.service.properties\n:\n\n\nfile\nvoms.hostname = \nhostname\n\n\nReplacing \n with the public hostname of your VOMS server.\n\n\n\n\n\n\nEL 5 systems are already configured to use Java email services.\n\n\nAdd and configure a VO\n\n\nYou must add at least one VO before you can start VOMS Admin. You must start the services as instructed during the configuration steps. You can stop them at the end if you don't want them running yet.\n\n\n\n\n\n\nRun \nvoms-admin-configure\n script to create a new VO. Note that if you have multiple VOs, each VO must have a unique VOMS_PORT\n\n\nconsole\n[root@voms ~]$ voms-admin-configure install \\\n    --dbtype mysql \\\n    --vo VO_NAME \\\n    --createdb \\\n    --deploy-database  \\\n    --dbauser root  \\\n    --dbapwd MYSQL_ROOT_PASSD \\\n    --dbusername  admin-VO_NAME \\\n    --dbpassword secret  \\\n    --dbport  DB_PORT \\\n    --port VOMS_PORT  \\\n    --mail-from email \\\n    --smtp-host smtp.domain \\\n    --sqlloc /usr/lib64/voms/libvomsmysql.so \\\n    --cert /etc/grid-security/voms/vomscert.pem \\\n    --key  /etc/grid-security/voms/vomskey.pem \\\n    --read-access-for-authenticated-clients\n\n\nFor example for the test1 VO:\n\n\nconsole\n[root@voms ~]$ voms-admin-configure install --dbtype mysql --vo test1 --createdb \\\n     --deploy-database --dbauser root --dbapwd  top_secret \\\n     --dbusername admin_test1 --dbpassword secret --dbport 3306 \\\n     --port 15001 --mail-from tlevshin@fnal.gov --smtp-host smtp.fnal.gov \\\n     --sqlloc /usr/lib64/voms/libvomsmysql.so --cert /etc/grid-security/voms/vomscert.pem \\\n     --key  /etc/grid-security/voms/vomskey.pem --read-access-for-authenticated-clients\n\n\nIf the command fails with a \nIOError: [Errno 32] Broken pipe\n error, check that you entered the correct database password.\nIf the above command is executed correctly a lot of sql stuff will scroll across the screen, followed by\n\n\n``` console\nDeploying voms database...\nDatabase deployed correctly!\n\n\nVO test1 installation finished.\n\n\nYou can start the voms services using the following commands:\n    /etc/init.d/voms start test1\n    /etc/init.d/voms-admin start test1\n```\n\n\n\n\n\n\nStart VOMS and VOMS Admin for the VO you just added:\n\n\nconsole\n[root@voms ~]$ /sbin/service voms start VO_NAME\n[root@voms ~]$ /sbin/service voms-admin start VO_NAME\n\n\nFor example:\n\n\nconsole\n[root@voms ~]$ /sbin/service voms start test1\n[root@voms ~]$ /sbin/service voms-admin start test1\n\n\nNote that the voms-admin init script really isn't an init script. Instead it deploys a Tomcat webapp into Tomcat. In normal usage, you never need to run this command again, and you should \nnot\n run \n/sbin/service voms-admin stop VO_NAME\n.\n\n\n\n\n\n\nAdd a local admin:\n\n\nconsole\n[root@voms ~]$ /usr/sbin/voms-db-deploy.py add-admin --vo VO_NAME  \\\n        --dn  HOST_CERT_SUBJECT \\\n        --ca HOST_CERT_ISSUER\n\n\nFor example:\n\n\nconsole\n[root@voms ~]$ /usr/sbin/voms-db-deploy.py add-admin --vo test1 \\\n     --dn /DC=org/DC=doegrids/OU=Services/CN=fermicloud002.fnal.gov \\\n     --ca \"/DC=org/DC=DOEGrids/OU=Certificate Authorities/CN=DOEGrids CA 1\"\n\n\n\n\n\n\nIn order to complete the next step you need to start also Tomcat if it is not already running. And since Tomcat requires valid CRLs you need to fetch the CRLs if it is a fresh installation they are not there:\n\n\nconsole\n[root@voms ~]$ /usr/sbin/fetch-crl  # If there are no CRLs in /etc/grid-security/certificates\n[root@voms ~]$ /sbin/service tomcat6 start\n\n\n\n\n\n\nConfigure VOMS Admin to allow GUMS and edg-mkgridmap to query your VO for the set of users. Note that you must have a valid certificate; voms-admin needs a certificate authorized to perform the operation (ACL editing) on the VO:\n\n\nconsole\n[root@voms ~]$ voms-admin --nousercert --vo VO_NAME add-ACL-entry /VO_NAME ANYONE VOMS_CA \"CONTAINER_READ,MEMBERSHIP_READ\" true\n\n\nFor example:\n\n\nconsole\n[root@voms ~]$ voms-admin --nousercert --vo test1 add-ACL-entry /test1 ANYONE VOMS_CA \"CONTAINER_READ,MEMBERSHIP_READ\" true\n\n\n\n\n\n\nAllow compatibility with \nGUMS\n and \nedg-mkgridmap\n. Edit \n/etc/voms-admin/VO_NAME/voms.service.properties\n by adding the following line to the file:\n\n\nconsole\nvoms.csrf.log_only = true\n\n\n\n\n\n\nIf you don't want to leave your services running you can stop them after you are done with adding the VO:\n\n\n[root@voms ~]$\n /sbin/service voms stop\n\n[root@voms ~]$\n /sbin/service tomcat6 stop\n\n\n\n\n\nServices\n\n\nVOMS has its persistent repository in a MySQL database, that requires \nmysql\n to be running in order to be queried. All grid interactions require updated CA certificates and CRLs. Then the \nvoms server\n is a daemon servicing authentication requests and \nvoms-admin\n, requiring Tomcat, is a Web UI/service to manage the VO membership.\n\n\nStarting and Enabling Services\n\n\nTo start VOMS you need to start several services:\n\n\n\n\n\n\n\n\nMySQL server needs to be running if it is not already:\n\n\n\n\nconsole\n[root@voms ~]$ /sbin/service mysqld start\n\n\n\n\n\n\nStart voms server\n\n\nconsole\n[root@voms ~]$ service voms start\n\n\n\n\n\n\nStart Tomcat, required by voms-admin:\n\n\nconsole\n[root@voms ~]$ service tomcat6 start\n\n\n\n\n\n\nYou should also enable the appropriate services so that they are automatically started when your system is powered on:\n\n\n\n\n\n\n\n\nTo enable the other services:\n\n\n\n\nconsole\n[root@voms ~]$ /sbin/chkconfig mysqld on\n[root@voms ~]$ /sbin/chkconfig voms on\n[root@voms ~]$ /sbin/chkconfig tomcat6 on\n\n\n\n\n\n\nNote\n: You do \nnot\n need to run chkconfig on the voms-admin service. This service is only need to do the initial \"deployment\" of the VOMS Admin service into Tomcat and is not needed thereafter unless you add another VO.\n\n\nStopping and Disabling Services\n\n\nTo stop VOMS you need to stop voms and voms-admin, and also the services they use if nothing else is using them:\n\n\n\n\n\n\nStop voms server\n\n\nconsole\n[root@voms ~]$ service voms stop\n\n\n\n\n\n\n, Stop Tomcat, if no other application is using it:\n\n\nconsole\n[root@voms ~]$ service tomcat6 stop\n\n\n\n\n\n\nStop MySQL server if no other application is using it:\n\n\nconsole\n[root@voms ~]$ service mysqld stop\n\n\n\n\n\n\nIn addition, you can disable services by running the following commands. However, you don't need to do this normally.\n\n\n\n\n\n\n\n\nTo disable the other services:\n\n\n\n\nconsole\n[root@voms ~]$ /sbin/chkconfig mysqld off\n[root@voms ~]$ /sbin/chkconfig voms off\n[root@voms ~]$ /sbin/chkconfig tomcat6 off\n\n\n\n\n\n\n\n\nNote\n\n\nYou do not need to run chkconfig on the voms-admin service. See above for more details.\n\n\n\n\nAdvertise your VOMS server\n\n\nA working VOMS server can be contacted to sign certificates extensions (e.g., by \nvoms-proxy-init\n) or to verify VO memberships only if it is on the list of valid VOMS servers (\n/etc/vomses\n) and if there is an LSC file for that VO. Some grid services (glite-FTS and glite-WMS) even require a copy of the VOMS server certificate.\n\n\nThe file \n/etc/vomses\n contains a list of VOs and their VOMS servers. A VOMS server may appear on multiple lines if it is serving multiple VOs on different ports. All lines contain:\n\n\nalias\n \nHOST_NAME\n \nPORT\n \nHOST_DN\n \nVO_NAME\n\n\n\n\n\n\nFor example:\n\n\ntest1\n \nfermicloud002.fnal.gov\n \n15001\n \n/DC=org/DC=doegrids/OU=Services/CN=fermicloud002.fnal.gov\n \ntest1\n\n\n\n\n\n\nThe \nLSC file\n (LiSt of Certificates) is another description of your VOMS server:\n\n\n\n\nIt is located in \n$X509_VOMS_DIR/${VO}\n, by default \n/etc/grid-security/vomsdir/${VO}\n\n\nThe filename is the fully qualified hostname (FQDN) of your server, followed by \n.lsc\n\n\nThe first line contains the subject DN of the VOMS server host certificate, without quotes\n\n\nThe second line contains the subject DN of the CA that issued the VOMS server host certificate, also without quotes\n\n\n\n\nFor example, for the VO named \ntest1\n hosted on \nfermicloud002\n, the LSC file named \n/etc/grid-security/vomsdir/test1/fermicloud002.fnal.gov.lsc\n might contain:\n\n\n/DC=org/DC=doegrids/OU=Services/CN=fermicloud002.fnal.gov\n/DC=org/DC=DOEGrids/OU=Certificate Authorities/CN=DOEGrids CA 1\n\n\n\n\n\nThe VOMS server manager must distribute the line for \n/etc/vomses\n and the LSC file to all hosts that will contact the VOMS server. OSG distributes with all its client and server installations a default list of VOMS servers (\n/etc/vomses\n) and a default set of LSC files. These are part of the \nvo-client\n RPM.\n\n\nTo be included, use the VO Registration function at \nOIM VO Registration\n. Even if you are already registered as a VO you need to notify the Grid Operations Center if the information in the vomses file or the LSC file has changed.\n\n\nTest\n\n\n\n\nNote\n\n\nBefore performing any of the tests you must start all the services as \ndescribed above\n\n\n\n\nTo test VOMS you can add a member (her/his certificate) to a VO using voms-admin, login in the Web UI and then:\n\n\n\n\nuse the voms server to generate a proxy for that certificate.\n\n\nor generate a grid-mapfile\n\n\nor query with GUMS\n\n\n\n\nVoms-admin test\n\n\nAdd a member(yourself) to the VO:\n\n\n[root@voms ~]$\n voms-admin --vo VO_NAME --host HOST_NAME \n\\\n\n       --nousercert create-user USER_CERT_SUBJECT USER_CERT_ISSUER \n\\\n\n       USER_COMMON_NAME email \n\n\n\n\n\nFor example:\n\n\n[root@voms ~]$\n voms-admin --vo test1 --host fermicloud002 --nousercert create-user \n\\\n\n   \n/DC=org/DC=doegrids/OU=People/CN=Joe Doe 12345\n \n/DC=org/DC=DOEGrids/OU=Certificate Authorities/CN=DOEGrids CA 1\n \n\n   \nJoe Doe 12345\n \njdoe@fnal.gov\n\n\n\n\n\n\nAssign VOAdmin role to yourself, so you can manage the VO using the Web UI:\n\n\nvoms-admin --vo VO_NAME --host HOST_NAME \\\n\n\n    --nousercert assign-role  /VO_NAME  VO-Admin  \\\n\n\n      USER_CERT_SUBJECT USRT_CERT_ISSUER \n\n\n\n\n\n\nFor example:\n\n\n[root@voms ~]$\n voms-admin --vo test1 --host fermicloud002 --nousercert assign-role /test1   VO-Admin  \n\\\n\n   \n/DC=org/DC=doegrids/OU=People/CN=Joe Doe 12345\n \n/DC=org/DC=DOEGrids/OU=Certificate Authorities/CN=DOEGrids CA 1\n \n\n\n\n\n\nCheck the Web UI by accessing \nhttps://hostname:8443/voms/voname\n. If you have your certificate loaded into the browser you will be able to manage the VO.\n\n\nVoms-core test:\n\n\nTo perform this test you need the \nvoms-client\n RPM.\n\n\nTo use your VOMS you need to add a line with its description (similar to the following) to \n/etc/vomses\n, a file containing a list of all valid voms servers:\n\n\nalias\n \nHOST_NAME\n \nPORT\n \nHOST DN\n \nVO_NAME\n\n\n\n\n\n\nLook at \n/etc/voms-admin/VO_NAME/vomses\n to find the correct line for your VO. For example:\n\n\ntest1\n \nfermicloud002.fnal.gov\n \n15001\n \n/DC=org/DC=doegrids/OU=Services/CN=fermicloud002.fnal.gov\n \ntest1\n\n\n\n\n\n\nIf it is not there already, add your certificate to the VO using \nvoms-admin\n \ncreate-user\n as in the previous section. Being the local admin set with \nvoms-db-deploy.py add-admin\n does not add the certificate to the VO.\n\n\nUse \nvoms-proxy-init\n to generate proxy. Login as user, create \nvomses\n file (e.g. \n~/.vomses\n). Generate voms proxy\n\n\n[user@voms ~]$\n voms-proxy-init -voms VO_NAME -vomses ~/vomses\n\n\n\n\n\nTesting edg-mkgridmap\n\n\nThe \nedg-mkgridmap\n command is run on a site's Compute Element node to generate a \ngrid-mapfile\n file. This script is not provided as a part of the VOMS installation, so you will have to go on a CE node to test this. You do not have to be the \nroot\n user on the CE node, but you will need to have a personal certificate available.\n\n\nThe \nmkgridmap\n file syntax/values for each VO you create, can be viewed on the WEB UI for your VO by selecting \nConfiguration information\n on the left hand menu of the main page. You will get a line like:\n\n\n group vomss://HOST_NAME:8443/voms/VO_NAME  group_name\n\n\n\n\n\ne.g. for the OSG VO :\n\n\n group vomss://cms-xen3.fnal.gov:8443/voms/OSG  osg\n\n\n\n\n\n\n\nNote\n\n\nThe \nosg\n (3rd token) is an EDG format for the account name. You will likely want to change it to the group unix account you want assigned for your CE node. The example above is for a \ngroup\n account assignment.\n\n\n\n\nTo test, you will need to do this on a CE node where the authorization mode is \"gridmap\" (lcmaps or PRIMA are not enabled):\n\n\n\n\n\n\nPopulate \n/etc/edg-mkgridmap.conf\n with the example configuration file on your VOMS server. It will be the only entry.\n\n\nconsole\n[root@voms ~]$ echo 'group vomss://cms-xen3.fnal.gov:8443/voms/OSG  osg' \n /etc/edg-mkgridmap.conf\n\n\n\n\n\n\nExecute edg-mkgridmap:\n\n\nconsole\n[root@voms ~]$ edg-mkgridmap\n  ... the output will be in /etc/grid-security/grid-mapfile and will look as follows ...\n  \"/DC=org/DC=doegrids/OU=People/CN=John Weigand 458491\"  vdt\n  \"/DC=org/DC=doegrids/OU=Services/CN=cms-xen3.fnal.gov\"  vdt\n\n\n\n\n\n\nYou can find more in the \ncompute element install document\n.\n\n\nTesting the GUMS interface\n\n\nThe GUMS authorization server periodically 'pulls' the VO membership data for it's authorization and account assignment functions from the various VO's VOMS servers.\n\n\nUnfortunately, there is no way of testing this except with an installed GUMS server which this document does not address.\n\n\nSee the \nGUMS install guide\n for more information.\n\n\nRemoving a VO\n\n\nIf you added the VO just for testing purposes, you can remove it using \nvoms-admin-configure remove --vo VONAME\n as documented in the \nuser guide\n, e.g.:\n\n\n#\n# Stop the service \n\n\n[root@voms ~]$\n service voms-admin stop test1\n\n[root@voms ~]$\n service voms stop test1  \n\n#\n# Remove it.\n\n\n[root@voms ~]$\n voms-admin-configure remove --vo test1 --undeploy-database --dropdb --dbapwd top_secret\n\nvoms-admin-configure, version 2.6.1\n\n\n\nRemoving vo  test1\n\n\nVO test1 succesfully removed.\n\n\n#\n# Restart to update the voms-server webpage \n\n\n[root@voms ~]$\n service tomcat6 restart\n\n\n\n\n\nNote that the --undeploy-database and --dropdb will remove the database that stores all the membership information. You cannot undo this option. If you're not quite sure that you really want to remove the database, then do not pass this option. The VO will be unaccessible and unusable, but the membership information will be retained in case you recreate the VO later.\n\n\nTroubleshooting\n\n\nUseful configuration and log files\n\n\nConfiguration Files\n\n\n\n\n\n\n\n\nService or Process\n\n\nConfiguration File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nvoms\n\n\n/etc/voms/VO_NAME/voms.conf\n\n\n\n\n\n\n\n\n/etc/voms/VO_NAME/voms.pass\n\n\nVOMS server configuration and password for the VO database\n\n\n\n\n\n\n\n\nvoms-admin\n\n\n/etc/voms-admin/voms-siblings.xml\n\n\nAuxiliary voms-admin application configuration\n\n\n\n\n\n\n\n\n/etc/voms-admin/VO_NAME/logback.runtime.xml\n\n\nLogging configuration\n\n\n\n\n\n\n\n\n/etc/voms-admin/VO_NAME/vo-aup.txt\n\n\nVO acceptable use policy\n\n\n\n\n\n\n\n\n/etc/voms-admin/VO_NAME/voms-admin-VO_NAME.xml\n\n\n\n\n\n\n\n\n/etc/voms-admin/VO_NAME/voms.service.properties\n\n\nVOMS Admin configuration\n\n\n\n\n\n\n\n\n\n\n/etc/voms-admin/VO_NAME/voms.database.properties\n\n\nVO database handler configuration\n\n\n\n\n\n\n\n\n/etc/voms-admin/VO_NAME/vomses\n\n\nvomses line\n\n\n\n\n\n\nmysql\n\n\n/var/lib/mysql/VO_NAME\n\n\nVOMS database for the VO\n\n\n\n\n\n\n\n\n/etc/my.cnf\n\n\nMySQL configuration, e.g. server port\n\n\n\n\n\n\ntomcat\n\n\n/etc/tomcat6/\n\n\nTomcat configuration files\n\n\n\n\n\n\n\n\n/etc/tomcat6/Catalina/localhost/*\n\n\nVOMS related tomcat configuration\n\n\n\n\n\n\n\n\nLog files\n\n\n\n\n\n\n\n\nService or Process\n\n\nLog File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n\n\n/var/log/tomcat5/voms-admin-VO_NAME.log\n\n\nThis is the vo-by-vo log for voms-admin Web UI\n\n\n\n\n\n\n\n\n/var/log/tomcat5/trustmanager.log\n\n\nThe trustmanager handles things related to authentication. Useful errors are sometimes here.\n\n\n\n\n\n\ntomcat\n\n\n/var/log/tomcat6/catalina.out\n\n\nThis is the Tomcat log file. Problems (and a lot of noise) are reported here.\n\n\n\n\n\n\n\n\n/var/log/tomcat6/voms-admin-VO_NAME.log\n\n\nThis is the vo-by-vo log for voms-admin Web UI\n\n\n\n\n\n\n\n\n/var/log/tomcat6/trustmanager.log\n\n\nThe trustmanager handles things related to authentication. Useful errors are sometimes here.\n\n\n\n\n\n\nvoms\n\n\n/var/log/voms/voms.VO_NAME\n\n\nThis is the log of the VOMS daemon for each VO\n\n\n\n\n\n\n\n\nFix these errors\n\n\nVOMS Server for VO_NAME not known!\n\n\nIf you saw an error like \"VOMS Server for VO_NAME not known!\" probably you did not add the VO VO_NAME (e.g. test1) to \n/etc/vomses\n\n\nTomcat is not starting\n\n\nIf Tomcat is not starting and \n/var/log/tomcat5/catalina.out\n contains an error like \"/usr/bin/tomcat5: line 331: /usr/lib/java/bin/java: No such file or directory\", then you may have to uncomment the line \nJAVA_HOME=\"/usr/lib/jvm/java\"\n in \n/etc/tomcat5/tomcat5.conf\n. The Jpackage repository is known to provide a misconfigured tomcat5 package.\n\n\nProblems running voms-admin\n\n\nVerify if voms-admin was configured and started correctly:\n\n\n\n\nIf voms-admin is not responding and you get a \"Socket error 111\" try restarting tomcat6 and voms-admin\n\n\nIf there are BC (bouncycastle) related errors in Tomcat's log file (\n/var/log/tomcat6/catalina.out\n), the trust manager may not be configured correctly. Try to run \n/var/lib/trustmanager-tomcat/configure.sh\n.\n\n\nIf there are OpenSAML or XML parser related errors in Tomcat's log file (\n/var/log/tomcat6/catalina.out\n), the JAVA_ENDORSED_DIRS variable may not be set in \n/etc/tomcat6/tomcat6.conf\n. See the \nConfigure Tomcat options\n section above.\n\n\nIf you get an error like \"Socket error: (1, 'error:14094416:SSL routines:SSL3_READ_BYTES:sslv3 alert certificate unknown')\" probably you have no CRLs for your CA certificates. You can see in \n/var/log/tomcat6/trustmanager.log\n that Tomcat is discarding CA certificates without CRL. Run \nfetch-crl\n.\n\n\n\n\nWrong URLs in VOMS admin pages/Connection timed out/Host unavailable\n\n\nIf you can reach the first page but all the links are failing, check the URL in the links. Tomcat by default constructs the absolute URLs in the web pages using the hostname/IP and port used for the request. If the server resides behind a Firewall or NAT that enforces port redirection, these may be the server IP or the port on the private network, that likely are not reachable by your Web browser. To fix the problem you must use \nTomcat's proxy support\n by setting \nproxyName\n and/or \nproxyPort\n in Tomcat's configuration file (\n/etc/tomcat5/server.xml\n).\n\n\nAfter changing \nserver.xml\n \nstop and restart\n Tomcat in order for the changes to take effect.\n\n\nHow to get Help?\n\n\nTo get assistance please use \nHelp Procedure\n.\n\n\nReferences\n\n\nEMI documentation:\n\n\n\n\nEMI VOMS documentation\n\n\nVOMS Admin User Guide\n\n\nVOMS System Admin Guide\n\n\nHow to configure VOMS LSC files\n\n\n\n\nOpenssl commands:\n\n\nhttp://security.ncsa.illinois.edu/research/grid-howtos/usefulopenssl.html", 
            "title": "Install VOMS"
        }, 
        {
            "location": "/other/install-voms/#install-voms", 
            "text": "Warning  As of the June 2017 release of OSG 3.4.0, this software is officially deprecated.  Support is scheduled to end as of May 2018.   This document is for VO System Administrators.  Here we describe how to install and configure VOMS on your Linux machine. When you install VOMS as described here, you get all three parts: MySQL database, VOMS admin and VOMS server. Instructions for creating your VO are also on this page, in the configuration section.  We also describe how to remove a VO from VOMS if the need arises.  The installation section will give you a bare-bones installation. In order to use VOMS you need to go through the configuration: run  configure_voms  to create your VO, probably enable read-only access to your VO and probably set a VO-admin to add/remove users. The other sections help in troubleshooting and knowing what is running.  This is not a complete reference. If you need to administer a VO, please check the  VOMS User guide  linked also in the  reference  section.", 
            "title": "Install VOMS"
        }, 
        {
            "location": "/other/install-voms/#introduction", 
            "text": "The  Virtual Organization Membership Service  allows to manage the members of a VO and their privileges (groups and roles). It has three main parts:   MySQL database which is a persistent repository for VO membership information,  VOMS admin which provides the Web UI/services to maintain the VO membership. This requires Apache/Tomcat.  VOMS server, a daemon process which services the  voms-proxy-init  requests.", 
            "title": "Introduction"
        }, 
        {
            "location": "/other/install-voms/#requirements", 
            "text": "", 
            "title": "Requirements"
        }, 
        {
            "location": "/other/install-voms/#host-and-os", 
            "text": "A host to install the VOMS Service (Pristine node)  OS is Red Hat Enterprise Linux 6, 7, and variants (see  details... ). Currently most of our testing has been done on Scientific Linux 5.  Time must be synchronized (e.g.  ntpd ).   Root access", 
            "title": "Host and OS"
        }, 
        {
            "location": "/other/install-voms/#users", 
            "text": "The VOMS and voms-admin installation will create two users unless they are already created.     User  Default uid  Comment      voms  none  Runs the VOMS daemons, one per VO.    tomcat  91  Runs tomcat6 and owns voms-admin configuration and certificates.     Note that if uid 91 is already taken but not used for the tomcat user, you will experience errors.  Details...", 
            "title": "Users"
        }, 
        {
            "location": "/other/install-voms/#certificates", 
            "text": "You will need two service certificates.  Here  are instructions to request a host certificate.      Certificate  User that owns certificate  Path to certificate      VOMS service certificate  voms  /etc/grid-security/voms/vomscert.pem    /etc/grid-security/voms/vomskey.pem      Tomcat service certificate  tomcat  /etc/grid-security/http/httpcert.pem    /etc/grid-security/http/httpkey.pem", 
            "title": "Certificates"
        }, 
        {
            "location": "/other/install-voms/#networking", 
            "text": "Service Name  Protocol  Port Number  Inbound  Outbound  Comment      VOMS  tcp  15001+  Y   range of ports, increment by 1 for each VO supported    VOMS Admin  tcp  8443  Y   VOMS Admin (which runs within Tomcat) web interface, it must be available to VO administrators and users registering online      Note  * 15001 is the port specified in the  configuration command . You can choose a different one or have multiple ones if you support multiple VOs.", 
            "title": "Networking"
        }, 
        {
            "location": "/other/install-voms/#additional-requirements", 
            "text": "Testing requirements:  The Subject and Issuer of a voms admin certificate (it could be your certificate)  Access to your own certificate if you want to create your voms proxy  Your certificate uploaded into a browser if you want to test voms-admin WEB UI      Note  The configuration and testing require some familiarity with openssl commands, e.g. to extract the subject (DN) and the issuer of a certificate. If you are not familiar please check the guides in the  reference section :", 
            "title": "Additional Requirements"
        }, 
        {
            "location": "/other/install-voms/#installation-procedure", 
            "text": "", 
            "title": "Installation Procedure"
        }, 
        {
            "location": "/other/install-voms/#install-voms_1", 
            "text": "Install Java using  these instructions   Install OSG-VOMS:  console\n[root@voms ~]$ yum install osg-voms", 
            "title": "Install VOMS"
        }, 
        {
            "location": "/other/install-voms/#configure", 
            "text": "The configuration requires some initial steps plus the addition of one or more VO. You will not be able to start voms-admin if you don't add at least one VO.", 
            "title": "Configure"
        }, 
        {
            "location": "/other/install-voms/#configure-mysql-database", 
            "text": "Before you will be able configure VOMS you will need to have access to MySQL database. The MySQL software is installed with the  osg-voms  package. By default it will be using port 3306 and will not have password for  root .  It is not necessary to change the port number if you don't want to. If you want to modify the port you will need to edit  /etc/my.cnf  file and add a new port:  [mysqld]  datadir = /var/lib/mysql  socket = /var/lib/mysql/mysql.sock  port = MYSQL_PORT_NUMBER   It is wise to set a password for the  root  user. Run the following command to set the password:  [root@voms ~]$  /etc/init.d/mysqld start [root@voms ~]$  mysqladmin -u root password top_secret", 
            "title": "Configure MySQL Database"
        }, 
        {
            "location": "/other/install-voms/#setup-the-service-certificates", 
            "text": "Make sure that you have the host, VOMS and Tomcat service certificates/keys in the right directories and with the correct owners and permissions ( 600  for key and  644  the certificate).  Host certificate  is required and must be in  /etc/grid-security/hostcert.pem  and  hostkey.pem . It is recommended to provide also service certificates for VOMS and Tomcat. If you don't, here are the instructions to copy the host certificate/key and set the correct owner (permissions are preserved). Note that if you did not previously have a voms or tomcat user on your system, they were added during the RPM install:    VOMS-core daemon is running as user  voms  and requires its service certificate and key in  /etc/grid-security/voms . To copy the host certificate/key for this service, you need to do the following:  console\n[root@voms ~]$ cd /etc/grid-security\n[root@voms ~]$ cp hostcert.pem voms/vomscert.pem\n[root@voms ~]$ cp hostkey.pem voms/vomskey.pem\n[root@voms ~]$ chown -R voms:voms voms    Tomcat service is running as user  tomcat  and requires its service certificate and key in  /etc/grid-security/http . To copy the host certificate/key for this service, you need to do the following:  console\n[root@voms ~]$ cd /etc/grid-security\n[root@voms ~]$ mkdir /etc/grid-security/http\n[root@voms ~]$ cp hostcert.pem http/httpcert.pem\n[root@voms ~]$ cp hostkey.pem http/httpkey.pem\n[root@voms ~]$ chown -R tomcat:tomcat http", 
            "title": "Setup the Service Certificates"
        }, 
        {
            "location": "/other/install-voms/#configure-tomcat-options", 
            "text": "Edit  /etc/tomcat6/tomcat6.conf  to add the following line to the bottom of the file. This is important for good performance and essential if you run more than one VO.  CATALINA_OPTS= ${ CATALINA_OPTS }  -XX:MaxPermSize=256m   If you run a large number of VOs, you might need to increase the amount of memory used by Tomcat. You can do this by adding one more line to the bottom of  /etc/tomcat6/tomcat6.conf :  JAVA_OPTS= ${ JAVA_OPTS }  -Xmx2048m   You must add the following line to  /etc/tomcat6/tomcat6.conf  to use VOMS-Admin successfully:  JAVA_ENDORSED_DIRS= ${ JAVA_ENDORSED_DIRS } :/usr/share/voms-admin/endorsed", 
            "title": "Configure Tomcat options"
        }, 
        {
            "location": "/other/install-voms/#disable-sslv3-in-tomcat", 
            "text": "The OSG Security team has concluded that the POODLE SSLv3 vulnerability is not a  critical  concern to OSG software installations. Most services are not affected, and those that are affected are difficult to exploit in a meaningful way. Nonetheless, the recommendation is to disable support for SSLv3 where reasonable.  OSG software includes VOMS Admin Server (currently, version 2.7.0), which runs within Tomcat. By default Tomcat allows SSLv3 connections, but that is easy to change. To disable SSLv3 support from a Tomcat instance that contains VOMS Admin Server, set  protocols=\"TLSv1.1,TLSv1.2\"  in  /etc/tomcat[56]/server.xml , as shown below.  Note: OSG Software has tested this change only for Tomcat and VOMS Admin Server. But this is a server-level Tomcat configuration change and thus affects all Tomcat web applications running on the same server. For now, we recommend making this change to Tomcat servers that run only VOMS Admin Server. There are known issues with applying this change to a Tomcat instance that runs GUMS, in that dCache clients (at least) fail to work with the changed GUMS server.  --- server.xml.old      2014-10-28 11:27:11.000000000 -0500  +++ server.xml  2014-10-28 11:29:34.000000000 -0500  @@ -20,19 +20,20 @@ \n      Connector port= 8443  SSLEnabled= true \n               maxThreads= 150  minSpareThreads= 25  maxSpareThreads= 75 \n               enableLookups= false  disableUploadTimeout= true \n               acceptCount= 100  debug= 0  scheme= https  secure= true \n               sSLImplementation= org.glite.security.trustmanager.tomcat.TMSSLImplementation \n               trustStoreDir= /etc/grid-security/certificates \n               sslCertFile= /etc/grid-security/http/httpcert.pem \n               sslKey= /etc/grid-security/http/httpkey.pem \n               crlUpdateInterval= 2h \n               log4jConfFile= /usr/share/tomcat6/conf/log4j-trustmanager.properties \n               clientAuth= true  sslProtocol= TLS  sslEnabledProtocols= TLSv1  +              protocols= TLSv1.1,TLSv1.2 \n               crlEnabled= true  crlRequired= true / \n\n     Engine name= Catalina  defaultHost= localhost \n\n       Host name= localhost  appBase= webapps  / \n     /Engine \n   /Service  /Server", 
            "title": "Disable SSLv3 in Tomcat"
        }, 
        {
            "location": "/other/install-voms/#allow-tomcat-to-have-more-open-files-and-processes", 
            "text": "If you run several VOs, you may need to extend the limits for Tomcat so that it can open more file and create more processes. To do this, add the following lines to  /etc/security/limits.conf :  tomcat          soft    nofile  63536\ntomcat          hard    nofile  63536\n\ntomcat          soft    nproc   16384\ntomcat          hard    nproc   16384  Note that this is not needed if you are running just a single VO, this is only needed if you run many VOs.", 
            "title": "Allow Tomcat to have more open files and processes"
        }, 
        {
            "location": "/other/install-voms/#configure-trust-manager", 
            "text": "Configure Tomcat's trust manager. This must be done once, before starting up Tomcat for the first time:    You have to configure Tomcat in order to enable EMI trustmanager:  console\n[root@voms ~]$ /var/lib/trustmanager-tomcat/configure.sh\nInfo: using default install root: /\nInfo: using default configuration file: //var/lib/trustmanager-tomcat/config.properties\nInfo: using default configuration directory: //var/lib/trustmanager-tomcat\nInfo: you can clean up using the following commands\n      mv -f /etc/tomcat5/server.xml.old-trustmanager /etc/tomcat5/server.xml\n      rm -f /usr/share/tomcat5/server/lib/bcprov*.jar\n      rm -f /usr/share/tomcat5/server/lib/log4j*.jar\n      rm -f /usr/share/tomcat5/server/lib/trustmanager-*.jar\n      rm -f /etc/tomcat5/log4j-trustmanager.properties\n      rm -f //var/lib/trustmanager-tomcat/server.xml    (the output may look different on EL6)", 
            "title": "Configure Trust Manager"
        }, 
        {
            "location": "/other/install-voms/#configure-email-services-for-the-voms-server", 
            "text": "The VOMS server may need to send administrative emails. To enable this feature  on EL 6 systems , you must complete a one-time initialization of Java email services within Tomcat:    Run the  build-jar-repository  command:  console\n[root@voms ~]$ /usr/bin/build-jar-repository /usr/share/tomcat6/lib javamail    Set the hostname for reminder e-mails in  /etc/voms-admin/VO_NAME/voms.service.properties :  file\nvoms.hostname =  hostname  Replacing   with the public hostname of your VOMS server.    EL 5 systems are already configured to use Java email services.", 
            "title": "Configure Email Services for the VOMS Server"
        }, 
        {
            "location": "/other/install-voms/#add-and-configure-a-vo", 
            "text": "You must add at least one VO before you can start VOMS Admin. You must start the services as instructed during the configuration steps. You can stop them at the end if you don't want them running yet.    Run  voms-admin-configure  script to create a new VO. Note that if you have multiple VOs, each VO must have a unique VOMS_PORT  console\n[root@voms ~]$ voms-admin-configure install \\\n    --dbtype mysql \\\n    --vo VO_NAME \\\n    --createdb \\\n    --deploy-database  \\\n    --dbauser root  \\\n    --dbapwd MYSQL_ROOT_PASSD \\\n    --dbusername  admin-VO_NAME \\\n    --dbpassword secret  \\\n    --dbport  DB_PORT \\\n    --port VOMS_PORT  \\\n    --mail-from email \\\n    --smtp-host smtp.domain \\\n    --sqlloc /usr/lib64/voms/libvomsmysql.so \\\n    --cert /etc/grid-security/voms/vomscert.pem \\\n    --key  /etc/grid-security/voms/vomskey.pem \\\n    --read-access-for-authenticated-clients  For example for the test1 VO:  console\n[root@voms ~]$ voms-admin-configure install --dbtype mysql --vo test1 --createdb \\\n     --deploy-database --dbauser root --dbapwd  top_secret \\\n     --dbusername admin_test1 --dbpassword secret --dbport 3306 \\\n     --port 15001 --mail-from tlevshin@fnal.gov --smtp-host smtp.fnal.gov \\\n     --sqlloc /usr/lib64/voms/libvomsmysql.so --cert /etc/grid-security/voms/vomscert.pem \\\n     --key  /etc/grid-security/voms/vomskey.pem --read-access-for-authenticated-clients  If the command fails with a  IOError: [Errno 32] Broken pipe  error, check that you entered the correct database password.\nIf the above command is executed correctly a lot of sql stuff will scroll across the screen, followed by  ``` console\nDeploying voms database...\nDatabase deployed correctly!  VO test1 installation finished.  You can start the voms services using the following commands:\n    /etc/init.d/voms start test1\n    /etc/init.d/voms-admin start test1\n```    Start VOMS and VOMS Admin for the VO you just added:  console\n[root@voms ~]$ /sbin/service voms start VO_NAME\n[root@voms ~]$ /sbin/service voms-admin start VO_NAME  For example:  console\n[root@voms ~]$ /sbin/service voms start test1\n[root@voms ~]$ /sbin/service voms-admin start test1  Note that the voms-admin init script really isn't an init script. Instead it deploys a Tomcat webapp into Tomcat. In normal usage, you never need to run this command again, and you should  not  run  /sbin/service voms-admin stop VO_NAME .    Add a local admin:  console\n[root@voms ~]$ /usr/sbin/voms-db-deploy.py add-admin --vo VO_NAME  \\\n        --dn  HOST_CERT_SUBJECT \\\n        --ca HOST_CERT_ISSUER  For example:  console\n[root@voms ~]$ /usr/sbin/voms-db-deploy.py add-admin --vo test1 \\\n     --dn /DC=org/DC=doegrids/OU=Services/CN=fermicloud002.fnal.gov \\\n     --ca \"/DC=org/DC=DOEGrids/OU=Certificate Authorities/CN=DOEGrids CA 1\"    In order to complete the next step you need to start also Tomcat if it is not already running. And since Tomcat requires valid CRLs you need to fetch the CRLs if it is a fresh installation they are not there:  console\n[root@voms ~]$ /usr/sbin/fetch-crl  # If there are no CRLs in /etc/grid-security/certificates\n[root@voms ~]$ /sbin/service tomcat6 start    Configure VOMS Admin to allow GUMS and edg-mkgridmap to query your VO for the set of users. Note that you must have a valid certificate; voms-admin needs a certificate authorized to perform the operation (ACL editing) on the VO:  console\n[root@voms ~]$ voms-admin --nousercert --vo VO_NAME add-ACL-entry /VO_NAME ANYONE VOMS_CA \"CONTAINER_READ,MEMBERSHIP_READ\" true  For example:  console\n[root@voms ~]$ voms-admin --nousercert --vo test1 add-ACL-entry /test1 ANYONE VOMS_CA \"CONTAINER_READ,MEMBERSHIP_READ\" true    Allow compatibility with  GUMS  and  edg-mkgridmap . Edit  /etc/voms-admin/VO_NAME/voms.service.properties  by adding the following line to the file:  console\nvoms.csrf.log_only = true    If you don't want to leave your services running you can stop them after you are done with adding the VO:  [root@voms ~]$  /sbin/service voms stop [root@voms ~]$  /sbin/service tomcat6 stop", 
            "title": "Add and configure a VO"
        }, 
        {
            "location": "/other/install-voms/#services", 
            "text": "VOMS has its persistent repository in a MySQL database, that requires  mysql  to be running in order to be queried. All grid interactions require updated CA certificates and CRLs. Then the  voms server  is a daemon servicing authentication requests and  voms-admin , requiring Tomcat, is a Web UI/service to manage the VO membership.", 
            "title": "Services"
        }, 
        {
            "location": "/other/install-voms/#starting-and-enabling-services", 
            "text": "To start VOMS you need to start several services:     MySQL server needs to be running if it is not already:   console\n[root@voms ~]$ /sbin/service mysqld start    Start voms server  console\n[root@voms ~]$ service voms start    Start Tomcat, required by voms-admin:  console\n[root@voms ~]$ service tomcat6 start    You should also enable the appropriate services so that they are automatically started when your system is powered on:     To enable the other services:   console\n[root@voms ~]$ /sbin/chkconfig mysqld on\n[root@voms ~]$ /sbin/chkconfig voms on\n[root@voms ~]$ /sbin/chkconfig tomcat6 on    Note : You do  not  need to run chkconfig on the voms-admin service. This service is only need to do the initial \"deployment\" of the VOMS Admin service into Tomcat and is not needed thereafter unless you add another VO.", 
            "title": "Starting and Enabling Services"
        }, 
        {
            "location": "/other/install-voms/#stopping-and-disabling-services", 
            "text": "To stop VOMS you need to stop voms and voms-admin, and also the services they use if nothing else is using them:    Stop voms server  console\n[root@voms ~]$ service voms stop    , Stop Tomcat, if no other application is using it:  console\n[root@voms ~]$ service tomcat6 stop    Stop MySQL server if no other application is using it:  console\n[root@voms ~]$ service mysqld stop    In addition, you can disable services by running the following commands. However, you don't need to do this normally.     To disable the other services:   console\n[root@voms ~]$ /sbin/chkconfig mysqld off\n[root@voms ~]$ /sbin/chkconfig voms off\n[root@voms ~]$ /sbin/chkconfig tomcat6 off     Note  You do not need to run chkconfig on the voms-admin service. See above for more details.", 
            "title": "Stopping and Disabling Services"
        }, 
        {
            "location": "/other/install-voms/#advertise-your-voms-server", 
            "text": "A working VOMS server can be contacted to sign certificates extensions (e.g., by  voms-proxy-init ) or to verify VO memberships only if it is on the list of valid VOMS servers ( /etc/vomses ) and if there is an LSC file for that VO. Some grid services (glite-FTS and glite-WMS) even require a copy of the VOMS server certificate.  The file  /etc/vomses  contains a list of VOs and their VOMS servers. A VOMS server may appear on multiple lines if it is serving multiple VOs on different ports. All lines contain:  alias   HOST_NAME   PORT   HOST_DN   VO_NAME   For example:  test1   fermicloud002.fnal.gov   15001   /DC=org/DC=doegrids/OU=Services/CN=fermicloud002.fnal.gov   test1   The  LSC file  (LiSt of Certificates) is another description of your VOMS server:   It is located in  $X509_VOMS_DIR/${VO} , by default  /etc/grid-security/vomsdir/${VO}  The filename is the fully qualified hostname (FQDN) of your server, followed by  .lsc  The first line contains the subject DN of the VOMS server host certificate, without quotes  The second line contains the subject DN of the CA that issued the VOMS server host certificate, also without quotes   For example, for the VO named  test1  hosted on  fermicloud002 , the LSC file named  /etc/grid-security/vomsdir/test1/fermicloud002.fnal.gov.lsc  might contain:  /DC=org/DC=doegrids/OU=Services/CN=fermicloud002.fnal.gov\n/DC=org/DC=DOEGrids/OU=Certificate Authorities/CN=DOEGrids CA 1  The VOMS server manager must distribute the line for  /etc/vomses  and the LSC file to all hosts that will contact the VOMS server. OSG distributes with all its client and server installations a default list of VOMS servers ( /etc/vomses ) and a default set of LSC files. These are part of the  vo-client  RPM.  To be included, use the VO Registration function at  OIM VO Registration . Even if you are already registered as a VO you need to notify the Grid Operations Center if the information in the vomses file or the LSC file has changed.", 
            "title": "Advertise your VOMS server"
        }, 
        {
            "location": "/other/install-voms/#test", 
            "text": "Note  Before performing any of the tests you must start all the services as  described above   To test VOMS you can add a member (her/his certificate) to a VO using voms-admin, login in the Web UI and then:   use the voms server to generate a proxy for that certificate.  or generate a grid-mapfile  or query with GUMS", 
            "title": "Test"
        }, 
        {
            "location": "/other/install-voms/#voms-admin-test", 
            "text": "Add a member(yourself) to the VO:  [root@voms ~]$  voms-admin --vo VO_NAME --host HOST_NAME  \\ \n       --nousercert create-user USER_CERT_SUBJECT USER_CERT_ISSUER  \\ \n       USER_COMMON_NAME email   For example:  [root@voms ~]$  voms-admin --vo test1 --host fermicloud002 --nousercert create-user  \\ \n    /DC=org/DC=doegrids/OU=People/CN=Joe Doe 12345   /DC=org/DC=DOEGrids/OU=Certificate Authorities/CN=DOEGrids CA 1       Joe Doe 12345   jdoe@fnal.gov   Assign VOAdmin role to yourself, so you can manage the VO using the Web UI:  voms-admin --vo VO_NAME --host HOST_NAME \\      --nousercert assign-role  /VO_NAME  VO-Admin  \\        USER_CERT_SUBJECT USRT_CERT_ISSUER    For example:  [root@voms ~]$  voms-admin --vo test1 --host fermicloud002 --nousercert assign-role /test1   VO-Admin   \\ \n    /DC=org/DC=doegrids/OU=People/CN=Joe Doe 12345   /DC=org/DC=DOEGrids/OU=Certificate Authorities/CN=DOEGrids CA 1    Check the Web UI by accessing  https://hostname:8443/voms/voname . If you have your certificate loaded into the browser you will be able to manage the VO.", 
            "title": "Voms-admin test"
        }, 
        {
            "location": "/other/install-voms/#voms-core-test", 
            "text": "To perform this test you need the  voms-client  RPM.  To use your VOMS you need to add a line with its description (similar to the following) to  /etc/vomses , a file containing a list of all valid voms servers:  alias   HOST_NAME   PORT   HOST DN   VO_NAME   Look at  /etc/voms-admin/VO_NAME/vomses  to find the correct line for your VO. For example:  test1   fermicloud002.fnal.gov   15001   /DC=org/DC=doegrids/OU=Services/CN=fermicloud002.fnal.gov   test1   If it is not there already, add your certificate to the VO using  voms-admin   create-user  as in the previous section. Being the local admin set with  voms-db-deploy.py add-admin  does not add the certificate to the VO.  Use  voms-proxy-init  to generate proxy. Login as user, create  vomses  file (e.g.  ~/.vomses ). Generate voms proxy  [user@voms ~]$  voms-proxy-init -voms VO_NAME -vomses ~/vomses", 
            "title": "Voms-core test:"
        }, 
        {
            "location": "/other/install-voms/#testing-edg-mkgridmap", 
            "text": "The  edg-mkgridmap  command is run on a site's Compute Element node to generate a  grid-mapfile  file. This script is not provided as a part of the VOMS installation, so you will have to go on a CE node to test this. You do not have to be the  root  user on the CE node, but you will need to have a personal certificate available.  The  mkgridmap  file syntax/values for each VO you create, can be viewed on the WEB UI for your VO by selecting  Configuration information  on the left hand menu of the main page. You will get a line like:   group vomss://HOST_NAME:8443/voms/VO_NAME  group_name  e.g. for the OSG VO :   group vomss://cms-xen3.fnal.gov:8443/voms/OSG  osg   Note  The  osg  (3rd token) is an EDG format for the account name. You will likely want to change it to the group unix account you want assigned for your CE node. The example above is for a  group  account assignment.   To test, you will need to do this on a CE node where the authorization mode is \"gridmap\" (lcmaps or PRIMA are not enabled):    Populate  /etc/edg-mkgridmap.conf  with the example configuration file on your VOMS server. It will be the only entry.  console\n[root@voms ~]$ echo 'group vomss://cms-xen3.fnal.gov:8443/voms/OSG  osg'   /etc/edg-mkgridmap.conf    Execute edg-mkgridmap:  console\n[root@voms ~]$ edg-mkgridmap\n  ... the output will be in /etc/grid-security/grid-mapfile and will look as follows ...\n  \"/DC=org/DC=doegrids/OU=People/CN=John Weigand 458491\"  vdt\n  \"/DC=org/DC=doegrids/OU=Services/CN=cms-xen3.fnal.gov\"  vdt    You can find more in the  compute element install document .", 
            "title": "Testing edg-mkgridmap"
        }, 
        {
            "location": "/other/install-voms/#testing-the-gums-interface", 
            "text": "The GUMS authorization server periodically 'pulls' the VO membership data for it's authorization and account assignment functions from the various VO's VOMS servers.  Unfortunately, there is no way of testing this except with an installed GUMS server which this document does not address.  See the  GUMS install guide  for more information.", 
            "title": "Testing the GUMS interface"
        }, 
        {
            "location": "/other/install-voms/#removing-a-vo", 
            "text": "If you added the VO just for testing purposes, you can remove it using  voms-admin-configure remove --vo VONAME  as documented in the  user guide , e.g.:  # # Stop the service   [root@voms ~]$  service voms-admin stop test1 [root@voms ~]$  service voms stop test1   # # Remove it.  [root@voms ~]$  voms-admin-configure remove --vo test1 --undeploy-database --dropdb --dbapwd top_secret voms-admin-configure, version 2.6.1  Removing vo  test1  VO test1 succesfully removed.  # # Restart to update the voms-server webpage   [root@voms ~]$  service tomcat6 restart  Note that the --undeploy-database and --dropdb will remove the database that stores all the membership information. You cannot undo this option. If you're not quite sure that you really want to remove the database, then do not pass this option. The VO will be unaccessible and unusable, but the membership information will be retained in case you recreate the VO later.", 
            "title": "Removing a VO"
        }, 
        {
            "location": "/other/install-voms/#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/other/install-voms/#useful-configuration-and-log-files", 
            "text": "Configuration Files     Service or Process  Configuration File  Description      voms  /etc/voms/VO_NAME/voms.conf     /etc/voms/VO_NAME/voms.pass  VOMS server configuration and password for the VO database     voms-admin  /etc/voms-admin/voms-siblings.xml  Auxiliary voms-admin application configuration     /etc/voms-admin/VO_NAME/logback.runtime.xml  Logging configuration     /etc/voms-admin/VO_NAME/vo-aup.txt  VO acceptable use policy     /etc/voms-admin/VO_NAME/voms-admin-VO_NAME.xml     /etc/voms-admin/VO_NAME/voms.service.properties  VOMS Admin configuration      /etc/voms-admin/VO_NAME/voms.database.properties  VO database handler configuration     /etc/voms-admin/VO_NAME/vomses  vomses line    mysql  /var/lib/mysql/VO_NAME  VOMS database for the VO     /etc/my.cnf  MySQL configuration, e.g. server port    tomcat  /etc/tomcat6/  Tomcat configuration files     /etc/tomcat6/Catalina/localhost/*  VOMS related tomcat configuration     Log files     Service or Process  Log File  Description       /var/log/tomcat5/voms-admin-VO_NAME.log  This is the vo-by-vo log for voms-admin Web UI     /var/log/tomcat5/trustmanager.log  The trustmanager handles things related to authentication. Useful errors are sometimes here.    tomcat  /var/log/tomcat6/catalina.out  This is the Tomcat log file. Problems (and a lot of noise) are reported here.     /var/log/tomcat6/voms-admin-VO_NAME.log  This is the vo-by-vo log for voms-admin Web UI     /var/log/tomcat6/trustmanager.log  The trustmanager handles things related to authentication. Useful errors are sometimes here.    voms  /var/log/voms/voms.VO_NAME  This is the log of the VOMS daemon for each VO", 
            "title": "Useful configuration and log files"
        }, 
        {
            "location": "/other/install-voms/#fix-these-errors", 
            "text": "", 
            "title": "Fix these errors"
        }, 
        {
            "location": "/other/install-voms/#voms-server-for-vo95name-not-known", 
            "text": "If you saw an error like \"VOMS Server for VO_NAME not known!\" probably you did not add the VO VO_NAME (e.g. test1) to  /etc/vomses", 
            "title": "VOMS Server for VO_NAME not known!"
        }, 
        {
            "location": "/other/install-voms/#tomcat-is-not-starting", 
            "text": "If Tomcat is not starting and  /var/log/tomcat5/catalina.out  contains an error like \"/usr/bin/tomcat5: line 331: /usr/lib/java/bin/java: No such file or directory\", then you may have to uncomment the line  JAVA_HOME=\"/usr/lib/jvm/java\"  in  /etc/tomcat5/tomcat5.conf . The Jpackage repository is known to provide a misconfigured tomcat5 package.", 
            "title": "Tomcat is not starting"
        }, 
        {
            "location": "/other/install-voms/#problems-running-voms-admin", 
            "text": "Verify if voms-admin was configured and started correctly:   If voms-admin is not responding and you get a \"Socket error 111\" try restarting tomcat6 and voms-admin  If there are BC (bouncycastle) related errors in Tomcat's log file ( /var/log/tomcat6/catalina.out ), the trust manager may not be configured correctly. Try to run  /var/lib/trustmanager-tomcat/configure.sh .  If there are OpenSAML or XML parser related errors in Tomcat's log file ( /var/log/tomcat6/catalina.out ), the JAVA_ENDORSED_DIRS variable may not be set in  /etc/tomcat6/tomcat6.conf . See the  Configure Tomcat options  section above.  If you get an error like \"Socket error: (1, 'error:14094416:SSL routines:SSL3_READ_BYTES:sslv3 alert certificate unknown')\" probably you have no CRLs for your CA certificates. You can see in  /var/log/tomcat6/trustmanager.log  that Tomcat is discarding CA certificates without CRL. Run  fetch-crl .", 
            "title": "Problems running voms-admin"
        }, 
        {
            "location": "/other/install-voms/#wrong-urls-in-voms-admin-pagesconnection-timed-outhost-unavailable", 
            "text": "If you can reach the first page but all the links are failing, check the URL in the links. Tomcat by default constructs the absolute URLs in the web pages using the hostname/IP and port used for the request. If the server resides behind a Firewall or NAT that enforces port redirection, these may be the server IP or the port on the private network, that likely are not reachable by your Web browser. To fix the problem you must use  Tomcat's proxy support  by setting  proxyName  and/or  proxyPort  in Tomcat's configuration file ( /etc/tomcat5/server.xml ).  After changing  server.xml   stop and restart  Tomcat in order for the changes to take effect.", 
            "title": "Wrong URLs in VOMS admin pages/Connection timed out/Host unavailable"
        }, 
        {
            "location": "/other/install-voms/#how-to-get-help", 
            "text": "To get assistance please use  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/other/install-voms/#references", 
            "text": "EMI documentation:   EMI VOMS documentation  VOMS Admin User Guide  VOMS System Admin Guide  How to configure VOMS LSC files   Openssl commands:  http://security.ncsa.illinois.edu/research/grid-howtos/usefulopenssl.html", 
            "title": "References"
        }, 
        {
            "location": "/common/help/", 
            "text": "How to Get Help\n\n\nThis page is aimed at OSG site administrators looking for support. Help for OSG users is found at \nour support desk\n.\n\n\nGrid Operations Center\n\n\nThe Grid Operations Center (GOC) is available to coordinate users, site admins, and developers around an issue.  Additionally, the GOC can provide basic monitoring and troubleshooting.  There are several ways to receive support:\n\n\n\n\nYou can \nsubmit a trouble ticket\n or send an email to \ngoc@opensciencegrid.org\n (which also accept general inquiries not intended for tickets.)\n\n\nThe \ntrouble ticket system\n is searchable.  Historical tickets may contain the solution for similar problems others have encountered.\n\n\nThe \noperations blog\n contains information about recent software releases, and important outage and maintenance notifications of central OSG services.\n\n\nFor emergencies, the OSG Grid Operation Center provides extended support. Operators are on hand 24x7 at the GOC and can be reached via phone at +1 317-278-9699.   Non-emergency issues can be opened 24x7 but will be handled during normal business hours.\n\n\n\n\nSecurity incident\n\n\nSecurity incidents can be reported by following the instructions on the \nIncident Discovery and Reporting\n page.  Additional steps to aid in the incident handling process are also linked from that page.\n\n\nInformation Required to Help You\n\n\nIf you came to this page from an installation or other document in this website, then follow instructions in the \nTroubleshooting\n and \nDebugging\n sections of that document and include results in your support inquiry, no matter which channel you choose (email, trouble ticket, web chat, ...)\n\n\nFor problems with installation of some software run \nosg-system-profiler\n:\n\n\nroot@host #\n osg-system-profiler\n\n\n\n\n\nAttach the generated \nosg-profile.txt\n to your support inquiry.\n\n\nCommunity-specific Resources\n\n\nSome OSG VOs have dedicated forums or mechanisms for community-specific support.  If your VO provides user support, that should be a user's first line of support because the VO is most familiar with your applications and requirements.\n\n\n\n\nThe list of support contacts for OSG VOs can be found in the \nSupport Center Tab on MyOSG\n.\n\n\nResources for \nCMS\n sites:\n\n\nhttp://www.uscms.org/uscms_at_work/physics/computing/grid/index.shtml\n\n\nCMS Hyper News: \nhttps://hypernews.cern.ch/HyperNews/CMS/get/osg-tier3.html\n\n\nCMS Twiki: \nhttps://twiki.cern.ch/twiki/bin/viewauth/CMS/USTier3Computing", 
            "title": "Get Help"
        }, 
        {
            "location": "/common/help/#how-to-get-help", 
            "text": "This page is aimed at OSG site administrators looking for support. Help for OSG users is found at  our support desk .", 
            "title": "How to Get Help"
        }, 
        {
            "location": "/common/help/#grid-operations-center", 
            "text": "The Grid Operations Center (GOC) is available to coordinate users, site admins, and developers around an issue.  Additionally, the GOC can provide basic monitoring and troubleshooting.  There are several ways to receive support:   You can  submit a trouble ticket  or send an email to  goc@opensciencegrid.org  (which also accept general inquiries not intended for tickets.)  The  trouble ticket system  is searchable.  Historical tickets may contain the solution for similar problems others have encountered.  The  operations blog  contains information about recent software releases, and important outage and maintenance notifications of central OSG services.  For emergencies, the OSG Grid Operation Center provides extended support. Operators are on hand 24x7 at the GOC and can be reached via phone at +1 317-278-9699.   Non-emergency issues can be opened 24x7 but will be handled during normal business hours.", 
            "title": "Grid Operations Center"
        }, 
        {
            "location": "/common/help/#security-incident", 
            "text": "Security incidents can be reported by following the instructions on the  Incident Discovery and Reporting  page.  Additional steps to aid in the incident handling process are also linked from that page.", 
            "title": "Security incident"
        }, 
        {
            "location": "/common/help/#information-required-to-help-you", 
            "text": "If you came to this page from an installation or other document in this website, then follow instructions in the  Troubleshooting  and  Debugging  sections of that document and include results in your support inquiry, no matter which channel you choose (email, trouble ticket, web chat, ...)  For problems with installation of some software run  osg-system-profiler :  root@host #  osg-system-profiler  Attach the generated  osg-profile.txt  to your support inquiry.", 
            "title": "Information Required to Help You"
        }, 
        {
            "location": "/common/help/#community-specific-resources", 
            "text": "Some OSG VOs have dedicated forums or mechanisms for community-specific support.  If your VO provides user support, that should be a user's first line of support because the VO is most familiar with your applications and requirements.   The list of support contacts for OSG VOs can be found in the  Support Center Tab on MyOSG .  Resources for  CMS  sites:  http://www.uscms.org/uscms_at_work/physics/computing/grid/index.shtml  CMS Hyper News:  https://hypernews.cern.ch/HyperNews/CMS/get/osg-tier3.html  CMS Twiki:  https://twiki.cern.ch/twiki/bin/viewauth/CMS/USTier3Computing", 
            "title": "Community-specific Resources"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-2/", 
            "text": "OSG Software Stack -- Data Release -- 3.4.4-2\n\n\nRelease Date\n: 2017-10-11\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nCA Certificates based on \nIGTF 1.86\n\n\nupdated MaGrid CA with extended validity period (MA)\n\n\nremoved discontinued pkIRISGrid CA (ES)\n\n\n\n\n\n\nVO Package v75\n\n\nAdd CMS wildcard to default map file\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nigtf-ca-certs-1.86-1.osg34.el6\n\n\nosg-ca-certs-1.66-1.osg34.el6\n\n\nvo-client-75-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nigtf-ca-certs-1.86-1.osg34.el7\n\n\nosg-ca-certs-1.66-1.osg34.el7\n\n\nvo-client-75-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nigtf-ca-certs osg-ca-certs osg-gums-config vo-client vo-client-edgmkgridmap vo-client-lcmaps-voms\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nigtf-ca-certs-1.86-1.osg34.el6\nosg-ca-certs-1.66-1.osg34.el6\nosg-gums-config-75-1.osg34.el6\nvo-client-75-1.osg34.el6\nvo-client-edgmkgridmap-75-1.osg34.el6\nvo-client-lcmaps-voms-75-1.osg34.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nigtf-ca-certs-1.86-1.osg34.el7\nosg-ca-certs-1.66-1.osg34.el7\nosg-gums-config-75-1.osg34.el7\nvo-client-75-1.osg34.el7\nvo-client-edgmkgridmap-75-1.osg34.el7\nvo-client-lcmaps-voms-75-1.osg34.el7", 
            "title": "OSG Release 3.4.4-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-2/#osg-software-stack-data-release-344-2", 
            "text": "Release Date : 2017-10-11", 
            "title": "OSG Software Stack -- Data Release -- 3.4.4-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-2/#summary-of-changes", 
            "text": "This release contains:   CA Certificates based on  IGTF 1.86  updated MaGrid CA with extended validity period (MA)  removed discontinued pkIRISGrid CA (ES)    VO Package v75  Add CMS wildcard to default map file     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-2/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-2/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-2/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-2/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-2/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-2/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-2/#enterprise-linux-6", 
            "text": "igtf-ca-certs-1.86-1.osg34.el6  osg-ca-certs-1.66-1.osg34.el6  vo-client-75-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-2/#enterprise-linux-7", 
            "text": "igtf-ca-certs-1.86-1.osg34.el7  osg-ca-certs-1.66-1.osg34.el7  vo-client-75-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-2/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  igtf-ca-certs osg-ca-certs osg-gums-config vo-client vo-client-edgmkgridmap vo-client-lcmaps-voms  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-2/#enterprise-linux-6_1", 
            "text": "igtf-ca-certs-1.86-1.osg34.el6\nosg-ca-certs-1.66-1.osg34.el6\nosg-gums-config-75-1.osg34.el6\nvo-client-75-1.osg34.el6\nvo-client-edgmkgridmap-75-1.osg34.el6\nvo-client-lcmaps-voms-75-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-2/#enterprise-linux-7_1", 
            "text": "igtf-ca-certs-1.86-1.osg34.el7\nosg-ca-certs-1.66-1.osg34.el7\nosg-gums-config-75-1.osg34.el7\nvo-client-75-1.osg34.el7\nvo-client-edgmkgridmap-75-1.osg34.el7\nvo-client-lcmaps-voms-75-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-29-2/", 
            "text": "OSG Software Stack -- Data Release -- 3.3.29-2\n\n\nRelease Date\n: 2017-10-11\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nCA Certificates based on \nIGTF 1.86\n\n\nupdated MaGrid CA with extended validity period (MA)\n\n\nremoved discontinued pkIRISGrid CA (ES)\n\n\n\n\n\n\nVO Package v75\n\n\nAdd CMS wildcard to default map file\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nigtf-ca-certs-1.86-1.osg33.el6\n\n\nosg-ca-certs-1.66-1.osg33.el6\n\n\nvo-client-75-1.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nigtf-ca-certs-1.86-1.osg33.el7\n\n\nosg-ca-certs-1.66-1.osg33.el7\n\n\nvo-client-75-1.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nigtf-ca-certs osg-ca-certs osg-gums-config vo-client vo-client-edgmkgridmap vo-client-lcmaps-voms\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nigtf-ca-certs-1.86-1.osg33.el6\nosg-ca-certs-1.66-1.osg33.el6\nosg-gums-config-75-1.osg33.el6\nvo-client-75-1.osg33.el6\nvo-client-edgmkgridmap-75-1.osg33.el6\nvo-client-lcmaps-voms-75-1.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nigtf-ca-certs-1.86-1.osg33.el7\nosg-ca-certs-1.66-1.osg33.el7\nosg-gums-config-75-1.osg33.el7\nvo-client-75-1.osg33.el7\nvo-client-edgmkgridmap-75-1.osg33.el7\nvo-client-lcmaps-voms-75-1.osg33.el7", 
            "title": "OSG Release 3.3.29-2"
        }, 
        {
            "location": "/release/3.3/release-3-3-29-2/#osg-software-stack-data-release-3329-2", 
            "text": "Release Date : 2017-10-11", 
            "title": "OSG Software Stack -- Data Release -- 3.3.29-2"
        }, 
        {
            "location": "/release/3.3/release-3-3-29-2/#summary-of-changes", 
            "text": "This release contains:   CA Certificates based on  IGTF 1.86  updated MaGrid CA with extended validity period (MA)  removed discontinued pkIRISGrid CA (ES)    VO Package v75  Add CMS wildcard to default map file     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-29-2/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-29-2/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-29-2/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-29-2/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-29-2/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-29-2/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-29-2/#enterprise-linux-6", 
            "text": "igtf-ca-certs-1.86-1.osg33.el6  osg-ca-certs-1.66-1.osg33.el6  vo-client-75-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-29-2/#enterprise-linux-7", 
            "text": "igtf-ca-certs-1.86-1.osg33.el7  osg-ca-certs-1.66-1.osg33.el7  vo-client-75-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-29-2/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  igtf-ca-certs osg-ca-certs osg-gums-config vo-client vo-client-edgmkgridmap vo-client-lcmaps-voms  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-29-2/#enterprise-linux-6_1", 
            "text": "igtf-ca-certs-1.86-1.osg33.el6\nosg-ca-certs-1.66-1.osg33.el6\nosg-gums-config-75-1.osg33.el6\nvo-client-75-1.osg33.el6\nvo-client-edgmkgridmap-75-1.osg33.el6\nvo-client-lcmaps-voms-75-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-29-2/#enterprise-linux-7_1", 
            "text": "igtf-ca-certs-1.86-1.osg33.el7\nosg-ca-certs-1.66-1.osg33.el7\nosg-gums-config-75-1.osg33.el7\nvo-client-75-1.osg33.el7\nvo-client-edgmkgridmap-75-1.osg33.el7\nvo-client-lcmaps-voms-75-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/", 
            "text": "OSG Software Release 3.4.4\n\n\nRelease Date\n: 2017-10-10\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nUpdated gsi-openssh-server to interoperate with clients using OpenSSL 1.1\n\n\nSingularity 2.3.2\n: Now works with Docker's updated registry RESTful API\n\n\nHTCondor 8.6.6\n: Bug fix release\n\n\nglobus-gridftp-server-control 5.2: Allow 400 responses to stat failures\n\n\nosg-ca-scripts now properly requires wget to be installed\n\n\nUpdated osg-configure\n\n\nto work properly when fetch-crl is missing\n\n\nto work properly with HTCondor 8.7.2+\n\n\n\n\n\n\nHTCondor 8.7.3\n in Upcoming\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n\n\nNote\n\n\nOSG 3.4 contains only 64-bit components.\n\n\n\n\n\n\nNote\n\n\nStashCache is supported on EL7 only.\n\n\n\n\n\n\nNote\n\n\nxrootd-lcmaps will remain at 1.2.1-2 on EL6.\n\n\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nKnown Issues\n\n\n\n\nBecause the Gratia system has been turned off, RSV emits the following warning: \nWARNING: Gratia server gratia-osg-prod.opensciencegrid.org:80 not responding to obtain last contact details\n. This warning can safely be ignored.\n\n\nUsing the LCMAPS VOMS will result in a failing \"supported VO\" RSV test (\nSOFTWARE-2763\n). This can be ignored and a fix is targeted for the November release.\n\n\nIn GlideinWMS, a small configuration change must be added to account for changes in HTCondor 8.6. Add the following line to the HTCondor configuration.\nCOLLECTOR.USE_SHARED_PORT=False\n\n\n\n\n\n\n\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\ncondor-8.6.6-1.osg34.el6\n\n\nglobus-gridftp-server-control-5.2-1.1.osg34.el6\n\n\ngsi-openssh-7.3p1c-1.1.osg34.el6\n\n\nosg-build-1.10.2-1.osg34.el6\n\n\nosg-ca-scripts-1.1.7-2.osg34.el6\n\n\nosg-configure-2.2.1-1.osg34.el6\n\n\nosg-release-3.4-2.osg34.el6\n\n\nosg-release-itb-3.4-2.osg34.el6\n\n\nosg-tested-internal-3.4-5.osg34.el6\n\n\nosg-version-3.4.4-1.osg34.el6\n\n\nsingularity-2.3.2-0.1.1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\ncondor-8.6.6-1.osg34.el7\n\n\nglobus-gridftp-server-control-5.2-1.1.osg34.el7\n\n\ngsi-openssh-7.3p1c-1.1.osg34.el7\n\n\nosg-build-1.10.2-1.osg34.el7\n\n\nosg-ca-scripts-1.1.7-2.osg34.el7\n\n\nosg-configure-2.2.1-1.osg34.el7\n\n\nosg-release-3.4-2.osg34.el7\n\n\nosg-release-itb-3.4-2.osg34.el7\n\n\nosg-tested-internal-3.4-5.osg34.el7\n\n\nosg-version-3.4.4-1.osg34.el7\n\n\nsingularity-2.3.2-0.1.1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\ncondor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp globus-gridftp-server-control globus-gridftp-server-control-debuginfo globus-gridftp-server-control-devel gsi-openssh gsi-openssh-clients gsi-openssh-debuginfo gsi-openssh-server osg-build osg-build-base osg-build-koji osg-build-mock osg-build-tests osg-ca-scripts osg-configure osg-configure-bosco osg-configure-ce osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-release osg-release-itb osg-tested-internal osg-version singularity singularity-debuginfo singularity-devel singularity-runtime\n\n\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\ncondor-8.6.6-1.osg34.el6\ncondor-all-8.6.6-1.osg34.el6\ncondor-bosco-8.6.6-1.osg34.el6\ncondor-classads-8.6.6-1.osg34.el6\ncondor-classads-devel-8.6.6-1.osg34.el6\ncondor-cream-gahp-8.6.6-1.osg34.el6\ncondor-debuginfo-8.6.6-1.osg34.el6\ncondor-kbdd-8.6.6-1.osg34.el6\ncondor-procd-8.6.6-1.osg34.el6\ncondor-python-8.6.6-1.osg34.el6\ncondor-std-universe-8.6.6-1.osg34.el6\ncondor-test-8.6.6-1.osg34.el6\ncondor-vm-gahp-8.6.6-1.osg34.el6\nglobus-gridftp-server-control-5.2-1.1.osg34.el6\nglobus-gridftp-server-control-debuginfo-5.2-1.1.osg34.el6\nglobus-gridftp-server-control-devel-5.2-1.1.osg34.el6\ngsi-openssh-7.3p1c-1.1.osg34.el6\ngsi-openssh-clients-7.3p1c-1.1.osg34.el6\ngsi-openssh-debuginfo-7.3p1c-1.1.osg34.el6\ngsi-openssh-server-7.3p1c-1.1.osg34.el6\nosg-build-1.10.2-1.osg34.el6\nosg-build-base-1.10.2-1.osg34.el6\nosg-build-koji-1.10.2-1.osg34.el6\nosg-build-mock-1.10.2-1.osg34.el6\nosg-build-tests-1.10.2-1.osg34.el6\nosg-ca-scripts-1.1.7-2.osg34.el6\nosg-configure-2.2.1-1.osg34.el6\nosg-configure-bosco-2.2.1-1.osg34.el6\nosg-configure-ce-2.2.1-1.osg34.el6\nosg-configure-condor-2.2.1-1.osg34.el6\nosg-configure-gateway-2.2.1-1.osg34.el6\nosg-configure-gip-2.2.1-1.osg34.el6\nosg-configure-gratia-2.2.1-1.osg34.el6\nosg-configure-infoservices-2.2.1-1.osg34.el6\nosg-configure-lsf-2.2.1-1.osg34.el6\nosg-configure-managedfork-2.2.1-1.osg34.el6\nosg-configure-misc-2.2.1-1.osg34.el6\nosg-configure-network-2.2.1-1.osg34.el6\nosg-configure-pbs-2.2.1-1.osg34.el6\nosg-configure-rsv-2.2.1-1.osg34.el6\nosg-configure-sge-2.2.1-1.osg34.el6\nosg-configure-slurm-2.2.1-1.osg34.el6\nosg-configure-squid-2.2.1-1.osg34.el6\nosg-configure-tests-2.2.1-1.osg34.el6\nosg-release-3.4-2.osg34.el6\nosg-release-itb-3.4-2.osg34.el6\nosg-tested-internal-3.4-5.osg34.el6\nosg-version-3.4.4-1.osg34.el6\nsingularity-2.3.2-0.1.1.osg34.el6\nsingularity-debuginfo-2.3.2-0.1.1.osg34.el6\nsingularity-devel-2.3.2-0.1.1.osg34.el6\nsingularity-runtime-2.3.2-0.1.1.osg34.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\ncondor-8.6.6-1.osg34.el7\ncondor-all-8.6.6-1.osg34.el7\ncondor-bosco-8.6.6-1.osg34.el7\ncondor-classads-8.6.6-1.osg34.el7\ncondor-classads-devel-8.6.6-1.osg34.el7\ncondor-cream-gahp-8.6.6-1.osg34.el7\ncondor-debuginfo-8.6.6-1.osg34.el7\ncondor-kbdd-8.6.6-1.osg34.el7\ncondor-procd-8.6.6-1.osg34.el7\ncondor-python-8.6.6-1.osg34.el7\ncondor-test-8.6.6-1.osg34.el7\ncondor-vm-gahp-8.6.6-1.osg34.el7\nglobus-gridftp-server-control-5.2-1.1.osg34.el7\nglobus-gridftp-server-control-debuginfo-5.2-1.1.osg34.el7\nglobus-gridftp-server-control-devel-5.2-1.1.osg34.el7\ngsi-openssh-7.3p1c-1.1.osg34.el7\ngsi-openssh-clients-7.3p1c-1.1.osg34.el7\ngsi-openssh-debuginfo-7.3p1c-1.1.osg34.el7\ngsi-openssh-server-7.3p1c-1.1.osg34.el7\nosg-build-1.10.2-1.osg34.el7\nosg-build-base-1.10.2-1.osg34.el7\nosg-build-koji-1.10.2-1.osg34.el7\nosg-build-mock-1.10.2-1.osg34.el7\nosg-build-tests-1.10.2-1.osg34.el7\nosg-ca-scripts-1.1.7-2.osg34.el7\nosg-configure-2.2.1-1.osg34.el7\nosg-configure-bosco-2.2.1-1.osg34.el7\nosg-configure-ce-2.2.1-1.osg34.el7\nosg-configure-condor-2.2.1-1.osg34.el7\nosg-configure-gateway-2.2.1-1.osg34.el7\nosg-configure-gip-2.2.1-1.osg34.el7\nosg-configure-gratia-2.2.1-1.osg34.el7\nosg-configure-infoservices-2.2.1-1.osg34.el7\nosg-configure-lsf-2.2.1-1.osg34.el7\nosg-configure-managedfork-2.2.1-1.osg34.el7\nosg-configure-misc-2.2.1-1.osg34.el7\nosg-configure-network-2.2.1-1.osg34.el7\nosg-configure-pbs-2.2.1-1.osg34.el7\nosg-configure-rsv-2.2.1-1.osg34.el7\nosg-configure-sge-2.2.1-1.osg34.el7\nosg-configure-slurm-2.2.1-1.osg34.el7\nosg-configure-squid-2.2.1-1.osg34.el7\nosg-configure-tests-2.2.1-1.osg34.el7\nosg-release-3.4-2.osg34.el7\nosg-release-itb-3.4-2.osg34.el7\nosg-tested-internal-3.4-5.osg34.el7\nosg-version-3.4.4-1.osg34.el7\nsingularity-2.3.2-0.1.1.osg34.el7\nsingularity-debuginfo-2.3.2-0.1.1.osg34.el7\nsingularity-devel-2.3.2-0.1.1.osg34.el7\nsingularity-runtime-2.3.2-0.1.1.osg34.el7\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\ncondor-8.7.3-1.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\ncondor-8.7.3-1.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\ncondor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\ncondor-8.7.3-1.osgup.el6\ncondor-all-8.7.3-1.osgup.el6\ncondor-annex-ec2-8.7.3-1.osgup.el6\ncondor-bosco-8.7.3-1.osgup.el6\ncondor-classads-8.7.3-1.osgup.el6\ncondor-classads-devel-8.7.3-1.osgup.el6\ncondor-cream-gahp-8.7.3-1.osgup.el6\ncondor-debuginfo-8.7.3-1.osgup.el6\ncondor-kbdd-8.7.3-1.osgup.el6\ncondor-procd-8.7.3-1.osgup.el6\ncondor-python-8.7.3-1.osgup.el6\ncondor-std-universe-8.7.3-1.osgup.el6\ncondor-test-8.7.3-1.osgup.el6\ncondor-vm-gahp-8.7.3-1.osgup.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\ncondor-8.7.3-1.osgup.el7\ncondor-all-8.7.3-1.osgup.el7\ncondor-annex-ec2-8.7.3-1.osgup.el7\ncondor-bosco-8.7.3-1.osgup.el7\ncondor-classads-8.7.3-1.osgup.el7\ncondor-classads-devel-8.7.3-1.osgup.el7\ncondor-cream-gahp-8.7.3-1.osgup.el7\ncondor-debuginfo-8.7.3-1.osgup.el7\ncondor-kbdd-8.7.3-1.osgup.el7\ncondor-procd-8.7.3-1.osgup.el7\ncondor-python-8.7.3-1.osgup.el7\ncondor-test-8.7.3-1.osgup.el7\ncondor-vm-gahp-8.7.3-1.osgup.el7", 
            "title": "OSG Release 3.4.4"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#osg-software-release-344", 
            "text": "Release Date : 2017-10-10", 
            "title": "OSG Software Release 3.4.4"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#summary-of-changes", 
            "text": "This release contains:   Updated gsi-openssh-server to interoperate with clients using OpenSSL 1.1  Singularity 2.3.2 : Now works with Docker's updated registry RESTful API  HTCondor 8.6.6 : Bug fix release  globus-gridftp-server-control 5.2: Allow 400 responses to stat failures  osg-ca-scripts now properly requires wget to be installed  Updated osg-configure  to work properly when fetch-crl is missing  to work properly with HTCondor 8.7.2+    HTCondor 8.7.3  in Upcoming   These  JIRA tickets  were addressed in this release.   Note  OSG 3.4 contains only 64-bit components.    Note  StashCache is supported on EL7 only.    Note  xrootd-lcmaps will remain at 1.2.1-2 on EL6.   Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#known-issues", 
            "text": "Because the Gratia system has been turned off, RSV emits the following warning:  WARNING: Gratia server gratia-osg-prod.opensciencegrid.org:80 not responding to obtain last contact details . This warning can safely be ignored.  Using the LCMAPS VOMS will result in a failing \"supported VO\" RSV test ( SOFTWARE-2763 ). This can be ignored and a fix is targeted for the November release.  In GlideinWMS, a small configuration change must be added to account for changes in HTCondor 8.6. Add the following line to the HTCondor configuration. COLLECTOR.USE_SHARED_PORT=False", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#enterprise-linux-6", 
            "text": "condor-8.6.6-1.osg34.el6  globus-gridftp-server-control-5.2-1.1.osg34.el6  gsi-openssh-7.3p1c-1.1.osg34.el6  osg-build-1.10.2-1.osg34.el6  osg-ca-scripts-1.1.7-2.osg34.el6  osg-configure-2.2.1-1.osg34.el6  osg-release-3.4-2.osg34.el6  osg-release-itb-3.4-2.osg34.el6  osg-tested-internal-3.4-5.osg34.el6  osg-version-3.4.4-1.osg34.el6  singularity-2.3.2-0.1.1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#enterprise-linux-7", 
            "text": "condor-8.6.6-1.osg34.el7  globus-gridftp-server-control-5.2-1.1.osg34.el7  gsi-openssh-7.3p1c-1.1.osg34.el7  osg-build-1.10.2-1.osg34.el7  osg-ca-scripts-1.1.7-2.osg34.el7  osg-configure-2.2.1-1.osg34.el7  osg-release-3.4-2.osg34.el7  osg-release-itb-3.4-2.osg34.el7  osg-tested-internal-3.4-5.osg34.el7  osg-version-3.4.4-1.osg34.el7  singularity-2.3.2-0.1.1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp globus-gridftp-server-control globus-gridftp-server-control-debuginfo globus-gridftp-server-control-devel gsi-openssh gsi-openssh-clients gsi-openssh-debuginfo gsi-openssh-server osg-build osg-build-base osg-build-koji osg-build-mock osg-build-tests osg-ca-scripts osg-configure osg-configure-bosco osg-configure-ce osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-release osg-release-itb osg-tested-internal osg-version singularity singularity-debuginfo singularity-devel singularity-runtime   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#enterprise-linux-6_1", 
            "text": "condor-8.6.6-1.osg34.el6\ncondor-all-8.6.6-1.osg34.el6\ncondor-bosco-8.6.6-1.osg34.el6\ncondor-classads-8.6.6-1.osg34.el6\ncondor-classads-devel-8.6.6-1.osg34.el6\ncondor-cream-gahp-8.6.6-1.osg34.el6\ncondor-debuginfo-8.6.6-1.osg34.el6\ncondor-kbdd-8.6.6-1.osg34.el6\ncondor-procd-8.6.6-1.osg34.el6\ncondor-python-8.6.6-1.osg34.el6\ncondor-std-universe-8.6.6-1.osg34.el6\ncondor-test-8.6.6-1.osg34.el6\ncondor-vm-gahp-8.6.6-1.osg34.el6\nglobus-gridftp-server-control-5.2-1.1.osg34.el6\nglobus-gridftp-server-control-debuginfo-5.2-1.1.osg34.el6\nglobus-gridftp-server-control-devel-5.2-1.1.osg34.el6\ngsi-openssh-7.3p1c-1.1.osg34.el6\ngsi-openssh-clients-7.3p1c-1.1.osg34.el6\ngsi-openssh-debuginfo-7.3p1c-1.1.osg34.el6\ngsi-openssh-server-7.3p1c-1.1.osg34.el6\nosg-build-1.10.2-1.osg34.el6\nosg-build-base-1.10.2-1.osg34.el6\nosg-build-koji-1.10.2-1.osg34.el6\nosg-build-mock-1.10.2-1.osg34.el6\nosg-build-tests-1.10.2-1.osg34.el6\nosg-ca-scripts-1.1.7-2.osg34.el6\nosg-configure-2.2.1-1.osg34.el6\nosg-configure-bosco-2.2.1-1.osg34.el6\nosg-configure-ce-2.2.1-1.osg34.el6\nosg-configure-condor-2.2.1-1.osg34.el6\nosg-configure-gateway-2.2.1-1.osg34.el6\nosg-configure-gip-2.2.1-1.osg34.el6\nosg-configure-gratia-2.2.1-1.osg34.el6\nosg-configure-infoservices-2.2.1-1.osg34.el6\nosg-configure-lsf-2.2.1-1.osg34.el6\nosg-configure-managedfork-2.2.1-1.osg34.el6\nosg-configure-misc-2.2.1-1.osg34.el6\nosg-configure-network-2.2.1-1.osg34.el6\nosg-configure-pbs-2.2.1-1.osg34.el6\nosg-configure-rsv-2.2.1-1.osg34.el6\nosg-configure-sge-2.2.1-1.osg34.el6\nosg-configure-slurm-2.2.1-1.osg34.el6\nosg-configure-squid-2.2.1-1.osg34.el6\nosg-configure-tests-2.2.1-1.osg34.el6\nosg-release-3.4-2.osg34.el6\nosg-release-itb-3.4-2.osg34.el6\nosg-tested-internal-3.4-5.osg34.el6\nosg-version-3.4.4-1.osg34.el6\nsingularity-2.3.2-0.1.1.osg34.el6\nsingularity-debuginfo-2.3.2-0.1.1.osg34.el6\nsingularity-devel-2.3.2-0.1.1.osg34.el6\nsingularity-runtime-2.3.2-0.1.1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#enterprise-linux-7_1", 
            "text": "condor-8.6.6-1.osg34.el7\ncondor-all-8.6.6-1.osg34.el7\ncondor-bosco-8.6.6-1.osg34.el7\ncondor-classads-8.6.6-1.osg34.el7\ncondor-classads-devel-8.6.6-1.osg34.el7\ncondor-cream-gahp-8.6.6-1.osg34.el7\ncondor-debuginfo-8.6.6-1.osg34.el7\ncondor-kbdd-8.6.6-1.osg34.el7\ncondor-procd-8.6.6-1.osg34.el7\ncondor-python-8.6.6-1.osg34.el7\ncondor-test-8.6.6-1.osg34.el7\ncondor-vm-gahp-8.6.6-1.osg34.el7\nglobus-gridftp-server-control-5.2-1.1.osg34.el7\nglobus-gridftp-server-control-debuginfo-5.2-1.1.osg34.el7\nglobus-gridftp-server-control-devel-5.2-1.1.osg34.el7\ngsi-openssh-7.3p1c-1.1.osg34.el7\ngsi-openssh-clients-7.3p1c-1.1.osg34.el7\ngsi-openssh-debuginfo-7.3p1c-1.1.osg34.el7\ngsi-openssh-server-7.3p1c-1.1.osg34.el7\nosg-build-1.10.2-1.osg34.el7\nosg-build-base-1.10.2-1.osg34.el7\nosg-build-koji-1.10.2-1.osg34.el7\nosg-build-mock-1.10.2-1.osg34.el7\nosg-build-tests-1.10.2-1.osg34.el7\nosg-ca-scripts-1.1.7-2.osg34.el7\nosg-configure-2.2.1-1.osg34.el7\nosg-configure-bosco-2.2.1-1.osg34.el7\nosg-configure-ce-2.2.1-1.osg34.el7\nosg-configure-condor-2.2.1-1.osg34.el7\nosg-configure-gateway-2.2.1-1.osg34.el7\nosg-configure-gip-2.2.1-1.osg34.el7\nosg-configure-gratia-2.2.1-1.osg34.el7\nosg-configure-infoservices-2.2.1-1.osg34.el7\nosg-configure-lsf-2.2.1-1.osg34.el7\nosg-configure-managedfork-2.2.1-1.osg34.el7\nosg-configure-misc-2.2.1-1.osg34.el7\nosg-configure-network-2.2.1-1.osg34.el7\nosg-configure-pbs-2.2.1-1.osg34.el7\nosg-configure-rsv-2.2.1-1.osg34.el7\nosg-configure-sge-2.2.1-1.osg34.el7\nosg-configure-slurm-2.2.1-1.osg34.el7\nosg-configure-squid-2.2.1-1.osg34.el7\nosg-configure-tests-2.2.1-1.osg34.el7\nosg-release-3.4-2.osg34.el7\nosg-release-itb-3.4-2.osg34.el7\nosg-tested-internal-3.4-5.osg34.el7\nosg-version-3.4.4-1.osg34.el7\nsingularity-2.3.2-0.1.1.osg34.el7\nsingularity-debuginfo-2.3.2-0.1.1.osg34.el7\nsingularity-devel-2.3.2-0.1.1.osg34.el7\nsingularity-runtime-2.3.2-0.1.1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#enterprise-linux-6_2", 
            "text": "condor-8.7.3-1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#enterprise-linux-7_2", 
            "text": "condor-8.7.3-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#enterprise-linux-6_3", 
            "text": "condor-8.7.3-1.osgup.el6\ncondor-all-8.7.3-1.osgup.el6\ncondor-annex-ec2-8.7.3-1.osgup.el6\ncondor-bosco-8.7.3-1.osgup.el6\ncondor-classads-8.7.3-1.osgup.el6\ncondor-classads-devel-8.7.3-1.osgup.el6\ncondor-cream-gahp-8.7.3-1.osgup.el6\ncondor-debuginfo-8.7.3-1.osgup.el6\ncondor-kbdd-8.7.3-1.osgup.el6\ncondor-procd-8.7.3-1.osgup.el6\ncondor-python-8.7.3-1.osgup.el6\ncondor-std-universe-8.7.3-1.osgup.el6\ncondor-test-8.7.3-1.osgup.el6\ncondor-vm-gahp-8.7.3-1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#enterprise-linux-7_3", 
            "text": "condor-8.7.3-1.osgup.el7\ncondor-all-8.7.3-1.osgup.el7\ncondor-annex-ec2-8.7.3-1.osgup.el7\ncondor-bosco-8.7.3-1.osgup.el7\ncondor-classads-8.7.3-1.osgup.el7\ncondor-classads-devel-8.7.3-1.osgup.el7\ncondor-cream-gahp-8.7.3-1.osgup.el7\ncondor-debuginfo-8.7.3-1.osgup.el7\ncondor-kbdd-8.7.3-1.osgup.el7\ncondor-procd-8.7.3-1.osgup.el7\ncondor-python-8.7.3-1.osgup.el7\ncondor-test-8.7.3-1.osgup.el7\ncondor-vm-gahp-8.7.3-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-29/", 
            "text": "OSG Software Release 3.3.29\n\n\nRelease Date\n: 2017-10-10\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nvoms-admin-server: Updated to fix Apache struts vulnerability\n\n\nUpdated gsi-openssh-server to interoperate with clients using OpenSSL 1.1\n\n\nglobus-gridftp-server-control 5.2: Allow 400 responses to stat failures\n\n\nosg-ca-scripts now properly declares its dependency on the wget package\n\n\nUpdated osg-configure to work properly when fetch-crl is missing\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n\n\nNote\n\n\nStashCache is supported on EL7 only.\n\n\n\n\n\n\nNote\n\n\nxrootd-lcmaps will remain at 1.2.1-1 on EL6.\n\n\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nKnown Issues\n\n\n\n\nBecause the Gratia system has been turned off, RSV emits the following warning: \nWARNING: Gratia server gratia-osg-prod.opensciencegrid.org:80 not responding to obtain last contact details.\n. This warning can safely be ignored.\n\n\nUsing the LCMAPS VOMS will result in a failing \"supported VO\" RSV test (\nSOFTWARE-2763\n). This can be ignored and a fix is targeted for the November release.\n\n\n\n\nUpdates to VOMS admin server require the updated emi-trustmanager-tomcat and re-running the configure script:\n\n\nroot@host #\n /var/lib/trustmanager-tomcat/configure.sh\n\n\n\n\n\n\n\n\n\nVOMS admin server shows an error when modifying/adding/signing AUPs, but all the actions still work.\n\n\n\n\n\n\nAfter updating OSG-CE to version 3.3-12, please disable and remove OSG Info Services via the following procedure:\n\n\nroot@host #\n service osg-info-services stop\n\nroot@host #\n yum erase gip osg-info-services\n\n\n\n\n\n\n\n\n\nThe Koji client config has changed in the new version of Koji: \npkgurl=http://koji.chtc.wisc.edu/packages\n has been replaced by \ntopurl=http://koji.chtc.wisc.edu\n and the Koji client will give a harmless but annoying warning when it finds \npkgurl\n. To get rid of the warning, update to osg-build \n \n1.8.0\n, rerun \nosg-koji setup\n, and say 'yes' when asked to replace the Koji configuration file; or, you may make the above change manually.\n\n\n\n\n\n\nA previous version (\n1.17.0-2.5\n) of the Gratia probes required changes in the HTCondor configuration to operate properly. Now, these changes need to be reverted to use verison \n1.17.0-2.6\n and later of the probes. The lines below are likely to be found in \ncondor_config.local\n or a new file in \n/etc/condor/config.d\n directory. Delete these lines and run \ncondor_reconfig\n.\n\n\n# GlideinWMS Gratia commands\n\n\nPER_JOB_HISTORY_DIR\n \n=\n /var/lib/gratia/data\n\nJOBGLIDEIN_ResourceName\n=\n$$\n([IfThenElse(IsUndefined(TARGET.GLIDEIN_ResourceName), IfThenElse(IsUndefined(TARGET.GLIDEIN_Site), IfThenElse(IsUndefined(TARGET.FileSystemDomain), \\\nLocal Job\\\n, TARGET.FileSystemDomain), TARGET.GLIDEIN_Site), TARGET.GLIDEIN_ResourceName)])\n\n\nSUBMIT_EXPRS\n \n=\n \n$(\nSUBMIT_EXPRS\n)\n JOBGLIDEIN_ResourceName\n\n\n\n\n\n\n\n\n\nOn EL7, your version of voms-proxy-init may come from the voms-clients-java package; this version will continue to make legacy proxies by default. If you want the new behavior, install the voms-clients-cpp package and uninstall the voms-clients-java package.\n\n\n\n\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNote\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n\n\n\n\nNote\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nglobus-gridftp-server-control-5.2-1.1.osg33.el6\n\n\ngsi-openssh-7.3p1c-1.1.osg33.el6\n\n\nosg-build-1.10.2-1.osg33.el6\n\n\nosg-ca-scripts-1.1.7-2.osg33.el6\n\n\nosg-configure-1.10.1-1.osg33.el6\n\n\nosg-release-3.3-6.osg33.el6\n\n\nosg-release-itb-3.3-6.osg33.el6\n\n\nosg-version-3.3.29-1.osg33.el6\n\n\nvoms-admin-server-2.7.0-1.23.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nglobus-gridftp-server-control-5.2-1.1.osg33.el6\n\n\ngsi-openssh-7.3p1c-1.1.osg33.el6\n\n\nosg-build-1.10.2-1.osg33.el6\n\n\nosg-ca-scripts-1.1.7-2.osg33.el6\n\n\nosg-configure-1.10.1-1.osg33.el6\n\n\nosg-release-3.3-6.osg33.el6\n\n\nosg-release-itb-3.3-6.osg33.el6\n\n\nosg-version-3.3.29-1.osg33.el6\n\n\nvoms-admin-server-2.7.0-1.23.osg33.el6\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nglobus-gridftp-server-control globus-gridftp-server-control-debuginfo globus-gridftp-server-control-devel gsi-openssh gsi-openssh-clients gsi-openssh-debuginfo gsi-openssh-server osg-build osg-build-base osg-build-koji osg-build-mock osg-build-tests osg-ca-scripts osg-configure osg-configure-bosco osg-configure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-release osg-release-itb osg-version voms-admin-server\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nbus-gridftp-server-control-5.2-1.1.osg33.el6\nglobus-gridftp-server-control-debuginfo-5.2-1.1.osg33.el6\nglobus-gridftp-server-control-devel-5.2-1.1.osg33.el6\ngsi-openssh-7.3p1c-1.1.osg33.el6\ngsi-openssh-clients-7.3p1c-1.1.osg33.el6\ngsi-openssh-debuginfo-7.3p1c-1.1.osg33.el6\ngsi-openssh-server-7.3p1c-1.1.osg33.el6\nosg-build-1.10.2-1.osg33.el6\nosg-build-base-1.10.2-1.osg33.el6\nosg-build-koji-1.10.2-1.osg33.el6\nosg-build-mock-1.10.2-1.osg33.el6\nosg-build-tests-1.10.2-1.osg33.el6\nosg-ca-scripts-1.1.7-2.osg33.el6\nosg-configure-1.10.1-1.osg33.el6\nosg-configure-bosco-1.10.1-1.osg33.el6\nosg-configure-ce-1.10.1-1.osg33.el6\nosg-configure-cemon-1.10.1-1.osg33.el6\nosg-configure-condor-1.10.1-1.osg33.el6\nosg-configure-gateway-1.10.1-1.osg33.el6\nosg-configure-gip-1.10.1-1.osg33.el6\nosg-configure-gratia-1.10.1-1.osg33.el6\nosg-configure-infoservices-1.10.1-1.osg33.el6\nosg-configure-lsf-1.10.1-1.osg33.el6\nosg-configure-managedfork-1.10.1-1.osg33.el6\nosg-configure-misc-1.10.1-1.osg33.el6\nosg-configure-monalisa-1.10.1-1.osg33.el6\nosg-configure-network-1.10.1-1.osg33.el6\nosg-configure-pbs-1.10.1-1.osg33.el6\nosg-configure-rsv-1.10.1-1.osg33.el6\nosg-configure-sge-1.10.1-1.osg33.el6\nosg-configure-slurm-1.10.1-1.osg33.el6\nosg-configure-squid-1.10.1-1.osg33.el6\nosg-configure-tests-1.10.1-1.osg33.el6\nosg-release-3.3-6.osg33.el6\nosg-release-itb-3.3-6.osg33.el6\nosg-version-3.3.29-1.osg33.el6\nvoms-admin-server-2.7.0-1.23.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nglobus-gridftp-server-control-5.2-1.1.osg33.el7\nglobus-gridftp-server-control-debuginfo-5.2-1.1.osg33.el7\nglobus-gridftp-server-control-devel-5.2-1.1.osg33.el7\ngsi-openssh-7.3p1c-1.1.osg33.el7\ngsi-openssh-clients-7.3p1c-1.1.osg33.el7\ngsi-openssh-debuginfo-7.3p1c-1.1.osg33.el7\ngsi-openssh-server-7.3p1c-1.1.osg33.el7\nosg-build-1.10.2-1.osg33.el7\nosg-build-base-1.10.2-1.osg33.el7\nosg-build-koji-1.10.2-1.osg33.el7\nosg-build-mock-1.10.2-1.osg33.el7\nosg-build-tests-1.10.2-1.osg33.el7\nosg-ca-scripts-1.1.7-2.osg33.el7\nosg-configure-1.10.1-1.osg33.el7\nosg-configure-bosco-1.10.1-1.osg33.el7\nosg-configure-ce-1.10.1-1.osg33.el7\nosg-configure-cemon-1.10.1-1.osg33.el7\nosg-configure-condor-1.10.1-1.osg33.el7\nosg-configure-gateway-1.10.1-1.osg33.el7\nosg-configure-gip-1.10.1-1.osg33.el7\nosg-configure-gratia-1.10.1-1.osg33.el7\nosg-configure-infoservices-1.10.1-1.osg33.el7\nosg-configure-lsf-1.10.1-1.osg33.el7\nosg-configure-managedfork-1.10.1-1.osg33.el7\nosg-configure-misc-1.10.1-1.osg33.el7\nosg-configure-monalisa-1.10.1-1.osg33.el7\nosg-configure-network-1.10.1-1.osg33.el7\nosg-configure-pbs-1.10.1-1.osg33.el7\nosg-configure-rsv-1.10.1-1.osg33.el7\nosg-configure-sge-1.10.1-1.osg33.el7\nosg-configure-slurm-1.10.1-1.osg33.el7\nosg-configure-squid-1.10.1-1.osg33.el7\nosg-configure-tests-1.10.1-1.osg33.el7\nosg-release-3.3-6.osg33.el7\nosg-release-itb-3.3-6.osg33.el7\nosg-version-3.3.29-1.osg33.el7", 
            "title": "OSG Release 3.3.29"
        }, 
        {
            "location": "/release/3.3/release-3-3-29/#osg-software-release-3329", 
            "text": "Release Date : 2017-10-10", 
            "title": "OSG Software Release 3.3.29"
        }, 
        {
            "location": "/release/3.3/release-3-3-29/#summary-of-changes", 
            "text": "This release contains:   voms-admin-server: Updated to fix Apache struts vulnerability  Updated gsi-openssh-server to interoperate with clients using OpenSSL 1.1  globus-gridftp-server-control 5.2: Allow 400 responses to stat failures  osg-ca-scripts now properly declares its dependency on the wget package  Updated osg-configure to work properly when fetch-crl is missing   These  JIRA tickets  were addressed in this release.   Note  StashCache is supported on EL7 only.    Note  xrootd-lcmaps will remain at 1.2.1-1 on EL6.   Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-29/#known-issues", 
            "text": "Because the Gratia system has been turned off, RSV emits the following warning:  WARNING: Gratia server gratia-osg-prod.opensciencegrid.org:80 not responding to obtain last contact details. . This warning can safely be ignored.  Using the LCMAPS VOMS will result in a failing \"supported VO\" RSV test ( SOFTWARE-2763 ). This can be ignored and a fix is targeted for the November release.   Updates to VOMS admin server require the updated emi-trustmanager-tomcat and re-running the configure script:  root@host #  /var/lib/trustmanager-tomcat/configure.sh    VOMS admin server shows an error when modifying/adding/signing AUPs, but all the actions still work.    After updating OSG-CE to version 3.3-12, please disable and remove OSG Info Services via the following procedure:  root@host #  service osg-info-services stop root@host #  yum erase gip osg-info-services    The Koji client config has changed in the new version of Koji:  pkgurl=http://koji.chtc.wisc.edu/packages  has been replaced by  topurl=http://koji.chtc.wisc.edu  and the Koji client will give a harmless but annoying warning when it finds  pkgurl . To get rid of the warning, update to osg-build    1.8.0 , rerun  osg-koji setup , and say 'yes' when asked to replace the Koji configuration file; or, you may make the above change manually.    A previous version ( 1.17.0-2.5 ) of the Gratia probes required changes in the HTCondor configuration to operate properly. Now, these changes need to be reverted to use verison  1.17.0-2.6  and later of the probes. The lines below are likely to be found in  condor_config.local  or a new file in  /etc/condor/config.d  directory. Delete these lines and run  condor_reconfig .  # GlideinWMS Gratia commands  PER_JOB_HISTORY_DIR   =  /var/lib/gratia/data JOBGLIDEIN_ResourceName = $$ ([IfThenElse(IsUndefined(TARGET.GLIDEIN_ResourceName), IfThenElse(IsUndefined(TARGET.GLIDEIN_Site), IfThenElse(IsUndefined(TARGET.FileSystemDomain), \\ Local Job\\ , TARGET.FileSystemDomain), TARGET.GLIDEIN_Site), TARGET.GLIDEIN_ResourceName)])  SUBMIT_EXPRS   =   $( SUBMIT_EXPRS )  JOBGLIDEIN_ResourceName    On EL7, your version of voms-proxy-init may come from the voms-clients-java package; this version will continue to make legacy proxies by default. If you want the new behavior, install the voms-clients-cpp package and uninstall the voms-clients-java package.", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.3/release-3-3-29/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-29/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-29/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Note  Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.    Note  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-29/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-29/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-29/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-29/#enterprise-linux-6", 
            "text": "globus-gridftp-server-control-5.2-1.1.osg33.el6  gsi-openssh-7.3p1c-1.1.osg33.el6  osg-build-1.10.2-1.osg33.el6  osg-ca-scripts-1.1.7-2.osg33.el6  osg-configure-1.10.1-1.osg33.el6  osg-release-3.3-6.osg33.el6  osg-release-itb-3.3-6.osg33.el6  osg-version-3.3.29-1.osg33.el6  voms-admin-server-2.7.0-1.23.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-29/#enterprise-linux-7", 
            "text": "globus-gridftp-server-control-5.2-1.1.osg33.el6  gsi-openssh-7.3p1c-1.1.osg33.el6  osg-build-1.10.2-1.osg33.el6  osg-ca-scripts-1.1.7-2.osg33.el6  osg-configure-1.10.1-1.osg33.el6  osg-release-3.3-6.osg33.el6  osg-release-itb-3.3-6.osg33.el6  osg-version-3.3.29-1.osg33.el6  voms-admin-server-2.7.0-1.23.osg33.el6", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-29/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  globus-gridftp-server-control globus-gridftp-server-control-debuginfo globus-gridftp-server-control-devel gsi-openssh gsi-openssh-clients gsi-openssh-debuginfo gsi-openssh-server osg-build osg-build-base osg-build-koji osg-build-mock osg-build-tests osg-ca-scripts osg-configure osg-configure-bosco osg-configure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-release osg-release-itb osg-version voms-admin-server  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-29/#enterprise-linux-6_1", 
            "text": "bus-gridftp-server-control-5.2-1.1.osg33.el6\nglobus-gridftp-server-control-debuginfo-5.2-1.1.osg33.el6\nglobus-gridftp-server-control-devel-5.2-1.1.osg33.el6\ngsi-openssh-7.3p1c-1.1.osg33.el6\ngsi-openssh-clients-7.3p1c-1.1.osg33.el6\ngsi-openssh-debuginfo-7.3p1c-1.1.osg33.el6\ngsi-openssh-server-7.3p1c-1.1.osg33.el6\nosg-build-1.10.2-1.osg33.el6\nosg-build-base-1.10.2-1.osg33.el6\nosg-build-koji-1.10.2-1.osg33.el6\nosg-build-mock-1.10.2-1.osg33.el6\nosg-build-tests-1.10.2-1.osg33.el6\nosg-ca-scripts-1.1.7-2.osg33.el6\nosg-configure-1.10.1-1.osg33.el6\nosg-configure-bosco-1.10.1-1.osg33.el6\nosg-configure-ce-1.10.1-1.osg33.el6\nosg-configure-cemon-1.10.1-1.osg33.el6\nosg-configure-condor-1.10.1-1.osg33.el6\nosg-configure-gateway-1.10.1-1.osg33.el6\nosg-configure-gip-1.10.1-1.osg33.el6\nosg-configure-gratia-1.10.1-1.osg33.el6\nosg-configure-infoservices-1.10.1-1.osg33.el6\nosg-configure-lsf-1.10.1-1.osg33.el6\nosg-configure-managedfork-1.10.1-1.osg33.el6\nosg-configure-misc-1.10.1-1.osg33.el6\nosg-configure-monalisa-1.10.1-1.osg33.el6\nosg-configure-network-1.10.1-1.osg33.el6\nosg-configure-pbs-1.10.1-1.osg33.el6\nosg-configure-rsv-1.10.1-1.osg33.el6\nosg-configure-sge-1.10.1-1.osg33.el6\nosg-configure-slurm-1.10.1-1.osg33.el6\nosg-configure-squid-1.10.1-1.osg33.el6\nosg-configure-tests-1.10.1-1.osg33.el6\nosg-release-3.3-6.osg33.el6\nosg-release-itb-3.3-6.osg33.el6\nosg-version-3.3.29-1.osg33.el6\nvoms-admin-server-2.7.0-1.23.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-29/#enterprise-linux-7_1", 
            "text": "globus-gridftp-server-control-5.2-1.1.osg33.el7\nglobus-gridftp-server-control-debuginfo-5.2-1.1.osg33.el7\nglobus-gridftp-server-control-devel-5.2-1.1.osg33.el7\ngsi-openssh-7.3p1c-1.1.osg33.el7\ngsi-openssh-clients-7.3p1c-1.1.osg33.el7\ngsi-openssh-debuginfo-7.3p1c-1.1.osg33.el7\ngsi-openssh-server-7.3p1c-1.1.osg33.el7\nosg-build-1.10.2-1.osg33.el7\nosg-build-base-1.10.2-1.osg33.el7\nosg-build-koji-1.10.2-1.osg33.el7\nosg-build-mock-1.10.2-1.osg33.el7\nosg-build-tests-1.10.2-1.osg33.el7\nosg-ca-scripts-1.1.7-2.osg33.el7\nosg-configure-1.10.1-1.osg33.el7\nosg-configure-bosco-1.10.1-1.osg33.el7\nosg-configure-ce-1.10.1-1.osg33.el7\nosg-configure-cemon-1.10.1-1.osg33.el7\nosg-configure-condor-1.10.1-1.osg33.el7\nosg-configure-gateway-1.10.1-1.osg33.el7\nosg-configure-gip-1.10.1-1.osg33.el7\nosg-configure-gratia-1.10.1-1.osg33.el7\nosg-configure-infoservices-1.10.1-1.osg33.el7\nosg-configure-lsf-1.10.1-1.osg33.el7\nosg-configure-managedfork-1.10.1-1.osg33.el7\nosg-configure-misc-1.10.1-1.osg33.el7\nosg-configure-monalisa-1.10.1-1.osg33.el7\nosg-configure-network-1.10.1-1.osg33.el7\nosg-configure-pbs-1.10.1-1.osg33.el7\nosg-configure-rsv-1.10.1-1.osg33.el7\nosg-configure-sge-1.10.1-1.osg33.el7\nosg-configure-slurm-1.10.1-1.osg33.el7\nosg-configure-squid-1.10.1-1.osg33.el7\nosg-configure-tests-1.10.1-1.osg33.el7\nosg-release-3.3-6.osg33.el7\nosg-release-itb-3.3-6.osg33.el7\nosg-version-3.3.29-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/", 
            "text": "OSG Software Release 3.4.3\n\n\nRelease Date\n: 2017-09-12\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nUpdated to \nCVMFS 2.4.1\n\n\nUpdated to \nSingularity 2.3.1\n\n\nUpdated to BLAHP 1.18.33\n\n\nProperly parses times from Slurm's sacct command\n\n\npbs_pro does not need to be defined to query PBS for job status\n\n\n\n\n\n\nUpdated to \nXRootD 4.7.0\n\n\nUpdated to \nStashCache 0.8\n\n\nUpdated Globus packages to latest EPEL versions\n\n\nosg-ca-scripts now use HTTPS to download CA certificates\n\n\nAdded the ability to limit transfer load in the globus-gridftp-osg-extensions\n\n\nFixed a few memory management bugs in \nxrootd-lcmaps\n\n\nHTCondor CE 3.0.2\n reports an error if \nJOB_ROUTER_ENTRIES\n are not defined\n\n\nosg-configure 2.2.0 - remove last vestiges of GRAM\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n\n\nNote\n\n\nOSG 3.4 contains only 64-bit components.\n\n\n\n\n\n\nNote\n\n\nStashCache is supported on EL7 only.\n\n\n\n\n\n\nNote\n\n\nxrootd-lcmaps will remain at 1.2.1-2 on EL6.\n\n\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nKnown Issues\n\n\n\n\nBecause the Gratia system has been turned off, RSV emits the following warning: \nWARNING: Gratia server gratia-osg-prod.opensciencegrid.org:80 not responding to obtain last contact details\n. This warning can safely be ignored.\n\n\nUsing the LCMAPS VOMS will result in a failing \"supported VO\" RSV test (\nSOFTWARE-2763\n). This can be ignored and a fix is targeted for the October release.\n\n\nIn GlideinWMS, a small configuration change must be added to account for changes in HTCondor 8.6. Add the following line to the HTCondor configuration.\nCOLLECTOR.USE_SHARED_PORT=False\n\n\n\n\n\n\n\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.33.bosco-1.osg34.el6\n\n\ncvmfs-2.4.1-1.osg34.el6\n\n\nglobus-ftp-client-8.36-1.1.osg34.el6\n\n\nglobus-gridftp-osg-extensions-0.4-1.osg34.el6\n\n\nglobus-gridftp-server-12.2-1.1.osg34.el6\n\n\nglobus-gridftp-server-control-5.1-1.1.osg34.el6\n\n\nhtcondor-ce-3.0.2-1.osg34.el6\n\n\nmyproxy-6.1.28-1.1.osg34.el6\n\n\nosg-ca-scripts-1.1.7-1.osg34.el6\n\n\nosg-configure-2.2.0-1.osg34.el6\n\n\nosg-oasis-8-1.osg34.el6\n\n\nosg-test-1.11.2-1.osg34.el6\n\n\nosg-version-3.4.3-1.osg34.el6\n\n\nsingularity-2.3.1-0.1.4.osg34.el6\n\n\nxrootd-4.7.0-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.33.bosco-1.osg34.el7\n\n\ncvmfs-2.4.1-1.osg34.el7\n\n\nglobus-ftp-client-8.36-1.1.osg34.el7\n\n\nglobus-gridftp-osg-extensions-0.4-1.osg34.el7\n\n\nglobus-gridftp-server-12.2-1.1.osg34.el7\n\n\nglobus-gridftp-server-control-5.1-1.1.osg34.el7\n\n\nhtcondor-ce-3.0.2-1.osg34.el7\n\n\nmyproxy-6.1.28-1.1.osg34.el7\n\n\nosg-ca-scripts-1.1.7-1.osg34.el7\n\n\nosg-configure-2.2.0-1.osg34.el7\n\n\nosg-oasis-8-1.osg34.el7\n\n\nosg-test-1.11.2-1.osg34.el7\n\n\nosg-version-3.4.3-1.osg34.el7\n\n\nsingularity-2.3.1-0.1.4.osg34.el7\n\n\nstashcache-0.8-1.osg34.el7\n\n\nxrootd-4.7.0-1.osg34.el7\n\n\nxrootd-lcmaps-1.3.4-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp blahp-debuginfo cvmfs cvmfs-devel cvmfs-server cvmfs-unittests globus-ftp-client globus-ftp-client-debuginfo globus-ftp-client-devel globus-ftp-client-doc globus-gridftp-osg-extensions globus-gridftp-osg-extensions-debuginfo globus-gridftp-server globus-gridftp-server-control globus-gridftp-server-control-debuginfo globus-gridftp-server-control-devel globus-gridftp-server-debuginfo globus-gridftp-server-devel globus-gridftp-server-progs htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-slurm htcondor-ce-view igtf-ca-certs myproxy myproxy-admin myproxy-debuginfo myproxy-devel myproxy-doc myproxy-libs myproxy-server myproxy-voms osg-ca-certs osg-ca-scripts osg-configure osg-configure-bosco osg-configure-ce osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-oasis osg-test osg-test-log-viewer osg-version singularity singularity-debuginfo singularity-devel singularity-runtime xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-libs xrootd-private-devel xrootd-python xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs\n\n\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp-1.18.33.bosco-1.osg34.el6\nblahp-debuginfo-1.18.33.bosco-1.osg34.el6\ncvmfs-2.4.1-1.osg34.el6\ncvmfs-devel-2.4.1-1.osg34.el6\ncvmfs-server-2.4.1-1.osg34.el6\ncvmfs-unittests-2.4.1-1.osg34.el6\nglobus-ftp-client-8.36-1.1.osg34.el6\nglobus-ftp-client-debuginfo-8.36-1.1.osg34.el6\nglobus-ftp-client-devel-8.36-1.1.osg34.el6\nglobus-ftp-client-doc-8.36-1.1.osg34.el6\nglobus-gridftp-osg-extensions-0.4-1.osg34.el6\nglobus-gridftp-osg-extensions-debuginfo-0.4-1.osg34.el6\nglobus-gridftp-server-12.2-1.1.osg34.el6\nglobus-gridftp-server-control-5.1-1.1.osg34.el6\nglobus-gridftp-server-control-debuginfo-5.1-1.1.osg34.el6\nglobus-gridftp-server-control-devel-5.1-1.1.osg34.el6\nglobus-gridftp-server-debuginfo-12.2-1.1.osg34.el6\nglobus-gridftp-server-devel-12.2-1.1.osg34.el6\nglobus-gridftp-server-progs-12.2-1.1.osg34.el6\nhtcondor-ce-3.0.2-1.osg34.el6\nhtcondor-ce-bosco-3.0.2-1.osg34.el6\nhtcondor-ce-client-3.0.2-1.osg34.el6\nhtcondor-ce-collector-3.0.2-1.osg34.el6\nhtcondor-ce-condor-3.0.2-1.osg34.el6\nhtcondor-ce-lsf-3.0.2-1.osg34.el6\nhtcondor-ce-pbs-3.0.2-1.osg34.el6\nhtcondor-ce-sge-3.0.2-1.osg34.el6\nhtcondor-ce-slurm-3.0.2-1.osg34.el6\nhtcondor-ce-view-3.0.2-1.osg34.el6\nmyproxy-6.1.28-1.1.osg34.el6\nmyproxy-admin-6.1.28-1.1.osg34.el6\nmyproxy-debuginfo-6.1.28-1.1.osg34.el6\nmyproxy-devel-6.1.28-1.1.osg34.el6\nmyproxy-doc-6.1.28-1.1.osg34.el6\nmyproxy-libs-6.1.28-1.1.osg34.el6\nmyproxy-server-6.1.28-1.1.osg34.el6\nmyproxy-voms-6.1.28-1.1.osg34.el6\nosg-ca-scripts-1.1.7-1.osg34.el6\nosg-configure-2.2.0-1.osg34.el6\nosg-configure-bosco-2.2.0-1.osg34.el6\nosg-configure-ce-2.2.0-1.osg34.el6\nosg-configure-condor-2.2.0-1.osg34.el6\nosg-configure-gateway-2.2.0-1.osg34.el6\nosg-configure-gip-2.2.0-1.osg34.el6\nosg-configure-gratia-2.2.0-1.osg34.el6\nosg-configure-infoservices-2.2.0-1.osg34.el6\nosg-configure-lsf-2.2.0-1.osg34.el6\nosg-configure-managedfork-2.2.0-1.osg34.el6\nosg-configure-misc-2.2.0-1.osg34.el6\nosg-configure-network-2.2.0-1.osg34.el6\nosg-configure-pbs-2.2.0-1.osg34.el6\nosg-configure-rsv-2.2.0-1.osg34.el6\nosg-configure-sge-2.2.0-1.osg34.el6\nosg-configure-slurm-2.2.0-1.osg34.el6\nosg-configure-squid-2.2.0-1.osg34.el6\nosg-configure-tests-2.2.0-1.osg34.el6\nosg-oasis-8-1.osg34.el6\nosg-test-1.11.2-1.osg34.el6\nosg-test-log-viewer-1.11.2-1.osg34.el6\nosg-version-3.4.3-1.osg34.el6\nsingularity-2.3.1-0.1.4.osg34.el6\nsingularity-debuginfo-2.3.1-0.1.4.osg34.el6\nsingularity-devel-2.3.1-0.1.4.osg34.el6\nsingularity-runtime-2.3.1-0.1.4.osg34.el6\nxrootd-4.7.0-1.osg34.el6\nxrootd-client-4.7.0-1.osg34.el6\nxrootd-client-devel-4.7.0-1.osg34.el6\nxrootd-client-libs-4.7.0-1.osg34.el6\nxrootd-debuginfo-4.7.0-1.osg34.el6\nxrootd-devel-4.7.0-1.osg34.el6\nxrootd-doc-4.7.0-1.osg34.el6\nxrootd-fuse-4.7.0-1.osg34.el6\nxrootd-libs-4.7.0-1.osg34.el6\nxrootd-private-devel-4.7.0-1.osg34.el6\nxrootd-python-4.7.0-1.osg34.el6\nxrootd-selinux-4.7.0-1.osg34.el6\nxrootd-server-4.7.0-1.osg34.el6\nxrootd-server-devel-4.7.0-1.osg34.el6\nxrootd-server-libs-4.7.0-1.osg34.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp-1.18.33.bosco-1.osg34.el7\nblahp-debuginfo-1.18.33.bosco-1.osg34.el7\ncvmfs-2.4.1-1.osg34.el7\ncvmfs-devel-2.4.1-1.osg34.el7\ncvmfs-server-2.4.1-1.osg34.el7\ncvmfs-unittests-2.4.1-1.osg34.el7\nglobus-ftp-client-8.36-1.1.osg34.el7\nglobus-ftp-client-debuginfo-8.36-1.1.osg34.el7\nglobus-ftp-client-devel-8.36-1.1.osg34.el7\nglobus-ftp-client-doc-8.36-1.1.osg34.el7\nglobus-gridftp-osg-extensions-0.4-1.osg34.el7\nglobus-gridftp-osg-extensions-debuginfo-0.4-1.osg34.el7\nglobus-gridftp-server-12.2-1.1.osg34.el7\nglobus-gridftp-server-control-5.1-1.1.osg34.el7\nglobus-gridftp-server-control-debuginfo-5.1-1.1.osg34.el7\nglobus-gridftp-server-control-devel-5.1-1.1.osg34.el7\nglobus-gridftp-server-debuginfo-12.2-1.1.osg34.el7\nglobus-gridftp-server-devel-12.2-1.1.osg34.el7\nglobus-gridftp-server-progs-12.2-1.1.osg34.el7\nhtcondor-ce-3.0.2-1.osg34.el7\nhtcondor-ce-bosco-3.0.2-1.osg34.el7\nhtcondor-ce-client-3.0.2-1.osg34.el7\nhtcondor-ce-collector-3.0.2-1.osg34.el7\nhtcondor-ce-condor-3.0.2-1.osg34.el7\nhtcondor-ce-lsf-3.0.2-1.osg34.el7\nhtcondor-ce-pbs-3.0.2-1.osg34.el7\nhtcondor-ce-sge-3.0.2-1.osg34.el7\nhtcondor-ce-slurm-3.0.2-1.osg34.el7\nhtcondor-ce-view-3.0.2-1.osg34.el7\nmyproxy-6.1.28-1.1.osg34.el7\nmyproxy-admin-6.1.28-1.1.osg34.el7\nmyproxy-debuginfo-6.1.28-1.1.osg34.el7\nmyproxy-devel-6.1.28-1.1.osg34.el7\nmyproxy-doc-6.1.28-1.1.osg34.el7\nmyproxy-libs-6.1.28-1.1.osg34.el7\nmyproxy-server-6.1.28-1.1.osg34.el7\nmyproxy-voms-6.1.28-1.1.osg34.el7\nosg-ca-scripts-1.1.7-1.osg34.el7\nosg-configure-2.2.0-1.osg34.el7\nosg-configure-bosco-2.2.0-1.osg34.el7\nosg-configure-ce-2.2.0-1.osg34.el7\nosg-configure-condor-2.2.0-1.osg34.el7\nosg-configure-gateway-2.2.0-1.osg34.el7\nosg-configure-gip-2.2.0-1.osg34.el7\nosg-configure-gratia-2.2.0-1.osg34.el7\nosg-configure-infoservices-2.2.0-1.osg34.el7\nosg-configure-lsf-2.2.0-1.osg34.el7\nosg-configure-managedfork-2.2.0-1.osg34.el7\nosg-configure-misc-2.2.0-1.osg34.el7\nosg-configure-network-2.2.0-1.osg34.el7\nosg-configure-pbs-2.2.0-1.osg34.el7\nosg-configure-rsv-2.2.0-1.osg34.el7\nosg-configure-sge-2.2.0-1.osg34.el7\nosg-configure-slurm-2.2.0-1.osg34.el7\nosg-configure-squid-2.2.0-1.osg34.el7\nosg-configure-tests-2.2.0-1.osg34.el7\nosg-oasis-8-1.osg34.el7\nosg-test-1.11.2-1.osg34.el7\nosg-test-log-viewer-1.11.2-1.osg34.el7\nosg-version-3.4.3-1.osg34.el7\nsingularity-2.3.1-0.1.4.osg34.el7\nsingularity-debuginfo-2.3.1-0.1.4.osg34.el7\nsingularity-devel-2.3.1-0.1.4.osg34.el7\nsingularity-runtime-2.3.1-0.1.4.osg34.el7\nstashcache-0.8-1.osg34.el7\nstashcache-cache-server-0.8-1.osg34.el7\nstashcache-daemon-0.8-1.osg34.el7\nstashcache-origin-server-0.8-1.osg34.el7\nxrootd-4.7.0-1.osg34.el7\nxrootd-client-4.7.0-1.osg34.el7\nxrootd-client-devel-4.7.0-1.osg34.el7\nxrootd-client-libs-4.7.0-1.osg34.el7\nxrootd-debuginfo-4.7.0-1.osg34.el7\nxrootd-devel-4.7.0-1.osg34.el7\nxrootd-doc-4.7.0-1.osg34.el7\nxrootd-fuse-4.7.0-1.osg34.el7\nxrootd-lcmaps-1.3.4-1.osg34.el7\nxrootd-lcmaps-debuginfo-1.3.4-1.osg34.el7\nxrootd-libs-4.7.0-1.osg34.el7\nxrootd-private-devel-4.7.0-1.osg34.el7\nxrootd-python-4.7.0-1.osg34.el7\nxrootd-selinux-4.7.0-1.osg34.el7\nxrootd-server-4.7.0-1.osg34.el7\nxrootd-server-devel-4.7.0-1.osg34.el7\nxrootd-server-libs-4.7.0-1.osg34.el7\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.33.bosco-1.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.33.bosco-1.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp blahp-debuginfo\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp-1.18.33.bosco-1.osgup.el6\nblahp-debuginfo-1.18.33.bosco-1.osgup.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp-1.18.33.bosco-1.osgup.el7\nblahp-debuginfo-1.18.33.bosco-1.osgup.el7", 
            "title": "OSG Release 3.4.3"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#osg-software-release-343", 
            "text": "Release Date : 2017-09-12", 
            "title": "OSG Software Release 3.4.3"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#summary-of-changes", 
            "text": "This release contains:   Updated to  CVMFS 2.4.1  Updated to  Singularity 2.3.1  Updated to BLAHP 1.18.33  Properly parses times from Slurm's sacct command  pbs_pro does not need to be defined to query PBS for job status    Updated to  XRootD 4.7.0  Updated to  StashCache 0.8  Updated Globus packages to latest EPEL versions  osg-ca-scripts now use HTTPS to download CA certificates  Added the ability to limit transfer load in the globus-gridftp-osg-extensions  Fixed a few memory management bugs in  xrootd-lcmaps  HTCondor CE 3.0.2  reports an error if  JOB_ROUTER_ENTRIES  are not defined  osg-configure 2.2.0 - remove last vestiges of GRAM   These  JIRA tickets  were addressed in this release.   Note  OSG 3.4 contains only 64-bit components.    Note  StashCache is supported on EL7 only.    Note  xrootd-lcmaps will remain at 1.2.1-2 on EL6.   Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#known-issues", 
            "text": "Because the Gratia system has been turned off, RSV emits the following warning:  WARNING: Gratia server gratia-osg-prod.opensciencegrid.org:80 not responding to obtain last contact details . This warning can safely be ignored.  Using the LCMAPS VOMS will result in a failing \"supported VO\" RSV test ( SOFTWARE-2763 ). This can be ignored and a fix is targeted for the October release.  In GlideinWMS, a small configuration change must be added to account for changes in HTCondor 8.6. Add the following line to the HTCondor configuration. COLLECTOR.USE_SHARED_PORT=False", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#enterprise-linux-6", 
            "text": "blahp-1.18.33.bosco-1.osg34.el6  cvmfs-2.4.1-1.osg34.el6  globus-ftp-client-8.36-1.1.osg34.el6  globus-gridftp-osg-extensions-0.4-1.osg34.el6  globus-gridftp-server-12.2-1.1.osg34.el6  globus-gridftp-server-control-5.1-1.1.osg34.el6  htcondor-ce-3.0.2-1.osg34.el6  myproxy-6.1.28-1.1.osg34.el6  osg-ca-scripts-1.1.7-1.osg34.el6  osg-configure-2.2.0-1.osg34.el6  osg-oasis-8-1.osg34.el6  osg-test-1.11.2-1.osg34.el6  osg-version-3.4.3-1.osg34.el6  singularity-2.3.1-0.1.4.osg34.el6  xrootd-4.7.0-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#enterprise-linux-7", 
            "text": "blahp-1.18.33.bosco-1.osg34.el7  cvmfs-2.4.1-1.osg34.el7  globus-ftp-client-8.36-1.1.osg34.el7  globus-gridftp-osg-extensions-0.4-1.osg34.el7  globus-gridftp-server-12.2-1.1.osg34.el7  globus-gridftp-server-control-5.1-1.1.osg34.el7  htcondor-ce-3.0.2-1.osg34.el7  myproxy-6.1.28-1.1.osg34.el7  osg-ca-scripts-1.1.7-1.osg34.el7  osg-configure-2.2.0-1.osg34.el7  osg-oasis-8-1.osg34.el7  osg-test-1.11.2-1.osg34.el7  osg-version-3.4.3-1.osg34.el7  singularity-2.3.1-0.1.4.osg34.el7  stashcache-0.8-1.osg34.el7  xrootd-4.7.0-1.osg34.el7  xrootd-lcmaps-1.3.4-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp blahp-debuginfo cvmfs cvmfs-devel cvmfs-server cvmfs-unittests globus-ftp-client globus-ftp-client-debuginfo globus-ftp-client-devel globus-ftp-client-doc globus-gridftp-osg-extensions globus-gridftp-osg-extensions-debuginfo globus-gridftp-server globus-gridftp-server-control globus-gridftp-server-control-debuginfo globus-gridftp-server-control-devel globus-gridftp-server-debuginfo globus-gridftp-server-devel globus-gridftp-server-progs htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-slurm htcondor-ce-view igtf-ca-certs myproxy myproxy-admin myproxy-debuginfo myproxy-devel myproxy-doc myproxy-libs myproxy-server myproxy-voms osg-ca-certs osg-ca-scripts osg-configure osg-configure-bosco osg-configure-ce osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-oasis osg-test osg-test-log-viewer osg-version singularity singularity-debuginfo singularity-devel singularity-runtime xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-libs xrootd-private-devel xrootd-python xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#enterprise-linux-6_1", 
            "text": "blahp-1.18.33.bosco-1.osg34.el6\nblahp-debuginfo-1.18.33.bosco-1.osg34.el6\ncvmfs-2.4.1-1.osg34.el6\ncvmfs-devel-2.4.1-1.osg34.el6\ncvmfs-server-2.4.1-1.osg34.el6\ncvmfs-unittests-2.4.1-1.osg34.el6\nglobus-ftp-client-8.36-1.1.osg34.el6\nglobus-ftp-client-debuginfo-8.36-1.1.osg34.el6\nglobus-ftp-client-devel-8.36-1.1.osg34.el6\nglobus-ftp-client-doc-8.36-1.1.osg34.el6\nglobus-gridftp-osg-extensions-0.4-1.osg34.el6\nglobus-gridftp-osg-extensions-debuginfo-0.4-1.osg34.el6\nglobus-gridftp-server-12.2-1.1.osg34.el6\nglobus-gridftp-server-control-5.1-1.1.osg34.el6\nglobus-gridftp-server-control-debuginfo-5.1-1.1.osg34.el6\nglobus-gridftp-server-control-devel-5.1-1.1.osg34.el6\nglobus-gridftp-server-debuginfo-12.2-1.1.osg34.el6\nglobus-gridftp-server-devel-12.2-1.1.osg34.el6\nglobus-gridftp-server-progs-12.2-1.1.osg34.el6\nhtcondor-ce-3.0.2-1.osg34.el6\nhtcondor-ce-bosco-3.0.2-1.osg34.el6\nhtcondor-ce-client-3.0.2-1.osg34.el6\nhtcondor-ce-collector-3.0.2-1.osg34.el6\nhtcondor-ce-condor-3.0.2-1.osg34.el6\nhtcondor-ce-lsf-3.0.2-1.osg34.el6\nhtcondor-ce-pbs-3.0.2-1.osg34.el6\nhtcondor-ce-sge-3.0.2-1.osg34.el6\nhtcondor-ce-slurm-3.0.2-1.osg34.el6\nhtcondor-ce-view-3.0.2-1.osg34.el6\nmyproxy-6.1.28-1.1.osg34.el6\nmyproxy-admin-6.1.28-1.1.osg34.el6\nmyproxy-debuginfo-6.1.28-1.1.osg34.el6\nmyproxy-devel-6.1.28-1.1.osg34.el6\nmyproxy-doc-6.1.28-1.1.osg34.el6\nmyproxy-libs-6.1.28-1.1.osg34.el6\nmyproxy-server-6.1.28-1.1.osg34.el6\nmyproxy-voms-6.1.28-1.1.osg34.el6\nosg-ca-scripts-1.1.7-1.osg34.el6\nosg-configure-2.2.0-1.osg34.el6\nosg-configure-bosco-2.2.0-1.osg34.el6\nosg-configure-ce-2.2.0-1.osg34.el6\nosg-configure-condor-2.2.0-1.osg34.el6\nosg-configure-gateway-2.2.0-1.osg34.el6\nosg-configure-gip-2.2.0-1.osg34.el6\nosg-configure-gratia-2.2.0-1.osg34.el6\nosg-configure-infoservices-2.2.0-1.osg34.el6\nosg-configure-lsf-2.2.0-1.osg34.el6\nosg-configure-managedfork-2.2.0-1.osg34.el6\nosg-configure-misc-2.2.0-1.osg34.el6\nosg-configure-network-2.2.0-1.osg34.el6\nosg-configure-pbs-2.2.0-1.osg34.el6\nosg-configure-rsv-2.2.0-1.osg34.el6\nosg-configure-sge-2.2.0-1.osg34.el6\nosg-configure-slurm-2.2.0-1.osg34.el6\nosg-configure-squid-2.2.0-1.osg34.el6\nosg-configure-tests-2.2.0-1.osg34.el6\nosg-oasis-8-1.osg34.el6\nosg-test-1.11.2-1.osg34.el6\nosg-test-log-viewer-1.11.2-1.osg34.el6\nosg-version-3.4.3-1.osg34.el6\nsingularity-2.3.1-0.1.4.osg34.el6\nsingularity-debuginfo-2.3.1-0.1.4.osg34.el6\nsingularity-devel-2.3.1-0.1.4.osg34.el6\nsingularity-runtime-2.3.1-0.1.4.osg34.el6\nxrootd-4.7.0-1.osg34.el6\nxrootd-client-4.7.0-1.osg34.el6\nxrootd-client-devel-4.7.0-1.osg34.el6\nxrootd-client-libs-4.7.0-1.osg34.el6\nxrootd-debuginfo-4.7.0-1.osg34.el6\nxrootd-devel-4.7.0-1.osg34.el6\nxrootd-doc-4.7.0-1.osg34.el6\nxrootd-fuse-4.7.0-1.osg34.el6\nxrootd-libs-4.7.0-1.osg34.el6\nxrootd-private-devel-4.7.0-1.osg34.el6\nxrootd-python-4.7.0-1.osg34.el6\nxrootd-selinux-4.7.0-1.osg34.el6\nxrootd-server-4.7.0-1.osg34.el6\nxrootd-server-devel-4.7.0-1.osg34.el6\nxrootd-server-libs-4.7.0-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#enterprise-linux-7_1", 
            "text": "blahp-1.18.33.bosco-1.osg34.el7\nblahp-debuginfo-1.18.33.bosco-1.osg34.el7\ncvmfs-2.4.1-1.osg34.el7\ncvmfs-devel-2.4.1-1.osg34.el7\ncvmfs-server-2.4.1-1.osg34.el7\ncvmfs-unittests-2.4.1-1.osg34.el7\nglobus-ftp-client-8.36-1.1.osg34.el7\nglobus-ftp-client-debuginfo-8.36-1.1.osg34.el7\nglobus-ftp-client-devel-8.36-1.1.osg34.el7\nglobus-ftp-client-doc-8.36-1.1.osg34.el7\nglobus-gridftp-osg-extensions-0.4-1.osg34.el7\nglobus-gridftp-osg-extensions-debuginfo-0.4-1.osg34.el7\nglobus-gridftp-server-12.2-1.1.osg34.el7\nglobus-gridftp-server-control-5.1-1.1.osg34.el7\nglobus-gridftp-server-control-debuginfo-5.1-1.1.osg34.el7\nglobus-gridftp-server-control-devel-5.1-1.1.osg34.el7\nglobus-gridftp-server-debuginfo-12.2-1.1.osg34.el7\nglobus-gridftp-server-devel-12.2-1.1.osg34.el7\nglobus-gridftp-server-progs-12.2-1.1.osg34.el7\nhtcondor-ce-3.0.2-1.osg34.el7\nhtcondor-ce-bosco-3.0.2-1.osg34.el7\nhtcondor-ce-client-3.0.2-1.osg34.el7\nhtcondor-ce-collector-3.0.2-1.osg34.el7\nhtcondor-ce-condor-3.0.2-1.osg34.el7\nhtcondor-ce-lsf-3.0.2-1.osg34.el7\nhtcondor-ce-pbs-3.0.2-1.osg34.el7\nhtcondor-ce-sge-3.0.2-1.osg34.el7\nhtcondor-ce-slurm-3.0.2-1.osg34.el7\nhtcondor-ce-view-3.0.2-1.osg34.el7\nmyproxy-6.1.28-1.1.osg34.el7\nmyproxy-admin-6.1.28-1.1.osg34.el7\nmyproxy-debuginfo-6.1.28-1.1.osg34.el7\nmyproxy-devel-6.1.28-1.1.osg34.el7\nmyproxy-doc-6.1.28-1.1.osg34.el7\nmyproxy-libs-6.1.28-1.1.osg34.el7\nmyproxy-server-6.1.28-1.1.osg34.el7\nmyproxy-voms-6.1.28-1.1.osg34.el7\nosg-ca-scripts-1.1.7-1.osg34.el7\nosg-configure-2.2.0-1.osg34.el7\nosg-configure-bosco-2.2.0-1.osg34.el7\nosg-configure-ce-2.2.0-1.osg34.el7\nosg-configure-condor-2.2.0-1.osg34.el7\nosg-configure-gateway-2.2.0-1.osg34.el7\nosg-configure-gip-2.2.0-1.osg34.el7\nosg-configure-gratia-2.2.0-1.osg34.el7\nosg-configure-infoservices-2.2.0-1.osg34.el7\nosg-configure-lsf-2.2.0-1.osg34.el7\nosg-configure-managedfork-2.2.0-1.osg34.el7\nosg-configure-misc-2.2.0-1.osg34.el7\nosg-configure-network-2.2.0-1.osg34.el7\nosg-configure-pbs-2.2.0-1.osg34.el7\nosg-configure-rsv-2.2.0-1.osg34.el7\nosg-configure-sge-2.2.0-1.osg34.el7\nosg-configure-slurm-2.2.0-1.osg34.el7\nosg-configure-squid-2.2.0-1.osg34.el7\nosg-configure-tests-2.2.0-1.osg34.el7\nosg-oasis-8-1.osg34.el7\nosg-test-1.11.2-1.osg34.el7\nosg-test-log-viewer-1.11.2-1.osg34.el7\nosg-version-3.4.3-1.osg34.el7\nsingularity-2.3.1-0.1.4.osg34.el7\nsingularity-debuginfo-2.3.1-0.1.4.osg34.el7\nsingularity-devel-2.3.1-0.1.4.osg34.el7\nsingularity-runtime-2.3.1-0.1.4.osg34.el7\nstashcache-0.8-1.osg34.el7\nstashcache-cache-server-0.8-1.osg34.el7\nstashcache-daemon-0.8-1.osg34.el7\nstashcache-origin-server-0.8-1.osg34.el7\nxrootd-4.7.0-1.osg34.el7\nxrootd-client-4.7.0-1.osg34.el7\nxrootd-client-devel-4.7.0-1.osg34.el7\nxrootd-client-libs-4.7.0-1.osg34.el7\nxrootd-debuginfo-4.7.0-1.osg34.el7\nxrootd-devel-4.7.0-1.osg34.el7\nxrootd-doc-4.7.0-1.osg34.el7\nxrootd-fuse-4.7.0-1.osg34.el7\nxrootd-lcmaps-1.3.4-1.osg34.el7\nxrootd-lcmaps-debuginfo-1.3.4-1.osg34.el7\nxrootd-libs-4.7.0-1.osg34.el7\nxrootd-private-devel-4.7.0-1.osg34.el7\nxrootd-python-4.7.0-1.osg34.el7\nxrootd-selinux-4.7.0-1.osg34.el7\nxrootd-server-4.7.0-1.osg34.el7\nxrootd-server-devel-4.7.0-1.osg34.el7\nxrootd-server-libs-4.7.0-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#enterprise-linux-6_2", 
            "text": "blahp-1.18.33.bosco-1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#enterprise-linux-7_2", 
            "text": "blahp-1.18.33.bosco-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp blahp-debuginfo  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#enterprise-linux-6_3", 
            "text": "blahp-1.18.33.bosco-1.osgup.el6\nblahp-debuginfo-1.18.33.bosco-1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#enterprise-linux-7_3", 
            "text": "blahp-1.18.33.bosco-1.osgup.el7\nblahp-debuginfo-1.18.33.bosco-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-28/", 
            "text": "OSG Software Release 3.3.28\n\n\nRelease Date\n: 2017-09-12\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nUpdated to BLAHP 1.18.33\n\n\nProperly parses times from Slurm's sacct command\n\n\npbs_pro does not need to be defined to query PBS for job status\n\n\n\n\n\n\nUpdated to \nXRootD 4.7.0\n\n\nUpdated to \nStashCache 0.8\n\n\nUpdated Globus packages to latest EPEL versions\n\n\nosg-ca-scripts now use HTTPS to download CA certificates\n\n\nAdded the ability to limit transfer load in the globus-gridftp-osg-extensions\n\n\nFixed a few memory management bugs in \nxrootd-lcmaps\n\n\nUpdated to \nxrootd-hdfs 1.9.2\n\n\nHTCondor CE 2.2.3\n reports an error if \nJOB_ROUTER_ENTRIES\n are not defined\n\n\nGridFTP-HDFS now built from OSG sources\n\n\nUpdated SELinux profile to allow GUMS to access the MySQL port\n\n\nosg-configure 1.10.0 - removed the last vestiges of GRAM\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n\n\nNote\n\n\nStashCache is supported on EL7 only.\n\n\n\n\n\n\nNote\n\n\nxrootd-lcmaps will remain at 1.2.1-1 on EL6.\n\n\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nKnown Issues\n\n\n\n\nBecause the Gratia system has been turned off, RSV emits the following warning: \nWARNING: Gratia server gratia-osg-prod.opensciencegrid.org:80 not responding to obtain last contact details.\n. This warning can safely be ignored.\n\n\nUsing the LCMAPS VOMS will result in a failing \"supported VO\" RSV test (\nSOFTWARE-2763\n). This can be ignored and a fix is targeted for the October release.\n\n\n\n\nUpdates to VOMS admin server require the updated emi-trustmanager-tomcat and re-running the configure script:\n\n\nroot@host #\n /var/lib/trustmanager-tomcat/configure.sh\n\n\n\n\n\n\n\n\n\nVOMS admin server shows an error when modifying/adding/signing AUPs, but all the actions still work.\n\n\n\n\n\n\nAfter updating OSG-CE to version 3.3-12, please disable and remove OSG Info Services via the following procedure:\n\n\nroot@host #\n service osg-info-services stop\n\nroot@host #\n yum erase gip osg-info-services\n\n\n\n\n\n\n\n\n\nThe Koji client config has changed in the new version of Koji: \npkgurl=http://koji.chtc.wisc.edu/packages\n has been replaced by \ntopurl=http://koji.chtc.wisc.edu\n and the Koji client will give a harmless but annoying warning when it finds \npkgurl\n. To get rid of the warning, update to osg-build \n \n1.8.0\n, rerun \nosg-koji setup\n, and say 'yes' when asked to replace the Koji configuration file; or, you may make the above change manually.\n\n\n\n\n\n\nA previous version (\n1.17.0-2.5\n) of the Gratia probes required changes in the HTCondor configuration to operate properly. Now, these changes need to be reverted to use verison \n1.17.0-2.6\n and later of the probes. The lines below are likely to be found in \ncondor_config.local\n or a new file in \n/etc/condor/config.d\n directory. Delete these lines and run \ncondor_reconfig\n.\n\n\n# GlideinWMS Gratia commands\n\n\nPER_JOB_HISTORY_DIR\n \n=\n /var/lib/gratia/data\n\nJOBGLIDEIN_ResourceName\n=\n$$\n([IfThenElse(IsUndefined(TARGET.GLIDEIN_ResourceName), IfThenElse(IsUndefined(TARGET.GLIDEIN_Site), IfThenElse(IsUndefined(TARGET.FileSystemDomain), \\\nLocal Job\\\n, TARGET.FileSystemDomain), TARGET.GLIDEIN_Site), TARGET.GLIDEIN_ResourceName)])\n\n\nSUBMIT_EXPRS\n \n=\n \n$(\nSUBMIT_EXPRS\n)\n JOBGLIDEIN_ResourceName\n\n\n\n\n\n\n\n\n\nOn EL7, your version of voms-proxy-init may come from the voms-clients-java package; this version will continue to make legacy proxies by default. If you want the new behavior, install the voms-clients-cpp package and uninstall the voms-clients-java package.\n\n\n\n\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNote\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n\n\n\n\nNote\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.33.bosco-1.osg33.el6\n\n\nglobus-authz-3.15-1.osg33.el6\n\n\nglobus-authz-callout-error-3.6-1.osg33.el6\n\n\nglobus-callout-3.15-1.osg33.el6\n\n\nglobus-common-17.1-1.osg33.el6\n\n\nglobus-ftp-client-8.36-1.1.osg33.el6\n\n\nglobus-ftp-control-7.8-1.osg33.el6\n\n\nglobus-gass-cache-9.10-1.osg33.el6\n\n\nglobus-gass-cache-program-6.7-1.osg33.el6\n\n\nglobus-gass-copy-9.27-1.osg33.el6\n\n\nglobus-gass-server-ez-5.8-1.osg33.el6\n\n\nglobus-gass-transfer-8.10-1.osg33.el6\n\n\nglobus-gfork-4.9-1.osg33.el6\n\n\nglobus-gridftp-osg-extensions-0.4-1.osg33.el6\n\n\nglobus-gridftp-server-12.2-1.1.osg33.el6\n\n\nglobus-gridftp-server-control-5.1-1.1.osg33.el6\n\n\nglobus-gridmap-callout-error-2.5-1.osg33.el6\n\n\nglobus-gsi-callback-5.13-1.osg33.el6\n\n\nglobus-gsi-cert-utils-9.16-1.osg33.el6\n\n\nglobus-gsi-credential-7.11-1.osg33.el6\n\n\nglobus-gsi-openssl-error-3.8-1.osg33.el6\n\n\nglobus-gsi-proxy-core-8.6-1.osg33.el6\n\n\nglobus-gsi-proxy-ssl-5.10-1.osg33.el6\n\n\nglobus-gsi-sysconfig-6.11-1.osg33.el6\n\n\nglobus-gss-assist-10.21-1.osg33.el6\n\n\nglobus-gssapi-error-5.5-1.osg33.el6\n\n\nglobus-gssapi-gsi-12.17-3.osg33.el6\n\n\nglobus-io-11.9-1.osg33.el6\n\n\nglobus-openssl-module-4.8-1.osg33.el6\n\n\nglobus-proxy-utils-6.19-1.osg33.el6\n\n\nglobus-rsl-10.11-1.osg33.el6\n\n\nglobus-simple-ca-4.24-1.osg33.el6\n\n\nglobus-usage-4.5-1.osg33.el6\n\n\nglobus-xio-5.16-1.osg33.el6\n\n\nglobus-xio-gsi-driver-3.11-1.osg33.el6\n\n\nglobus-xio-pipe-driver-3.10-1.osg33.el6\n\n\nglobus-xio-popen-driver-3.6-1.osg33.el6\n\n\nglobus-xio-udt-driver-1.28-1.osg33.el6\n\n\nglobus-xioperf-4.5-1.osg33.el6\n\n\ngridftp-hdfs-1.0-1.1.osg33.el6\n\n\ngums-1.5.2-10.osg33.el6\n\n\nhtcondor-ce-2.2.3-1.osg33.el6\n\n\nmyproxy-6.1.28-1.1.osg33.el6\n\n\nosg-ca-scripts-1.1.7-1.osg33.el6\n\n\nosg-configure-1.10.0-1.osg33.el6\n\n\nosg-test-1.11.2-1.osg33.el6\n\n\nosg-version-3.3.28-1.osg33.el6\n\n\nxrootd-4.7.0-1.osg33.el6\n\n\nxrootd-hdfs-1.9.2-2.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.33.bosco-1.osg33.el7\n\n\nglobus-authz-3.15-1.osg33.el7\n\n\nglobus-authz-callout-error-3.6-1.osg33.el7\n\n\nglobus-callout-3.15-1.osg33.el7\n\n\nglobus-common-17.1-1.osg33.el7\n\n\nglobus-ftp-client-8.36-1.1.osg33.el7\n\n\nglobus-ftp-control-7.8-1.osg33.el7\n\n\nglobus-gass-cache-9.10-1.osg33.el7\n\n\nglobus-gass-cache-program-6.7-1.osg33.el7\n\n\nglobus-gass-copy-9.27-1.osg33.el7\n\n\nglobus-gass-server-ez-5.8-1.osg33.el7\n\n\nglobus-gass-transfer-8.10-1.osg33.el7\n\n\nglobus-gfork-4.9-1.osg33.el7\n\n\nglobus-gridftp-osg-extensions-0.4-1.osg33.el7\n\n\nglobus-gridftp-server-12.2-1.1.osg33.el7\n\n\nglobus-gridftp-server-control-5.1-1.1.osg33.el7\n\n\nglobus-gridmap-callout-error-2.5-1.osg33.el7\n\n\nglobus-gsi-callback-5.13-1.osg33.el7\n\n\nglobus-gsi-cert-utils-9.16-1.osg33.el7\n\n\nglobus-gsi-credential-7.11-1.osg33.el7\n\n\nglobus-gsi-openssl-error-3.8-1.osg33.el7\n\n\nglobus-gsi-proxy-core-8.6-1.osg33.el7\n\n\nglobus-gsi-proxy-ssl-5.10-1.osg33.el7\n\n\nglobus-gsi-sysconfig-6.11-1.osg33.el7\n\n\nglobus-gss-assist-10.21-1.osg33.el7\n\n\nglobus-gssapi-error-5.5-1.osg33.el7\n\n\nglobus-gssapi-gsi-12.17-3.osg33.el7\n\n\nglobus-io-11.9-1.osg33.el7\n\n\nglobus-openssl-module-4.8-1.osg33.el7\n\n\nglobus-proxy-utils-6.19-1.osg33.el7\n\n\nglobus-rsl-10.11-1.osg33.el7\n\n\nglobus-simple-ca-4.24-1.osg33.el7\n\n\nglobus-usage-4.5-1.osg33.el7\n\n\nglobus-xio-5.16-1.osg33.el7\n\n\nglobus-xio-gsi-driver-3.11-1.osg33.el7\n\n\nglobus-xio-pipe-driver-3.10-1.osg33.el7\n\n\nglobus-xio-popen-driver-3.6-1.osg33.el7\n\n\nglobus-xio-udt-driver-1.28-1.osg33.el7\n\n\nglobus-xioperf-4.5-1.osg33.el7\n\n\ngridftp-hdfs-1.0-1.1.osg33.el7\n\n\ngums-1.5.2-10.osg33.el7\n\n\nhtcondor-ce-2.2.3-1.osg33.el7\n\n\nmyproxy-6.1.28-1.1.osg33.el7\n\n\nosg-ca-scripts-1.1.7-1.osg33.el7\n\n\nosg-configure-1.10.0-1.osg33.el7\n\n\nosg-test-1.11.2-1.osg33.el7\n\n\nosg-version-3.3.28-1.osg33.el7\n\n\nstashcache-0.8-1.osg33.el7\n\n\nxrootd-4.7.0-1.osg33.el7\n\n\nxrootd-hdfs-1.9.2-2.osg33.el7\n\n\nxrootd-lcmaps-1.3.4-1.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp blahp-debuginfo globus-authz globus-authz-callout-error globus-authz-callout-error-debuginfo globus-authz-callout-error-devel globus-authz-callout-error-doc globus-authz-debuginfo globus-authz-devel globus-authz-doc globus-callout globus-callout-debuginfo globus-callout-devel globus-callout-doc globus-common globus-common-debuginfo globus-common-devel globus-common-doc globus-common-progs globus-ftp-client globus-ftp-client-debuginfo globus-ftp-client-devel globus-ftp-client-doc globus-ftp-control globus-ftp-control-debuginfo globus-ftp-control-devel globus-ftp-control-doc globus-gass-cache globus-gass-cache-debuginfo globus-gass-cache-devel globus-gass-cache-doc globus-gass-cache-program globus-gass-cache-program-debuginfo globus-gass-copy globus-gass-copy-debuginfo globus-gass-copy-devel globus-gass-copy-doc globus-gass-copy-progs globus-gass-server-ez globus-gass-server-ez-debuginfo globus-gass-server-ez-devel globus-gass-server-ez-progs globus-gass-transfer globus-gass-transfer-debuginfo globus-gass-transfer-devel globus-gass-transfer-doc globus-gfork globus-gfork-debuginfo globus-gfork-devel globus-gfork-progs globus-gridftp-osg-extensions globus-gridftp-osg-extensions-debuginfo globus-gridftp-server globus-gridftp-server-control globus-gridftp-server-control-debuginfo globus-gridftp-server-control-devel globus-gridftp-server-debuginfo globus-gridftp-server-devel globus-gridftp-server-progs globus-gridmap-callout-error globus-gridmap-callout-error-debuginfo globus-gridmap-callout-error-devel globus-gridmap-callout-error-doc globus-gsi-callback globus-gsi-callback-debuginfo globus-gsi-callback-devel globus-gsi-callback-doc globus-gsi-cert-utils globus-gsi-cert-utils-debuginfo globus-gsi-cert-utils-devel globus-gsi-cert-utils-doc globus-gsi-cert-utils-progs globus-gsi-credential globus-gsi-credential-debuginfo globus-gsi-credential-devel globus-gsi-credential-doc globus-gsi-openssl-error globus-gsi-openssl-error-debuginfo globus-gsi-openssl-error-devel globus-gsi-openssl-error-doc globus-gsi-proxy-core globus-gsi-proxy-core-debuginfo globus-gsi-proxy-core-devel globus-gsi-proxy-core-doc globus-gsi-proxy-ssl globus-gsi-proxy-ssl-debuginfo globus-gsi-proxy-ssl-devel globus-gsi-proxy-ssl-doc globus-gsi-sysconfig globus-gsi-sysconfig-debuginfo globus-gsi-sysconfig-devel globus-gsi-sysconfig-doc globus-gssapi-error globus-gssapi-error-debuginfo globus-gssapi-error-devel globus-gssapi-error-doc globus-gssapi-gsi globus-gssapi-gsi-debuginfo globus-gssapi-gsi-devel globus-gssapi-gsi-doc globus-gss-assist globus-gss-assist-debuginfo globus-gss-assist-devel globus-gss-assist-doc globus-gss-assist-progs globus-io globus-io-debuginfo globus-io-devel globus-openssl-module globus-openssl-module-debuginfo globus-openssl-module-devel globus-openssl-module-doc globus-proxy-utils globus-proxy-utils-debuginfo globus-rsl globus-rsl-debuginfo globus-rsl-devel globus-rsl-doc globus-simple-ca globus-usage globus-usage-debuginfo globus-usage-devel globus-xio globus-xio-debuginfo globus-xio-devel globus-xio-doc globus-xio-gsi-driver globus-xio-gsi-driver-debuginfo globus-xio-gsi-driver-devel globus-xio-gsi-driver-doc globus-xioperf globus-xioperf-debuginfo globus-xio-pipe-driver globus-xio-pipe-driver-debuginfo globus-xio-pipe-driver-devel globus-xio-popen-driver globus-xio-popen-driver-debuginfo globus-xio-popen-driver-devel globus-xio-udt-driver globus-xio-udt-driver-debuginfo globus-xio-udt-driver-devel gridftp-hdfs gridftp-hdfs-debuginfo gums gums-client gums-service htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-slurm htcondor-ce-view igtf-ca-certs myproxy myproxy-admin myproxy-debuginfo myproxy-devel myproxy-doc myproxy-libs myproxy-server myproxy-voms osg-ca-certs osg-ca-scripts osg-configure osg-configure-bosco osg-configure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-test osg-test-log-viewer osg-version xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-hdfs xrootd-hdfs-debuginfo xrootd-hdfs-devel xrootd-libs xrootd-private-devel xrootd-python xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp-1.18.33.bosco-1.osg33.el6\nblahp-debuginfo-1.18.33.bosco-1.osg33.el6\nglobus-authz-3.15-1.osg33.el6\nglobus-authz-callout-error-3.6-1.osg33.el6\nglobus-authz-callout-error-debuginfo-3.6-1.osg33.el6\nglobus-authz-callout-error-devel-3.6-1.osg33.el6\nglobus-authz-callout-error-doc-3.6-1.osg33.el6\nglobus-authz-debuginfo-3.15-1.osg33.el6\nglobus-authz-devel-3.15-1.osg33.el6\nglobus-authz-doc-3.15-1.osg33.el6\nglobus-callout-3.15-1.osg33.el6\nglobus-callout-debuginfo-3.15-1.osg33.el6\nglobus-callout-devel-3.15-1.osg33.el6\nglobus-callout-doc-3.15-1.osg33.el6\nglobus-common-17.1-1.osg33.el6\nglobus-common-debuginfo-17.1-1.osg33.el6\nglobus-common-devel-17.1-1.osg33.el6\nglobus-common-doc-17.1-1.osg33.el6\nglobus-common-progs-17.1-1.osg33.el6\nglobus-ftp-client-8.36-1.1.osg33.el6\nglobus-ftp-client-debuginfo-8.36-1.1.osg33.el6\nglobus-ftp-client-devel-8.36-1.1.osg33.el6\nglobus-ftp-client-doc-8.36-1.1.osg33.el6\nglobus-ftp-control-7.8-1.osg33.el6\nglobus-ftp-control-debuginfo-7.8-1.osg33.el6\nglobus-ftp-control-devel-7.8-1.osg33.el6\nglobus-ftp-control-doc-7.8-1.osg33.el6\nglobus-gass-cache-9.10-1.osg33.el6\nglobus-gass-cache-debuginfo-9.10-1.osg33.el6\nglobus-gass-cache-devel-9.10-1.osg33.el6\nglobus-gass-cache-doc-9.10-1.osg33.el6\nglobus-gass-cache-program-6.7-1.osg33.el6\nglobus-gass-cache-program-debuginfo-6.7-1.osg33.el6\nglobus-gass-copy-9.27-1.osg33.el6\nglobus-gass-copy-debuginfo-9.27-1.osg33.el6\nglobus-gass-copy-devel-9.27-1.osg33.el6\nglobus-gass-copy-doc-9.27-1.osg33.el6\nglobus-gass-copy-progs-9.27-1.osg33.el6\nglobus-gass-server-ez-5.8-1.osg33.el6\nglobus-gass-server-ez-debuginfo-5.8-1.osg33.el6\nglobus-gass-server-ez-devel-5.8-1.osg33.el6\nglobus-gass-server-ez-progs-5.8-1.osg33.el6\nglobus-gass-transfer-8.10-1.osg33.el6\nglobus-gass-transfer-debuginfo-8.10-1.osg33.el6\nglobus-gass-transfer-devel-8.10-1.osg33.el6\nglobus-gass-transfer-doc-8.10-1.osg33.el6\nglobus-gfork-4.9-1.osg33.el6\nglobus-gfork-debuginfo-4.9-1.osg33.el6\nglobus-gfork-devel-4.9-1.osg33.el6\nglobus-gfork-progs-4.9-1.osg33.el6\nglobus-gridftp-osg-extensions-0.4-1.osg33.el6\nglobus-gridftp-osg-extensions-debuginfo-0.4-1.osg33.el6\nglobus-gridftp-server-12.2-1.1.osg33.el6\nglobus-gridftp-server-control-5.1-1.1.osg33.el6\nglobus-gridftp-server-control-debuginfo-5.1-1.1.osg33.el6\nglobus-gridftp-server-control-devel-5.1-1.1.osg33.el6\nglobus-gridftp-server-debuginfo-12.2-1.1.osg33.el6\nglobus-gridftp-server-devel-12.2-1.1.osg33.el6\nglobus-gridftp-server-progs-12.2-1.1.osg33.el6\nglobus-gridmap-callout-error-2.5-1.osg33.el6\nglobus-gridmap-callout-error-debuginfo-2.5-1.osg33.el6\nglobus-gridmap-callout-error-devel-2.5-1.osg33.el6\nglobus-gridmap-callout-error-doc-2.5-1.osg33.el6\nglobus-gsi-callback-5.13-1.osg33.el6\nglobus-gsi-callback-debuginfo-5.13-1.osg33.el6\nglobus-gsi-callback-devel-5.13-1.osg33.el6\nglobus-gsi-callback-doc-5.13-1.osg33.el6\nglobus-gsi-cert-utils-9.16-1.osg33.el6\nglobus-gsi-cert-utils-debuginfo-9.16-1.osg33.el6\nglobus-gsi-cert-utils-devel-9.16-1.osg33.el6\nglobus-gsi-cert-utils-doc-9.16-1.osg33.el6\nglobus-gsi-cert-utils-progs-9.16-1.osg33.el6\nglobus-gsi-credential-7.11-1.osg33.el6\nglobus-gsi-credential-debuginfo-7.11-1.osg33.el6\nglobus-gsi-credential-devel-7.11-1.osg33.el6\nglobus-gsi-credential-doc-7.11-1.osg33.el6\nglobus-gsi-openssl-error-3.8-1.osg33.el6\nglobus-gsi-openssl-error-debuginfo-3.8-1.osg33.el6\nglobus-gsi-openssl-error-devel-3.8-1.osg33.el6\nglobus-gsi-openssl-error-doc-3.8-1.osg33.el6\nglobus-gsi-proxy-core-8.6-1.osg33.el6\nglobus-gsi-proxy-core-debuginfo-8.6-1.osg33.el6\nglobus-gsi-proxy-core-devel-8.6-1.osg33.el6\nglobus-gsi-proxy-core-doc-8.6-1.osg33.el6\nglobus-gsi-proxy-ssl-5.10-1.osg33.el6\nglobus-gsi-proxy-ssl-debuginfo-5.10-1.osg33.el6\nglobus-gsi-proxy-ssl-devel-5.10-1.osg33.el6\nglobus-gsi-proxy-ssl-doc-5.10-1.osg33.el6\nglobus-gsi-sysconfig-6.11-1.osg33.el6\nglobus-gsi-sysconfig-debuginfo-6.11-1.osg33.el6\nglobus-gsi-sysconfig-devel-6.11-1.osg33.el6\nglobus-gsi-sysconfig-doc-6.11-1.osg33.el6\nglobus-gssapi-error-5.5-1.osg33.el6\nglobus-gssapi-error-debuginfo-5.5-1.osg33.el6\nglobus-gssapi-error-devel-5.5-1.osg33.el6\nglobus-gssapi-error-doc-5.5-1.osg33.el6\nglobus-gssapi-gsi-12.17-3.osg33.el6\nglobus-gssapi-gsi-debuginfo-12.17-3.osg33.el6\nglobus-gssapi-gsi-devel-12.17-3.osg33.el6\nglobus-gssapi-gsi-doc-12.17-3.osg33.el6\nglobus-gss-assist-10.21-1.osg33.el6\nglobus-gss-assist-debuginfo-10.21-1.osg33.el6\nglobus-gss-assist-devel-10.21-1.osg33.el6\nglobus-gss-assist-doc-10.21-1.osg33.el6\nglobus-gss-assist-progs-10.21-1.osg33.el6\nglobus-io-11.9-1.osg33.el6\nglobus-io-debuginfo-11.9-1.osg33.el6\nglobus-io-devel-11.9-1.osg33.el6\nglobus-openssl-module-4.8-1.osg33.el6\nglobus-openssl-module-debuginfo-4.8-1.osg33.el6\nglobus-openssl-module-devel-4.8-1.osg33.el6\nglobus-openssl-module-doc-4.8-1.osg33.el6\nglobus-proxy-utils-6.19-1.osg33.el6\nglobus-proxy-utils-debuginfo-6.19-1.osg33.el6\nglobus-rsl-10.11-1.osg33.el6\nglobus-rsl-debuginfo-10.11-1.osg33.el6\nglobus-rsl-devel-10.11-1.osg33.el6\nglobus-rsl-doc-10.11-1.osg33.el6\nglobus-simple-ca-4.24-1.osg33.el6\nglobus-usage-4.5-1.osg33.el6\nglobus-usage-debuginfo-4.5-1.osg33.el6\nglobus-usage-devel-4.5-1.osg33.el6\nglobus-xio-5.16-1.osg33.el6\nglobus-xio-debuginfo-5.16-1.osg33.el6\nglobus-xio-devel-5.16-1.osg33.el6\nglobus-xio-doc-5.16-1.osg33.el6\nglobus-xio-gsi-driver-3.11-1.osg33.el6\nglobus-xio-gsi-driver-debuginfo-3.11-1.osg33.el6\nglobus-xio-gsi-driver-devel-3.11-1.osg33.el6\nglobus-xio-gsi-driver-doc-3.11-1.osg33.el6\nglobus-xioperf-4.5-1.osg33.el6\nglobus-xioperf-debuginfo-4.5-1.osg33.el6\nglobus-xio-pipe-driver-3.10-1.osg33.el6\nglobus-xio-pipe-driver-debuginfo-3.10-1.osg33.el6\nglobus-xio-pipe-driver-devel-3.10-1.osg33.el6\nglobus-xio-popen-driver-3.6-1.osg33.el6\nglobus-xio-popen-driver-debuginfo-3.6-1.osg33.el6\nglobus-xio-popen-driver-devel-3.6-1.osg33.el6\nglobus-xio-udt-driver-1.28-1.osg33.el6\nglobus-xio-udt-driver-debuginfo-1.28-1.osg33.el6\nglobus-xio-udt-driver-devel-1.28-1.osg33.el6\ngridftp-hdfs-1.0-1.1.osg33.el6\ngridftp-hdfs-debuginfo-1.0-1.1.osg33.el6\ngums-1.5.2-10.osg33.el6\ngums-client-1.5.2-10.osg33.el6\ngums-service-1.5.2-10.osg33.el6\nhtcondor-ce-2.2.3-1.osg33.el6\nhtcondor-ce-bosco-2.2.3-1.osg33.el6\nhtcondor-ce-client-2.2.3-1.osg33.el6\nhtcondor-ce-collector-2.2.3-1.osg33.el6\nhtcondor-ce-condor-2.2.3-1.osg33.el6\nhtcondor-ce-lsf-2.2.3-1.osg33.el6\nhtcondor-ce-pbs-2.2.3-1.osg33.el6\nhtcondor-ce-sge-2.2.3-1.osg33.el6\nhtcondor-ce-slurm-2.2.3-1.osg33.el6\nhtcondor-ce-view-2.2.3-1.osg33.el6\nmyproxy-6.1.28-1.1.osg33.el6\nmyproxy-admin-6.1.28-1.1.osg33.el6\nmyproxy-debuginfo-6.1.28-1.1.osg33.el6\nmyproxy-devel-6.1.28-1.1.osg33.el6\nmyproxy-doc-6.1.28-1.1.osg33.el6\nmyproxy-libs-6.1.28-1.1.osg33.el6\nmyproxy-server-6.1.28-1.1.osg33.el6\nmyproxy-voms-6.1.28-1.1.osg33.el6\nosg-ca-scripts-1.1.7-1.osg33.el6\nosg-configure-1.10.0-1.osg33.el6\nosg-configure-bosco-1.10.0-1.osg33.el6\nosg-configure-ce-1.10.0-1.osg33.el6\nosg-configure-cemon-1.10.0-1.osg33.el6\nosg-configure-condor-1.10.0-1.osg33.el6\nosg-configure-gateway-1.10.0-1.osg33.el6\nosg-configure-gip-1.10.0-1.osg33.el6\nosg-configure-gratia-1.10.0-1.osg33.el6\nosg-configure-infoservices-1.10.0-1.osg33.el6\nosg-configure-lsf-1.10.0-1.osg33.el6\nosg-configure-managedfork-1.10.0-1.osg33.el6\nosg-configure-misc-1.10.0-1.osg33.el6\nosg-configure-monalisa-1.10.0-1.osg33.el6\nosg-configure-network-1.10.0-1.osg33.el6\nosg-configure-pbs-1.10.0-1.osg33.el6\nosg-configure-rsv-1.10.0-1.osg33.el6\nosg-configure-sge-1.10.0-1.osg33.el6\nosg-configure-slurm-1.10.0-1.osg33.el6\nosg-configure-squid-1.10.0-1.osg33.el6\nosg-configure-tests-1.10.0-1.osg33.el6\nosg-test-1.11.2-1.osg33.el6\nosg-test-log-viewer-1.11.2-1.osg33.el6\nosg-version-3.3.28-1.osg33.el6\nxrootd-4.7.0-1.osg33.el6\nxrootd-client-4.7.0-1.osg33.el6\nxrootd-client-devel-4.7.0-1.osg33.el6\nxrootd-client-libs-4.7.0-1.osg33.el6\nxrootd-debuginfo-4.7.0-1.osg33.el6\nxrootd-devel-4.7.0-1.osg33.el6\nxrootd-doc-4.7.0-1.osg33.el6\nxrootd-fuse-4.7.0-1.osg33.el6\nxrootd-hdfs-1.9.2-2.osg33.el6\nxrootd-hdfs-debuginfo-1.9.2-2.osg33.el6\nxrootd-hdfs-devel-1.9.2-2.osg33.el6\nxrootd-libs-4.7.0-1.osg33.el6\nxrootd-private-devel-4.7.0-1.osg33.el6\nxrootd-python-4.7.0-1.osg33.el6\nxrootd-selinux-4.7.0-1.osg33.el6\nxrootd-server-4.7.0-1.osg33.el6\nxrootd-server-devel-4.7.0-1.osg33.el6\nxrootd-server-libs-4.7.0-1.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp-1.18.33.bosco-1.osg33.el7\nblahp-debuginfo-1.18.33.bosco-1.osg33.el7\nglobus-authz-3.15-1.osg33.el7\nglobus-authz-callout-error-3.6-1.osg33.el7\nglobus-authz-callout-error-debuginfo-3.6-1.osg33.el7\nglobus-authz-callout-error-devel-3.6-1.osg33.el7\nglobus-authz-callout-error-doc-3.6-1.osg33.el7\nglobus-authz-debuginfo-3.15-1.osg33.el7\nglobus-authz-devel-3.15-1.osg33.el7\nglobus-authz-doc-3.15-1.osg33.el7\nglobus-callout-3.15-1.osg33.el7\nglobus-callout-debuginfo-3.15-1.osg33.el7\nglobus-callout-devel-3.15-1.osg33.el7\nglobus-callout-doc-3.15-1.osg33.el7\nglobus-common-17.1-1.osg33.el7\nglobus-common-debuginfo-17.1-1.osg33.el7\nglobus-common-devel-17.1-1.osg33.el7\nglobus-common-doc-17.1-1.osg33.el7\nglobus-common-progs-17.1-1.osg33.el7\nglobus-ftp-client-8.36-1.1.osg33.el7\nglobus-ftp-client-debuginfo-8.36-1.1.osg33.el7\nglobus-ftp-client-devel-8.36-1.1.osg33.el7\nglobus-ftp-client-doc-8.36-1.1.osg33.el7\nglobus-ftp-control-7.8-1.osg33.el7\nglobus-ftp-control-debuginfo-7.8-1.osg33.el7\nglobus-ftp-control-devel-7.8-1.osg33.el7\nglobus-ftp-control-doc-7.8-1.osg33.el7\nglobus-gass-cache-9.10-1.osg33.el7\nglobus-gass-cache-debuginfo-9.10-1.osg33.el7\nglobus-gass-cache-devel-9.10-1.osg33.el7\nglobus-gass-cache-doc-9.10-1.osg33.el7\nglobus-gass-cache-program-6.7-1.osg33.el7\nglobus-gass-cache-program-debuginfo-6.7-1.osg33.el7\nglobus-gass-copy-9.27-1.osg33.el7\nglobus-gass-copy-debuginfo-9.27-1.osg33.el7\nglobus-gass-copy-devel-9.27-1.osg33.el7\nglobus-gass-copy-doc-9.27-1.osg33.el7\nglobus-gass-copy-progs-9.27-1.osg33.el7\nglobus-gass-server-ez-5.8-1.osg33.el7\nglobus-gass-server-ez-debuginfo-5.8-1.osg33.el7\nglobus-gass-server-ez-devel-5.8-1.osg33.el7\nglobus-gass-server-ez-progs-5.8-1.osg33.el7\nglobus-gass-transfer-8.10-1.osg33.el7\nglobus-gass-transfer-debuginfo-8.10-1.osg33.el7\nglobus-gass-transfer-devel-8.10-1.osg33.el7\nglobus-gass-transfer-doc-8.10-1.osg33.el7\nglobus-gfork-4.9-1.osg33.el7\nglobus-gfork-debuginfo-4.9-1.osg33.el7\nglobus-gfork-devel-4.9-1.osg33.el7\nglobus-gfork-progs-4.9-1.osg33.el7\nglobus-gridftp-osg-extensions-0.4-1.osg33.el7\nglobus-gridftp-osg-extensions-debuginfo-0.4-1.osg33.el7\nglobus-gridftp-server-12.2-1.1.osg33.el7\nglobus-gridftp-server-control-5.1-1.1.osg33.el7\nglobus-gridftp-server-control-debuginfo-5.1-1.1.osg33.el7\nglobus-gridftp-server-control-devel-5.1-1.1.osg33.el7\nglobus-gridftp-server-debuginfo-12.2-1.1.osg33.el7\nglobus-gridftp-server-devel-12.2-1.1.osg33.el7\nglobus-gridftp-server-progs-12.2-1.1.osg33.el7\nglobus-gridmap-callout-error-2.5-1.osg33.el7\nglobus-gridmap-callout-error-debuginfo-2.5-1.osg33.el7\nglobus-gridmap-callout-error-devel-2.5-1.osg33.el7\nglobus-gridmap-callout-error-doc-2.5-1.osg33.el7\nglobus-gsi-callback-5.13-1.osg33.el7\nglobus-gsi-callback-debuginfo-5.13-1.osg33.el7\nglobus-gsi-callback-devel-5.13-1.osg33.el7\nglobus-gsi-callback-doc-5.13-1.osg33.el7\nglobus-gsi-cert-utils-9.16-1.osg33.el7\nglobus-gsi-cert-utils-debuginfo-9.16-1.osg33.el7\nglobus-gsi-cert-utils-devel-9.16-1.osg33.el7\nglobus-gsi-cert-utils-doc-9.16-1.osg33.el7\nglobus-gsi-cert-utils-progs-9.16-1.osg33.el7\nglobus-gsi-credential-7.11-1.osg33.el7\nglobus-gsi-credential-debuginfo-7.11-1.osg33.el7\nglobus-gsi-credential-devel-7.11-1.osg33.el7\nglobus-gsi-credential-doc-7.11-1.osg33.el7\nglobus-gsi-openssl-error-3.8-1.osg33.el7\nglobus-gsi-openssl-error-debuginfo-3.8-1.osg33.el7\nglobus-gsi-openssl-error-devel-3.8-1.osg33.el7\nglobus-gsi-openssl-error-doc-3.8-1.osg33.el7\nglobus-gsi-proxy-core-8.6-1.osg33.el7\nglobus-gsi-proxy-core-debuginfo-8.6-1.osg33.el7\nglobus-gsi-proxy-core-devel-8.6-1.osg33.el7\nglobus-gsi-proxy-core-doc-8.6-1.osg33.el7\nglobus-gsi-proxy-ssl-5.10-1.osg33.el7\nglobus-gsi-proxy-ssl-debuginfo-5.10-1.osg33.el7\nglobus-gsi-proxy-ssl-devel-5.10-1.osg33.el7\nglobus-gsi-proxy-ssl-doc-5.10-1.osg33.el7\nglobus-gsi-sysconfig-6.11-1.osg33.el7\nglobus-gsi-sysconfig-debuginfo-6.11-1.osg33.el7\nglobus-gsi-sysconfig-devel-6.11-1.osg33.el7\nglobus-gsi-sysconfig-doc-6.11-1.osg33.el7\nglobus-gssapi-error-5.5-1.osg33.el7\nglobus-gssapi-error-debuginfo-5.5-1.osg33.el7\nglobus-gssapi-error-devel-5.5-1.osg33.el7\nglobus-gssapi-error-doc-5.5-1.osg33.el7\nglobus-gssapi-gsi-12.17-3.osg33.el7\nglobus-gssapi-gsi-debuginfo-12.17-3.osg33.el7\nglobus-gssapi-gsi-devel-12.17-3.osg33.el7\nglobus-gssapi-gsi-doc-12.17-3.osg33.el7\nglobus-gss-assist-10.21-1.osg33.el7\nglobus-gss-assist-debuginfo-10.21-1.osg33.el7\nglobus-gss-assist-devel-10.21-1.osg33.el7\nglobus-gss-assist-doc-10.21-1.osg33.el7\nglobus-gss-assist-progs-10.21-1.osg33.el7\nglobus-io-11.9-1.osg33.el7\nglobus-io-debuginfo-11.9-1.osg33.el7\nglobus-io-devel-11.9-1.osg33.el7\nglobus-openssl-module-4.8-1.osg33.el7\nglobus-openssl-module-debuginfo-4.8-1.osg33.el7\nglobus-openssl-module-devel-4.8-1.osg33.el7\nglobus-openssl-module-doc-4.8-1.osg33.el7\nglobus-proxy-utils-6.19-1.osg33.el7\nglobus-proxy-utils-debuginfo-6.19-1.osg33.el7\nglobus-rsl-10.11-1.osg33.el7\nglobus-rsl-debuginfo-10.11-1.osg33.el7\nglobus-rsl-devel-10.11-1.osg33.el7\nglobus-rsl-doc-10.11-1.osg33.el7\nglobus-simple-ca-4.24-1.osg33.el7\nglobus-usage-4.5-1.osg33.el7\nglobus-usage-debuginfo-4.5-1.osg33.el7\nglobus-usage-devel-4.5-1.osg33.el7\nglobus-xio-5.16-1.osg33.el7\nglobus-xio-debuginfo-5.16-1.osg33.el7\nglobus-xio-devel-5.16-1.osg33.el7\nglobus-xio-doc-5.16-1.osg33.el7\nglobus-xio-gsi-driver-3.11-1.osg33.el7\nglobus-xio-gsi-driver-debuginfo-3.11-1.osg33.el7\nglobus-xio-gsi-driver-devel-3.11-1.osg33.el7\nglobus-xio-gsi-driver-doc-3.11-1.osg33.el7\nglobus-xioperf-4.5-1.osg33.el7\nglobus-xioperf-debuginfo-4.5-1.osg33.el7\nglobus-xio-pipe-driver-3.10-1.osg33.el7\nglobus-xio-pipe-driver-debuginfo-3.10-1.osg33.el7\nglobus-xio-pipe-driver-devel-3.10-1.osg33.el7\nglobus-xio-popen-driver-3.6-1.osg33.el7\nglobus-xio-popen-driver-debuginfo-3.6-1.osg33.el7\nglobus-xio-popen-driver-devel-3.6-1.osg33.el7\nglobus-xio-udt-driver-1.28-1.osg33.el7\nglobus-xio-udt-driver-debuginfo-1.28-1.osg33.el7\nglobus-xio-udt-driver-devel-1.28-1.osg33.el7\ngridftp-hdfs-1.0-1.1.osg33.el7\ngridftp-hdfs-debuginfo-1.0-1.1.osg33.el7\ngums-1.5.2-10.osg33.el7\ngums-client-1.5.2-10.osg33.el7\ngums-service-1.5.2-10.osg33.el7\nhtcondor-ce-2.2.3-1.osg33.el7\nhtcondor-ce-bosco-2.2.3-1.osg33.el7\nhtcondor-ce-client-2.2.3-1.osg33.el7\nhtcondor-ce-collector-2.2.3-1.osg33.el7\nhtcondor-ce-condor-2.2.3-1.osg33.el7\nhtcondor-ce-lsf-2.2.3-1.osg33.el7\nhtcondor-ce-pbs-2.2.3-1.osg33.el7\nhtcondor-ce-sge-2.2.3-1.osg33.el7\nhtcondor-ce-slurm-2.2.3-1.osg33.el7\nhtcondor-ce-view-2.2.3-1.osg33.el7\nmyproxy-6.1.28-1.1.osg33.el7\nmyproxy-admin-6.1.28-1.1.osg33.el7\nmyproxy-debuginfo-6.1.28-1.1.osg33.el7\nmyproxy-devel-6.1.28-1.1.osg33.el7\nmyproxy-doc-6.1.28-1.1.osg33.el7\nmyproxy-libs-6.1.28-1.1.osg33.el7\nmyproxy-server-6.1.28-1.1.osg33.el7\nmyproxy-voms-6.1.28-1.1.osg33.el7\nosg-ca-scripts-1.1.7-1.osg33.el7\nosg-configure-1.10.0-1.osg33.el7\nosg-configure-bosco-1.10.0-1.osg33.el7\nosg-configure-ce-1.10.0-1.osg33.el7\nosg-configure-cemon-1.10.0-1.osg33.el7\nosg-configure-condor-1.10.0-1.osg33.el7\nosg-configure-gateway-1.10.0-1.osg33.el7\nosg-configure-gip-1.10.0-1.osg33.el7\nosg-configure-gratia-1.10.0-1.osg33.el7\nosg-configure-infoservices-1.10.0-1.osg33.el7\nosg-configure-lsf-1.10.0-1.osg33.el7\nosg-configure-managedfork-1.10.0-1.osg33.el7\nosg-configure-misc-1.10.0-1.osg33.el7\nosg-configure-monalisa-1.10.0-1.osg33.el7\nosg-configure-network-1.10.0-1.osg33.el7\nosg-configure-pbs-1.10.0-1.osg33.el7\nosg-configure-rsv-1.10.0-1.osg33.el7\nosg-configure-sge-1.10.0-1.osg33.el7\nosg-configure-slurm-1.10.0-1.osg33.el7\nosg-configure-squid-1.10.0-1.osg33.el7\nosg-configure-tests-1.10.0-1.osg33.el7\nosg-test-1.11.2-1.osg33.el7\nosg-test-log-viewer-1.11.2-1.osg33.el7\nosg-version-3.3.28-1.osg33.el7\nstashcache-0.8-1.osg33.el7\nstashcache-cache-server-0.8-1.osg33.el7\nstashcache-daemon-0.8-1.osg33.el7\nstashcache-origin-server-0.8-1.osg33.el7\nxrootd-4.7.0-1.osg33.el7\nxrootd-client-4.7.0-1.osg33.el7\nxrootd-client-devel-4.7.0-1.osg33.el7\nxrootd-client-libs-4.7.0-1.osg33.el7\nxrootd-debuginfo-4.7.0-1.osg33.el7\nxrootd-devel-4.7.0-1.osg33.el7\nxrootd-doc-4.7.0-1.osg33.el7\nxrootd-fuse-4.7.0-1.osg33.el7\nxrootd-hdfs-1.9.2-2.osg33.el7\nxrootd-hdfs-debuginfo-1.9.2-2.osg33.el7\nxrootd-hdfs-devel-1.9.2-2.osg33.el7\nxrootd-lcmaps-1.3.4-1.osg33.el7\nxrootd-lcmaps-debuginfo-1.3.4-1.osg33.el7\nxrootd-libs-4.7.0-1.osg33.el7\nxrootd-private-devel-4.7.0-1.osg33.el7\nxrootd-python-4.7.0-1.osg33.el7\nxrootd-selinux-4.7.0-1.osg33.el7\nxrootd-server-4.7.0-1.osg33.el7\nxrootd-server-devel-4.7.0-1.osg33.el7\nxrootd-server-libs-4.7.0-1.osg33.el7", 
            "title": "OSG Release 3.3.28"
        }, 
        {
            "location": "/release/3.3/release-3-3-28/#osg-software-release-3328", 
            "text": "Release Date : 2017-09-12", 
            "title": "OSG Software Release 3.3.28"
        }, 
        {
            "location": "/release/3.3/release-3-3-28/#summary-of-changes", 
            "text": "This release contains:   Updated to BLAHP 1.18.33  Properly parses times from Slurm's sacct command  pbs_pro does not need to be defined to query PBS for job status    Updated to  XRootD 4.7.0  Updated to  StashCache 0.8  Updated Globus packages to latest EPEL versions  osg-ca-scripts now use HTTPS to download CA certificates  Added the ability to limit transfer load in the globus-gridftp-osg-extensions  Fixed a few memory management bugs in  xrootd-lcmaps  Updated to  xrootd-hdfs 1.9.2  HTCondor CE 2.2.3  reports an error if  JOB_ROUTER_ENTRIES  are not defined  GridFTP-HDFS now built from OSG sources  Updated SELinux profile to allow GUMS to access the MySQL port  osg-configure 1.10.0 - removed the last vestiges of GRAM   These  JIRA tickets  were addressed in this release.   Note  StashCache is supported on EL7 only.    Note  xrootd-lcmaps will remain at 1.2.1-1 on EL6.   Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-28/#known-issues", 
            "text": "Because the Gratia system has been turned off, RSV emits the following warning:  WARNING: Gratia server gratia-osg-prod.opensciencegrid.org:80 not responding to obtain last contact details. . This warning can safely be ignored.  Using the LCMAPS VOMS will result in a failing \"supported VO\" RSV test ( SOFTWARE-2763 ). This can be ignored and a fix is targeted for the October release.   Updates to VOMS admin server require the updated emi-trustmanager-tomcat and re-running the configure script:  root@host #  /var/lib/trustmanager-tomcat/configure.sh    VOMS admin server shows an error when modifying/adding/signing AUPs, but all the actions still work.    After updating OSG-CE to version 3.3-12, please disable and remove OSG Info Services via the following procedure:  root@host #  service osg-info-services stop root@host #  yum erase gip osg-info-services    The Koji client config has changed in the new version of Koji:  pkgurl=http://koji.chtc.wisc.edu/packages  has been replaced by  topurl=http://koji.chtc.wisc.edu  and the Koji client will give a harmless but annoying warning when it finds  pkgurl . To get rid of the warning, update to osg-build    1.8.0 , rerun  osg-koji setup , and say 'yes' when asked to replace the Koji configuration file; or, you may make the above change manually.    A previous version ( 1.17.0-2.5 ) of the Gratia probes required changes in the HTCondor configuration to operate properly. Now, these changes need to be reverted to use verison  1.17.0-2.6  and later of the probes. The lines below are likely to be found in  condor_config.local  or a new file in  /etc/condor/config.d  directory. Delete these lines and run  condor_reconfig .  # GlideinWMS Gratia commands  PER_JOB_HISTORY_DIR   =  /var/lib/gratia/data JOBGLIDEIN_ResourceName = $$ ([IfThenElse(IsUndefined(TARGET.GLIDEIN_ResourceName), IfThenElse(IsUndefined(TARGET.GLIDEIN_Site), IfThenElse(IsUndefined(TARGET.FileSystemDomain), \\ Local Job\\ , TARGET.FileSystemDomain), TARGET.GLIDEIN_Site), TARGET.GLIDEIN_ResourceName)])  SUBMIT_EXPRS   =   $( SUBMIT_EXPRS )  JOBGLIDEIN_ResourceName    On EL7, your version of voms-proxy-init may come from the voms-clients-java package; this version will continue to make legacy proxies by default. If you want the new behavior, install the voms-clients-cpp package and uninstall the voms-clients-java package.", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.3/release-3-3-28/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-28/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-28/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Note  Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.    Note  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-28/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-28/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-28/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-28/#enterprise-linux-6", 
            "text": "blahp-1.18.33.bosco-1.osg33.el6  globus-authz-3.15-1.osg33.el6  globus-authz-callout-error-3.6-1.osg33.el6  globus-callout-3.15-1.osg33.el6  globus-common-17.1-1.osg33.el6  globus-ftp-client-8.36-1.1.osg33.el6  globus-ftp-control-7.8-1.osg33.el6  globus-gass-cache-9.10-1.osg33.el6  globus-gass-cache-program-6.7-1.osg33.el6  globus-gass-copy-9.27-1.osg33.el6  globus-gass-server-ez-5.8-1.osg33.el6  globus-gass-transfer-8.10-1.osg33.el6  globus-gfork-4.9-1.osg33.el6  globus-gridftp-osg-extensions-0.4-1.osg33.el6  globus-gridftp-server-12.2-1.1.osg33.el6  globus-gridftp-server-control-5.1-1.1.osg33.el6  globus-gridmap-callout-error-2.5-1.osg33.el6  globus-gsi-callback-5.13-1.osg33.el6  globus-gsi-cert-utils-9.16-1.osg33.el6  globus-gsi-credential-7.11-1.osg33.el6  globus-gsi-openssl-error-3.8-1.osg33.el6  globus-gsi-proxy-core-8.6-1.osg33.el6  globus-gsi-proxy-ssl-5.10-1.osg33.el6  globus-gsi-sysconfig-6.11-1.osg33.el6  globus-gss-assist-10.21-1.osg33.el6  globus-gssapi-error-5.5-1.osg33.el6  globus-gssapi-gsi-12.17-3.osg33.el6  globus-io-11.9-1.osg33.el6  globus-openssl-module-4.8-1.osg33.el6  globus-proxy-utils-6.19-1.osg33.el6  globus-rsl-10.11-1.osg33.el6  globus-simple-ca-4.24-1.osg33.el6  globus-usage-4.5-1.osg33.el6  globus-xio-5.16-1.osg33.el6  globus-xio-gsi-driver-3.11-1.osg33.el6  globus-xio-pipe-driver-3.10-1.osg33.el6  globus-xio-popen-driver-3.6-1.osg33.el6  globus-xio-udt-driver-1.28-1.osg33.el6  globus-xioperf-4.5-1.osg33.el6  gridftp-hdfs-1.0-1.1.osg33.el6  gums-1.5.2-10.osg33.el6  htcondor-ce-2.2.3-1.osg33.el6  myproxy-6.1.28-1.1.osg33.el6  osg-ca-scripts-1.1.7-1.osg33.el6  osg-configure-1.10.0-1.osg33.el6  osg-test-1.11.2-1.osg33.el6  osg-version-3.3.28-1.osg33.el6  xrootd-4.7.0-1.osg33.el6  xrootd-hdfs-1.9.2-2.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-28/#enterprise-linux-7", 
            "text": "blahp-1.18.33.bosco-1.osg33.el7  globus-authz-3.15-1.osg33.el7  globus-authz-callout-error-3.6-1.osg33.el7  globus-callout-3.15-1.osg33.el7  globus-common-17.1-1.osg33.el7  globus-ftp-client-8.36-1.1.osg33.el7  globus-ftp-control-7.8-1.osg33.el7  globus-gass-cache-9.10-1.osg33.el7  globus-gass-cache-program-6.7-1.osg33.el7  globus-gass-copy-9.27-1.osg33.el7  globus-gass-server-ez-5.8-1.osg33.el7  globus-gass-transfer-8.10-1.osg33.el7  globus-gfork-4.9-1.osg33.el7  globus-gridftp-osg-extensions-0.4-1.osg33.el7  globus-gridftp-server-12.2-1.1.osg33.el7  globus-gridftp-server-control-5.1-1.1.osg33.el7  globus-gridmap-callout-error-2.5-1.osg33.el7  globus-gsi-callback-5.13-1.osg33.el7  globus-gsi-cert-utils-9.16-1.osg33.el7  globus-gsi-credential-7.11-1.osg33.el7  globus-gsi-openssl-error-3.8-1.osg33.el7  globus-gsi-proxy-core-8.6-1.osg33.el7  globus-gsi-proxy-ssl-5.10-1.osg33.el7  globus-gsi-sysconfig-6.11-1.osg33.el7  globus-gss-assist-10.21-1.osg33.el7  globus-gssapi-error-5.5-1.osg33.el7  globus-gssapi-gsi-12.17-3.osg33.el7  globus-io-11.9-1.osg33.el7  globus-openssl-module-4.8-1.osg33.el7  globus-proxy-utils-6.19-1.osg33.el7  globus-rsl-10.11-1.osg33.el7  globus-simple-ca-4.24-1.osg33.el7  globus-usage-4.5-1.osg33.el7  globus-xio-5.16-1.osg33.el7  globus-xio-gsi-driver-3.11-1.osg33.el7  globus-xio-pipe-driver-3.10-1.osg33.el7  globus-xio-popen-driver-3.6-1.osg33.el7  globus-xio-udt-driver-1.28-1.osg33.el7  globus-xioperf-4.5-1.osg33.el7  gridftp-hdfs-1.0-1.1.osg33.el7  gums-1.5.2-10.osg33.el7  htcondor-ce-2.2.3-1.osg33.el7  myproxy-6.1.28-1.1.osg33.el7  osg-ca-scripts-1.1.7-1.osg33.el7  osg-configure-1.10.0-1.osg33.el7  osg-test-1.11.2-1.osg33.el7  osg-version-3.3.28-1.osg33.el7  stashcache-0.8-1.osg33.el7  xrootd-4.7.0-1.osg33.el7  xrootd-hdfs-1.9.2-2.osg33.el7  xrootd-lcmaps-1.3.4-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-28/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp blahp-debuginfo globus-authz globus-authz-callout-error globus-authz-callout-error-debuginfo globus-authz-callout-error-devel globus-authz-callout-error-doc globus-authz-debuginfo globus-authz-devel globus-authz-doc globus-callout globus-callout-debuginfo globus-callout-devel globus-callout-doc globus-common globus-common-debuginfo globus-common-devel globus-common-doc globus-common-progs globus-ftp-client globus-ftp-client-debuginfo globus-ftp-client-devel globus-ftp-client-doc globus-ftp-control globus-ftp-control-debuginfo globus-ftp-control-devel globus-ftp-control-doc globus-gass-cache globus-gass-cache-debuginfo globus-gass-cache-devel globus-gass-cache-doc globus-gass-cache-program globus-gass-cache-program-debuginfo globus-gass-copy globus-gass-copy-debuginfo globus-gass-copy-devel globus-gass-copy-doc globus-gass-copy-progs globus-gass-server-ez globus-gass-server-ez-debuginfo globus-gass-server-ez-devel globus-gass-server-ez-progs globus-gass-transfer globus-gass-transfer-debuginfo globus-gass-transfer-devel globus-gass-transfer-doc globus-gfork globus-gfork-debuginfo globus-gfork-devel globus-gfork-progs globus-gridftp-osg-extensions globus-gridftp-osg-extensions-debuginfo globus-gridftp-server globus-gridftp-server-control globus-gridftp-server-control-debuginfo globus-gridftp-server-control-devel globus-gridftp-server-debuginfo globus-gridftp-server-devel globus-gridftp-server-progs globus-gridmap-callout-error globus-gridmap-callout-error-debuginfo globus-gridmap-callout-error-devel globus-gridmap-callout-error-doc globus-gsi-callback globus-gsi-callback-debuginfo globus-gsi-callback-devel globus-gsi-callback-doc globus-gsi-cert-utils globus-gsi-cert-utils-debuginfo globus-gsi-cert-utils-devel globus-gsi-cert-utils-doc globus-gsi-cert-utils-progs globus-gsi-credential globus-gsi-credential-debuginfo globus-gsi-credential-devel globus-gsi-credential-doc globus-gsi-openssl-error globus-gsi-openssl-error-debuginfo globus-gsi-openssl-error-devel globus-gsi-openssl-error-doc globus-gsi-proxy-core globus-gsi-proxy-core-debuginfo globus-gsi-proxy-core-devel globus-gsi-proxy-core-doc globus-gsi-proxy-ssl globus-gsi-proxy-ssl-debuginfo globus-gsi-proxy-ssl-devel globus-gsi-proxy-ssl-doc globus-gsi-sysconfig globus-gsi-sysconfig-debuginfo globus-gsi-sysconfig-devel globus-gsi-sysconfig-doc globus-gssapi-error globus-gssapi-error-debuginfo globus-gssapi-error-devel globus-gssapi-error-doc globus-gssapi-gsi globus-gssapi-gsi-debuginfo globus-gssapi-gsi-devel globus-gssapi-gsi-doc globus-gss-assist globus-gss-assist-debuginfo globus-gss-assist-devel globus-gss-assist-doc globus-gss-assist-progs globus-io globus-io-debuginfo globus-io-devel globus-openssl-module globus-openssl-module-debuginfo globus-openssl-module-devel globus-openssl-module-doc globus-proxy-utils globus-proxy-utils-debuginfo globus-rsl globus-rsl-debuginfo globus-rsl-devel globus-rsl-doc globus-simple-ca globus-usage globus-usage-debuginfo globus-usage-devel globus-xio globus-xio-debuginfo globus-xio-devel globus-xio-doc globus-xio-gsi-driver globus-xio-gsi-driver-debuginfo globus-xio-gsi-driver-devel globus-xio-gsi-driver-doc globus-xioperf globus-xioperf-debuginfo globus-xio-pipe-driver globus-xio-pipe-driver-debuginfo globus-xio-pipe-driver-devel globus-xio-popen-driver globus-xio-popen-driver-debuginfo globus-xio-popen-driver-devel globus-xio-udt-driver globus-xio-udt-driver-debuginfo globus-xio-udt-driver-devel gridftp-hdfs gridftp-hdfs-debuginfo gums gums-client gums-service htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-slurm htcondor-ce-view igtf-ca-certs myproxy myproxy-admin myproxy-debuginfo myproxy-devel myproxy-doc myproxy-libs myproxy-server myproxy-voms osg-ca-certs osg-ca-scripts osg-configure osg-configure-bosco osg-configure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-test osg-test-log-viewer osg-version xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-hdfs xrootd-hdfs-debuginfo xrootd-hdfs-devel xrootd-libs xrootd-private-devel xrootd-python xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-28/#enterprise-linux-6_1", 
            "text": "blahp-1.18.33.bosco-1.osg33.el6\nblahp-debuginfo-1.18.33.bosco-1.osg33.el6\nglobus-authz-3.15-1.osg33.el6\nglobus-authz-callout-error-3.6-1.osg33.el6\nglobus-authz-callout-error-debuginfo-3.6-1.osg33.el6\nglobus-authz-callout-error-devel-3.6-1.osg33.el6\nglobus-authz-callout-error-doc-3.6-1.osg33.el6\nglobus-authz-debuginfo-3.15-1.osg33.el6\nglobus-authz-devel-3.15-1.osg33.el6\nglobus-authz-doc-3.15-1.osg33.el6\nglobus-callout-3.15-1.osg33.el6\nglobus-callout-debuginfo-3.15-1.osg33.el6\nglobus-callout-devel-3.15-1.osg33.el6\nglobus-callout-doc-3.15-1.osg33.el6\nglobus-common-17.1-1.osg33.el6\nglobus-common-debuginfo-17.1-1.osg33.el6\nglobus-common-devel-17.1-1.osg33.el6\nglobus-common-doc-17.1-1.osg33.el6\nglobus-common-progs-17.1-1.osg33.el6\nglobus-ftp-client-8.36-1.1.osg33.el6\nglobus-ftp-client-debuginfo-8.36-1.1.osg33.el6\nglobus-ftp-client-devel-8.36-1.1.osg33.el6\nglobus-ftp-client-doc-8.36-1.1.osg33.el6\nglobus-ftp-control-7.8-1.osg33.el6\nglobus-ftp-control-debuginfo-7.8-1.osg33.el6\nglobus-ftp-control-devel-7.8-1.osg33.el6\nglobus-ftp-control-doc-7.8-1.osg33.el6\nglobus-gass-cache-9.10-1.osg33.el6\nglobus-gass-cache-debuginfo-9.10-1.osg33.el6\nglobus-gass-cache-devel-9.10-1.osg33.el6\nglobus-gass-cache-doc-9.10-1.osg33.el6\nglobus-gass-cache-program-6.7-1.osg33.el6\nglobus-gass-cache-program-debuginfo-6.7-1.osg33.el6\nglobus-gass-copy-9.27-1.osg33.el6\nglobus-gass-copy-debuginfo-9.27-1.osg33.el6\nglobus-gass-copy-devel-9.27-1.osg33.el6\nglobus-gass-copy-doc-9.27-1.osg33.el6\nglobus-gass-copy-progs-9.27-1.osg33.el6\nglobus-gass-server-ez-5.8-1.osg33.el6\nglobus-gass-server-ez-debuginfo-5.8-1.osg33.el6\nglobus-gass-server-ez-devel-5.8-1.osg33.el6\nglobus-gass-server-ez-progs-5.8-1.osg33.el6\nglobus-gass-transfer-8.10-1.osg33.el6\nglobus-gass-transfer-debuginfo-8.10-1.osg33.el6\nglobus-gass-transfer-devel-8.10-1.osg33.el6\nglobus-gass-transfer-doc-8.10-1.osg33.el6\nglobus-gfork-4.9-1.osg33.el6\nglobus-gfork-debuginfo-4.9-1.osg33.el6\nglobus-gfork-devel-4.9-1.osg33.el6\nglobus-gfork-progs-4.9-1.osg33.el6\nglobus-gridftp-osg-extensions-0.4-1.osg33.el6\nglobus-gridftp-osg-extensions-debuginfo-0.4-1.osg33.el6\nglobus-gridftp-server-12.2-1.1.osg33.el6\nglobus-gridftp-server-control-5.1-1.1.osg33.el6\nglobus-gridftp-server-control-debuginfo-5.1-1.1.osg33.el6\nglobus-gridftp-server-control-devel-5.1-1.1.osg33.el6\nglobus-gridftp-server-debuginfo-12.2-1.1.osg33.el6\nglobus-gridftp-server-devel-12.2-1.1.osg33.el6\nglobus-gridftp-server-progs-12.2-1.1.osg33.el6\nglobus-gridmap-callout-error-2.5-1.osg33.el6\nglobus-gridmap-callout-error-debuginfo-2.5-1.osg33.el6\nglobus-gridmap-callout-error-devel-2.5-1.osg33.el6\nglobus-gridmap-callout-error-doc-2.5-1.osg33.el6\nglobus-gsi-callback-5.13-1.osg33.el6\nglobus-gsi-callback-debuginfo-5.13-1.osg33.el6\nglobus-gsi-callback-devel-5.13-1.osg33.el6\nglobus-gsi-callback-doc-5.13-1.osg33.el6\nglobus-gsi-cert-utils-9.16-1.osg33.el6\nglobus-gsi-cert-utils-debuginfo-9.16-1.osg33.el6\nglobus-gsi-cert-utils-devel-9.16-1.osg33.el6\nglobus-gsi-cert-utils-doc-9.16-1.osg33.el6\nglobus-gsi-cert-utils-progs-9.16-1.osg33.el6\nglobus-gsi-credential-7.11-1.osg33.el6\nglobus-gsi-credential-debuginfo-7.11-1.osg33.el6\nglobus-gsi-credential-devel-7.11-1.osg33.el6\nglobus-gsi-credential-doc-7.11-1.osg33.el6\nglobus-gsi-openssl-error-3.8-1.osg33.el6\nglobus-gsi-openssl-error-debuginfo-3.8-1.osg33.el6\nglobus-gsi-openssl-error-devel-3.8-1.osg33.el6\nglobus-gsi-openssl-error-doc-3.8-1.osg33.el6\nglobus-gsi-proxy-core-8.6-1.osg33.el6\nglobus-gsi-proxy-core-debuginfo-8.6-1.osg33.el6\nglobus-gsi-proxy-core-devel-8.6-1.osg33.el6\nglobus-gsi-proxy-core-doc-8.6-1.osg33.el6\nglobus-gsi-proxy-ssl-5.10-1.osg33.el6\nglobus-gsi-proxy-ssl-debuginfo-5.10-1.osg33.el6\nglobus-gsi-proxy-ssl-devel-5.10-1.osg33.el6\nglobus-gsi-proxy-ssl-doc-5.10-1.osg33.el6\nglobus-gsi-sysconfig-6.11-1.osg33.el6\nglobus-gsi-sysconfig-debuginfo-6.11-1.osg33.el6\nglobus-gsi-sysconfig-devel-6.11-1.osg33.el6\nglobus-gsi-sysconfig-doc-6.11-1.osg33.el6\nglobus-gssapi-error-5.5-1.osg33.el6\nglobus-gssapi-error-debuginfo-5.5-1.osg33.el6\nglobus-gssapi-error-devel-5.5-1.osg33.el6\nglobus-gssapi-error-doc-5.5-1.osg33.el6\nglobus-gssapi-gsi-12.17-3.osg33.el6\nglobus-gssapi-gsi-debuginfo-12.17-3.osg33.el6\nglobus-gssapi-gsi-devel-12.17-3.osg33.el6\nglobus-gssapi-gsi-doc-12.17-3.osg33.el6\nglobus-gss-assist-10.21-1.osg33.el6\nglobus-gss-assist-debuginfo-10.21-1.osg33.el6\nglobus-gss-assist-devel-10.21-1.osg33.el6\nglobus-gss-assist-doc-10.21-1.osg33.el6\nglobus-gss-assist-progs-10.21-1.osg33.el6\nglobus-io-11.9-1.osg33.el6\nglobus-io-debuginfo-11.9-1.osg33.el6\nglobus-io-devel-11.9-1.osg33.el6\nglobus-openssl-module-4.8-1.osg33.el6\nglobus-openssl-module-debuginfo-4.8-1.osg33.el6\nglobus-openssl-module-devel-4.8-1.osg33.el6\nglobus-openssl-module-doc-4.8-1.osg33.el6\nglobus-proxy-utils-6.19-1.osg33.el6\nglobus-proxy-utils-debuginfo-6.19-1.osg33.el6\nglobus-rsl-10.11-1.osg33.el6\nglobus-rsl-debuginfo-10.11-1.osg33.el6\nglobus-rsl-devel-10.11-1.osg33.el6\nglobus-rsl-doc-10.11-1.osg33.el6\nglobus-simple-ca-4.24-1.osg33.el6\nglobus-usage-4.5-1.osg33.el6\nglobus-usage-debuginfo-4.5-1.osg33.el6\nglobus-usage-devel-4.5-1.osg33.el6\nglobus-xio-5.16-1.osg33.el6\nglobus-xio-debuginfo-5.16-1.osg33.el6\nglobus-xio-devel-5.16-1.osg33.el6\nglobus-xio-doc-5.16-1.osg33.el6\nglobus-xio-gsi-driver-3.11-1.osg33.el6\nglobus-xio-gsi-driver-debuginfo-3.11-1.osg33.el6\nglobus-xio-gsi-driver-devel-3.11-1.osg33.el6\nglobus-xio-gsi-driver-doc-3.11-1.osg33.el6\nglobus-xioperf-4.5-1.osg33.el6\nglobus-xioperf-debuginfo-4.5-1.osg33.el6\nglobus-xio-pipe-driver-3.10-1.osg33.el6\nglobus-xio-pipe-driver-debuginfo-3.10-1.osg33.el6\nglobus-xio-pipe-driver-devel-3.10-1.osg33.el6\nglobus-xio-popen-driver-3.6-1.osg33.el6\nglobus-xio-popen-driver-debuginfo-3.6-1.osg33.el6\nglobus-xio-popen-driver-devel-3.6-1.osg33.el6\nglobus-xio-udt-driver-1.28-1.osg33.el6\nglobus-xio-udt-driver-debuginfo-1.28-1.osg33.el6\nglobus-xio-udt-driver-devel-1.28-1.osg33.el6\ngridftp-hdfs-1.0-1.1.osg33.el6\ngridftp-hdfs-debuginfo-1.0-1.1.osg33.el6\ngums-1.5.2-10.osg33.el6\ngums-client-1.5.2-10.osg33.el6\ngums-service-1.5.2-10.osg33.el6\nhtcondor-ce-2.2.3-1.osg33.el6\nhtcondor-ce-bosco-2.2.3-1.osg33.el6\nhtcondor-ce-client-2.2.3-1.osg33.el6\nhtcondor-ce-collector-2.2.3-1.osg33.el6\nhtcondor-ce-condor-2.2.3-1.osg33.el6\nhtcondor-ce-lsf-2.2.3-1.osg33.el6\nhtcondor-ce-pbs-2.2.3-1.osg33.el6\nhtcondor-ce-sge-2.2.3-1.osg33.el6\nhtcondor-ce-slurm-2.2.3-1.osg33.el6\nhtcondor-ce-view-2.2.3-1.osg33.el6\nmyproxy-6.1.28-1.1.osg33.el6\nmyproxy-admin-6.1.28-1.1.osg33.el6\nmyproxy-debuginfo-6.1.28-1.1.osg33.el6\nmyproxy-devel-6.1.28-1.1.osg33.el6\nmyproxy-doc-6.1.28-1.1.osg33.el6\nmyproxy-libs-6.1.28-1.1.osg33.el6\nmyproxy-server-6.1.28-1.1.osg33.el6\nmyproxy-voms-6.1.28-1.1.osg33.el6\nosg-ca-scripts-1.1.7-1.osg33.el6\nosg-configure-1.10.0-1.osg33.el6\nosg-configure-bosco-1.10.0-1.osg33.el6\nosg-configure-ce-1.10.0-1.osg33.el6\nosg-configure-cemon-1.10.0-1.osg33.el6\nosg-configure-condor-1.10.0-1.osg33.el6\nosg-configure-gateway-1.10.0-1.osg33.el6\nosg-configure-gip-1.10.0-1.osg33.el6\nosg-configure-gratia-1.10.0-1.osg33.el6\nosg-configure-infoservices-1.10.0-1.osg33.el6\nosg-configure-lsf-1.10.0-1.osg33.el6\nosg-configure-managedfork-1.10.0-1.osg33.el6\nosg-configure-misc-1.10.0-1.osg33.el6\nosg-configure-monalisa-1.10.0-1.osg33.el6\nosg-configure-network-1.10.0-1.osg33.el6\nosg-configure-pbs-1.10.0-1.osg33.el6\nosg-configure-rsv-1.10.0-1.osg33.el6\nosg-configure-sge-1.10.0-1.osg33.el6\nosg-configure-slurm-1.10.0-1.osg33.el6\nosg-configure-squid-1.10.0-1.osg33.el6\nosg-configure-tests-1.10.0-1.osg33.el6\nosg-test-1.11.2-1.osg33.el6\nosg-test-log-viewer-1.11.2-1.osg33.el6\nosg-version-3.3.28-1.osg33.el6\nxrootd-4.7.0-1.osg33.el6\nxrootd-client-4.7.0-1.osg33.el6\nxrootd-client-devel-4.7.0-1.osg33.el6\nxrootd-client-libs-4.7.0-1.osg33.el6\nxrootd-debuginfo-4.7.0-1.osg33.el6\nxrootd-devel-4.7.0-1.osg33.el6\nxrootd-doc-4.7.0-1.osg33.el6\nxrootd-fuse-4.7.0-1.osg33.el6\nxrootd-hdfs-1.9.2-2.osg33.el6\nxrootd-hdfs-debuginfo-1.9.2-2.osg33.el6\nxrootd-hdfs-devel-1.9.2-2.osg33.el6\nxrootd-libs-4.7.0-1.osg33.el6\nxrootd-private-devel-4.7.0-1.osg33.el6\nxrootd-python-4.7.0-1.osg33.el6\nxrootd-selinux-4.7.0-1.osg33.el6\nxrootd-server-4.7.0-1.osg33.el6\nxrootd-server-devel-4.7.0-1.osg33.el6\nxrootd-server-libs-4.7.0-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-28/#enterprise-linux-7_1", 
            "text": "blahp-1.18.33.bosco-1.osg33.el7\nblahp-debuginfo-1.18.33.bosco-1.osg33.el7\nglobus-authz-3.15-1.osg33.el7\nglobus-authz-callout-error-3.6-1.osg33.el7\nglobus-authz-callout-error-debuginfo-3.6-1.osg33.el7\nglobus-authz-callout-error-devel-3.6-1.osg33.el7\nglobus-authz-callout-error-doc-3.6-1.osg33.el7\nglobus-authz-debuginfo-3.15-1.osg33.el7\nglobus-authz-devel-3.15-1.osg33.el7\nglobus-authz-doc-3.15-1.osg33.el7\nglobus-callout-3.15-1.osg33.el7\nglobus-callout-debuginfo-3.15-1.osg33.el7\nglobus-callout-devel-3.15-1.osg33.el7\nglobus-callout-doc-3.15-1.osg33.el7\nglobus-common-17.1-1.osg33.el7\nglobus-common-debuginfo-17.1-1.osg33.el7\nglobus-common-devel-17.1-1.osg33.el7\nglobus-common-doc-17.1-1.osg33.el7\nglobus-common-progs-17.1-1.osg33.el7\nglobus-ftp-client-8.36-1.1.osg33.el7\nglobus-ftp-client-debuginfo-8.36-1.1.osg33.el7\nglobus-ftp-client-devel-8.36-1.1.osg33.el7\nglobus-ftp-client-doc-8.36-1.1.osg33.el7\nglobus-ftp-control-7.8-1.osg33.el7\nglobus-ftp-control-debuginfo-7.8-1.osg33.el7\nglobus-ftp-control-devel-7.8-1.osg33.el7\nglobus-ftp-control-doc-7.8-1.osg33.el7\nglobus-gass-cache-9.10-1.osg33.el7\nglobus-gass-cache-debuginfo-9.10-1.osg33.el7\nglobus-gass-cache-devel-9.10-1.osg33.el7\nglobus-gass-cache-doc-9.10-1.osg33.el7\nglobus-gass-cache-program-6.7-1.osg33.el7\nglobus-gass-cache-program-debuginfo-6.7-1.osg33.el7\nglobus-gass-copy-9.27-1.osg33.el7\nglobus-gass-copy-debuginfo-9.27-1.osg33.el7\nglobus-gass-copy-devel-9.27-1.osg33.el7\nglobus-gass-copy-doc-9.27-1.osg33.el7\nglobus-gass-copy-progs-9.27-1.osg33.el7\nglobus-gass-server-ez-5.8-1.osg33.el7\nglobus-gass-server-ez-debuginfo-5.8-1.osg33.el7\nglobus-gass-server-ez-devel-5.8-1.osg33.el7\nglobus-gass-server-ez-progs-5.8-1.osg33.el7\nglobus-gass-transfer-8.10-1.osg33.el7\nglobus-gass-transfer-debuginfo-8.10-1.osg33.el7\nglobus-gass-transfer-devel-8.10-1.osg33.el7\nglobus-gass-transfer-doc-8.10-1.osg33.el7\nglobus-gfork-4.9-1.osg33.el7\nglobus-gfork-debuginfo-4.9-1.osg33.el7\nglobus-gfork-devel-4.9-1.osg33.el7\nglobus-gfork-progs-4.9-1.osg33.el7\nglobus-gridftp-osg-extensions-0.4-1.osg33.el7\nglobus-gridftp-osg-extensions-debuginfo-0.4-1.osg33.el7\nglobus-gridftp-server-12.2-1.1.osg33.el7\nglobus-gridftp-server-control-5.1-1.1.osg33.el7\nglobus-gridftp-server-control-debuginfo-5.1-1.1.osg33.el7\nglobus-gridftp-server-control-devel-5.1-1.1.osg33.el7\nglobus-gridftp-server-debuginfo-12.2-1.1.osg33.el7\nglobus-gridftp-server-devel-12.2-1.1.osg33.el7\nglobus-gridftp-server-progs-12.2-1.1.osg33.el7\nglobus-gridmap-callout-error-2.5-1.osg33.el7\nglobus-gridmap-callout-error-debuginfo-2.5-1.osg33.el7\nglobus-gridmap-callout-error-devel-2.5-1.osg33.el7\nglobus-gridmap-callout-error-doc-2.5-1.osg33.el7\nglobus-gsi-callback-5.13-1.osg33.el7\nglobus-gsi-callback-debuginfo-5.13-1.osg33.el7\nglobus-gsi-callback-devel-5.13-1.osg33.el7\nglobus-gsi-callback-doc-5.13-1.osg33.el7\nglobus-gsi-cert-utils-9.16-1.osg33.el7\nglobus-gsi-cert-utils-debuginfo-9.16-1.osg33.el7\nglobus-gsi-cert-utils-devel-9.16-1.osg33.el7\nglobus-gsi-cert-utils-doc-9.16-1.osg33.el7\nglobus-gsi-cert-utils-progs-9.16-1.osg33.el7\nglobus-gsi-credential-7.11-1.osg33.el7\nglobus-gsi-credential-debuginfo-7.11-1.osg33.el7\nglobus-gsi-credential-devel-7.11-1.osg33.el7\nglobus-gsi-credential-doc-7.11-1.osg33.el7\nglobus-gsi-openssl-error-3.8-1.osg33.el7\nglobus-gsi-openssl-error-debuginfo-3.8-1.osg33.el7\nglobus-gsi-openssl-error-devel-3.8-1.osg33.el7\nglobus-gsi-openssl-error-doc-3.8-1.osg33.el7\nglobus-gsi-proxy-core-8.6-1.osg33.el7\nglobus-gsi-proxy-core-debuginfo-8.6-1.osg33.el7\nglobus-gsi-proxy-core-devel-8.6-1.osg33.el7\nglobus-gsi-proxy-core-doc-8.6-1.osg33.el7\nglobus-gsi-proxy-ssl-5.10-1.osg33.el7\nglobus-gsi-proxy-ssl-debuginfo-5.10-1.osg33.el7\nglobus-gsi-proxy-ssl-devel-5.10-1.osg33.el7\nglobus-gsi-proxy-ssl-doc-5.10-1.osg33.el7\nglobus-gsi-sysconfig-6.11-1.osg33.el7\nglobus-gsi-sysconfig-debuginfo-6.11-1.osg33.el7\nglobus-gsi-sysconfig-devel-6.11-1.osg33.el7\nglobus-gsi-sysconfig-doc-6.11-1.osg33.el7\nglobus-gssapi-error-5.5-1.osg33.el7\nglobus-gssapi-error-debuginfo-5.5-1.osg33.el7\nglobus-gssapi-error-devel-5.5-1.osg33.el7\nglobus-gssapi-error-doc-5.5-1.osg33.el7\nglobus-gssapi-gsi-12.17-3.osg33.el7\nglobus-gssapi-gsi-debuginfo-12.17-3.osg33.el7\nglobus-gssapi-gsi-devel-12.17-3.osg33.el7\nglobus-gssapi-gsi-doc-12.17-3.osg33.el7\nglobus-gss-assist-10.21-1.osg33.el7\nglobus-gss-assist-debuginfo-10.21-1.osg33.el7\nglobus-gss-assist-devel-10.21-1.osg33.el7\nglobus-gss-assist-doc-10.21-1.osg33.el7\nglobus-gss-assist-progs-10.21-1.osg33.el7\nglobus-io-11.9-1.osg33.el7\nglobus-io-debuginfo-11.9-1.osg33.el7\nglobus-io-devel-11.9-1.osg33.el7\nglobus-openssl-module-4.8-1.osg33.el7\nglobus-openssl-module-debuginfo-4.8-1.osg33.el7\nglobus-openssl-module-devel-4.8-1.osg33.el7\nglobus-openssl-module-doc-4.8-1.osg33.el7\nglobus-proxy-utils-6.19-1.osg33.el7\nglobus-proxy-utils-debuginfo-6.19-1.osg33.el7\nglobus-rsl-10.11-1.osg33.el7\nglobus-rsl-debuginfo-10.11-1.osg33.el7\nglobus-rsl-devel-10.11-1.osg33.el7\nglobus-rsl-doc-10.11-1.osg33.el7\nglobus-simple-ca-4.24-1.osg33.el7\nglobus-usage-4.5-1.osg33.el7\nglobus-usage-debuginfo-4.5-1.osg33.el7\nglobus-usage-devel-4.5-1.osg33.el7\nglobus-xio-5.16-1.osg33.el7\nglobus-xio-debuginfo-5.16-1.osg33.el7\nglobus-xio-devel-5.16-1.osg33.el7\nglobus-xio-doc-5.16-1.osg33.el7\nglobus-xio-gsi-driver-3.11-1.osg33.el7\nglobus-xio-gsi-driver-debuginfo-3.11-1.osg33.el7\nglobus-xio-gsi-driver-devel-3.11-1.osg33.el7\nglobus-xio-gsi-driver-doc-3.11-1.osg33.el7\nglobus-xioperf-4.5-1.osg33.el7\nglobus-xioperf-debuginfo-4.5-1.osg33.el7\nglobus-xio-pipe-driver-3.10-1.osg33.el7\nglobus-xio-pipe-driver-debuginfo-3.10-1.osg33.el7\nglobus-xio-pipe-driver-devel-3.10-1.osg33.el7\nglobus-xio-popen-driver-3.6-1.osg33.el7\nglobus-xio-popen-driver-debuginfo-3.6-1.osg33.el7\nglobus-xio-popen-driver-devel-3.6-1.osg33.el7\nglobus-xio-udt-driver-1.28-1.osg33.el7\nglobus-xio-udt-driver-debuginfo-1.28-1.osg33.el7\nglobus-xio-udt-driver-devel-1.28-1.osg33.el7\ngridftp-hdfs-1.0-1.1.osg33.el7\ngridftp-hdfs-debuginfo-1.0-1.1.osg33.el7\ngums-1.5.2-10.osg33.el7\ngums-client-1.5.2-10.osg33.el7\ngums-service-1.5.2-10.osg33.el7\nhtcondor-ce-2.2.3-1.osg33.el7\nhtcondor-ce-bosco-2.2.3-1.osg33.el7\nhtcondor-ce-client-2.2.3-1.osg33.el7\nhtcondor-ce-collector-2.2.3-1.osg33.el7\nhtcondor-ce-condor-2.2.3-1.osg33.el7\nhtcondor-ce-lsf-2.2.3-1.osg33.el7\nhtcondor-ce-pbs-2.2.3-1.osg33.el7\nhtcondor-ce-sge-2.2.3-1.osg33.el7\nhtcondor-ce-slurm-2.2.3-1.osg33.el7\nhtcondor-ce-view-2.2.3-1.osg33.el7\nmyproxy-6.1.28-1.1.osg33.el7\nmyproxy-admin-6.1.28-1.1.osg33.el7\nmyproxy-debuginfo-6.1.28-1.1.osg33.el7\nmyproxy-devel-6.1.28-1.1.osg33.el7\nmyproxy-doc-6.1.28-1.1.osg33.el7\nmyproxy-libs-6.1.28-1.1.osg33.el7\nmyproxy-server-6.1.28-1.1.osg33.el7\nmyproxy-voms-6.1.28-1.1.osg33.el7\nosg-ca-scripts-1.1.7-1.osg33.el7\nosg-configure-1.10.0-1.osg33.el7\nosg-configure-bosco-1.10.0-1.osg33.el7\nosg-configure-ce-1.10.0-1.osg33.el7\nosg-configure-cemon-1.10.0-1.osg33.el7\nosg-configure-condor-1.10.0-1.osg33.el7\nosg-configure-gateway-1.10.0-1.osg33.el7\nosg-configure-gip-1.10.0-1.osg33.el7\nosg-configure-gratia-1.10.0-1.osg33.el7\nosg-configure-infoservices-1.10.0-1.osg33.el7\nosg-configure-lsf-1.10.0-1.osg33.el7\nosg-configure-managedfork-1.10.0-1.osg33.el7\nosg-configure-misc-1.10.0-1.osg33.el7\nosg-configure-monalisa-1.10.0-1.osg33.el7\nosg-configure-network-1.10.0-1.osg33.el7\nosg-configure-pbs-1.10.0-1.osg33.el7\nosg-configure-rsv-1.10.0-1.osg33.el7\nosg-configure-sge-1.10.0-1.osg33.el7\nosg-configure-slurm-1.10.0-1.osg33.el7\nosg-configure-squid-1.10.0-1.osg33.el7\nosg-configure-tests-1.10.0-1.osg33.el7\nosg-test-1.11.2-1.osg33.el7\nosg-test-log-viewer-1.11.2-1.osg33.el7\nosg-version-3.3.28-1.osg33.el7\nstashcache-0.8-1.osg33.el7\nstashcache-cache-server-0.8-1.osg33.el7\nstashcache-daemon-0.8-1.osg33.el7\nstashcache-origin-server-0.8-1.osg33.el7\nxrootd-4.7.0-1.osg33.el7\nxrootd-client-4.7.0-1.osg33.el7\nxrootd-client-devel-4.7.0-1.osg33.el7\nxrootd-client-libs-4.7.0-1.osg33.el7\nxrootd-debuginfo-4.7.0-1.osg33.el7\nxrootd-devel-4.7.0-1.osg33.el7\nxrootd-doc-4.7.0-1.osg33.el7\nxrootd-fuse-4.7.0-1.osg33.el7\nxrootd-hdfs-1.9.2-2.osg33.el7\nxrootd-hdfs-debuginfo-1.9.2-2.osg33.el7\nxrootd-hdfs-devel-1.9.2-2.osg33.el7\nxrootd-lcmaps-1.3.4-1.osg33.el7\nxrootd-lcmaps-debuginfo-1.3.4-1.osg33.el7\nxrootd-libs-4.7.0-1.osg33.el7\nxrootd-private-devel-4.7.0-1.osg33.el7\nxrootd-python-4.7.0-1.osg33.el7\nxrootd-selinux-4.7.0-1.osg33.el7\nxrootd-server-4.7.0-1.osg33.el7\nxrootd-server-devel-4.7.0-1.osg33.el7\nxrootd-server-libs-4.7.0-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/", 
            "text": "OSG Software Stack -- Data Release -- 3.4.2-2 and 3.3.27-2\n\n\nRelease Date\n: 2017-08-14\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nCA Certificates based on \nIGTF 1.85\n\n\nUpdated URL domain information for CyGrid (CY)\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nOSG 3.3\n\n\nEnterprise Linux 6\n\n\n\n\nigtf-ca-certs-1.85-1.osg33.el6\n\n\nosg-ca-certs-1.65-1.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nigtf-ca-certs-1.85-1.osg33.el7\n\n\nosg-ca-certs-1.65-1.osg33.el7\n\n\n\n\nOSG 3.4\n\n\nEnterprise Linux 6\n\n\n\n\nigtf-ca-certs-1.85-1.osg34.el6\n\n\nosg-ca-certs-1.65-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nigtf-ca-certs-1.85-1.osg34.el7\n\n\nosg-ca-certs-1.65-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nigtf-ca-certs osg-ca-certs\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nOSG 3.3\n\n\nEnterprise Linux 6\n\n\nigtf-ca-certs-1.85-1.osg33.el6\nosg-ca-certs-1.65-1.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nigtf-ca-certs-1.85-1.osg33.el7\nosg-ca-certs-1.65-1.osg33.el7\n\n\n\n\n\nOSG 3.4\n\n\nEnterprise Linux 6\n\n\nigtf-ca-certs-1.85-1.osg34.el6\nosg-ca-certs-1.65-1.osg34.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nigtf-ca-certs-1.85-1.osg34.el7\nosg-ca-certs-1.65-1.osg34.el7", 
            "title": "OSG Release 3.4.2-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/#osg-software-stack-data-release-342-2-and-3327-2", 
            "text": "Release Date : 2017-08-14", 
            "title": "OSG Software Stack -- Data Release -- 3.4.2-2 and 3.3.27-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/#summary-of-changes", 
            "text": "This release contains:   CA Certificates based on  IGTF 1.85  Updated URL domain information for CyGrid (CY)     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/#osg-33", 
            "text": "", 
            "title": "OSG 3.3"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/#enterprise-linux-6", 
            "text": "igtf-ca-certs-1.85-1.osg33.el6  osg-ca-certs-1.65-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/#enterprise-linux-7", 
            "text": "igtf-ca-certs-1.85-1.osg33.el7  osg-ca-certs-1.65-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/#osg-34", 
            "text": "", 
            "title": "OSG 3.4"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/#enterprise-linux-6_1", 
            "text": "igtf-ca-certs-1.85-1.osg34.el6  osg-ca-certs-1.65-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/#enterprise-linux-7_1", 
            "text": "igtf-ca-certs-1.85-1.osg34.el7  osg-ca-certs-1.65-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  igtf-ca-certs osg-ca-certs  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/#osg-33_1", 
            "text": "", 
            "title": "OSG 3.3"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/#enterprise-linux-6_2", 
            "text": "igtf-ca-certs-1.85-1.osg33.el6\nosg-ca-certs-1.65-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/#enterprise-linux-7_2", 
            "text": "igtf-ca-certs-1.85-1.osg33.el7\nosg-ca-certs-1.65-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/#osg-34_1", 
            "text": "", 
            "title": "OSG 3.4"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/#enterprise-linux-6_3", 
            "text": "igtf-ca-certs-1.85-1.osg34.el6\nosg-ca-certs-1.65-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/#enterprise-linux-7_3", 
            "text": "igtf-ca-certs-1.85-1.osg34.el7\nosg-ca-certs-1.65-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/", 
            "text": "OSG Software Release 3.4.2\n\n\nRelease Date\n: 2017-08-08\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nHTCondor 8.6.5\n\n\nUpdated SELinux profile which is required on Red Hat 7.4\n\n\nFixed a memory leak that would cause the condor_collector to slowly grow\n\n\nFixed several issues that occur when IPv6 is in use\n\n\nFixed a bug where condor_rm rarely removed another one of the user's jobs\n\n\nFixed a bug with parallel universe jobs starting on partitionable slots\n\n\n\n\n\n\nHTCondor-CE\n\n\nAdded pilot payload auditing\n\n\nDo not hold running jobs with an expired proxy\n\n\nInitialize \n$SPOOL/ceview/vos\n directory at installation time so that the VO tab functions in CEView before any pilots are received\n\n\nDon't warn about not running osg-configure, if osg-configure is not installed\n\n\n\n\n\n\nDefault configuration improvements for condor-cron\n\n\nClarified how to override the \nCONDOR_IDS\n\n\nDisable the unused GSI authentication that was producing spurious log messages.\n\n\n\n\n\n\nSeveral improvements to osg-configure\n\n\nEnsure that GUMS is configured before trying to query the GUMS server\n\n\nProgress updates (such as informing when an operation is expected to take a while) during the user VO file validation are presented to the user rather than being sent to the log file.\n\n\nosg-configure will issue same warnings and errors with -v option as it does with the -c option.\n\n\n\n\n\n\nAdded blahp configuration option for PBS Pro. This option must be used when the disambiguation code does not correctly determine which PBS is in use.\n\n\nReorganize the osg-ce packages \nSOFTWARE-2768\n\n\nUpcoming: patched HTCondor 8.7.2\n\n\nUpdated SELinux profile which is required on Red Hat 7.4\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n OSG 3.4 contains only 64-bit components. \n StashCache is supported on EL7 only. \n xrootd-lcmaps will remain at 1.2.1-2 on EL6.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nKnown Issues\n\n\n\n\nBecause the Gratia system has been turned off, RSV emits the following warning: \nWARNING: Gratia server gratia-osg-prod.opensciencegrid.org:80 not responding to obtain last contact details.\n. This warning can safely be ignored.\n\n\nUsing the LCMAPS VOMS will result in a failing \"supported VO\" RSV test (\nSOFTWARE-2763\n). This can be ignored and a fix is targeted for the August release.\n\n\nIn GlideinWMS, a small configuration change must be added to account for changes in HTCondor 8.6. Add the following line to the HTCondor configuration.\n\n\n\n\nCOLLECTOR.USE_SHARED_PORT=False\n\n\n\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.32.bosco-1.osg34.el6\n\n\ncondor-8.6.5-2.osg34.el6\n\n\ncondor-cron-1.1.3-1.osg34.el6\n\n\nhtcondor-ce-3.0.1-1.osg34.el6\n\n\nosg-ce-3.4-3.osg34.el6\n\n\nosg-configure-2.1.1-1.osg34.el6\n\n\nosg-test-1.11.1-1.osg34.el6\n\n\nosg-tested-internal-3.4-4.osg34.el6\n\n\nosg-version-3.4.2-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.32.bosco-1.osg34.el7\n\n\ncondor-8.6.5-2.osg34.el7\n\n\ncondor-cron-1.1.3-1.osg34.el7\n\n\nhtcondor-ce-3.0.1-1.osg34.el7\n\n\nosg-ce-3.4-3.osg34.el7\n\n\nosg-configure-2.1.1-1.osg34.el7\n\n\nosg-test-1.11.1-1.osg34.el7\n\n\nosg-tested-internal-3.4-4.osg34.el7\n\n\nosg-version-3.4.2-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-cron condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-slurm htcondor-ce-view igtf-ca-certs osg-ca-certs osg-ce osg-ce-bosco osg-ce-condor osg-ce-lsf osg-ce-pbs osg-ce-sge osg-ce-slurm osg-configure osg-configure-bosco osg-configure-ce osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-test osg-tested-internal osg-test-log-viewer osg-version\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp-1.18.32.bosco-1.osg34.el6\nblahp-debuginfo-1.18.32.bosco-1.osg34.el6\ncondor-8.6.5-2.osg34.el6\ncondor-all-8.6.5-2.osg34.el6\ncondor-bosco-8.6.5-2.osg34.el6\ncondor-classads-8.6.5-2.osg34.el6\ncondor-classads-devel-8.6.5-2.osg34.el6\ncondor-cream-gahp-8.6.5-2.osg34.el6\ncondor-cron-1.1.3-1.osg34.el6\ncondor-debuginfo-8.6.5-2.osg34.el6\ncondor-kbdd-8.6.5-2.osg34.el6\ncondor-procd-8.6.5-2.osg34.el6\ncondor-python-8.6.5-2.osg34.el6\ncondor-std-universe-8.6.5-2.osg34.el6\ncondor-test-8.6.5-2.osg34.el6\ncondor-vm-gahp-8.6.5-2.osg34.el6\nhtcondor-ce-3.0.1-1.osg34.el6\nhtcondor-ce-bosco-3.0.1-1.osg34.el6\nhtcondor-ce-client-3.0.1-1.osg34.el6\nhtcondor-ce-collector-3.0.1-1.osg34.el6\nhtcondor-ce-condor-3.0.1-1.osg34.el6\nhtcondor-ce-lsf-3.0.1-1.osg34.el6\nhtcondor-ce-pbs-3.0.1-1.osg34.el6\nhtcondor-ce-sge-3.0.1-1.osg34.el6\nhtcondor-ce-slurm-3.0.1-1.osg34.el6\nhtcondor-ce-view-3.0.1-1.osg34.el6\nosg-ce-3.4-3.osg34.el6\nosg-ce-bosco-3.4-3.osg34.el6\nosg-ce-condor-3.4-3.osg34.el6\nosg-ce-lsf-3.4-3.osg34.el6\nosg-ce-pbs-3.4-3.osg34.el6\nosg-ce-sge-3.4-3.osg34.el6\nosg-ce-slurm-3.4-3.osg34.el6\nosg-configure-2.1.1-1.osg34.el6\nosg-configure-bosco-2.1.1-1.osg34.el6\nosg-configure-ce-2.1.1-1.osg34.el6\nosg-configure-condor-2.1.1-1.osg34.el6\nosg-configure-gateway-2.1.1-1.osg34.el6\nosg-configure-gip-2.1.1-1.osg34.el6\nosg-configure-gratia-2.1.1-1.osg34.el6\nosg-configure-infoservices-2.1.1-1.osg34.el6\nosg-configure-lsf-2.1.1-1.osg34.el6\nosg-configure-managedfork-2.1.1-1.osg34.el6\nosg-configure-misc-2.1.1-1.osg34.el6\nosg-configure-network-2.1.1-1.osg34.el6\nosg-configure-pbs-2.1.1-1.osg34.el6\nosg-configure-rsv-2.1.1-1.osg34.el6\nosg-configure-sge-2.1.1-1.osg34.el6\nosg-configure-slurm-2.1.1-1.osg34.el6\nosg-configure-squid-2.1.1-1.osg34.el6\nosg-configure-tests-2.1.1-1.osg34.el6\nosg-test-1.11.1-1.osg34.el6\nosg-tested-internal-3.4-4.osg34.el6\nosg-test-log-viewer-1.11.1-1.osg34.el6\nosg-version-3.4.2-1.osg34.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp-1.18.32.bosco-1.osg34.el7\nblahp-debuginfo-1.18.32.bosco-1.osg34.el7\ncondor-8.6.5-2.osg34.el7\ncondor-all-8.6.5-2.osg34.el7\ncondor-bosco-8.6.5-2.osg34.el7\ncondor-classads-8.6.5-2.osg34.el7\ncondor-classads-devel-8.6.5-2.osg34.el7\ncondor-cream-gahp-8.6.5-2.osg34.el7\ncondor-cron-1.1.3-1.osg34.el7\ncondor-debuginfo-8.6.5-2.osg34.el7\ncondor-kbdd-8.6.5-2.osg34.el7\ncondor-procd-8.6.5-2.osg34.el7\ncondor-python-8.6.5-2.osg34.el7\ncondor-test-8.6.5-2.osg34.el7\ncondor-vm-gahp-8.6.5-2.osg34.el7\nhtcondor-ce-3.0.1-1.osg34.el7\nhtcondor-ce-bosco-3.0.1-1.osg34.el7\nhtcondor-ce-client-3.0.1-1.osg34.el7\nhtcondor-ce-collector-3.0.1-1.osg34.el7\nhtcondor-ce-condor-3.0.1-1.osg34.el7\nhtcondor-ce-lsf-3.0.1-1.osg34.el7\nhtcondor-ce-pbs-3.0.1-1.osg34.el7\nhtcondor-ce-sge-3.0.1-1.osg34.el7\nhtcondor-ce-slurm-3.0.1-1.osg34.el7\nhtcondor-ce-view-3.0.1-1.osg34.el7\nosg-ce-3.4-3.osg34.el7\nosg-ce-bosco-3.4-3.osg34.el7\nosg-ce-condor-3.4-3.osg34.el7\nosg-ce-lsf-3.4-3.osg34.el7\nosg-ce-pbs-3.4-3.osg34.el7\nosg-ce-sge-3.4-3.osg34.el7\nosg-ce-slurm-3.4-3.osg34.el7\nosg-configure-2.1.1-1.osg34.el7\nosg-configure-bosco-2.1.1-1.osg34.el7\nosg-configure-ce-2.1.1-1.osg34.el7\nosg-configure-condor-2.1.1-1.osg34.el7\nosg-configure-gateway-2.1.1-1.osg34.el7\nosg-configure-gip-2.1.1-1.osg34.el7\nosg-configure-gratia-2.1.1-1.osg34.el7\nosg-configure-infoservices-2.1.1-1.osg34.el7\nosg-configure-lsf-2.1.1-1.osg34.el7\nosg-configure-managedfork-2.1.1-1.osg34.el7\nosg-configure-misc-2.1.1-1.osg34.el7\nosg-configure-network-2.1.1-1.osg34.el7\nosg-configure-pbs-2.1.1-1.osg34.el7\nosg-configure-rsv-2.1.1-1.osg34.el7\nosg-configure-sge-2.1.1-1.osg34.el7\nosg-configure-slurm-2.1.1-1.osg34.el7\nosg-configure-squid-2.1.1-1.osg34.el7\nosg-configure-tests-2.1.1-1.osg34.el7\nosg-test-1.11.1-1.osg34.el7\nosg-tested-internal-3.4-4.osg34.el7\nosg-test-log-viewer-1.11.1-1.osg34.el7\nosg-version-3.4.2-1.osg34.el7\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.32.bosco-1.osgup.el6\n\n\ncondor-8.7.2-2.1.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.32.bosco-1.osgup.el7\n\n\ncondor-8.7.2-2.1.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp blahp-debuginfo condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp-1.18.32.bosco-1.osgup.el6\nblahp-debuginfo-1.18.32.bosco-1.osgup.el6\ncondor-8.7.2-2.1.osgup.el6\ncondor-all-8.7.2-2.1.osgup.el6\ncondor-annex-ec2-8.7.2-2.1.osgup.el6\ncondor-bosco-8.7.2-2.1.osgup.el6\ncondor-classads-8.7.2-2.1.osgup.el6\ncondor-classads-devel-8.7.2-2.1.osgup.el6\ncondor-cream-gahp-8.7.2-2.1.osgup.el6\ncondor-debuginfo-8.7.2-2.1.osgup.el6\ncondor-kbdd-8.7.2-2.1.osgup.el6\ncondor-procd-8.7.2-2.1.osgup.el6\ncondor-python-8.7.2-2.1.osgup.el6\ncondor-std-universe-8.7.2-2.1.osgup.el6\ncondor-test-8.7.2-2.1.osgup.el6\ncondor-vm-gahp-8.7.2-2.1.osgup.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp-1.18.32.bosco-1.osgup.el7\nblahp-debuginfo-1.18.32.bosco-1.osgup.el7\ncondor-8.7.2-2.1.osgup.el7\ncondor-all-8.7.2-2.1.osgup.el7\ncondor-annex-ec2-8.7.2-2.1.osgup.el7\ncondor-bosco-8.7.2-2.1.osgup.el7\ncondor-classads-8.7.2-2.1.osgup.el7\ncondor-classads-devel-8.7.2-2.1.osgup.el7\ncondor-cream-gahp-8.7.2-2.1.osgup.el7\ncondor-debuginfo-8.7.2-2.1.osgup.el7\ncondor-kbdd-8.7.2-2.1.osgup.el7\ncondor-procd-8.7.2-2.1.osgup.el7\ncondor-python-8.7.2-2.1.osgup.el7\ncondor-test-8.7.2-2.1.osgup.el7\ncondor-vm-gahp-8.7.2-2.1.osgup.el7", 
            "title": "OSG Release 3.4.2"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#osg-software-release-342", 
            "text": "Release Date : 2017-08-08", 
            "title": "OSG Software Release 3.4.2"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#summary-of-changes", 
            "text": "This release contains:   HTCondor 8.6.5  Updated SELinux profile which is required on Red Hat 7.4  Fixed a memory leak that would cause the condor_collector to slowly grow  Fixed several issues that occur when IPv6 is in use  Fixed a bug where condor_rm rarely removed another one of the user's jobs  Fixed a bug with parallel universe jobs starting on partitionable slots    HTCondor-CE  Added pilot payload auditing  Do not hold running jobs with an expired proxy  Initialize  $SPOOL/ceview/vos  directory at installation time so that the VO tab functions in CEView before any pilots are received  Don't warn about not running osg-configure, if osg-configure is not installed    Default configuration improvements for condor-cron  Clarified how to override the  CONDOR_IDS  Disable the unused GSI authentication that was producing spurious log messages.    Several improvements to osg-configure  Ensure that GUMS is configured before trying to query the GUMS server  Progress updates (such as informing when an operation is expected to take a while) during the user VO file validation are presented to the user rather than being sent to the log file.  osg-configure will issue same warnings and errors with -v option as it does with the -c option.    Added blahp configuration option for PBS Pro. This option must be used when the disambiguation code does not correctly determine which PBS is in use.  Reorganize the osg-ce packages  SOFTWARE-2768  Upcoming: patched HTCondor 8.7.2  Updated SELinux profile which is required on Red Hat 7.4     These  JIRA tickets  were addressed in this release.   OSG 3.4 contains only 64-bit components.   StashCache is supported on EL7 only.   xrootd-lcmaps will remain at 1.2.1-2 on EL6.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#known-issues", 
            "text": "Because the Gratia system has been turned off, RSV emits the following warning:  WARNING: Gratia server gratia-osg-prod.opensciencegrid.org:80 not responding to obtain last contact details. . This warning can safely be ignored.  Using the LCMAPS VOMS will result in a failing \"supported VO\" RSV test ( SOFTWARE-2763 ). This can be ignored and a fix is targeted for the August release.  In GlideinWMS, a small configuration change must be added to account for changes in HTCondor 8.6. Add the following line to the HTCondor configuration.   COLLECTOR.USE_SHARED_PORT=False", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#enterprise-linux-6", 
            "text": "blahp-1.18.32.bosco-1.osg34.el6  condor-8.6.5-2.osg34.el6  condor-cron-1.1.3-1.osg34.el6  htcondor-ce-3.0.1-1.osg34.el6  osg-ce-3.4-3.osg34.el6  osg-configure-2.1.1-1.osg34.el6  osg-test-1.11.1-1.osg34.el6  osg-tested-internal-3.4-4.osg34.el6  osg-version-3.4.2-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#enterprise-linux-7", 
            "text": "blahp-1.18.32.bosco-1.osg34.el7  condor-8.6.5-2.osg34.el7  condor-cron-1.1.3-1.osg34.el7  htcondor-ce-3.0.1-1.osg34.el7  osg-ce-3.4-3.osg34.el7  osg-configure-2.1.1-1.osg34.el7  osg-test-1.11.1-1.osg34.el7  osg-tested-internal-3.4-4.osg34.el7  osg-version-3.4.2-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-cron condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-slurm htcondor-ce-view igtf-ca-certs osg-ca-certs osg-ce osg-ce-bosco osg-ce-condor osg-ce-lsf osg-ce-pbs osg-ce-sge osg-ce-slurm osg-configure osg-configure-bosco osg-configure-ce osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-test osg-tested-internal osg-test-log-viewer osg-version  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#enterprise-linux-6_1", 
            "text": "blahp-1.18.32.bosco-1.osg34.el6\nblahp-debuginfo-1.18.32.bosco-1.osg34.el6\ncondor-8.6.5-2.osg34.el6\ncondor-all-8.6.5-2.osg34.el6\ncondor-bosco-8.6.5-2.osg34.el6\ncondor-classads-8.6.5-2.osg34.el6\ncondor-classads-devel-8.6.5-2.osg34.el6\ncondor-cream-gahp-8.6.5-2.osg34.el6\ncondor-cron-1.1.3-1.osg34.el6\ncondor-debuginfo-8.6.5-2.osg34.el6\ncondor-kbdd-8.6.5-2.osg34.el6\ncondor-procd-8.6.5-2.osg34.el6\ncondor-python-8.6.5-2.osg34.el6\ncondor-std-universe-8.6.5-2.osg34.el6\ncondor-test-8.6.5-2.osg34.el6\ncondor-vm-gahp-8.6.5-2.osg34.el6\nhtcondor-ce-3.0.1-1.osg34.el6\nhtcondor-ce-bosco-3.0.1-1.osg34.el6\nhtcondor-ce-client-3.0.1-1.osg34.el6\nhtcondor-ce-collector-3.0.1-1.osg34.el6\nhtcondor-ce-condor-3.0.1-1.osg34.el6\nhtcondor-ce-lsf-3.0.1-1.osg34.el6\nhtcondor-ce-pbs-3.0.1-1.osg34.el6\nhtcondor-ce-sge-3.0.1-1.osg34.el6\nhtcondor-ce-slurm-3.0.1-1.osg34.el6\nhtcondor-ce-view-3.0.1-1.osg34.el6\nosg-ce-3.4-3.osg34.el6\nosg-ce-bosco-3.4-3.osg34.el6\nosg-ce-condor-3.4-3.osg34.el6\nosg-ce-lsf-3.4-3.osg34.el6\nosg-ce-pbs-3.4-3.osg34.el6\nosg-ce-sge-3.4-3.osg34.el6\nosg-ce-slurm-3.4-3.osg34.el6\nosg-configure-2.1.1-1.osg34.el6\nosg-configure-bosco-2.1.1-1.osg34.el6\nosg-configure-ce-2.1.1-1.osg34.el6\nosg-configure-condor-2.1.1-1.osg34.el6\nosg-configure-gateway-2.1.1-1.osg34.el6\nosg-configure-gip-2.1.1-1.osg34.el6\nosg-configure-gratia-2.1.1-1.osg34.el6\nosg-configure-infoservices-2.1.1-1.osg34.el6\nosg-configure-lsf-2.1.1-1.osg34.el6\nosg-configure-managedfork-2.1.1-1.osg34.el6\nosg-configure-misc-2.1.1-1.osg34.el6\nosg-configure-network-2.1.1-1.osg34.el6\nosg-configure-pbs-2.1.1-1.osg34.el6\nosg-configure-rsv-2.1.1-1.osg34.el6\nosg-configure-sge-2.1.1-1.osg34.el6\nosg-configure-slurm-2.1.1-1.osg34.el6\nosg-configure-squid-2.1.1-1.osg34.el6\nosg-configure-tests-2.1.1-1.osg34.el6\nosg-test-1.11.1-1.osg34.el6\nosg-tested-internal-3.4-4.osg34.el6\nosg-test-log-viewer-1.11.1-1.osg34.el6\nosg-version-3.4.2-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#enterprise-linux-7_1", 
            "text": "blahp-1.18.32.bosco-1.osg34.el7\nblahp-debuginfo-1.18.32.bosco-1.osg34.el7\ncondor-8.6.5-2.osg34.el7\ncondor-all-8.6.5-2.osg34.el7\ncondor-bosco-8.6.5-2.osg34.el7\ncondor-classads-8.6.5-2.osg34.el7\ncondor-classads-devel-8.6.5-2.osg34.el7\ncondor-cream-gahp-8.6.5-2.osg34.el7\ncondor-cron-1.1.3-1.osg34.el7\ncondor-debuginfo-8.6.5-2.osg34.el7\ncondor-kbdd-8.6.5-2.osg34.el7\ncondor-procd-8.6.5-2.osg34.el7\ncondor-python-8.6.5-2.osg34.el7\ncondor-test-8.6.5-2.osg34.el7\ncondor-vm-gahp-8.6.5-2.osg34.el7\nhtcondor-ce-3.0.1-1.osg34.el7\nhtcondor-ce-bosco-3.0.1-1.osg34.el7\nhtcondor-ce-client-3.0.1-1.osg34.el7\nhtcondor-ce-collector-3.0.1-1.osg34.el7\nhtcondor-ce-condor-3.0.1-1.osg34.el7\nhtcondor-ce-lsf-3.0.1-1.osg34.el7\nhtcondor-ce-pbs-3.0.1-1.osg34.el7\nhtcondor-ce-sge-3.0.1-1.osg34.el7\nhtcondor-ce-slurm-3.0.1-1.osg34.el7\nhtcondor-ce-view-3.0.1-1.osg34.el7\nosg-ce-3.4-3.osg34.el7\nosg-ce-bosco-3.4-3.osg34.el7\nosg-ce-condor-3.4-3.osg34.el7\nosg-ce-lsf-3.4-3.osg34.el7\nosg-ce-pbs-3.4-3.osg34.el7\nosg-ce-sge-3.4-3.osg34.el7\nosg-ce-slurm-3.4-3.osg34.el7\nosg-configure-2.1.1-1.osg34.el7\nosg-configure-bosco-2.1.1-1.osg34.el7\nosg-configure-ce-2.1.1-1.osg34.el7\nosg-configure-condor-2.1.1-1.osg34.el7\nosg-configure-gateway-2.1.1-1.osg34.el7\nosg-configure-gip-2.1.1-1.osg34.el7\nosg-configure-gratia-2.1.1-1.osg34.el7\nosg-configure-infoservices-2.1.1-1.osg34.el7\nosg-configure-lsf-2.1.1-1.osg34.el7\nosg-configure-managedfork-2.1.1-1.osg34.el7\nosg-configure-misc-2.1.1-1.osg34.el7\nosg-configure-network-2.1.1-1.osg34.el7\nosg-configure-pbs-2.1.1-1.osg34.el7\nosg-configure-rsv-2.1.1-1.osg34.el7\nosg-configure-sge-2.1.1-1.osg34.el7\nosg-configure-slurm-2.1.1-1.osg34.el7\nosg-configure-squid-2.1.1-1.osg34.el7\nosg-configure-tests-2.1.1-1.osg34.el7\nosg-test-1.11.1-1.osg34.el7\nosg-tested-internal-3.4-4.osg34.el7\nosg-test-log-viewer-1.11.1-1.osg34.el7\nosg-version-3.4.2-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#enterprise-linux-6_2", 
            "text": "blahp-1.18.32.bosco-1.osgup.el6  condor-8.7.2-2.1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#enterprise-linux-7_2", 
            "text": "blahp-1.18.32.bosco-1.osgup.el7  condor-8.7.2-2.1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp blahp-debuginfo condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#enterprise-linux-6_3", 
            "text": "blahp-1.18.32.bosco-1.osgup.el6\nblahp-debuginfo-1.18.32.bosco-1.osgup.el6\ncondor-8.7.2-2.1.osgup.el6\ncondor-all-8.7.2-2.1.osgup.el6\ncondor-annex-ec2-8.7.2-2.1.osgup.el6\ncondor-bosco-8.7.2-2.1.osgup.el6\ncondor-classads-8.7.2-2.1.osgup.el6\ncondor-classads-devel-8.7.2-2.1.osgup.el6\ncondor-cream-gahp-8.7.2-2.1.osgup.el6\ncondor-debuginfo-8.7.2-2.1.osgup.el6\ncondor-kbdd-8.7.2-2.1.osgup.el6\ncondor-procd-8.7.2-2.1.osgup.el6\ncondor-python-8.7.2-2.1.osgup.el6\ncondor-std-universe-8.7.2-2.1.osgup.el6\ncondor-test-8.7.2-2.1.osgup.el6\ncondor-vm-gahp-8.7.2-2.1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#enterprise-linux-7_3", 
            "text": "blahp-1.18.32.bosco-1.osgup.el7\nblahp-debuginfo-1.18.32.bosco-1.osgup.el7\ncondor-8.7.2-2.1.osgup.el7\ncondor-all-8.7.2-2.1.osgup.el7\ncondor-annex-ec2-8.7.2-2.1.osgup.el7\ncondor-bosco-8.7.2-2.1.osgup.el7\ncondor-classads-8.7.2-2.1.osgup.el7\ncondor-classads-devel-8.7.2-2.1.osgup.el7\ncondor-cream-gahp-8.7.2-2.1.osgup.el7\ncondor-debuginfo-8.7.2-2.1.osgup.el7\ncondor-kbdd-8.7.2-2.1.osgup.el7\ncondor-procd-8.7.2-2.1.osgup.el7\ncondor-python-8.7.2-2.1.osgup.el7\ncondor-test-8.7.2-2.1.osgup.el7\ncondor-vm-gahp-8.7.2-2.1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/", 
            "text": "OSG Software Stack -- Data Release -- 3.4.1-2 and 3.3.26-2\n\n\nRelease Date\n: 2017-07-13\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nCA Certificates based on \nIGTF 1.84\n\n\nUpdated ROSA root certificate with extended 20yr valitity (RO)\n\n\nUpdated contact details for CyGrid CA following transition to CYNET (CY)\n\n\nRemoved obsoleted KISTI-2007 trust anchor - replaced by KISTIv3 (KR)\n\n\nRemoved expiring LACGrid trust anchor a9082267 (BR)\n\n\nAdded UK Pathfinder AAAI CA 1 to unaccredited (misc) area (UK)\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nOSG 3.3\n\n\nEnterprise Linux 6\n\n\n\n\nigtf-ca-certs-1.84-1.osg33.el6\n\n\nosg-ca-certs-1.64-1.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nigtf-ca-certs-1.84-1.osg33.el7\n\n\nosg-ca-certs-1.64-1.osg33.el7\n\n\n\n\nOSG 3.4\n\n\nEnterprise Linux 6\n\n\n\n\nigtf-ca-certs-1.84-1.osg34.el6\n\n\nosg-ca-certs-1.64-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nigtf-ca-certs-1.84-1.osg34.el7\n\n\nosg-ca-certs-1.64-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nigtf-ca-certs osg-ca-certs\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nOSG 3.3\n\n\nEnterprise Linux 6\n\n\nigtf-ca-certs-1.84-1.osg33.el6\nosg-ca-certs-1.64-1.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nigtf-ca-certs-1.84-1.osg33.el7\nosg-ca-certs-1.64-1.osg33.el7\n\n\n\n\n\nOSG 3.4\n\n\nEnterprise Linux 6\n\n\nigtf-ca-certs-1.84-1.osg34.el6\nosg-ca-certs-1.64-1.osg34.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nigtf-ca-certs-1.84-1.osg34.el7\nosg-ca-certs-1.64-1.osg34.el7", 
            "title": "OSG Release 3.4.1-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/#osg-software-stack-data-release-341-2-and-3326-2", 
            "text": "Release Date : 2017-07-13", 
            "title": "OSG Software Stack -- Data Release -- 3.4.1-2 and 3.3.26-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/#summary-of-changes", 
            "text": "This release contains:   CA Certificates based on  IGTF 1.84  Updated ROSA root certificate with extended 20yr valitity (RO)  Updated contact details for CyGrid CA following transition to CYNET (CY)  Removed obsoleted KISTI-2007 trust anchor - replaced by KISTIv3 (KR)  Removed expiring LACGrid trust anchor a9082267 (BR)  Added UK Pathfinder AAAI CA 1 to unaccredited (misc) area (UK)     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/#osg-33", 
            "text": "", 
            "title": "OSG 3.3"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/#enterprise-linux-6", 
            "text": "igtf-ca-certs-1.84-1.osg33.el6  osg-ca-certs-1.64-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/#enterprise-linux-7", 
            "text": "igtf-ca-certs-1.84-1.osg33.el7  osg-ca-certs-1.64-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/#osg-34", 
            "text": "", 
            "title": "OSG 3.4"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/#enterprise-linux-6_1", 
            "text": "igtf-ca-certs-1.84-1.osg34.el6  osg-ca-certs-1.64-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/#enterprise-linux-7_1", 
            "text": "igtf-ca-certs-1.84-1.osg34.el7  osg-ca-certs-1.64-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  igtf-ca-certs osg-ca-certs  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/#osg-33_1", 
            "text": "", 
            "title": "OSG 3.3"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/#enterprise-linux-6_2", 
            "text": "igtf-ca-certs-1.84-1.osg33.el6\nosg-ca-certs-1.64-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/#enterprise-linux-7_2", 
            "text": "igtf-ca-certs-1.84-1.osg33.el7\nosg-ca-certs-1.64-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/#osg-34_1", 
            "text": "", 
            "title": "OSG 3.4"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/#enterprise-linux-6_3", 
            "text": "igtf-ca-certs-1.84-1.osg34.el6\nosg-ca-certs-1.64-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/#enterprise-linux-7_3", 
            "text": "igtf-ca-certs-1.84-1.osg34.el7\nosg-ca-certs-1.64-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/", 
            "text": "OSG Software Release 3.4.1\n\n\nRelease Date\n: 2017-07-12\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nBug fix in LCMAPS plugin that could cause the HTCondor-CE schedd to crash\n\n\nosg-configure uses the new GUMS JSON interface\n\n\nThe BLAHP properly requests multi-core resources for Slurm batch systems\n\n\nHTCondor-CE 2.2.1\n\n\nFixed memory requirement requests to non-HTCondor batch systems\n\n\nCorrect CPU allocation for whole node jobs\n\n\n\n\n\n\nGratia probes\n\n\nsupport whole node jobs\n\n\ncan include arbitrary ClassAd attributes in Gratia usage records\n\n\n\n\n\n\nBug fix to CVMFS client to able to mount when large groups exist\n\n\nGridFTP server now uses correct configuration with a dsi plugin\n\n\ngridftp-dsi-posix replaces the xrootd-dsi plugin\n\n\nany local changes made to \n/etc/sysconfig/xrootd-dsi\n should be transferred over to \n/etc/sysconfig/gridftp-dsi-posix\n\n\n\n\n\n\nEnhanced gridftp-dsi-posix\n\n\nAdded MD5 checksum\n\n\nAdded GRIDFTP_APPEND_XROOTD_CGI hook to support XRootD space tokens\n\n\n\n\n\n\nHTCondor 8.6.4\n: BOSCO now works without CA certificates on remote cluster\n\n\nHTCondor 8.7.2\n: introducing the 8.7 series in the upcoming repository\n\n\nRSV\n\n\nreplace software.grid.iu.edu with repo.grid.iu.edu\n\n\nparse condor_cron condor_q output properly\n\n\n\n\n\n\nosg-gridftp now pulls in osg-configure-misc\n\n\ncondor_cron: eliminate email on restart\n\n\nInternal tools\n\n\nosg-build update\n\n\nDrop unused tests from osg-test\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n OSG 3.4 contains only 64-bit components. \n StashCache is supported on EL7 only. \n xrootd-lcmaps will remain at 1.2.1-2 on EL6.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nKnown Issues\n\n\n\n\nBecause the Gratia system has been turned off, RSV emits the following warning: \nWARNING: Gratia server gratia-osg-prod.opensciencegrid.org:80 not responding to obtain last contact details.\n. This warning can safely be ignored.\n\n\nUsing the LCMAPS VOMS will result in a failing \"supported VO\" RSV test (\nSOFTWARE-2763\n). This can be ignored and a fix is targeted for the August release.\n\n\nIn GlideinWMS, a small configuration change must be added to account for changes in HTCondor 8.6. Add the following line to the HTCondor configuration.\n\n\n\n\nCOLLECTOR.USE_SHARED_PORT=False\n\n\n\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.30.bosco-1.osg34.el6\n\n\ncondor-8.6.4-1.osg34.el6\n\n\ncondor-cron-1.1.2-1.osg34.el6\n\n\ncvmfs-2.3.5-1.1.osg34.el6\n\n\nglobus-gridftp-server-11.8-1.3.osg34.el6\n\n\ngratia-probe-1.18.1-1.osg34.el6\n\n\ngridftp-dsi-posix-1.4-2.osg34.el6\n\n\nhtcondor-ce-2.2.1-1.osg34.el6\n\n\nlcmaps-plugins-verify-proxy-1.5.9-1.2.osg34.el6\n\n\nosg-build-1.10.1-1.osg34.el6\n\n\nosg-configure-2.1.0-2.osg34.el6\n\n\nosg-gridftp-3.4-3.osg34.el6\n\n\nosg-test-1.11.0-1.osg34.el6\n\n\nosg-version-3.4.1-1.osg34.el6\n\n\nrsv-3.14.2-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.30.bosco-1.osg34.el7\n\n\ncondor-8.6.4-1.osg34.el7\n\n\ncondor-cron-1.1.2-1.osg34.el7\n\n\ncvmfs-2.3.5-1.1.osg34.el7\n\n\nglobus-gridftp-server-11.8-1.3.osg34.el7\n\n\ngratia-probe-1.18.1-1.osg34.el7\n\n\ngridftp-dsi-posix-1.4-2.osg34.el7\n\n\nhtcondor-ce-2.2.1-1.osg34.el7\n\n\nlcmaps-plugins-verify-proxy-1.5.9-1.2.osg34.el7\n\n\nosg-build-1.10.1-1.osg34.el7\n\n\nosg-configure-2.1.0-2.osg34.el7\n\n\nosg-gridftp-3.4-3.osg34.el7\n\n\nosg-test-1.11.0-1.osg34.el7\n\n\nosg-version-3.4.1-1.osg34.el7\n\n\nrsv-3.14.2-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-cron condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp cvmfs cvmfs-devel cvmfs-server cvmfs-unittests globus-gridftp-server globus-gridftp-server-debuginfo globus-gridftp-server-devel globus-gridftp-server-progs gratia-probe-bdii-status gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glexec gratia-probe-glideinwms gratia-probe-gram gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer gridftp-dsi-posix gridftp-dsi-posix-debuginfo htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-slurm htcondor-ce-view igtf-ca-certs lcmaps-plugins-verify-proxy lcmaps-plugins-verify-proxy-debuginfo osg-build osg-build-base osg-build-koji osg-build-mock osg-build-tests osg-ca-certs osg-configure osg-configure-bosco osg-configure-ce osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-gridftp osg-test osg-test-log-viewer osg-version rsv rsv-consumers rsv-core rsv-metrics\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp-1.18.30.bosco-1.osg34.el6\nblahp-debuginfo-1.18.30.bosco-1.osg34.el6\ncondor-8.6.4-1.osg34.el6\ncondor-all-8.6.4-1.osg34.el6\ncondor-bosco-8.6.4-1.osg34.el6\ncondor-classads-8.6.4-1.osg34.el6\ncondor-classads-devel-8.6.4-1.osg34.el6\ncondor-cream-gahp-8.6.4-1.osg34.el6\ncondor-cron-1.1.2-1.osg34.el6\ncondor-debuginfo-8.6.4-1.osg34.el6\ncondor-kbdd-8.6.4-1.osg34.el6\ncondor-procd-8.6.4-1.osg34.el6\ncondor-python-8.6.4-1.osg34.el6\ncondor-std-universe-8.6.4-1.osg34.el6\ncondor-test-8.6.4-1.osg34.el6\ncondor-vm-gahp-8.6.4-1.osg34.el6\ncvmfs-2.3.5-1.1.osg34.el6\ncvmfs-devel-2.3.5-1.1.osg34.el6\ncvmfs-server-2.3.5-1.1.osg34.el6\ncvmfs-unittests-2.3.5-1.1.osg34.el6\nglobus-gridftp-server-11.8-1.3.osg34.el6\nglobus-gridftp-server-debuginfo-11.8-1.3.osg34.el6\nglobus-gridftp-server-devel-11.8-1.3.osg34.el6\nglobus-gridftp-server-progs-11.8-1.3.osg34.el6\ngratia-probe-1.18.1-1.osg34.el6\ngratia-probe-bdii-status-1.18.1-1.osg34.el6\ngratia-probe-common-1.18.1-1.osg34.el6\ngratia-probe-condor-1.18.1-1.osg34.el6\ngratia-probe-condor-events-1.18.1-1.osg34.el6\ngratia-probe-dcache-storage-1.18.1-1.osg34.el6\ngratia-probe-dcache-storagegroup-1.18.1-1.osg34.el6\ngratia-probe-dcache-transfer-1.18.1-1.osg34.el6\ngratia-probe-debuginfo-1.18.1-1.osg34.el6\ngratia-probe-enstore-storage-1.18.1-1.osg34.el6\ngratia-probe-enstore-tapedrive-1.18.1-1.osg34.el6\ngratia-probe-enstore-transfer-1.18.1-1.osg34.el6\ngratia-probe-glexec-1.18.1-1.osg34.el6\ngratia-probe-glideinwms-1.18.1-1.osg34.el6\ngratia-probe-gram-1.18.1-1.osg34.el6\ngratia-probe-gridftp-transfer-1.18.1-1.osg34.el6\ngratia-probe-hadoop-storage-1.18.1-1.osg34.el6\ngratia-probe-htcondor-ce-1.18.1-1.osg34.el6\ngratia-probe-lsf-1.18.1-1.osg34.el6\ngratia-probe-metric-1.18.1-1.osg34.el6\ngratia-probe-onevm-1.18.1-1.osg34.el6\ngratia-probe-pbs-lsf-1.18.1-1.osg34.el6\ngratia-probe-services-1.18.1-1.osg34.el6\ngratia-probe-sge-1.18.1-1.osg34.el6\ngratia-probe-slurm-1.18.1-1.osg34.el6\ngratia-probe-xrootd-storage-1.18.1-1.osg34.el6\ngratia-probe-xrootd-transfer-1.18.1-1.osg34.el6\ngridftp-dsi-posix-1.4-2.osg34.el6\ngridftp-dsi-posix-debuginfo-1.4-2.osg34.el6\nhtcondor-ce-2.2.1-1.osg34.el6\nhtcondor-ce-bosco-2.2.1-1.osg34.el6\nhtcondor-ce-client-2.2.1-1.osg34.el6\nhtcondor-ce-collector-2.2.1-1.osg34.el6\nhtcondor-ce-condor-2.2.1-1.osg34.el6\nhtcondor-ce-lsf-2.2.1-1.osg34.el6\nhtcondor-ce-pbs-2.2.1-1.osg34.el6\nhtcondor-ce-sge-2.2.1-1.osg34.el6\nhtcondor-ce-slurm-2.2.1-1.osg34.el6\nhtcondor-ce-view-2.2.1-1.osg34.el6\nlcmaps-plugins-verify-proxy-1.5.9-1.2.osg34.el6\nlcmaps-plugins-verify-proxy-debuginfo-1.5.9-1.2.osg34.el6\nosg-build-1.10.1-1.osg34.el6\nosg-build-base-1.10.1-1.osg34.el6\nosg-build-koji-1.10.1-1.osg34.el6\nosg-build-mock-1.10.1-1.osg34.el6\nosg-build-tests-1.10.1-1.osg34.el6\nosg-configure-2.1.0-2.osg34.el6\nosg-configure-bosco-2.1.0-2.osg34.el6\nosg-configure-ce-2.1.0-2.osg34.el6\nosg-configure-condor-2.1.0-2.osg34.el6\nosg-configure-gateway-2.1.0-2.osg34.el6\nosg-configure-gip-2.1.0-2.osg34.el6\nosg-configure-gratia-2.1.0-2.osg34.el6\nosg-configure-infoservices-2.1.0-2.osg34.el6\nosg-configure-lsf-2.1.0-2.osg34.el6\nosg-configure-managedfork-2.1.0-2.osg34.el6\nosg-configure-misc-2.1.0-2.osg34.el6\nosg-configure-network-2.1.0-2.osg34.el6\nosg-configure-pbs-2.1.0-2.osg34.el6\nosg-configure-rsv-2.1.0-2.osg34.el6\nosg-configure-sge-2.1.0-2.osg34.el6\nosg-configure-slurm-2.1.0-2.osg34.el6\nosg-configure-squid-2.1.0-2.osg34.el6\nosg-configure-tests-2.1.0-2.osg34.el6\nosg-gridftp-3.4-3.osg34.el6\nosg-test-1.11.0-1.osg34.el6\nosg-test-log-viewer-1.11.0-1.osg34.el6\nosg-version-3.4.1-1.osg34.el6\nrsv-3.14.2-1.osg34.el6\nrsv-consumers-3.14.2-1.osg34.el6\nrsv-core-3.14.2-1.osg34.el6\nrsv-metrics-3.14.2-1.osg34.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp-1.18.30.bosco-1.osg34.el7\nblahp-debuginfo-1.18.30.bosco-1.osg34.el7\ncondor-8.6.4-1.osg34.el7\ncondor-all-8.6.4-1.osg34.el7\ncondor-bosco-8.6.4-1.osg34.el7\ncondor-classads-8.6.4-1.osg34.el7\ncondor-classads-devel-8.6.4-1.osg34.el7\ncondor-cream-gahp-8.6.4-1.osg34.el7\ncondor-cron-1.1.2-1.osg34.el7\ncondor-debuginfo-8.6.4-1.osg34.el7\ncondor-kbdd-8.6.4-1.osg34.el7\ncondor-procd-8.6.4-1.osg34.el7\ncondor-python-8.6.4-1.osg34.el7\ncondor-test-8.6.4-1.osg34.el7\ncondor-vm-gahp-8.6.4-1.osg34.el7\ncvmfs-2.3.5-1.1.osg34.el7\ncvmfs-devel-2.3.5-1.1.osg34.el7\ncvmfs-server-2.3.5-1.1.osg34.el7\ncvmfs-unittests-2.3.5-1.1.osg34.el7\nglobus-gridftp-server-11.8-1.3.osg34.el7\nglobus-gridftp-server-debuginfo-11.8-1.3.osg34.el7\nglobus-gridftp-server-devel-11.8-1.3.osg34.el7\nglobus-gridftp-server-progs-11.8-1.3.osg34.el7\ngratia-probe-1.18.1-1.osg34.el7\ngratia-probe-bdii-status-1.18.1-1.osg34.el7\ngratia-probe-common-1.18.1-1.osg34.el7\ngratia-probe-condor-1.18.1-1.osg34.el7\ngratia-probe-condor-events-1.18.1-1.osg34.el7\ngratia-probe-dcache-storage-1.18.1-1.osg34.el7\ngratia-probe-dcache-storagegroup-1.18.1-1.osg34.el7\ngratia-probe-dcache-transfer-1.18.1-1.osg34.el7\ngratia-probe-debuginfo-1.18.1-1.osg34.el7\ngratia-probe-enstore-storage-1.18.1-1.osg34.el7\ngratia-probe-enstore-tapedrive-1.18.1-1.osg34.el7\ngratia-probe-enstore-transfer-1.18.1-1.osg34.el7\ngratia-probe-glexec-1.18.1-1.osg34.el7\ngratia-probe-glideinwms-1.18.1-1.osg34.el7\ngratia-probe-gram-1.18.1-1.osg34.el7\ngratia-probe-gridftp-transfer-1.18.1-1.osg34.el7\ngratia-probe-hadoop-storage-1.18.1-1.osg34.el7\ngratia-probe-htcondor-ce-1.18.1-1.osg34.el7\ngratia-probe-lsf-1.18.1-1.osg34.el7\ngratia-probe-metric-1.18.1-1.osg34.el7\ngratia-probe-onevm-1.18.1-1.osg34.el7\ngratia-probe-pbs-lsf-1.18.1-1.osg34.el7\ngratia-probe-services-1.18.1-1.osg34.el7\ngratia-probe-sge-1.18.1-1.osg34.el7\ngratia-probe-slurm-1.18.1-1.osg34.el7\ngratia-probe-xrootd-storage-1.18.1-1.osg34.el7\ngratia-probe-xrootd-transfer-1.18.1-1.osg34.el7\ngridftp-dsi-posix-1.4-2.osg34.el7\ngridftp-dsi-posix-debuginfo-1.4-2.osg34.el7\nhtcondor-ce-2.2.1-1.osg34.el7\nhtcondor-ce-bosco-2.2.1-1.osg34.el7\nhtcondor-ce-client-2.2.1-1.osg34.el7\nhtcondor-ce-collector-2.2.1-1.osg34.el7\nhtcondor-ce-condor-2.2.1-1.osg34.el7\nhtcondor-ce-lsf-2.2.1-1.osg34.el7\nhtcondor-ce-pbs-2.2.1-1.osg34.el7\nhtcondor-ce-sge-2.2.1-1.osg34.el7\nhtcondor-ce-slurm-2.2.1-1.osg34.el7\nhtcondor-ce-view-2.2.1-1.osg34.el7\nlcmaps-plugins-verify-proxy-1.5.9-1.2.osg34.el7\nlcmaps-plugins-verify-proxy-debuginfo-1.5.9-1.2.osg34.el7\nosg-build-1.10.1-1.osg34.el7\nosg-build-base-1.10.1-1.osg34.el7\nosg-build-koji-1.10.1-1.osg34.el7\nosg-build-mock-1.10.1-1.osg34.el7\nosg-build-tests-1.10.1-1.osg34.el7\nosg-configure-2.1.0-2.osg34.el7\nosg-configure-bosco-2.1.0-2.osg34.el7\nosg-configure-ce-2.1.0-2.osg34.el7\nosg-configure-condor-2.1.0-2.osg34.el7\nosg-configure-gateway-2.1.0-2.osg34.el7\nosg-configure-gip-2.1.0-2.osg34.el7\nosg-configure-gratia-2.1.0-2.osg34.el7\nosg-configure-infoservices-2.1.0-2.osg34.el7\nosg-configure-lsf-2.1.0-2.osg34.el7\nosg-configure-managedfork-2.1.0-2.osg34.el7\nosg-configure-misc-2.1.0-2.osg34.el7\nosg-configure-network-2.1.0-2.osg34.el7\nosg-configure-pbs-2.1.0-2.osg34.el7\nosg-configure-rsv-2.1.0-2.osg34.el7\nosg-configure-sge-2.1.0-2.osg34.el7\nosg-configure-slurm-2.1.0-2.osg34.el7\nosg-configure-squid-2.1.0-2.osg34.el7\nosg-configure-tests-2.1.0-2.osg34.el7\nosg-gridftp-3.4-3.osg34.el7\nosg-test-1.11.0-1.osg34.el7\nosg-test-log-viewer-1.11.0-1.osg34.el7\nosg-version-3.4.1-1.osg34.el7\nrsv-3.14.2-1.osg34.el7\nrsv-consumers-3.14.2-1.osg34.el7\nrsv-core-3.14.2-1.osg34.el7\nrsv-metrics-3.14.2-1.osg34.el7\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.30.bosco-1.osgup.el6\n\n\ncondor-8.7.2-1.osgup.el6\n\n\nglite-ce-cream-client-api-c-1.15.4-2.4.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.30.bosco-1.osgup.el7\n\n\ncondor-8.7.2-1.osgup.el7\n\n\nglite-ce-cream-client-api-c-1.15.4-2.4.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp blahp-debuginfo condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone glite-ce-cream-client-api-c glite-ce-cream-client-devel\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp-1.18.30.bosco-1.osgup.el6\nblahp-debuginfo-1.18.30.bosco-1.osgup.el6\ncondor-8.7.2-1.osgup.el6\ncondor-all-8.7.2-1.osgup.el6\ncondor-annex-ec2-8.7.2-1.osgup.el6\ncondor-bosco-8.7.2-1.osgup.el6\ncondor-classads-8.7.2-1.osgup.el6\ncondor-classads-devel-8.7.2-1.osgup.el6\ncondor-cream-gahp-8.7.2-1.osgup.el6\ncondor-debuginfo-8.7.2-1.osgup.el6\ncondor-kbdd-8.7.2-1.osgup.el6\ncondor-procd-8.7.2-1.osgup.el6\ncondor-python-8.7.2-1.osgup.el6\ncondor-std-universe-8.7.2-1.osgup.el6\ncondor-test-8.7.2-1.osgup.el6\ncondor-vm-gahp-8.7.2-1.osgup.el6\nglite-ce-cream-client-api-c-1.15.4-2.4.osgup.el6\nglite-ce-cream-client-devel-1.15.4-2.4.osgup.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp-1.18.30.bosco-1.osgup.el7\nblahp-debuginfo-1.18.30.bosco-1.osgup.el7\ncondor-8.7.2-1.osgup.el7\ncondor-all-8.7.2-1.osgup.el7\ncondor-annex-ec2-8.7.2-1.osgup.el7\ncondor-bosco-8.7.2-1.osgup.el7\ncondor-classads-8.7.2-1.osgup.el7\ncondor-classads-devel-8.7.2-1.osgup.el7\ncondor-cream-gahp-8.7.2-1.osgup.el7\ncondor-debuginfo-8.7.2-1.osgup.el7\ncondor-kbdd-8.7.2-1.osgup.el7\ncondor-procd-8.7.2-1.osgup.el7\ncondor-python-8.7.2-1.osgup.el7\ncondor-test-8.7.2-1.osgup.el7\ncondor-vm-gahp-8.7.2-1.osgup.el7\nglite-ce-cream-client-api-c-1.15.4-2.4.osgup.el7\nglite-ce-cream-client-devel-1.15.4-2.4.osgup.el7", 
            "title": "OSG Release 3.4.1"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#osg-software-release-341", 
            "text": "Release Date : 2017-07-12", 
            "title": "OSG Software Release 3.4.1"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#summary-of-changes", 
            "text": "This release contains:   Bug fix in LCMAPS plugin that could cause the HTCondor-CE schedd to crash  osg-configure uses the new GUMS JSON interface  The BLAHP properly requests multi-core resources for Slurm batch systems  HTCondor-CE 2.2.1  Fixed memory requirement requests to non-HTCondor batch systems  Correct CPU allocation for whole node jobs    Gratia probes  support whole node jobs  can include arbitrary ClassAd attributes in Gratia usage records    Bug fix to CVMFS client to able to mount when large groups exist  GridFTP server now uses correct configuration with a dsi plugin  gridftp-dsi-posix replaces the xrootd-dsi plugin  any local changes made to  /etc/sysconfig/xrootd-dsi  should be transferred over to  /etc/sysconfig/gridftp-dsi-posix    Enhanced gridftp-dsi-posix  Added MD5 checksum  Added GRIDFTP_APPEND_XROOTD_CGI hook to support XRootD space tokens    HTCondor 8.6.4 : BOSCO now works without CA certificates on remote cluster  HTCondor 8.7.2 : introducing the 8.7 series in the upcoming repository  RSV  replace software.grid.iu.edu with repo.grid.iu.edu  parse condor_cron condor_q output properly    osg-gridftp now pulls in osg-configure-misc  condor_cron: eliminate email on restart  Internal tools  osg-build update  Drop unused tests from osg-test     These  JIRA tickets  were addressed in this release.   OSG 3.4 contains only 64-bit components.   StashCache is supported on EL7 only.   xrootd-lcmaps will remain at 1.2.1-2 on EL6.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#known-issues", 
            "text": "Because the Gratia system has been turned off, RSV emits the following warning:  WARNING: Gratia server gratia-osg-prod.opensciencegrid.org:80 not responding to obtain last contact details. . This warning can safely be ignored.  Using the LCMAPS VOMS will result in a failing \"supported VO\" RSV test ( SOFTWARE-2763 ). This can be ignored and a fix is targeted for the August release.  In GlideinWMS, a small configuration change must be added to account for changes in HTCondor 8.6. Add the following line to the HTCondor configuration.   COLLECTOR.USE_SHARED_PORT=False", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#enterprise-linux-6", 
            "text": "blahp-1.18.30.bosco-1.osg34.el6  condor-8.6.4-1.osg34.el6  condor-cron-1.1.2-1.osg34.el6  cvmfs-2.3.5-1.1.osg34.el6  globus-gridftp-server-11.8-1.3.osg34.el6  gratia-probe-1.18.1-1.osg34.el6  gridftp-dsi-posix-1.4-2.osg34.el6  htcondor-ce-2.2.1-1.osg34.el6  lcmaps-plugins-verify-proxy-1.5.9-1.2.osg34.el6  osg-build-1.10.1-1.osg34.el6  osg-configure-2.1.0-2.osg34.el6  osg-gridftp-3.4-3.osg34.el6  osg-test-1.11.0-1.osg34.el6  osg-version-3.4.1-1.osg34.el6  rsv-3.14.2-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#enterprise-linux-7", 
            "text": "blahp-1.18.30.bosco-1.osg34.el7  condor-8.6.4-1.osg34.el7  condor-cron-1.1.2-1.osg34.el7  cvmfs-2.3.5-1.1.osg34.el7  globus-gridftp-server-11.8-1.3.osg34.el7  gratia-probe-1.18.1-1.osg34.el7  gridftp-dsi-posix-1.4-2.osg34.el7  htcondor-ce-2.2.1-1.osg34.el7  lcmaps-plugins-verify-proxy-1.5.9-1.2.osg34.el7  osg-build-1.10.1-1.osg34.el7  osg-configure-2.1.0-2.osg34.el7  osg-gridftp-3.4-3.osg34.el7  osg-test-1.11.0-1.osg34.el7  osg-version-3.4.1-1.osg34.el7  rsv-3.14.2-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-cron condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp cvmfs cvmfs-devel cvmfs-server cvmfs-unittests globus-gridftp-server globus-gridftp-server-debuginfo globus-gridftp-server-devel globus-gridftp-server-progs gratia-probe-bdii-status gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glexec gratia-probe-glideinwms gratia-probe-gram gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer gridftp-dsi-posix gridftp-dsi-posix-debuginfo htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-slurm htcondor-ce-view igtf-ca-certs lcmaps-plugins-verify-proxy lcmaps-plugins-verify-proxy-debuginfo osg-build osg-build-base osg-build-koji osg-build-mock osg-build-tests osg-ca-certs osg-configure osg-configure-bosco osg-configure-ce osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-gridftp osg-test osg-test-log-viewer osg-version rsv rsv-consumers rsv-core rsv-metrics  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#enterprise-linux-6_1", 
            "text": "blahp-1.18.30.bosco-1.osg34.el6\nblahp-debuginfo-1.18.30.bosco-1.osg34.el6\ncondor-8.6.4-1.osg34.el6\ncondor-all-8.6.4-1.osg34.el6\ncondor-bosco-8.6.4-1.osg34.el6\ncondor-classads-8.6.4-1.osg34.el6\ncondor-classads-devel-8.6.4-1.osg34.el6\ncondor-cream-gahp-8.6.4-1.osg34.el6\ncondor-cron-1.1.2-1.osg34.el6\ncondor-debuginfo-8.6.4-1.osg34.el6\ncondor-kbdd-8.6.4-1.osg34.el6\ncondor-procd-8.6.4-1.osg34.el6\ncondor-python-8.6.4-1.osg34.el6\ncondor-std-universe-8.6.4-1.osg34.el6\ncondor-test-8.6.4-1.osg34.el6\ncondor-vm-gahp-8.6.4-1.osg34.el6\ncvmfs-2.3.5-1.1.osg34.el6\ncvmfs-devel-2.3.5-1.1.osg34.el6\ncvmfs-server-2.3.5-1.1.osg34.el6\ncvmfs-unittests-2.3.5-1.1.osg34.el6\nglobus-gridftp-server-11.8-1.3.osg34.el6\nglobus-gridftp-server-debuginfo-11.8-1.3.osg34.el6\nglobus-gridftp-server-devel-11.8-1.3.osg34.el6\nglobus-gridftp-server-progs-11.8-1.3.osg34.el6\ngratia-probe-1.18.1-1.osg34.el6\ngratia-probe-bdii-status-1.18.1-1.osg34.el6\ngratia-probe-common-1.18.1-1.osg34.el6\ngratia-probe-condor-1.18.1-1.osg34.el6\ngratia-probe-condor-events-1.18.1-1.osg34.el6\ngratia-probe-dcache-storage-1.18.1-1.osg34.el6\ngratia-probe-dcache-storagegroup-1.18.1-1.osg34.el6\ngratia-probe-dcache-transfer-1.18.1-1.osg34.el6\ngratia-probe-debuginfo-1.18.1-1.osg34.el6\ngratia-probe-enstore-storage-1.18.1-1.osg34.el6\ngratia-probe-enstore-tapedrive-1.18.1-1.osg34.el6\ngratia-probe-enstore-transfer-1.18.1-1.osg34.el6\ngratia-probe-glexec-1.18.1-1.osg34.el6\ngratia-probe-glideinwms-1.18.1-1.osg34.el6\ngratia-probe-gram-1.18.1-1.osg34.el6\ngratia-probe-gridftp-transfer-1.18.1-1.osg34.el6\ngratia-probe-hadoop-storage-1.18.1-1.osg34.el6\ngratia-probe-htcondor-ce-1.18.1-1.osg34.el6\ngratia-probe-lsf-1.18.1-1.osg34.el6\ngratia-probe-metric-1.18.1-1.osg34.el6\ngratia-probe-onevm-1.18.1-1.osg34.el6\ngratia-probe-pbs-lsf-1.18.1-1.osg34.el6\ngratia-probe-services-1.18.1-1.osg34.el6\ngratia-probe-sge-1.18.1-1.osg34.el6\ngratia-probe-slurm-1.18.1-1.osg34.el6\ngratia-probe-xrootd-storage-1.18.1-1.osg34.el6\ngratia-probe-xrootd-transfer-1.18.1-1.osg34.el6\ngridftp-dsi-posix-1.4-2.osg34.el6\ngridftp-dsi-posix-debuginfo-1.4-2.osg34.el6\nhtcondor-ce-2.2.1-1.osg34.el6\nhtcondor-ce-bosco-2.2.1-1.osg34.el6\nhtcondor-ce-client-2.2.1-1.osg34.el6\nhtcondor-ce-collector-2.2.1-1.osg34.el6\nhtcondor-ce-condor-2.2.1-1.osg34.el6\nhtcondor-ce-lsf-2.2.1-1.osg34.el6\nhtcondor-ce-pbs-2.2.1-1.osg34.el6\nhtcondor-ce-sge-2.2.1-1.osg34.el6\nhtcondor-ce-slurm-2.2.1-1.osg34.el6\nhtcondor-ce-view-2.2.1-1.osg34.el6\nlcmaps-plugins-verify-proxy-1.5.9-1.2.osg34.el6\nlcmaps-plugins-verify-proxy-debuginfo-1.5.9-1.2.osg34.el6\nosg-build-1.10.1-1.osg34.el6\nosg-build-base-1.10.1-1.osg34.el6\nosg-build-koji-1.10.1-1.osg34.el6\nosg-build-mock-1.10.1-1.osg34.el6\nosg-build-tests-1.10.1-1.osg34.el6\nosg-configure-2.1.0-2.osg34.el6\nosg-configure-bosco-2.1.0-2.osg34.el6\nosg-configure-ce-2.1.0-2.osg34.el6\nosg-configure-condor-2.1.0-2.osg34.el6\nosg-configure-gateway-2.1.0-2.osg34.el6\nosg-configure-gip-2.1.0-2.osg34.el6\nosg-configure-gratia-2.1.0-2.osg34.el6\nosg-configure-infoservices-2.1.0-2.osg34.el6\nosg-configure-lsf-2.1.0-2.osg34.el6\nosg-configure-managedfork-2.1.0-2.osg34.el6\nosg-configure-misc-2.1.0-2.osg34.el6\nosg-configure-network-2.1.0-2.osg34.el6\nosg-configure-pbs-2.1.0-2.osg34.el6\nosg-configure-rsv-2.1.0-2.osg34.el6\nosg-configure-sge-2.1.0-2.osg34.el6\nosg-configure-slurm-2.1.0-2.osg34.el6\nosg-configure-squid-2.1.0-2.osg34.el6\nosg-configure-tests-2.1.0-2.osg34.el6\nosg-gridftp-3.4-3.osg34.el6\nosg-test-1.11.0-1.osg34.el6\nosg-test-log-viewer-1.11.0-1.osg34.el6\nosg-version-3.4.1-1.osg34.el6\nrsv-3.14.2-1.osg34.el6\nrsv-consumers-3.14.2-1.osg34.el6\nrsv-core-3.14.2-1.osg34.el6\nrsv-metrics-3.14.2-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#enterprise-linux-7_1", 
            "text": "blahp-1.18.30.bosco-1.osg34.el7\nblahp-debuginfo-1.18.30.bosco-1.osg34.el7\ncondor-8.6.4-1.osg34.el7\ncondor-all-8.6.4-1.osg34.el7\ncondor-bosco-8.6.4-1.osg34.el7\ncondor-classads-8.6.4-1.osg34.el7\ncondor-classads-devel-8.6.4-1.osg34.el7\ncondor-cream-gahp-8.6.4-1.osg34.el7\ncondor-cron-1.1.2-1.osg34.el7\ncondor-debuginfo-8.6.4-1.osg34.el7\ncondor-kbdd-8.6.4-1.osg34.el7\ncondor-procd-8.6.4-1.osg34.el7\ncondor-python-8.6.4-1.osg34.el7\ncondor-test-8.6.4-1.osg34.el7\ncondor-vm-gahp-8.6.4-1.osg34.el7\ncvmfs-2.3.5-1.1.osg34.el7\ncvmfs-devel-2.3.5-1.1.osg34.el7\ncvmfs-server-2.3.5-1.1.osg34.el7\ncvmfs-unittests-2.3.5-1.1.osg34.el7\nglobus-gridftp-server-11.8-1.3.osg34.el7\nglobus-gridftp-server-debuginfo-11.8-1.3.osg34.el7\nglobus-gridftp-server-devel-11.8-1.3.osg34.el7\nglobus-gridftp-server-progs-11.8-1.3.osg34.el7\ngratia-probe-1.18.1-1.osg34.el7\ngratia-probe-bdii-status-1.18.1-1.osg34.el7\ngratia-probe-common-1.18.1-1.osg34.el7\ngratia-probe-condor-1.18.1-1.osg34.el7\ngratia-probe-condor-events-1.18.1-1.osg34.el7\ngratia-probe-dcache-storage-1.18.1-1.osg34.el7\ngratia-probe-dcache-storagegroup-1.18.1-1.osg34.el7\ngratia-probe-dcache-transfer-1.18.1-1.osg34.el7\ngratia-probe-debuginfo-1.18.1-1.osg34.el7\ngratia-probe-enstore-storage-1.18.1-1.osg34.el7\ngratia-probe-enstore-tapedrive-1.18.1-1.osg34.el7\ngratia-probe-enstore-transfer-1.18.1-1.osg34.el7\ngratia-probe-glexec-1.18.1-1.osg34.el7\ngratia-probe-glideinwms-1.18.1-1.osg34.el7\ngratia-probe-gram-1.18.1-1.osg34.el7\ngratia-probe-gridftp-transfer-1.18.1-1.osg34.el7\ngratia-probe-hadoop-storage-1.18.1-1.osg34.el7\ngratia-probe-htcondor-ce-1.18.1-1.osg34.el7\ngratia-probe-lsf-1.18.1-1.osg34.el7\ngratia-probe-metric-1.18.1-1.osg34.el7\ngratia-probe-onevm-1.18.1-1.osg34.el7\ngratia-probe-pbs-lsf-1.18.1-1.osg34.el7\ngratia-probe-services-1.18.1-1.osg34.el7\ngratia-probe-sge-1.18.1-1.osg34.el7\ngratia-probe-slurm-1.18.1-1.osg34.el7\ngratia-probe-xrootd-storage-1.18.1-1.osg34.el7\ngratia-probe-xrootd-transfer-1.18.1-1.osg34.el7\ngridftp-dsi-posix-1.4-2.osg34.el7\ngridftp-dsi-posix-debuginfo-1.4-2.osg34.el7\nhtcondor-ce-2.2.1-1.osg34.el7\nhtcondor-ce-bosco-2.2.1-1.osg34.el7\nhtcondor-ce-client-2.2.1-1.osg34.el7\nhtcondor-ce-collector-2.2.1-1.osg34.el7\nhtcondor-ce-condor-2.2.1-1.osg34.el7\nhtcondor-ce-lsf-2.2.1-1.osg34.el7\nhtcondor-ce-pbs-2.2.1-1.osg34.el7\nhtcondor-ce-sge-2.2.1-1.osg34.el7\nhtcondor-ce-slurm-2.2.1-1.osg34.el7\nhtcondor-ce-view-2.2.1-1.osg34.el7\nlcmaps-plugins-verify-proxy-1.5.9-1.2.osg34.el7\nlcmaps-plugins-verify-proxy-debuginfo-1.5.9-1.2.osg34.el7\nosg-build-1.10.1-1.osg34.el7\nosg-build-base-1.10.1-1.osg34.el7\nosg-build-koji-1.10.1-1.osg34.el7\nosg-build-mock-1.10.1-1.osg34.el7\nosg-build-tests-1.10.1-1.osg34.el7\nosg-configure-2.1.0-2.osg34.el7\nosg-configure-bosco-2.1.0-2.osg34.el7\nosg-configure-ce-2.1.0-2.osg34.el7\nosg-configure-condor-2.1.0-2.osg34.el7\nosg-configure-gateway-2.1.0-2.osg34.el7\nosg-configure-gip-2.1.0-2.osg34.el7\nosg-configure-gratia-2.1.0-2.osg34.el7\nosg-configure-infoservices-2.1.0-2.osg34.el7\nosg-configure-lsf-2.1.0-2.osg34.el7\nosg-configure-managedfork-2.1.0-2.osg34.el7\nosg-configure-misc-2.1.0-2.osg34.el7\nosg-configure-network-2.1.0-2.osg34.el7\nosg-configure-pbs-2.1.0-2.osg34.el7\nosg-configure-rsv-2.1.0-2.osg34.el7\nosg-configure-sge-2.1.0-2.osg34.el7\nosg-configure-slurm-2.1.0-2.osg34.el7\nosg-configure-squid-2.1.0-2.osg34.el7\nosg-configure-tests-2.1.0-2.osg34.el7\nosg-gridftp-3.4-3.osg34.el7\nosg-test-1.11.0-1.osg34.el7\nosg-test-log-viewer-1.11.0-1.osg34.el7\nosg-version-3.4.1-1.osg34.el7\nrsv-3.14.2-1.osg34.el7\nrsv-consumers-3.14.2-1.osg34.el7\nrsv-core-3.14.2-1.osg34.el7\nrsv-metrics-3.14.2-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#enterprise-linux-6_2", 
            "text": "blahp-1.18.30.bosco-1.osgup.el6  condor-8.7.2-1.osgup.el6  glite-ce-cream-client-api-c-1.15.4-2.4.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#enterprise-linux-7_2", 
            "text": "blahp-1.18.30.bosco-1.osgup.el7  condor-8.7.2-1.osgup.el7  glite-ce-cream-client-api-c-1.15.4-2.4.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp blahp-debuginfo condor condor-all condor-annex-ec2 condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone glite-ce-cream-client-api-c glite-ce-cream-client-devel  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#enterprise-linux-6_3", 
            "text": "blahp-1.18.30.bosco-1.osgup.el6\nblahp-debuginfo-1.18.30.bosco-1.osgup.el6\ncondor-8.7.2-1.osgup.el6\ncondor-all-8.7.2-1.osgup.el6\ncondor-annex-ec2-8.7.2-1.osgup.el6\ncondor-bosco-8.7.2-1.osgup.el6\ncondor-classads-8.7.2-1.osgup.el6\ncondor-classads-devel-8.7.2-1.osgup.el6\ncondor-cream-gahp-8.7.2-1.osgup.el6\ncondor-debuginfo-8.7.2-1.osgup.el6\ncondor-kbdd-8.7.2-1.osgup.el6\ncondor-procd-8.7.2-1.osgup.el6\ncondor-python-8.7.2-1.osgup.el6\ncondor-std-universe-8.7.2-1.osgup.el6\ncondor-test-8.7.2-1.osgup.el6\ncondor-vm-gahp-8.7.2-1.osgup.el6\nglite-ce-cream-client-api-c-1.15.4-2.4.osgup.el6\nglite-ce-cream-client-devel-1.15.4-2.4.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#enterprise-linux-7_3", 
            "text": "blahp-1.18.30.bosco-1.osgup.el7\nblahp-debuginfo-1.18.30.bosco-1.osgup.el7\ncondor-8.7.2-1.osgup.el7\ncondor-all-8.7.2-1.osgup.el7\ncondor-annex-ec2-8.7.2-1.osgup.el7\ncondor-bosco-8.7.2-1.osgup.el7\ncondor-classads-8.7.2-1.osgup.el7\ncondor-classads-devel-8.7.2-1.osgup.el7\ncondor-cream-gahp-8.7.2-1.osgup.el7\ncondor-debuginfo-8.7.2-1.osgup.el7\ncondor-kbdd-8.7.2-1.osgup.el7\ncondor-procd-8.7.2-1.osgup.el7\ncondor-python-8.7.2-1.osgup.el7\ncondor-test-8.7.2-1.osgup.el7\ncondor-vm-gahp-8.7.2-1.osgup.el7\nglite-ce-cream-client-api-c-1.15.4-2.4.osgup.el7\nglite-ce-cream-client-devel-1.15.4-2.4.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/", 
            "text": "OSG Software Stack -- Data Release -- 3.4.0-2 and 3.3.25-2\n\n\nRelease Date\n: 2017-06-15\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nCA Certificates based on \nIGTF 1.83\n\n\nAdded new trust anchor for accredited KISTI CA v3 (KR)\n\n\nRemoved obsolete GEANT TCS G1 and G2 (old Comodo-backed) trust anchors:\n\n\nUTN-USERFirst-Hardware\n\n\nTERENA-eScience-SSL-CA\n\n\nAAACertificateServices\n\n\nUTNAAAClient\n\n\nTERENAeSciencePersonalCA\n\n\nUTN-USERTrust-RSA-CA\n\n\nTERENA-eScience-SSL-CA-2\n\n\nTERENAeSciencePersonalCA2 (EU)\n\n\n\n\n\n\n\n\n\n\nVO Package v74\n\n\nFix the edg-mkgridmap entries for project8 and miniclean\n\n\nAdd new VOMS entry for CIGI\n\n\nAdd LIGO entry to GUMS template\n\n\nFix vo-client ATLAS mappings\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nOSG 3.3\n\n\nEnterprise Linux 6\n\n\n\n\nigtf-ca-certs-1.83-1.osg33.el6\n\n\nosg-ca-certs-1.63-1.osg33.el6\n\n\nvo-client-74-1.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nigtf-ca-certs-1.83-1.osg33.el7\n\n\nosg-ca-certs-1.63-1.osg33.el7\n\n\nvo-client-74-1.osg33.el7\n\n\n\n\nOSG 3.4\n\n\nEnterprise Linux 6\n\n\n\n\nigtf-ca-certs-1.83-1.osg34.el6\n\n\nosg-ca-certs-1.63-1.osg34.el6\n\n\nvo-client-74-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nigtf-ca-certs-1.83-1.osg34.el7\n\n\nosg-ca-certs-1.63-1.osg34.el7\n\n\nvo-client-74-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nigtf-ca-certs osg-ca-certs osg-gums-config vo-client vo-client-edgmkgridmap vo-client-lcmaps-voms\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nOSG 3.3\n\n\nEnterprise Linux 6\n\n\nigtf-ca-certs-1.83-1.osg33.el6\nosg-ca-certs-1.63-1.osg33.el6\nosg-gums-config-74-1.osg33.el6\nvo-client-74-1.osg33.el6\nvo-client-edgmkgridmap-74-1.osg33.el6\nvo-client-lcmaps-voms-74-1.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nigtf-ca-certs-1.83-1.osg33.el7\nosg-ca-certs-1.63-1.osg33.el7\nosg-gums-config-74-1.osg33.el7\nvo-client-74-1.osg33.el7\nvo-client-edgmkgridmap-74-1.osg33.el7\nvo-client-lcmaps-voms-74-1.osg33.el7\n\n\n\n\n\nOSG 3.4\n\n\nEnterprise Linux 6\n\n\nigtf-ca-certs-1.83-1.osg34.el6\nosg-ca-certs-1.63-1.osg34.el6\nosg-gums-config-74-1.osg34.el6\nvo-client-74-1.osg34.el6\nvo-client-edgmkgridmap-74-1.osg34.el6\nvo-client-lcmaps-voms-74-1.osg34.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nigtf-ca-certs-1.83-1.osg34.el7\nosg-ca-certs-1.63-1.osg34.el7\nosg-gums-config-74-1.osg34.el7\nvo-client-74-1.osg34.el7\nvo-client-edgmkgridmap-74-1.osg34.el7\nvo-client-lcmaps-voms-74-1.osg34.el7", 
            "title": "OSG Release 3.4.0-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/#osg-software-stack-data-release-340-2-and-3325-2", 
            "text": "Release Date : 2017-06-15", 
            "title": "OSG Software Stack -- Data Release -- 3.4.0-2 and 3.3.25-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/#summary-of-changes", 
            "text": "This release contains:   CA Certificates based on  IGTF 1.83  Added new trust anchor for accredited KISTI CA v3 (KR)  Removed obsolete GEANT TCS G1 and G2 (old Comodo-backed) trust anchors:  UTN-USERFirst-Hardware  TERENA-eScience-SSL-CA  AAACertificateServices  UTNAAAClient  TERENAeSciencePersonalCA  UTN-USERTrust-RSA-CA  TERENA-eScience-SSL-CA-2  TERENAeSciencePersonalCA2 (EU)      VO Package v74  Fix the edg-mkgridmap entries for project8 and miniclean  Add new VOMS entry for CIGI  Add LIGO entry to GUMS template  Fix vo-client ATLAS mappings     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/#osg-33", 
            "text": "", 
            "title": "OSG 3.3"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/#enterprise-linux-6", 
            "text": "igtf-ca-certs-1.83-1.osg33.el6  osg-ca-certs-1.63-1.osg33.el6  vo-client-74-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/#enterprise-linux-7", 
            "text": "igtf-ca-certs-1.83-1.osg33.el7  osg-ca-certs-1.63-1.osg33.el7  vo-client-74-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/#osg-34", 
            "text": "", 
            "title": "OSG 3.4"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/#enterprise-linux-6_1", 
            "text": "igtf-ca-certs-1.83-1.osg34.el6  osg-ca-certs-1.63-1.osg34.el6  vo-client-74-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/#enterprise-linux-7_1", 
            "text": "igtf-ca-certs-1.83-1.osg34.el7  osg-ca-certs-1.63-1.osg34.el7  vo-client-74-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  igtf-ca-certs osg-ca-certs osg-gums-config vo-client vo-client-edgmkgridmap vo-client-lcmaps-voms  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/#osg-33_1", 
            "text": "", 
            "title": "OSG 3.3"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/#enterprise-linux-6_2", 
            "text": "igtf-ca-certs-1.83-1.osg33.el6\nosg-ca-certs-1.63-1.osg33.el6\nosg-gums-config-74-1.osg33.el6\nvo-client-74-1.osg33.el6\nvo-client-edgmkgridmap-74-1.osg33.el6\nvo-client-lcmaps-voms-74-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/#enterprise-linux-7_2", 
            "text": "igtf-ca-certs-1.83-1.osg33.el7\nosg-ca-certs-1.63-1.osg33.el7\nosg-gums-config-74-1.osg33.el7\nvo-client-74-1.osg33.el7\nvo-client-edgmkgridmap-74-1.osg33.el7\nvo-client-lcmaps-voms-74-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/#osg-34_1", 
            "text": "", 
            "title": "OSG 3.4"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/#enterprise-linux-6_3", 
            "text": "igtf-ca-certs-1.83-1.osg34.el6\nosg-ca-certs-1.63-1.osg34.el6\nosg-gums-config-74-1.osg34.el6\nvo-client-74-1.osg34.el6\nvo-client-edgmkgridmap-74-1.osg34.el6\nvo-client-lcmaps-voms-74-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/#enterprise-linux-7_3", 
            "text": "igtf-ca-certs-1.83-1.osg34.el7\nosg-ca-certs-1.63-1.osg34.el7\nosg-gums-config-74-1.osg34.el7\nvo-client-74-1.osg34.el7\nvo-client-edgmkgridmap-74-1.osg34.el7\nvo-client-lcmaps-voms-74-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/", 
            "text": "OSG Software Release 3.4.0\n\n\nRelease Date\n: 2017-06-14\n\n\nWhat's New in OSG 3.4\n\n\nThe OSG 3.4.0 software stack features a more streamlined and consolidated package list. Specifically, the varied authentication solutions proved to be good candidates for consolidation and a new piece of software, the LCMAPS VOMS plugin, has been designed to replace both edg-mkgridmap and GUMS.\n\n\nSee \ninstall the LCMAPS VOMS plugin\n to replace GUMS + edg-mkgridmap.\n\n\nSee \nmigrating from edg-mkgridmap to lcmaps VOMS plugin\n to transistion from edg-mkgridmap.\n\n\nIn 3.4.0, we dropped HDFS 2.x with the intention of adding HDFS 3.x in a subsequent OSG 3.4 release when it becomes available upstream.\n\n\nIn addition to GUMS, edg-mkgridmap, and HDFS 2.x, we dropped packages related to the following software:\n\n\n\n\nVOMS Admin Server \u2212 \nRetirement Policy\n\n\nBeStMan \u2212 replaced by \nLoad Balanced GridFTP\n\n\nGLExec \u2212 replaced by \nSingularty\n\n\nGlobus GRAM \u2212 available from EPEL\n\n\nGIP and OSG Info Services \u2212 BDII servers retired\n\n\n\n\nThe aforementioned packages are still be available in OSG 3.3 and will receive regular support until December 2017 and security updates until June 2018 per our \nrelease policy\n. See \nthis section\n for the complete list of packages removed from OSG 3.4.\n\n\n OSG 3.4 contains only 64-bit components. \n StashCache is supported on EL7 only. \n xrootd-lcmaps will remain at 1.2.1-2 on EL6.\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nOSG 3.4.0\n\n\nHTCondor 8.6.3\n: See \nUpgrading from 8.4\n for additional information\n\n\nFrontier squid 3.5.24-3.1\n: See \nUpgrading from 2.x to 3.x\n for additional information\n\n\nUpdate to \nXRootD 4.6.1\n\n\nUpdate to xrootd-lcmaps 1.3.3 for EL7\n\n\n\n\n\n\nUpdate StashCache meta-packages to require XRootD 4.6.1\n\n\nUpdate to \nGlideinWMS 3.2.19\n\n\nFrontier squid 3.5.24-3.1\n: See \nUpgrading from 2.x to 3.x\n for additional information\n\n\nMake the LCMAPS VOMS plugin consider only the first FQAN to be consistent with GUMS\n\n\nHTCondor-CE: Add WholeNodeWanted ClassAd expression so jobs can request a whole node from the batch system\n\n\nHTCondor 8.6.3\n: See \nUpgrading from 8.4\n for additional information\n\n\nOSG CE 3.4\n\n\nAdd vo-client-lcmaps-voms dependency\n\n\nRemove gridftp dependency\n\n\nDrop client tools\n\n\n\n\n\n\nAdd vo-client-lcmaps-voms dependency to osg-gridftp\n\n\nFix osg-update-vos script to clean yum cache in order pick up the latest vo-client RPM\n\n\nosg-ca-scripts now refers to repo.grid.iu.edu (rather than the retired software.grid.iu.edu)\n\n\nosg-configure 2.0.0\n\n\nreject empty \nallowed_vos\n in subclusters\n\n\nget default \nallowed_vos\n from LCMAPS VOMS plugin\n\n\nissue warning (rather than error out) if OSG_APP or OSG_DATA directories are not present\n\n\ndrop 'RSV is not installed' warning\n\n\nremove configure-osg alias\n\n\ndeprecate GUMS support\n\n\ndisable GRAM configuration\n\n\ndrop managedfork and network modules\n\n\ndrop glexec support\n\n\nremove nonfunctional osg-cleanup\n\n\n\n\n\n\nDrop glexec and java from osg-wn-client\n\n\nBeSTMan 2 is no longer part of the OSG Software Stack\n\n\nGUMS is no longer part of the OSG Software Stack\n\n\nedg-mkgridmap in no longer part of the OSG Software Stack\n\n\nDrop bestman2 and globus*run RSV metrics\n\n\nosg-build 1.10.0\n\n\ndrop vdt-build alias\n\n\ndrop ~/.osg-build.ini configuration file\n\n\n\n\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nKnown Issues\n\n\n\n\nCurrently, OSG 3.4 CEs cannot be configured to authenticate via GUMS (\nSOFTWARE-2482\n). This issue is expected to be fixed in the July release.\n\n\nUsing the LCMAPS VOMS will result in a failing \"supported VO\" RSV test (\nSOFTWARE-2763\n). This can be ignored and a fix is targeted for the July release.\n\n\nIn GlideinWMS, a small configuration change must be added to account for changes in HTCondor 8.6. Add the following line to the HTCondor configuration.\n\n\n\n\nCOLLECTOR.USE_SHARED_PORT=False\n\n\n\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nautopyfactory-2.4.6-4.osg34.el6\n\n\nblahp-1.18.29.bosco-3.osg34.el6\n\n\nbwctl-1.4-7.osg34.el6\n\n\ncctools-4.4.3-1.osg34.el6\n\n\ncondor-8.6.3-1.1.osg34.el6\n\n\ncondor-cron-1.1.1-2.osg34.el6\n\n\ncvmfs-2.3.5-1.osg34.el6\n\n\ncvmfs-config-osg-2.0-2.osg34.el6\n\n\ncvmfs-x509-helper-1.0-1.osg34.el6\n\n\nfrontier-squid-3.5.24-3.1.osg34.el6\n\n\nglideinwms-3.2.19-2.osg34.el6\n\n\nglite-build-common-cpp-3.3.0.2-1.osg34.el6\n\n\nglite-ce-cream-client-api-c-1.15.4-2.3.osg34.el6\n\n\nglite-ce-wsdl-1.15.1-1.1.osg34.el6\n\n\nglite-lbjp-common-gsoap-plugin-3.2.12-1.1.osg34.el6\n\n\nglobus-ftp-client-8.29-1.1.osg34.el6\n\n\nglobus-gridftp-osg-extensions-0.3-2.osg34.el6\n\n\nglobus-gridftp-server-11.8-1.1.osg34.el6\n\n\nglobus-gridftp-server-control-4.1-1.3.osg34.el6\n\n\ngratia-probe-1.17.5-1.osg34.el6\n\n\ngsi-openssh-7.1p2f-1.2.osg34.el6\n\n\nhtcondor-ce-2.2.0-1.osg34.el6\n\n\nigtf-ca-certs-1.82-1.osg34.el6\n\n\njavascriptrrd-1.1.1-1.osg34.el6\n\n\nkoji-1.11.0-1.5.osg34.el6\n\n\nlcas-lcmaps-gt4-interface-0.3.1-1.2.osg34.el6\n\n\nlcmaps-1.6.6-1.6.osg34.el6\n\n\nlcmaps-plugins-basic-1.7.0-2.osg34.el6\n\n\nlcmaps-plugins-scas-client-0.5.6-1.osg34.el6\n\n\nlcmaps-plugins-verify-proxy-1.5.9-1.1.osg34.el6\n\n\nlcmaps-plugins-voms-1.7.1-1.4.osg34.el6\n\n\nllrun-0.1.3-1.3.osg34.el6\n\n\nmash-0.5.22-3.osg34.el6\n\n\nmyproxy-6.1.18-1.4.osg34.el6\n\n\nnuttcp-6.1.2-1.osg34.el6\n\n\nosg-build-1.10.0-1.osg34.el6\n\n\nosg-ca-certs-1.62-1.osg34.el6\n\n\nosg-ca-certs-updater-1.4-1.osg34.el6\n\n\nosg-ca-generator-1.2.0-1.osg34.el6\n\n\nosg-ca-scripts-1.1.6-1.osg34.el6\n\n\nosg-ce-3.4-2.osg34.el6\n\n\nosg-configure-2.0.0-3.osg34.el6\n\n\nosg-control-1.1.0-1.osg34.el6\n\n\nosg-gridftp-3.4-2.osg34.el6\n\n\nosg-gridftp-xrootd-3.4-1.osg34.el6\n\n\nosg-oasis-7-9.osg34.el6\n\n\nosg-pki-tools-1.2.20-1.osg34.el6\n\n\nosg-system-profiler-1.4.0-1.osg34.el6\n\n\nosg-test-1.10.1-1.osg34.el6\n\n\nosg-tested-internal-3.4-2.osg34.el6\n\n\nosg-update-vos-1.4.0-1.osg34.el6\n\n\nosg-version-3.4.0-1.osg34.el6\n\n\nosg-vo-map-0.0.2-1.osg34.el6\n\n\nosg-wn-client-3.4-1.osg34.el6\n\n\nowamp-3.2rc4-2.osg34.el6\n\n\npegasus-4.7.4-1.1.osg34.el6\n\n\nrsv-3.14.0-2.osg34.el6\n\n\nrsv-gwms-tester-1.1.2-1.osg34.el6\n\n\nuberftp-2.8-2.1.osg34.el6\n\n\nvo-client-73-1.osg34.el6\n\n\nvoms-2.0.14-1.3.osg34.el6\n\n\nxacml-1.5.0-1.osg34.el6\n\n\nxrootd-4.6.1-1.osg34.el6\n\n\nxrootd-dsi-3.0.4-22.osg34.el6\n\n\nxrootd-lcmaps-1.2.1-2.osg34.el6\n\n\nxrootd-voms-plugin-0.4.0-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nautopyfactory-2.4.6-4.osg34.el7\n\n\nblahp-1.18.29.bosco-3.osg34.el7\n\n\nbwctl-1.4-7.osg34.el7\n\n\ncctools-4.4.3-1.osg34.el7\n\n\ncondor-8.6.3-1.1.osg34.el7\n\n\ncondor-cron-1.1.1-2.osg34.el7\n\n\ncvmfs-2.3.5-1.osg34.el7\n\n\ncvmfs-config-osg-2.0-2.osg34.el7\n\n\ncvmfs-x509-helper-1.0-1.osg34.el7\n\n\nfrontier-squid-3.5.24-3.1.osg34.el7\n\n\nglideinwms-3.2.19-2.osg34.el7\n\n\nglite-build-common-cpp-3.3.0.2-1.osg34.el7\n\n\nglite-ce-cream-client-api-c-1.15.4-2.3.osg34.el7\n\n\nglite-ce-wsdl-1.15.1-1.1.osg34.el7\n\n\nglite-lbjp-common-gsoap-plugin-3.2.12-1.1.osg34.el7\n\n\nglobus-ftp-client-8.29-1.1.osg34.el7\n\n\nglobus-gridftp-osg-extensions-0.3-2.osg34.el7\n\n\nglobus-gridftp-server-11.8-1.1.osg34.el7\n\n\nglobus-gridftp-server-control-4.1-1.3.osg34.el7\n\n\ngratia-probe-1.17.5-1.osg34.el7\n\n\ngsi-openssh-7.1p2f-1.2.osg34.el7\n\n\nhtcondor-ce-2.2.0-1.osg34.el7\n\n\nigtf-ca-certs-1.82-1.osg34.el7\n\n\njavascriptrrd-1.1.1-1.osg34.el7\n\n\nkoji-1.11.0-1.5.osg34.el7\n\n\nlcas-lcmaps-gt4-interface-0.3.1-1.2.osg34.el7\n\n\nlcmaps-1.6.6-1.6.osg34.el7\n\n\nlcmaps-plugins-basic-1.7.0-2.osg34.el7\n\n\nlcmaps-plugins-scas-client-0.5.6-1.osg34.el7\n\n\nlcmaps-plugins-verify-proxy-1.5.9-1.1.osg34.el7\n\n\nlcmaps-plugins-voms-1.7.1-1.4.osg34.el7\n\n\nllrun-0.1.3-1.3.osg34.el7\n\n\nmash-0.5.22-3.osg34.el7\n\n\nmyproxy-6.1.18-1.4.osg34.el7\n\n\nnuttcp-6.1.2-1.osg34.el7\n\n\nosg-build-1.10.0-1.osg34.el7\n\n\nosg-ca-certs-1.62-1.osg34.el7\n\n\nosg-ca-certs-updater-1.4-1.osg34.el7\n\n\nosg-ca-generator-1.2.0-1.osg34.el7\n\n\nosg-ca-scripts-1.1.6-1.osg34.el7\n\n\nosg-ce-3.4-2.osg34.el7\n\n\nosg-configure-2.0.0-3.osg34.el7\n\n\nosg-control-1.1.0-1.osg34.el7\n\n\nosg-gridftp-3.4-2.osg34.el7\n\n\nosg-gridftp-xrootd-3.4-1.osg34.el7\n\n\nosg-oasis-7-9.osg34.el7\n\n\nosg-pki-tools-1.2.20-1.osg34.el7\n\n\nosg-system-profiler-1.4.0-1.osg34.el7\n\n\nosg-test-1.10.1-1.osg34.el7\n\n\nosg-tested-internal-3.4-2.osg34.el7\n\n\nosg-update-vos-1.4.0-1.osg34.el7\n\n\nosg-version-3.4.0-1.osg34.el7\n\n\nosg-vo-map-0.0.2-1.osg34.el7\n\n\nosg-wn-client-3.4-1.osg34.el7\n\n\nowamp-3.2rc4-2.osg34.el7\n\n\npegasus-4.7.4-1.1.osg34.el7\n\n\nrsv-3.14.0-2.osg34.el7\n\n\nrsv-gwms-tester-1.1.2-1.osg34.el7\n\n\nstashcache-0.7-2.osg34.el7\n\n\nuberftp-2.8-2.1.osg34.el7\n\n\nvo-client-73-1.osg34.el7\n\n\nvoms-2.0.14-1.3.osg34.el7\n\n\nxacml-1.5.0-1.osg34.el7\n\n\nxrootd-4.6.1-1.osg34.el7\n\n\nxrootd-dsi-3.0.4-22.osg34.el7\n\n\nxrootd-lcmaps-1.3.3-3.osg34.el7\n\n\nxrootd-voms-plugin-0.4.0-1.osg34.el7\n\n\n\n\n#PackagesRemoved\n\n\nPackages Removed from OSG 3.4\n\n\nMany packages that were available in OSG 3.3 will not be included in the OSG 3.4 software stack. The following packages are not in the 3.4 repositories.\n\n\nEnterprise Linux 6\n\n\nbestman2\nbigtop-jsvc\nbigtop-utils\ncilogon-openid-ca-cert\ncilogon-osg-ca-cert\ncog-jglobus-axis\nedg-mkgridmap\nemi-trustmanager\nemi-trustmanager-axis\nemi-trustmanager-tomcat\ngip\nglexec\nglexec-wrapper-scripts\nglite-ce-cream-utils\nglite-lbjp-common-gss\nglobus-authz\nglobus-authz-callout-error\nglobus-callout\nglobus-common\nglobus-ftp-control\nglobus-gass-cache\nglobus-gass-cache-program\nglobus-gass-copy\nglobus-gass-server-ez\nglobus-gass-transfer\nglobus-gatekeeper\nglobus-gfork\nglobus-gram-audit\nglobus-gram-client\nglobus-gram-client-tools\nglobus-gram-job-manager\nglobus-gram-job-manager-callout-error\nglobus-gram-job-manager-condor\nglobus-gram-job-manager-fork\nglobus-gram-job-manager-lsf\nglobus-gram-job-manager-managedfork\nglobus-gram-job-manager-pbs\nglobus-gram-job-manager-scripts\nglobus-gram-job-manager-sge\nglobus-gram-protocol\nglobus-gridmap-callout-error\nglobus-gsi-callback\nglobus-gsi-cert-utils\nglobus-gsi-credential\nglobus-gsi-openssl-error\nglobus-gsi-proxy-core\nglobus-gsi-proxy-ssl\nglobus-gsi-sysconfig\nglobus-gssapi-error\nglobus-gssapi-gsi\nglobus-gss-assist\nglobus-io\nglobus-openssl-module\nglobus-proxy-utils\nglobus-rsl\nglobus-scheduler-event-generator\nglobus-simple-ca\nglobus-usage\nglobus-xio\nglobus-xio-gsi-driver\nglobus-xioperf\nglobus-xio-pipe-driver\nglobus-xio-popen-driver\nglobus-xio-udt-driver\ngratia\ngratia-reporting-email\ngridftp-hdfs\ngums\nhadoop\nI2util\njetty\njglobus\njoda-time\nlcmaps-plugins-glexec-tracking\nlcmaps-plugins-gums-client\nlcmaps-plugins-mount-under-scratch\nlcmaps-plugins-process-tracking\nmkgltempdir\nndt\nnetlogger\nosg-cert-scripts\nosg-cleanup\nosg-gridftp-hdfs\nosg-gums\nosg-info-services\nosg-java7-compat\nosg-release\nosg-release-itb\nosg-se-bestman\nosg-se-bestman-xrootd\nosg-se-hadoop\nosg-voms\nosg-webapp-common\nprivilege-xacml\nrsv-vo-gwms\nstashcache\nstashcache-daemon\nvoms-admin-client\nvoms-admin-server\nvoms-api-java\nvoms-mysql-plugin\nweb100_userland\nxrootd-hdfs\nxrootd-status-probe\nzookeeper\n\n\n\n\n\nEnterprise Linux 7\n\n\naxis\nbestman2\nbigtop-jsvc\nbigtop-utils\ncilogon-openid-ca-cert\ncilogon-osg-ca-cert\ncog-jglobus-axis\nedg-mkgridmap\nemi-trustmanager\nemi-trustmanager-axis\nemi-trustmanager-tomcat\ngip\nglexec\nglexec-wrapper-scripts\nglite-lbjp-common-gss\nglobus-authz\nglobus-authz-callout-error\nglobus-callout\nglobus-common\nglobus-ftp-control\nglobus-gass-cache\nglobus-gass-cache-program\nglobus-gass-copy\nglobus-gass-server-ez\nglobus-gass-transfer\nglobus-gatekeeper\nglobus-gfork\nglobus-gram-audit\nglobus-gram-client\nglobus-gram-client-tools\nglobus-gram-job-manager\nglobus-gram-job-manager-callout-error\nglobus-gram-job-manager-condor\nglobus-gram-job-manager-fork\nglobus-gram-job-manager-lsf\nglobus-gram-job-manager-managedfork\nglobus-gram-job-manager-pbs\nglobus-gram-job-manager-scripts\nglobus-gram-job-manager-sge\nglobus-gram-protocol\nglobus-gridmap-callout-error\nglobus-gsi-callback\nglobus-gsi-cert-utils\nglobus-gsi-credential\nglobus-gsi-openssl-error\nglobus-gsi-proxy-core\nglobus-gsi-proxy-ssl\nglobus-gsi-sysconfig\nglobus-gssapi-error\nglobus-gssapi-gsi\nglobus-gss-assist\nglobus-io\nglobus-openssl-module\nglobus-proxy-utils\nglobus-rsl\nglobus-scheduler-event-generator\nglobus-simple-ca\nglobus-usage\nglobus-xio\nglobus-xio-gsi-driver\nglobus-xioperf\nglobus-xio-pipe-driver\nglobus-xio-popen-driver\nglobus-xio-udt-driver\ngratia\ngratia-reporting-email\ngridftp-hdfs\ngums\nhadoop\nI2util\njavamail\njetty\njglobus\njoda-time\nlcmaps-plugins-glexec-tracking\nlcmaps-plugins-gums-client\nlcmaps-plugins-mount-under-scratch\nlcmaps-plugins-process-tracking\nmkgltempdir\nndt\nnetlogger\nosg-cert-scripts\nosg-cleanup\nosg-gridftp-hdfs\nosg-gums\nosg-info-services\nosg-java7-compat\nosg-release\nosg-release-itb\nosg-se-bestman\nosg-se-bestman-xrootd\nosg-se-hadoop\nosg-voms\nosg-webapp-common\nprivilege-xacml\npython-ZSI\nPyXML\nrsv-vo-gwms\nstashcache-daemon\nvoms-admin-client\nvoms-mysql-plugin\nweb100_userland\nwsdl4j\nxrootd-hdfs\nxrootd-status-probe\nzookeeper\n\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nautopyfactory-cloud autopyfactory-common autopyfactory-panda autopyfactory-plugins-cloud autopyfactory-plugins-local autopyfactory-plugins-monitor autopyfactory-plugins-panda autopyfactory-plugins-remote autopyfactory-plugins-scheds autopyfactory-proxymanager autopyfactory-remote autopyfactory-wms blahp blahp-debuginfo bwctl bwctl-client bwctl-debuginfo bwctl-devel bwctl-server cctools-chirp cctools-debuginfo cctools-doc cctools-dttools cctools-makeflow cctools-parrot cctools-resource_monitor cctools-sand cctools-wavefront cctools-weaver cctools-work_queue condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-cron condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp cvmfs cvmfs-config-osg cvmfs-devel cvmfs-server cvmfs-unittests cvmfs-x509-helper cvmfs-x509-helper-debuginfo frontier-squid frontier-squid-debuginfo glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone glite-build-common-cpp glite-ce-cream-client-api-c glite-ce-cream-client-devel glite-ce-wsdl glite-lbjp-common-gsoap-plugin glite-lbjp-common-gsoap-plugin-debuginfo glite-lbjp-common-gsoap-plugin-devel globus-ftp-client globus-ftp-client-debuginfo globus-ftp-client-devel globus-ftp-client-doc globus-gridftp-osg-extensions globus-gridftp-osg-extensions-debuginfo globus-gridftp-server globus-gridftp-server-control globus-gridftp-server-control-debuginfo globus-gridftp-server-control-devel globus-gridftp-server-debuginfo globus-gridftp-server-devel globus-gridftp-server-progs gratia-probe-bdii-status gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glexec gratia-probe-glideinwms gratia-probe-gram gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer gsi-openssh gsi-openssh-clients gsi-openssh-debuginfo gsi-openssh-server htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-slurm htcondor-ce-view igtf-ca-certs javascriptrrd koji koji-builder koji-hub koji-hub-plugins koji-utils koji-vm koji-web lcas-lcmaps-gt4-interface lcas-lcmaps-gt4-interface-debuginfo lcmaps lcmaps-common-devel lcmaps-db-templates lcmaps-debuginfo lcmaps-devel lcmaps-plugins-basic lcmaps-plugins-basic-debuginfo lcmaps-plugins-basic-ldap lcmaps-plugins-scas-client lcmaps-plugins-scas-client-debuginfo lcmaps-plugins-verify-proxy lcmaps-plugins-verify-proxy-debuginfo lcmaps-plugins-voms lcmaps-plugins-voms-debuginfo lcmaps-without-gsi lcmaps-without-gsi-devel llrun llrun-debuginfo mash myproxy myproxy-admin myproxy-debuginfo myproxy-devel myproxy-doc myproxy-libs myproxy-server myproxy-voms nuttcp nuttcp-debuginfo osg-base-ce osg-base-ce-bosco osg-base-ce-condor osg-base-ce-lsf osg-base-ce-pbs osg-base-ce-sge osg-base-ce-slurm osg-build osg-build-base osg-build-koji osg-build-mock osg-build-tests osg-ca-certs osg-ca-certs-updater osg-ca-generator osg-ca-scripts osg-ce osg-ce-bosco osg-ce-condor osg-ce-lsf osg-ce-pbs osg-ce-sge osg-ce-slurm osg-configure osg-configure-bosco osg-configure-ce osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-control osg-gridftp osg-gridftp-xrootd osg-gums-config osg-htcondor-ce osg-htcondor-ce-bosco osg-htcondor-ce-condor osg-htcondor-ce-lsf osg-htcondor-ce-pbs osg-htcondor-ce-sge osg-htcondor-ce-slurm osg-oasis osg-pki-tools osg-pki-tools-tests osg-release osg-release-itb osg-system-profiler osg-system-profiler-viewer osg-test osg-tested-internal osg-test-log-viewer osg-update-data osg-update-vos osg-version osg-vo-map osg-wn-client owamp owamp-client owamp-debuginfo owamp-server pegasus pegasus-debuginfo rsv rsv-consumers rsv-core rsv-gwms-tester rsv-metrics uberftp uberftp-debuginfo vo-client vo-client-edgmkgridmap vo-client-lcmaps-voms voms voms-clients-cpp voms-debuginfo voms-devel voms-doc voms-server xacml xacml-debuginfo xacml-devel xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-dsi xrootd-dsi-debuginfo xrootd-fuse xrootd-lcmaps xrootd-lcmaps-debuginfo xrootd-libs xrootd-private-devel xrootd-python xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs xrootd-voms-plugin xrootd-voms-plugin-debuginfo xrootd-voms-plugin-devel\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nautopyfactory-2.4.6-4.osg34.el6\nautopyfactory-cloud-2.4.6-4.osg34.el6\nautopyfactory-common-2.4.6-4.osg34.el6\nautopyfactory-panda-2.4.6-4.osg34.el6\nautopyfactory-plugins-cloud-2.4.6-4.osg34.el6\nautopyfactory-plugins-local-2.4.6-4.osg34.el6\nautopyfactory-plugins-monitor-2.4.6-4.osg34.el6\nautopyfactory-plugins-panda-2.4.6-4.osg34.el6\nautopyfactory-plugins-remote-2.4.6-4.osg34.el6\nautopyfactory-plugins-scheds-2.4.6-4.osg34.el6\nautopyfactory-proxymanager-2.4.6-4.osg34.el6\nautopyfactory-remote-2.4.6-4.osg34.el6\nautopyfactory-wms-2.4.6-4.osg34.el6\nblahp-1.18.29.bosco-3.osg34.el6\nblahp-debuginfo-1.18.29.bosco-3.osg34.el6\nbwctl-1.4-7.osg34.el6\nbwctl-client-1.4-7.osg34.el6\nbwctl-debuginfo-1.4-7.osg34.el6\nbwctl-devel-1.4-7.osg34.el6\nbwctl-server-1.4-7.osg34.el6\ncctools-4.4.3-1.osg34.el6\ncctools-chirp-4.4.3-1.osg34.el6\ncctools-debuginfo-4.4.3-1.osg34.el6\ncctools-doc-4.4.3-1.osg34.el6\ncctools-dttools-4.4.3-1.osg34.el6\ncctools-makeflow-4.4.3-1.osg34.el6\ncctools-parrot-4.4.3-1.osg34.el6\ncctools-resource_monitor-4.4.3-1.osg34.el6\ncctools-sand-4.4.3-1.osg34.el6\ncctools-wavefront-4.4.3-1.osg34.el6\ncctools-weaver-4.4.3-1.osg34.el6\ncctools-work_queue-4.4.3-1.osg34.el6\ncondor-8.6.3-1.1.osg34.el6\ncondor-all-8.6.3-1.1.osg34.el6\ncondor-bosco-8.6.3-1.1.osg34.el6\ncondor-classads-8.6.3-1.1.osg34.el6\ncondor-classads-devel-8.6.3-1.1.osg34.el6\ncondor-cream-gahp-8.6.3-1.1.osg34.el6\ncondor-cron-1.1.1-2.osg34.el6\ncondor-debuginfo-8.6.3-1.1.osg34.el6\ncondor-kbdd-8.6.3-1.1.osg34.el6\ncondor-procd-8.6.3-1.1.osg34.el6\ncondor-python-8.6.3-1.1.osg34.el6\ncondor-std-universe-8.6.3-1.1.osg34.el6\ncondor-test-8.6.3-1.1.osg34.el6\ncondor-vm-gahp-8.6.3-1.1.osg34.el6\ncvmfs-2.3.5-1.osg34.el6\ncvmfs-config-osg-2.0-2.osg34.el6\ncvmfs-devel-2.3.5-1.osg34.el6\ncvmfs-server-2.3.5-1.osg34.el6\ncvmfs-unittests-2.3.5-1.osg34.el6\ncvmfs-x509-helper-1.0-1.osg34.el6\ncvmfs-x509-helper-debuginfo-1.0-1.osg34.el6\nfrontier-squid-3.5.24-3.1.osg34.el6\nfrontier-squid-debuginfo-3.5.24-3.1.osg34.el6\nglideinwms-3.2.19-2.osg34.el6\nglideinwms-common-tools-3.2.19-2.osg34.el6\nglideinwms-condor-common-config-3.2.19-2.osg34.el6\nglideinwms-factory-3.2.19-2.osg34.el6\nglideinwms-factory-condor-3.2.19-2.osg34.el6\nglideinwms-glidecondor-tools-3.2.19-2.osg34.el6\nglideinwms-libs-3.2.19-2.osg34.el6\nglideinwms-minimal-condor-3.2.19-2.osg34.el6\nglideinwms-usercollector-3.2.19-2.osg34.el6\nglideinwms-userschedd-3.2.19-2.osg34.el6\nglideinwms-vofrontend-3.2.19-2.osg34.el6\nglideinwms-vofrontend-standalone-3.2.19-2.osg34.el6\nglite-build-common-cpp-3.3.0.2-1.osg34.el6\nglite-ce-cream-client-api-c-1.15.4-2.3.osg34.el6\nglite-ce-cream-client-devel-1.15.4-2.3.osg34.el6\nglite-ce-wsdl-1.15.1-1.1.osg34.el6\nglite-lbjp-common-gsoap-plugin-3.2.12-1.1.osg34.el6\nglite-lbjp-common-gsoap-plugin-debuginfo-3.2.12-1.1.osg34.el6\nglite-lbjp-common-gsoap-plugin-devel-3.2.12-1.1.osg34.el6\nglobus-ftp-client-8.29-1.1.osg34.el6\nglobus-ftp-client-debuginfo-8.29-1.1.osg34.el6\nglobus-ftp-client-devel-8.29-1.1.osg34.el6\nglobus-ftp-client-doc-8.29-1.1.osg34.el6\nglobus-gridftp-osg-extensions-0.3-2.osg34.el6\nglobus-gridftp-osg-extensions-debuginfo-0.3-2.osg34.el6\nglobus-gridftp-server-11.8-1.1.osg34.el6\nglobus-gridftp-server-control-4.1-1.3.osg34.el6\nglobus-gridftp-server-control-debuginfo-4.1-1.3.osg34.el6\nglobus-gridftp-server-control-devel-4.1-1.3.osg34.el6\nglobus-gridftp-server-debuginfo-11.8-1.1.osg34.el6\nglobus-gridftp-server-devel-11.8-1.1.osg34.el6\nglobus-gridftp-server-progs-11.8-1.1.osg34.el6\ngratia-probe-1.17.5-1.osg34.el6\ngratia-probe-bdii-status-1.17.5-1.osg34.el6\ngratia-probe-common-1.17.5-1.osg34.el6\ngratia-probe-condor-1.17.5-1.osg34.el6\ngratia-probe-condor-events-1.17.5-1.osg34.el6\ngratia-probe-dcache-storage-1.17.5-1.osg34.el6\ngratia-probe-dcache-storagegroup-1.17.5-1.osg34.el6\ngratia-probe-dcache-transfer-1.17.5-1.osg34.el6\ngratia-probe-debuginfo-1.17.5-1.osg34.el6\ngratia-probe-enstore-storage-1.17.5-1.osg34.el6\ngratia-probe-enstore-tapedrive-1.17.5-1.osg34.el6\ngratia-probe-enstore-transfer-1.17.5-1.osg34.el6\ngratia-probe-glexec-1.17.5-1.osg34.el6\ngratia-probe-glideinwms-1.17.5-1.osg34.el6\ngratia-probe-gram-1.17.5-1.osg34.el6\ngratia-probe-gridftp-transfer-1.17.5-1.osg34.el6\ngratia-probe-hadoop-storage-1.17.5-1.osg34.el6\ngratia-probe-htcondor-ce-1.17.5-1.osg34.el6\ngratia-probe-lsf-1.17.5-1.osg34.el6\ngratia-probe-metric-1.17.5-1.osg34.el6\ngratia-probe-onevm-1.17.5-1.osg34.el6\ngratia-probe-pbs-lsf-1.17.5-1.osg34.el6\ngratia-probe-services-1.17.5-1.osg34.el6\ngratia-probe-sge-1.17.5-1.osg34.el6\ngratia-probe-slurm-1.17.5-1.osg34.el6\ngratia-probe-xrootd-storage-1.17.5-1.osg34.el6\ngratia-probe-xrootd-transfer-1.17.5-1.osg34.el6\ngsi-openssh-7.1p2f-1.2.osg34.el6\ngsi-openssh-clients-7.1p2f-1.2.osg34.el6\ngsi-openssh-debuginfo-7.1p2f-1.2.osg34.el6\ngsi-openssh-server-7.1p2f-1.2.osg34.el6\nhtcondor-ce-2.2.0-1.osg34.el6\nhtcondor-ce-bosco-2.2.0-1.osg34.el6\nhtcondor-ce-client-2.2.0-1.osg34.el6\nhtcondor-ce-collector-2.2.0-1.osg34.el6\nhtcondor-ce-condor-2.2.0-1.osg34.el6\nhtcondor-ce-lsf-2.2.0-1.osg34.el6\nhtcondor-ce-pbs-2.2.0-1.osg34.el6\nhtcondor-ce-sge-2.2.0-1.osg34.el6\nhtcondor-ce-slurm-2.2.0-1.osg34.el6\nhtcondor-ce-view-2.2.0-1.osg34.el6\nigtf-ca-certs-1.82-1.osg34.el6\njavascriptrrd-1.1.1-1.osg34.el6\nkoji-1.11.0-1.5.osg34.el6\nkoji-builder-1.11.0-1.5.osg34.el6\nkoji-hub-1.11.0-1.5.osg34.el6\nkoji-hub-plugins-1.11.0-1.5.osg34.el6\nkoji-utils-1.11.0-1.5.osg34.el6\nkoji-vm-1.11.0-1.5.osg34.el6\nkoji-web-1.11.0-1.5.osg34.el6\nlcas-lcmaps-gt4-interface-0.3.1-1.2.osg34.el6\nlcas-lcmaps-gt4-interface-debuginfo-0.3.1-1.2.osg34.el6\nlcmaps-1.6.6-1.6.osg34.el6\nlcmaps-common-devel-1.6.6-1.6.osg34.el6\nlcmaps-db-templates-1.6.6-1.6.osg34.el6\nlcmaps-debuginfo-1.6.6-1.6.osg34.el6\nlcmaps-devel-1.6.6-1.6.osg34.el6\nlcmaps-plugins-basic-1.7.0-2.osg34.el6\nlcmaps-plugins-basic-debuginfo-1.7.0-2.osg34.el6\nlcmaps-plugins-basic-ldap-1.7.0-2.osg34.el6\nlcmaps-plugins-scas-client-0.5.6-1.osg34.el6\nlcmaps-plugins-scas-client-debuginfo-0.5.6-1.osg34.el6\nlcmaps-plugins-verify-proxy-1.5.9-1.1.osg34.el6\nlcmaps-plugins-verify-proxy-debuginfo-1.5.9-1.1.osg34.el6\nlcmaps-plugins-voms-1.7.1-1.4.osg34.el6\nlcmaps-plugins-voms-debuginfo-1.7.1-1.4.osg34.el6\nlcmaps-without-gsi-1.6.6-1.6.osg34.el6\nlcmaps-without-gsi-devel-1.6.6-1.6.osg34.el6\nllrun-0.1.3-1.3.osg34.el6\nllrun-debuginfo-0.1.3-1.3.osg34.el6\nmash-0.5.22-3.osg34.el6\nmyproxy-6.1.18-1.4.osg34.el6\nmyproxy-admin-6.1.18-1.4.osg34.el6\nmyproxy-debuginfo-6.1.18-1.4.osg34.el6\nmyproxy-devel-6.1.18-1.4.osg34.el6\nmyproxy-doc-6.1.18-1.4.osg34.el6\nmyproxy-libs-6.1.18-1.4.osg34.el6\nmyproxy-server-6.1.18-1.4.osg34.el6\nmyproxy-voms-6.1.18-1.4.osg34.el6\nnuttcp-6.1.2-1.osg34.el6\nnuttcp-debuginfo-6.1.2-1.osg34.el6\nosg-base-ce-3.4-2.osg34.el6\nosg-base-ce-bosco-3.4-2.osg34.el6\nosg-base-ce-condor-3.4-2.osg34.el6\nosg-base-ce-lsf-3.4-2.osg34.el6\nosg-base-ce-pbs-3.4-2.osg34.el6\nosg-base-ce-sge-3.4-2.osg34.el6\nosg-base-ce-slurm-3.4-2.osg34.el6\nosg-build-1.10.0-1.osg34.el6\nosg-build-base-1.10.0-1.osg34.el6\nosg-build-koji-1.10.0-1.osg34.el6\nosg-build-mock-1.10.0-1.osg34.el6\nosg-build-tests-1.10.0-1.osg34.el6\nosg-ca-certs-1.62-1.osg34.el6\nosg-ca-certs-updater-1.4-1.osg34.el6\nosg-ca-generator-1.2.0-1.osg34.el6\nosg-ca-scripts-1.1.6-1.osg34.el6\nosg-ce-3.4-2.osg34.el6\nosg-ce-bosco-3.4-2.osg34.el6\nosg-ce-condor-3.4-2.osg34.el6\nosg-ce-lsf-3.4-2.osg34.el6\nosg-ce-pbs-3.4-2.osg34.el6\nosg-ce-sge-3.4-2.osg34.el6\nosg-ce-slurm-3.4-2.osg34.el6\nosg-configure-2.0.0-3.osg34.el6\nosg-configure-bosco-2.0.0-3.osg34.el6\nosg-configure-ce-2.0.0-3.osg34.el6\nosg-configure-condor-2.0.0-3.osg34.el6\nosg-configure-gateway-2.0.0-3.osg34.el6\nosg-configure-gip-2.0.0-3.osg34.el6\nosg-configure-gratia-2.0.0-3.osg34.el6\nosg-configure-infoservices-2.0.0-3.osg34.el6\nosg-configure-lsf-2.0.0-3.osg34.el6\nosg-configure-managedfork-2.0.0-3.osg34.el6\nosg-configure-misc-2.0.0-3.osg34.el6\nosg-configure-network-2.0.0-3.osg34.el6\nosg-configure-pbs-2.0.0-3.osg34.el6\nosg-configure-rsv-2.0.0-3.osg34.el6\nosg-configure-sge-2.0.0-3.osg34.el6\nosg-configure-slurm-2.0.0-3.osg34.el6\nosg-configure-squid-2.0.0-3.osg34.el6\nosg-configure-tests-2.0.0-3.osg34.el6\nosg-control-1.1.0-1.osg34.el6\nosg-gridftp-3.4-2.osg34.el6\nosg-gridftp-xrootd-3.4-1.osg34.el6\nosg-gums-config-73-1.osg34.el6\nosg-htcondor-ce-3.4-2.osg34.el6\nosg-htcondor-ce-bosco-3.4-2.osg34.el6\nosg-htcondor-ce-condor-3.4-2.osg34.el6\nosg-htcondor-ce-lsf-3.4-2.osg34.el6\nosg-htcondor-ce-pbs-3.4-2.osg34.el6\nosg-htcondor-ce-sge-3.4-2.osg34.el6\nosg-htcondor-ce-slurm-3.4-2.osg34.el6\nosg-oasis-7-9.osg34.el6\nosg-pki-tools-1.2.20-1.osg34.el6\nosg-pki-tools-tests-1.2.20-1.osg34.el6\nosg-system-profiler-1.4.0-1.osg34.el6\nosg-system-profiler-viewer-1.4.0-1.osg34.el6\nosg-test-1.10.1-1.osg34.el6\nosg-tested-internal-3.4-2.osg34.el6\nosg-test-log-viewer-1.10.1-1.osg34.el6\nosg-update-data-1.4.0-1.osg34.el6\nosg-update-vos-1.4.0-1.osg34.el6\nosg-version-3.4.0-1.osg34.el6\nosg-vo-map-0.0.2-1.osg34.el6\nosg-wn-client-3.4-1.osg34.el6\nowamp-3.2rc4-2.osg34.el6\nowamp-client-3.2rc4-2.osg34.el6\nowamp-debuginfo-3.2rc4-2.osg34.el6\nowamp-server-3.2rc4-2.osg34.el6\npegasus-4.7.4-1.1.osg34.el6\npegasus-debuginfo-4.7.4-1.1.osg34.el6\nrsv-3.14.0-2.osg34.el6\nrsv-consumers-3.14.0-2.osg34.el6\nrsv-core-3.14.0-2.osg34.el6\nrsv-gwms-tester-1.1.2-1.osg34.el6\nrsv-metrics-3.14.0-2.osg34.el6\nuberftp-2.8-2.1.osg34.el6\nuberftp-debuginfo-2.8-2.1.osg34.el6\nvo-client-73-1.osg34.el6\nvo-client-edgmkgridmap-73-1.osg34.el6\nvo-client-lcmaps-voms-73-1.osg34.el6\nvoms-2.0.14-1.3.osg34.el6\nvoms-clients-cpp-2.0.14-1.3.osg34.el6\nvoms-debuginfo-2.0.14-1.3.osg34.el6\nvoms-devel-2.0.14-1.3.osg34.el6\nvoms-doc-2.0.14-1.3.osg34.el6\nvoms-server-2.0.14-1.3.osg34.el6\nxacml-1.5.0-1.osg34.el6\nxacml-debuginfo-1.5.0-1.osg34.el6\nxacml-devel-1.5.0-1.osg34.el6\nxrootd-4.6.1-1.osg34.el6\nxrootd-client-4.6.1-1.osg34.el6\nxrootd-client-devel-4.6.1-1.osg34.el6\nxrootd-client-libs-4.6.1-1.osg34.el6\nxrootd-debuginfo-4.6.1-1.osg34.el6\nxrootd-devel-4.6.1-1.osg34.el6\nxrootd-doc-4.6.1-1.osg34.el6\nxrootd-dsi-3.0.4-22.osg34.el6\nxrootd-dsi-debuginfo-3.0.4-22.osg34.el6\nxrootd-fuse-4.6.1-1.osg34.el6\nxrootd-lcmaps-1.2.1-2.osg34.el6\nxrootd-lcmaps-debuginfo-1.2.1-2.osg34.el6\nxrootd-libs-4.6.1-1.osg34.el6\nxrootd-private-devel-4.6.1-1.osg34.el6\nxrootd-python-4.6.1-1.osg34.el6\nxrootd-selinux-4.6.1-1.osg34.el6\nxrootd-server-4.6.1-1.osg34.el6\nxrootd-server-devel-4.6.1-1.osg34.el6\nxrootd-server-libs-4.6.1-1.osg34.el6\nxrootd-voms-plugin-0.4.0-1.osg34.el6\nxrootd-voms-plugin-debuginfo-0.4.0-1.osg34.el6\nxrootd-voms-plugin-devel-0.4.0-1.osg34.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nautopyfactory-2.4.6-4.osg34.el7\nautopyfactory-cloud-2.4.6-4.osg34.el7\nautopyfactory-common-2.4.6-4.osg34.el7\nautopyfactory-panda-2.4.6-4.osg34.el7\nautopyfactory-plugins-cloud-2.4.6-4.osg34.el7\nautopyfactory-plugins-local-2.4.6-4.osg34.el7\nautopyfactory-plugins-monitor-2.4.6-4.osg34.el7\nautopyfactory-plugins-panda-2.4.6-4.osg34.el7\nautopyfactory-plugins-remote-2.4.6-4.osg34.el7\nautopyfactory-plugins-scheds-2.4.6-4.osg34.el7\nautopyfactory-proxymanager-2.4.6-4.osg34.el7\nautopyfactory-remote-2.4.6-4.osg34.el7\nautopyfactory-wms-2.4.6-4.osg34.el7\nblahp-1.18.29.bosco-3.osg34.el7\nblahp-debuginfo-1.18.29.bosco-3.osg34.el7\nbwctl-1.4-7.osg34.el7\nbwctl-client-1.4-7.osg34.el7\nbwctl-debuginfo-1.4-7.osg34.el7\nbwctl-devel-1.4-7.osg34.el7\nbwctl-server-1.4-7.osg34.el7\ncctools-4.4.3-1.osg34.el7\ncctools-chirp-4.4.3-1.osg34.el7\ncctools-debuginfo-4.4.3-1.osg34.el7\ncctools-doc-4.4.3-1.osg34.el7\ncctools-dttools-4.4.3-1.osg34.el7\ncctools-makeflow-4.4.3-1.osg34.el7\ncctools-parrot-4.4.3-1.osg34.el7\ncctools-resource_monitor-4.4.3-1.osg34.el7\ncctools-sand-4.4.3-1.osg34.el7\ncctools-wavefront-4.4.3-1.osg34.el7\ncctools-weaver-4.4.3-1.osg34.el7\ncctools-work_queue-4.4.3-1.osg34.el7\ncondor-8.6.3-1.1.osg34.el7\ncondor-all-8.6.3-1.1.osg34.el7\ncondor-bosco-8.6.3-1.1.osg34.el7\ncondor-classads-8.6.3-1.1.osg34.el7\ncondor-classads-devel-8.6.3-1.1.osg34.el7\ncondor-cream-gahp-8.6.3-1.1.osg34.el7\ncondor-cron-1.1.1-2.osg34.el7\ncondor-debuginfo-8.6.3-1.1.osg34.el7\ncondor-kbdd-8.6.3-1.1.osg34.el7\ncondor-procd-8.6.3-1.1.osg34.el7\ncondor-python-8.6.3-1.1.osg34.el7\ncondor-test-8.6.3-1.1.osg34.el7\ncondor-vm-gahp-8.6.3-1.1.osg34.el7\ncvmfs-2.3.5-1.osg34.el7\ncvmfs-config-osg-2.0-2.osg34.el7\ncvmfs-devel-2.3.5-1.osg34.el7\ncvmfs-server-2.3.5-1.osg34.el7\ncvmfs-unittests-2.3.5-1.osg34.el7\ncvmfs-x509-helper-1.0-1.osg34.el7\ncvmfs-x509-helper-debuginfo-1.0-1.osg34.el7\nfrontier-squid-3.5.24-3.1.osg34.el7\nfrontier-squid-debuginfo-3.5.24-3.1.osg34.el7\nglideinwms-3.2.19-2.osg34.el7\nglideinwms-common-tools-3.2.19-2.osg34.el7\nglideinwms-condor-common-config-3.2.19-2.osg34.el7\nglideinwms-factory-3.2.19-2.osg34.el7\nglideinwms-factory-condor-3.2.19-2.osg34.el7\nglideinwms-glidecondor-tools-3.2.19-2.osg34.el7\nglideinwms-libs-3.2.19-2.osg34.el7\nglideinwms-minimal-condor-3.2.19-2.osg34.el7\nglideinwms-usercollector-3.2.19-2.osg34.el7\nglideinwms-userschedd-3.2.19-2.osg34.el7\nglideinwms-vofrontend-3.2.19-2.osg34.el7\nglideinwms-vofrontend-standalone-3.2.19-2.osg34.el7\nglite-build-common-cpp-3.3.0.2-1.osg34.el7\nglite-ce-cream-client-api-c-1.15.4-2.3.osg34.el7\nglite-ce-cream-client-devel-1.15.4-2.3.osg34.el7\nglite-ce-wsdl-1.15.1-1.1.osg34.el7\nglite-lbjp-common-gsoap-plugin-3.2.12-1.1.osg34.el7\nglite-lbjp-common-gsoap-plugin-debuginfo-3.2.12-1.1.osg34.el7\nglite-lbjp-common-gsoap-plugin-devel-3.2.12-1.1.osg34.el7\nglobus-ftp-client-8.29-1.1.osg34.el7\nglobus-ftp-client-debuginfo-8.29-1.1.osg34.el7\nglobus-ftp-client-devel-8.29-1.1.osg34.el7\nglobus-ftp-client-doc-8.29-1.1.osg34.el7\nglobus-gridftp-osg-extensions-0.3-2.osg34.el7\nglobus-gridftp-osg-extensions-debuginfo-0.3-2.osg34.el7\nglobus-gridftp-server-11.8-1.1.osg34.el7\nglobus-gridftp-server-control-4.1-1.3.osg34.el7\nglobus-gridftp-server-control-debuginfo-4.1-1.3.osg34.el7\nglobus-gridftp-server-control-devel-4.1-1.3.osg34.el7\nglobus-gridftp-server-debuginfo-11.8-1.1.osg34.el7\nglobus-gridftp-server-devel-11.8-1.1.osg34.el7\nglobus-gridftp-server-progs-11.8-1.1.osg34.el7\ngratia-probe-1.17.5-1.osg34.el7\ngratia-probe-bdii-status-1.17.5-1.osg34.el7\ngratia-probe-common-1.17.5-1.osg34.el7\ngratia-probe-condor-1.17.5-1.osg34.el7\ngratia-probe-condor-events-1.17.5-1.osg34.el7\ngratia-probe-dcache-storage-1.17.5-1.osg34.el7\ngratia-probe-dcache-storagegroup-1.17.5-1.osg34.el7\ngratia-probe-dcache-transfer-1.17.5-1.osg34.el7\ngratia-probe-debuginfo-1.17.5-1.osg34.el7\ngratia-probe-enstore-storage-1.17.5-1.osg34.el7\ngratia-probe-enstore-tapedrive-1.17.5-1.osg34.el7\ngratia-probe-enstore-transfer-1.17.5-1.osg34.el7\ngratia-probe-glexec-1.17.5-1.osg34.el7\ngratia-probe-glideinwms-1.17.5-1.osg34.el7\ngratia-probe-gram-1.17.5-1.osg34.el7\ngratia-probe-gridftp-transfer-1.17.5-1.osg34.el7\ngratia-probe-hadoop-storage-1.17.5-1.osg34.el7\ngratia-probe-htcondor-ce-1.17.5-1.osg34.el7\ngratia-probe-lsf-1.17.5-1.osg34.el7\ngratia-probe-metric-1.17.5-1.osg34.el7\ngratia-probe-onevm-1.17.5-1.osg34.el7\ngratia-probe-pbs-lsf-1.17.5-1.osg34.el7\ngratia-probe-services-1.17.5-1.osg34.el7\ngratia-probe-sge-1.17.5-1.osg34.el7\ngratia-probe-slurm-1.17.5-1.osg34.el7\ngratia-probe-xrootd-storage-1.17.5-1.osg34.el7\ngratia-probe-xrootd-transfer-1.17.5-1.osg34.el7\ngsi-openssh-7.1p2f-1.2.osg34.el7\ngsi-openssh-clients-7.1p2f-1.2.osg34.el7\ngsi-openssh-debuginfo-7.1p2f-1.2.osg34.el7\ngsi-openssh-server-7.1p2f-1.2.osg34.el7\nhtcondor-ce-2.2.0-1.osg34.el7\nhtcondor-ce-bosco-2.2.0-1.osg34.el7\nhtcondor-ce-client-2.2.0-1.osg34.el7\nhtcondor-ce-collector-2.2.0-1.osg34.el7\nhtcondor-ce-condor-2.2.0-1.osg34.el7\nhtcondor-ce-lsf-2.2.0-1.osg34.el7\nhtcondor-ce-pbs-2.2.0-1.osg34.el7\nhtcondor-ce-sge-2.2.0-1.osg34.el7\nhtcondor-ce-slurm-2.2.0-1.osg34.el7\nhtcondor-ce-view-2.2.0-1.osg34.el7\nigtf-ca-certs-1.82-1.osg34.el7\njavascriptrrd-1.1.1-1.osg34.el7\nkoji-1.11.0-1.5.osg34.el7\nkoji-builder-1.11.0-1.5.osg34.el7\nkoji-hub-1.11.0-1.5.osg34.el7\nkoji-hub-plugins-1.11.0-1.5.osg34.el7\nkoji-utils-1.11.0-1.5.osg34.el7\nkoji-vm-1.11.0-1.5.osg34.el7\nkoji-web-1.11.0-1.5.osg34.el7\nlcas-lcmaps-gt4-interface-0.3.1-1.2.osg34.el7\nlcas-lcmaps-gt4-interface-debuginfo-0.3.1-1.2.osg34.el7\nlcmaps-1.6.6-1.6.osg34.el7\nlcmaps-common-devel-1.6.6-1.6.osg34.el7\nlcmaps-db-templates-1.6.6-1.6.osg34.el7\nlcmaps-debuginfo-1.6.6-1.6.osg34.el7\nlcmaps-devel-1.6.6-1.6.osg34.el7\nlcmaps-plugins-basic-1.7.0-2.osg34.el7\nlcmaps-plugins-basic-debuginfo-1.7.0-2.osg34.el7\nlcmaps-plugins-basic-ldap-1.7.0-2.osg34.el7\nlcmaps-plugins-scas-client-0.5.6-1.osg34.el7\nlcmaps-plugins-scas-client-debuginfo-0.5.6-1.osg34.el7\nlcmaps-plugins-verify-proxy-1.5.9-1.1.osg34.el7\nlcmaps-plugins-verify-proxy-debuginfo-1.5.9-1.1.osg34.el7\nlcmaps-plugins-voms-1.7.1-1.4.osg34.el7\nlcmaps-plugins-voms-debuginfo-1.7.1-1.4.osg34.el7\nlcmaps-without-gsi-1.6.6-1.6.osg34.el7\nlcmaps-without-gsi-devel-1.6.6-1.6.osg34.el7\nllrun-0.1.3-1.3.osg34.el7\nllrun-debuginfo-0.1.3-1.3.osg34.el7\nmash-0.5.22-3.osg34.el7\nmyproxy-6.1.18-1.4.osg34.el7\nmyproxy-admin-6.1.18-1.4.osg34.el7\nmyproxy-debuginfo-6.1.18-1.4.osg34.el7\nmyproxy-devel-6.1.18-1.4.osg34.el7\nmyproxy-doc-6.1.18-1.4.osg34.el7\nmyproxy-libs-6.1.18-1.4.osg34.el7\nmyproxy-server-6.1.18-1.4.osg34.el7\nmyproxy-voms-6.1.18-1.4.osg34.el7\nnuttcp-6.1.2-1.osg34.el7\nnuttcp-debuginfo-6.1.2-1.osg34.el7\nosg-base-ce-3.4-2.osg34.el7\nosg-base-ce-bosco-3.4-2.osg34.el7\nosg-base-ce-condor-3.4-2.osg34.el7\nosg-base-ce-lsf-3.4-2.osg34.el7\nosg-base-ce-pbs-3.4-2.osg34.el7\nosg-base-ce-sge-3.4-2.osg34.el7\nosg-base-ce-slurm-3.4-2.osg34.el7\nosg-build-1.10.0-1.osg34.el7\nosg-build-base-1.10.0-1.osg34.el7\nosg-build-koji-1.10.0-1.osg34.el7\nosg-build-mock-1.10.0-1.osg34.el7\nosg-build-tests-1.10.0-1.osg34.el7\nosg-ca-certs-1.62-1.osg34.el7\nosg-ca-certs-updater-1.4-1.osg34.el7\nosg-ca-generator-1.2.0-1.osg34.el7\nosg-ca-scripts-1.1.6-1.osg34.el7\nosg-ce-3.4-2.osg34.el7\nosg-ce-bosco-3.4-2.osg34.el7\nosg-ce-condor-3.4-2.osg34.el7\nosg-ce-lsf-3.4-2.osg34.el7\nosg-ce-pbs-3.4-2.osg34.el7\nosg-ce-sge-3.4-2.osg34.el7\nosg-ce-slurm-3.4-2.osg34.el7\nosg-configure-2.0.0-3.osg34.el7\nosg-configure-bosco-2.0.0-3.osg34.el7\nosg-configure-ce-2.0.0-3.osg34.el7\nosg-configure-condor-2.0.0-3.osg34.el7\nosg-configure-gateway-2.0.0-3.osg34.el7\nosg-configure-gip-2.0.0-3.osg34.el7\nosg-configure-gratia-2.0.0-3.osg34.el7\nosg-configure-infoservices-2.0.0-3.osg34.el7\nosg-configure-lsf-2.0.0-3.osg34.el7\nosg-configure-managedfork-2.0.0-3.osg34.el7\nosg-configure-misc-2.0.0-3.osg34.el7\nosg-configure-network-2.0.0-3.osg34.el7\nosg-configure-pbs-2.0.0-3.osg34.el7\nosg-configure-rsv-2.0.0-3.osg34.el7\nosg-configure-sge-2.0.0-3.osg34.el7\nosg-configure-slurm-2.0.0-3.osg34.el7\nosg-configure-squid-2.0.0-3.osg34.el7\nosg-configure-tests-2.0.0-3.osg34.el7\nosg-control-1.1.0-1.osg34.el7\nosg-gridftp-3.4-2.osg34.el7\nosg-gridftp-xrootd-3.4-1.osg34.el7\nosg-gums-config-73-1.osg34.el7\nosg-htcondor-ce-3.4-2.osg34.el7\nosg-htcondor-ce-bosco-3.4-2.osg34.el7\nosg-htcondor-ce-condor-3.4-2.osg34.el7\nosg-htcondor-ce-lsf-3.4-2.osg34.el7\nosg-htcondor-ce-pbs-3.4-2.osg34.el7\nosg-htcondor-ce-sge-3.4-2.osg34.el7\nosg-htcondor-ce-slurm-3.4-2.osg34.el7\nosg-oasis-7-9.osg34.el7\nosg-pki-tools-1.2.20-1.osg34.el7\nosg-pki-tools-tests-1.2.20-1.osg34.el7\nosg-system-profiler-1.4.0-1.osg34.el7\nosg-system-profiler-viewer-1.4.0-1.osg34.el7\nosg-test-1.10.1-1.osg34.el7\nosg-tested-internal-3.4-2.osg34.el7\nosg-test-log-viewer-1.10.1-1.osg34.el7\nosg-update-data-1.4.0-1.osg34.el7\nosg-update-vos-1.4.0-1.osg34.el7\nosg-version-3.4.0-1.osg34.el7\nosg-vo-map-0.0.2-1.osg34.el7\nosg-wn-client-3.4-1.osg34.el7\nowamp-3.2rc4-2.osg34.el7\nowamp-client-3.2rc4-2.osg34.el7\nowamp-debuginfo-3.2rc4-2.osg34.el7\nowamp-server-3.2rc4-2.osg34.el7\npegasus-4.7.4-1.1.osg34.el7\npegasus-debuginfo-4.7.4-1.1.osg34.el7\nrsv-3.14.0-2.osg34.el7\nrsv-consumers-3.14.0-2.osg34.el7\nrsv-core-3.14.0-2.osg34.el7\nrsv-gwms-tester-1.1.2-1.osg34.el7\nrsv-metrics-3.14.0-2.osg34.el7\nstashcache-0.7-2.osg34.el7\nstashcache-cache-server-0.7-2.osg34.el7\nstashcache-daemon-0.7-2.osg34.el7\nstashcache-origin-server-0.7-2.osg34.el7\nuberftp-2.8-2.1.osg34.el7\nuberftp-debuginfo-2.8-2.1.osg34.el7\nvo-client-73-1.osg34.el7\nvo-client-edgmkgridmap-73-1.osg34.el7\nvo-client-lcmaps-voms-73-1.osg34.el7\nvoms-2.0.14-1.3.osg34.el7\nvoms-clients-cpp-2.0.14-1.3.osg34.el7\nvoms-debuginfo-2.0.14-1.3.osg34.el7\nvoms-devel-2.0.14-1.3.osg34.el7\nvoms-doc-2.0.14-1.3.osg34.el7\nvoms-server-2.0.14-1.3.osg34.el7\nxacml-1.5.0-1.osg34.el7\nxacml-debuginfo-1.5.0-1.osg34.el7\nxacml-devel-1.5.0-1.osg34.el7\nxrootd-4.6.1-1.osg34.el7\nxrootd-client-4.6.1-1.osg34.el7\nxrootd-client-devel-4.6.1-1.osg34.el7\nxrootd-client-libs-4.6.1-1.osg34.el7\nxrootd-debuginfo-4.6.1-1.osg34.el7\nxrootd-devel-4.6.1-1.osg34.el7\nxrootd-doc-4.6.1-1.osg34.el7\nxrootd-dsi-3.0.4-22.osg34.el7\nxrootd-dsi-debuginfo-3.0.4-22.osg34.el7\nxrootd-fuse-4.6.1-1.osg34.el7\nxrootd-lcmaps-1.3.3-3.osg34.el7\nxrootd-lcmaps-debuginfo-1.3.3-3.osg34.el7\nxrootd-libs-4.6.1-1.osg34.el7\nxrootd-private-devel-4.6.1-1.osg34.el7\nxrootd-python-4.6.1-1.osg34.el7\nxrootd-selinux-4.6.1-1.osg34.el7\nxrootd-server-4.6.1-1.osg34.el7\nxrootd-server-devel-4.6.1-1.osg34.el7\nxrootd-server-libs-4.6.1-1.osg34.el7\nxrootd-voms-plugin-0.4.0-1.osg34.el7\nxrootd-voms-plugin-debuginfo-0.4.0-1.osg34.el7\nxrootd-voms-plugin-devel-0.4.0-1.osg34.el7\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nglideinwms-3.3.2-2.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nglideinwms-3.3.2-2.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nglideinwms glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nglideinwms-3.3.2-2.osgup.el6\nglideinwms-common-tools-3.3.2-2.osgup.el6\nglideinwms-condor-common-config-3.3.2-2.osgup.el6\nglideinwms-factory-3.3.2-2.osgup.el6\nglideinwms-factory-condor-3.3.2-2.osgup.el6\nglideinwms-glidecondor-tools-3.3.2-2.osgup.el6\nglideinwms-libs-3.3.2-2.osgup.el6\nglideinwms-minimal-condor-3.3.2-2.osgup.el6\nglideinwms-usercollector-3.3.2-2.osgup.el6\nglideinwms-userschedd-3.3.2-2.osgup.el6\nglideinwms-vofrontend-3.3.2-2.osgup.el6\nglideinwms-vofrontend-standalone-3.3.2-2.osgup.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nglideinwms-3.3.2-2.osgup.el7\nglideinwms-common-tools-3.3.2-2.osgup.el7\nglideinwms-condor-common-config-3.3.2-2.osgup.el7\nglideinwms-factory-3.3.2-2.osgup.el7\nglideinwms-factory-condor-3.3.2-2.osgup.el7\nglideinwms-glidecondor-tools-3.3.2-2.osgup.el7\nglideinwms-libs-3.3.2-2.osgup.el7\nglideinwms-minimal-condor-3.3.2-2.osgup.el7\nglideinwms-usercollector-3.3.2-2.osgup.el7\nglideinwms-userschedd-3.3.2-2.osgup.el7\nglideinwms-vofrontend-3.3.2-2.osgup.el7\nglideinwms-vofrontend-standalone-3.3.2-2.osgup.el7", 
            "title": "OSG Release 3.4.0"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#osg-software-release-340", 
            "text": "Release Date : 2017-06-14", 
            "title": "OSG Software Release 3.4.0"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#whats-new-in-osg-34", 
            "text": "The OSG 3.4.0 software stack features a more streamlined and consolidated package list. Specifically, the varied authentication solutions proved to be good candidates for consolidation and a new piece of software, the LCMAPS VOMS plugin, has been designed to replace both edg-mkgridmap and GUMS.  See  install the LCMAPS VOMS plugin  to replace GUMS + edg-mkgridmap.  See  migrating from edg-mkgridmap to lcmaps VOMS plugin  to transistion from edg-mkgridmap.  In 3.4.0, we dropped HDFS 2.x with the intention of adding HDFS 3.x in a subsequent OSG 3.4 release when it becomes available upstream.  In addition to GUMS, edg-mkgridmap, and HDFS 2.x, we dropped packages related to the following software:   VOMS Admin Server \u2212  Retirement Policy  BeStMan \u2212 replaced by  Load Balanced GridFTP  GLExec \u2212 replaced by  Singularty  Globus GRAM \u2212 available from EPEL  GIP and OSG Info Services \u2212 BDII servers retired   The aforementioned packages are still be available in OSG 3.3 and will receive regular support until December 2017 and security updates until June 2018 per our  release policy . See  this section  for the complete list of packages removed from OSG 3.4.   OSG 3.4 contains only 64-bit components.   StashCache is supported on EL7 only.   xrootd-lcmaps will remain at 1.2.1-2 on EL6.", 
            "title": "What's New in OSG 3.4"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#summary-of-changes", 
            "text": "This release contains:   OSG 3.4.0  HTCondor 8.6.3 : See  Upgrading from 8.4  for additional information  Frontier squid 3.5.24-3.1 : See  Upgrading from 2.x to 3.x  for additional information  Update to  XRootD 4.6.1  Update to xrootd-lcmaps 1.3.3 for EL7    Update StashCache meta-packages to require XRootD 4.6.1  Update to  GlideinWMS 3.2.19  Frontier squid 3.5.24-3.1 : See  Upgrading from 2.x to 3.x  for additional information  Make the LCMAPS VOMS plugin consider only the first FQAN to be consistent with GUMS  HTCondor-CE: Add WholeNodeWanted ClassAd expression so jobs can request a whole node from the batch system  HTCondor 8.6.3 : See  Upgrading from 8.4  for additional information  OSG CE 3.4  Add vo-client-lcmaps-voms dependency  Remove gridftp dependency  Drop client tools    Add vo-client-lcmaps-voms dependency to osg-gridftp  Fix osg-update-vos script to clean yum cache in order pick up the latest vo-client RPM  osg-ca-scripts now refers to repo.grid.iu.edu (rather than the retired software.grid.iu.edu)  osg-configure 2.0.0  reject empty  allowed_vos  in subclusters  get default  allowed_vos  from LCMAPS VOMS plugin  issue warning (rather than error out) if OSG_APP or OSG_DATA directories are not present  drop 'RSV is not installed' warning  remove configure-osg alias  deprecate GUMS support  disable GRAM configuration  drop managedfork and network modules  drop glexec support  remove nonfunctional osg-cleanup    Drop glexec and java from osg-wn-client  BeSTMan 2 is no longer part of the OSG Software Stack  GUMS is no longer part of the OSG Software Stack  edg-mkgridmap in no longer part of the OSG Software Stack  Drop bestman2 and globus*run RSV metrics  osg-build 1.10.0  drop vdt-build alias  drop ~/.osg-build.ini configuration file       These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#known-issues", 
            "text": "Currently, OSG 3.4 CEs cannot be configured to authenticate via GUMS ( SOFTWARE-2482 ). This issue is expected to be fixed in the July release.  Using the LCMAPS VOMS will result in a failing \"supported VO\" RSV test ( SOFTWARE-2763 ). This can be ignored and a fix is targeted for the July release.  In GlideinWMS, a small configuration change must be added to account for changes in HTCondor 8.6. Add the following line to the HTCondor configuration.   COLLECTOR.USE_SHARED_PORT=False", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#enterprise-linux-6", 
            "text": "autopyfactory-2.4.6-4.osg34.el6  blahp-1.18.29.bosco-3.osg34.el6  bwctl-1.4-7.osg34.el6  cctools-4.4.3-1.osg34.el6  condor-8.6.3-1.1.osg34.el6  condor-cron-1.1.1-2.osg34.el6  cvmfs-2.3.5-1.osg34.el6  cvmfs-config-osg-2.0-2.osg34.el6  cvmfs-x509-helper-1.0-1.osg34.el6  frontier-squid-3.5.24-3.1.osg34.el6  glideinwms-3.2.19-2.osg34.el6  glite-build-common-cpp-3.3.0.2-1.osg34.el6  glite-ce-cream-client-api-c-1.15.4-2.3.osg34.el6  glite-ce-wsdl-1.15.1-1.1.osg34.el6  glite-lbjp-common-gsoap-plugin-3.2.12-1.1.osg34.el6  globus-ftp-client-8.29-1.1.osg34.el6  globus-gridftp-osg-extensions-0.3-2.osg34.el6  globus-gridftp-server-11.8-1.1.osg34.el6  globus-gridftp-server-control-4.1-1.3.osg34.el6  gratia-probe-1.17.5-1.osg34.el6  gsi-openssh-7.1p2f-1.2.osg34.el6  htcondor-ce-2.2.0-1.osg34.el6  igtf-ca-certs-1.82-1.osg34.el6  javascriptrrd-1.1.1-1.osg34.el6  koji-1.11.0-1.5.osg34.el6  lcas-lcmaps-gt4-interface-0.3.1-1.2.osg34.el6  lcmaps-1.6.6-1.6.osg34.el6  lcmaps-plugins-basic-1.7.0-2.osg34.el6  lcmaps-plugins-scas-client-0.5.6-1.osg34.el6  lcmaps-plugins-verify-proxy-1.5.9-1.1.osg34.el6  lcmaps-plugins-voms-1.7.1-1.4.osg34.el6  llrun-0.1.3-1.3.osg34.el6  mash-0.5.22-3.osg34.el6  myproxy-6.1.18-1.4.osg34.el6  nuttcp-6.1.2-1.osg34.el6  osg-build-1.10.0-1.osg34.el6  osg-ca-certs-1.62-1.osg34.el6  osg-ca-certs-updater-1.4-1.osg34.el6  osg-ca-generator-1.2.0-1.osg34.el6  osg-ca-scripts-1.1.6-1.osg34.el6  osg-ce-3.4-2.osg34.el6  osg-configure-2.0.0-3.osg34.el6  osg-control-1.1.0-1.osg34.el6  osg-gridftp-3.4-2.osg34.el6  osg-gridftp-xrootd-3.4-1.osg34.el6  osg-oasis-7-9.osg34.el6  osg-pki-tools-1.2.20-1.osg34.el6  osg-system-profiler-1.4.0-1.osg34.el6  osg-test-1.10.1-1.osg34.el6  osg-tested-internal-3.4-2.osg34.el6  osg-update-vos-1.4.0-1.osg34.el6  osg-version-3.4.0-1.osg34.el6  osg-vo-map-0.0.2-1.osg34.el6  osg-wn-client-3.4-1.osg34.el6  owamp-3.2rc4-2.osg34.el6  pegasus-4.7.4-1.1.osg34.el6  rsv-3.14.0-2.osg34.el6  rsv-gwms-tester-1.1.2-1.osg34.el6  uberftp-2.8-2.1.osg34.el6  vo-client-73-1.osg34.el6  voms-2.0.14-1.3.osg34.el6  xacml-1.5.0-1.osg34.el6  xrootd-4.6.1-1.osg34.el6  xrootd-dsi-3.0.4-22.osg34.el6  xrootd-lcmaps-1.2.1-2.osg34.el6  xrootd-voms-plugin-0.4.0-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#enterprise-linux-7", 
            "text": "autopyfactory-2.4.6-4.osg34.el7  blahp-1.18.29.bosco-3.osg34.el7  bwctl-1.4-7.osg34.el7  cctools-4.4.3-1.osg34.el7  condor-8.6.3-1.1.osg34.el7  condor-cron-1.1.1-2.osg34.el7  cvmfs-2.3.5-1.osg34.el7  cvmfs-config-osg-2.0-2.osg34.el7  cvmfs-x509-helper-1.0-1.osg34.el7  frontier-squid-3.5.24-3.1.osg34.el7  glideinwms-3.2.19-2.osg34.el7  glite-build-common-cpp-3.3.0.2-1.osg34.el7  glite-ce-cream-client-api-c-1.15.4-2.3.osg34.el7  glite-ce-wsdl-1.15.1-1.1.osg34.el7  glite-lbjp-common-gsoap-plugin-3.2.12-1.1.osg34.el7  globus-ftp-client-8.29-1.1.osg34.el7  globus-gridftp-osg-extensions-0.3-2.osg34.el7  globus-gridftp-server-11.8-1.1.osg34.el7  globus-gridftp-server-control-4.1-1.3.osg34.el7  gratia-probe-1.17.5-1.osg34.el7  gsi-openssh-7.1p2f-1.2.osg34.el7  htcondor-ce-2.2.0-1.osg34.el7  igtf-ca-certs-1.82-1.osg34.el7  javascriptrrd-1.1.1-1.osg34.el7  koji-1.11.0-1.5.osg34.el7  lcas-lcmaps-gt4-interface-0.3.1-1.2.osg34.el7  lcmaps-1.6.6-1.6.osg34.el7  lcmaps-plugins-basic-1.7.0-2.osg34.el7  lcmaps-plugins-scas-client-0.5.6-1.osg34.el7  lcmaps-plugins-verify-proxy-1.5.9-1.1.osg34.el7  lcmaps-plugins-voms-1.7.1-1.4.osg34.el7  llrun-0.1.3-1.3.osg34.el7  mash-0.5.22-3.osg34.el7  myproxy-6.1.18-1.4.osg34.el7  nuttcp-6.1.2-1.osg34.el7  osg-build-1.10.0-1.osg34.el7  osg-ca-certs-1.62-1.osg34.el7  osg-ca-certs-updater-1.4-1.osg34.el7  osg-ca-generator-1.2.0-1.osg34.el7  osg-ca-scripts-1.1.6-1.osg34.el7  osg-ce-3.4-2.osg34.el7  osg-configure-2.0.0-3.osg34.el7  osg-control-1.1.0-1.osg34.el7  osg-gridftp-3.4-2.osg34.el7  osg-gridftp-xrootd-3.4-1.osg34.el7  osg-oasis-7-9.osg34.el7  osg-pki-tools-1.2.20-1.osg34.el7  osg-system-profiler-1.4.0-1.osg34.el7  osg-test-1.10.1-1.osg34.el7  osg-tested-internal-3.4-2.osg34.el7  osg-update-vos-1.4.0-1.osg34.el7  osg-version-3.4.0-1.osg34.el7  osg-vo-map-0.0.2-1.osg34.el7  osg-wn-client-3.4-1.osg34.el7  owamp-3.2rc4-2.osg34.el7  pegasus-4.7.4-1.1.osg34.el7  rsv-3.14.0-2.osg34.el7  rsv-gwms-tester-1.1.2-1.osg34.el7  stashcache-0.7-2.osg34.el7  uberftp-2.8-2.1.osg34.el7  vo-client-73-1.osg34.el7  voms-2.0.14-1.3.osg34.el7  xacml-1.5.0-1.osg34.el7  xrootd-4.6.1-1.osg34.el7  xrootd-dsi-3.0.4-22.osg34.el7  xrootd-lcmaps-1.3.3-3.osg34.el7  xrootd-voms-plugin-0.4.0-1.osg34.el7   #PackagesRemoved", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#packages-removed-from-osg-34", 
            "text": "Many packages that were available in OSG 3.3 will not be included in the OSG 3.4 software stack. The following packages are not in the 3.4 repositories.", 
            "title": "Packages Removed from OSG 3.4"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#enterprise-linux-6_1", 
            "text": "bestman2\nbigtop-jsvc\nbigtop-utils\ncilogon-openid-ca-cert\ncilogon-osg-ca-cert\ncog-jglobus-axis\nedg-mkgridmap\nemi-trustmanager\nemi-trustmanager-axis\nemi-trustmanager-tomcat\ngip\nglexec\nglexec-wrapper-scripts\nglite-ce-cream-utils\nglite-lbjp-common-gss\nglobus-authz\nglobus-authz-callout-error\nglobus-callout\nglobus-common\nglobus-ftp-control\nglobus-gass-cache\nglobus-gass-cache-program\nglobus-gass-copy\nglobus-gass-server-ez\nglobus-gass-transfer\nglobus-gatekeeper\nglobus-gfork\nglobus-gram-audit\nglobus-gram-client\nglobus-gram-client-tools\nglobus-gram-job-manager\nglobus-gram-job-manager-callout-error\nglobus-gram-job-manager-condor\nglobus-gram-job-manager-fork\nglobus-gram-job-manager-lsf\nglobus-gram-job-manager-managedfork\nglobus-gram-job-manager-pbs\nglobus-gram-job-manager-scripts\nglobus-gram-job-manager-sge\nglobus-gram-protocol\nglobus-gridmap-callout-error\nglobus-gsi-callback\nglobus-gsi-cert-utils\nglobus-gsi-credential\nglobus-gsi-openssl-error\nglobus-gsi-proxy-core\nglobus-gsi-proxy-ssl\nglobus-gsi-sysconfig\nglobus-gssapi-error\nglobus-gssapi-gsi\nglobus-gss-assist\nglobus-io\nglobus-openssl-module\nglobus-proxy-utils\nglobus-rsl\nglobus-scheduler-event-generator\nglobus-simple-ca\nglobus-usage\nglobus-xio\nglobus-xio-gsi-driver\nglobus-xioperf\nglobus-xio-pipe-driver\nglobus-xio-popen-driver\nglobus-xio-udt-driver\ngratia\ngratia-reporting-email\ngridftp-hdfs\ngums\nhadoop\nI2util\njetty\njglobus\njoda-time\nlcmaps-plugins-glexec-tracking\nlcmaps-plugins-gums-client\nlcmaps-plugins-mount-under-scratch\nlcmaps-plugins-process-tracking\nmkgltempdir\nndt\nnetlogger\nosg-cert-scripts\nosg-cleanup\nosg-gridftp-hdfs\nosg-gums\nosg-info-services\nosg-java7-compat\nosg-release\nosg-release-itb\nosg-se-bestman\nosg-se-bestman-xrootd\nosg-se-hadoop\nosg-voms\nosg-webapp-common\nprivilege-xacml\nrsv-vo-gwms\nstashcache\nstashcache-daemon\nvoms-admin-client\nvoms-admin-server\nvoms-api-java\nvoms-mysql-plugin\nweb100_userland\nxrootd-hdfs\nxrootd-status-probe\nzookeeper", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#enterprise-linux-7_1", 
            "text": "axis\nbestman2\nbigtop-jsvc\nbigtop-utils\ncilogon-openid-ca-cert\ncilogon-osg-ca-cert\ncog-jglobus-axis\nedg-mkgridmap\nemi-trustmanager\nemi-trustmanager-axis\nemi-trustmanager-tomcat\ngip\nglexec\nglexec-wrapper-scripts\nglite-lbjp-common-gss\nglobus-authz\nglobus-authz-callout-error\nglobus-callout\nglobus-common\nglobus-ftp-control\nglobus-gass-cache\nglobus-gass-cache-program\nglobus-gass-copy\nglobus-gass-server-ez\nglobus-gass-transfer\nglobus-gatekeeper\nglobus-gfork\nglobus-gram-audit\nglobus-gram-client\nglobus-gram-client-tools\nglobus-gram-job-manager\nglobus-gram-job-manager-callout-error\nglobus-gram-job-manager-condor\nglobus-gram-job-manager-fork\nglobus-gram-job-manager-lsf\nglobus-gram-job-manager-managedfork\nglobus-gram-job-manager-pbs\nglobus-gram-job-manager-scripts\nglobus-gram-job-manager-sge\nglobus-gram-protocol\nglobus-gridmap-callout-error\nglobus-gsi-callback\nglobus-gsi-cert-utils\nglobus-gsi-credential\nglobus-gsi-openssl-error\nglobus-gsi-proxy-core\nglobus-gsi-proxy-ssl\nglobus-gsi-sysconfig\nglobus-gssapi-error\nglobus-gssapi-gsi\nglobus-gss-assist\nglobus-io\nglobus-openssl-module\nglobus-proxy-utils\nglobus-rsl\nglobus-scheduler-event-generator\nglobus-simple-ca\nglobus-usage\nglobus-xio\nglobus-xio-gsi-driver\nglobus-xioperf\nglobus-xio-pipe-driver\nglobus-xio-popen-driver\nglobus-xio-udt-driver\ngratia\ngratia-reporting-email\ngridftp-hdfs\ngums\nhadoop\nI2util\njavamail\njetty\njglobus\njoda-time\nlcmaps-plugins-glexec-tracking\nlcmaps-plugins-gums-client\nlcmaps-plugins-mount-under-scratch\nlcmaps-plugins-process-tracking\nmkgltempdir\nndt\nnetlogger\nosg-cert-scripts\nosg-cleanup\nosg-gridftp-hdfs\nosg-gums\nosg-info-services\nosg-java7-compat\nosg-release\nosg-release-itb\nosg-se-bestman\nosg-se-bestman-xrootd\nosg-se-hadoop\nosg-voms\nosg-webapp-common\nprivilege-xacml\npython-ZSI\nPyXML\nrsv-vo-gwms\nstashcache-daemon\nvoms-admin-client\nvoms-mysql-plugin\nweb100_userland\nwsdl4j\nxrootd-hdfs\nxrootd-status-probe\nzookeeper", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  autopyfactory-cloud autopyfactory-common autopyfactory-panda autopyfactory-plugins-cloud autopyfactory-plugins-local autopyfactory-plugins-monitor autopyfactory-plugins-panda autopyfactory-plugins-remote autopyfactory-plugins-scheds autopyfactory-proxymanager autopyfactory-remote autopyfactory-wms blahp blahp-debuginfo bwctl bwctl-client bwctl-debuginfo bwctl-devel bwctl-server cctools-chirp cctools-debuginfo cctools-doc cctools-dttools cctools-makeflow cctools-parrot cctools-resource_monitor cctools-sand cctools-wavefront cctools-weaver cctools-work_queue condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-cron condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp cvmfs cvmfs-config-osg cvmfs-devel cvmfs-server cvmfs-unittests cvmfs-x509-helper cvmfs-x509-helper-debuginfo frontier-squid frontier-squid-debuginfo glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone glite-build-common-cpp glite-ce-cream-client-api-c glite-ce-cream-client-devel glite-ce-wsdl glite-lbjp-common-gsoap-plugin glite-lbjp-common-gsoap-plugin-debuginfo glite-lbjp-common-gsoap-plugin-devel globus-ftp-client globus-ftp-client-debuginfo globus-ftp-client-devel globus-ftp-client-doc globus-gridftp-osg-extensions globus-gridftp-osg-extensions-debuginfo globus-gridftp-server globus-gridftp-server-control globus-gridftp-server-control-debuginfo globus-gridftp-server-control-devel globus-gridftp-server-debuginfo globus-gridftp-server-devel globus-gridftp-server-progs gratia-probe-bdii-status gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glexec gratia-probe-glideinwms gratia-probe-gram gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer gsi-openssh gsi-openssh-clients gsi-openssh-debuginfo gsi-openssh-server htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-slurm htcondor-ce-view igtf-ca-certs javascriptrrd koji koji-builder koji-hub koji-hub-plugins koji-utils koji-vm koji-web lcas-lcmaps-gt4-interface lcas-lcmaps-gt4-interface-debuginfo lcmaps lcmaps-common-devel lcmaps-db-templates lcmaps-debuginfo lcmaps-devel lcmaps-plugins-basic lcmaps-plugins-basic-debuginfo lcmaps-plugins-basic-ldap lcmaps-plugins-scas-client lcmaps-plugins-scas-client-debuginfo lcmaps-plugins-verify-proxy lcmaps-plugins-verify-proxy-debuginfo lcmaps-plugins-voms lcmaps-plugins-voms-debuginfo lcmaps-without-gsi lcmaps-without-gsi-devel llrun llrun-debuginfo mash myproxy myproxy-admin myproxy-debuginfo myproxy-devel myproxy-doc myproxy-libs myproxy-server myproxy-voms nuttcp nuttcp-debuginfo osg-base-ce osg-base-ce-bosco osg-base-ce-condor osg-base-ce-lsf osg-base-ce-pbs osg-base-ce-sge osg-base-ce-slurm osg-build osg-build-base osg-build-koji osg-build-mock osg-build-tests osg-ca-certs osg-ca-certs-updater osg-ca-generator osg-ca-scripts osg-ce osg-ce-bosco osg-ce-condor osg-ce-lsf osg-ce-pbs osg-ce-sge osg-ce-slurm osg-configure osg-configure-bosco osg-configure-ce osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-control osg-gridftp osg-gridftp-xrootd osg-gums-config osg-htcondor-ce osg-htcondor-ce-bosco osg-htcondor-ce-condor osg-htcondor-ce-lsf osg-htcondor-ce-pbs osg-htcondor-ce-sge osg-htcondor-ce-slurm osg-oasis osg-pki-tools osg-pki-tools-tests osg-release osg-release-itb osg-system-profiler osg-system-profiler-viewer osg-test osg-tested-internal osg-test-log-viewer osg-update-data osg-update-vos osg-version osg-vo-map osg-wn-client owamp owamp-client owamp-debuginfo owamp-server pegasus pegasus-debuginfo rsv rsv-consumers rsv-core rsv-gwms-tester rsv-metrics uberftp uberftp-debuginfo vo-client vo-client-edgmkgridmap vo-client-lcmaps-voms voms voms-clients-cpp voms-debuginfo voms-devel voms-doc voms-server xacml xacml-debuginfo xacml-devel xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-dsi xrootd-dsi-debuginfo xrootd-fuse xrootd-lcmaps xrootd-lcmaps-debuginfo xrootd-libs xrootd-private-devel xrootd-python xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs xrootd-voms-plugin xrootd-voms-plugin-debuginfo xrootd-voms-plugin-devel  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#enterprise-linux-6_2", 
            "text": "autopyfactory-2.4.6-4.osg34.el6\nautopyfactory-cloud-2.4.6-4.osg34.el6\nautopyfactory-common-2.4.6-4.osg34.el6\nautopyfactory-panda-2.4.6-4.osg34.el6\nautopyfactory-plugins-cloud-2.4.6-4.osg34.el6\nautopyfactory-plugins-local-2.4.6-4.osg34.el6\nautopyfactory-plugins-monitor-2.4.6-4.osg34.el6\nautopyfactory-plugins-panda-2.4.6-4.osg34.el6\nautopyfactory-plugins-remote-2.4.6-4.osg34.el6\nautopyfactory-plugins-scheds-2.4.6-4.osg34.el6\nautopyfactory-proxymanager-2.4.6-4.osg34.el6\nautopyfactory-remote-2.4.6-4.osg34.el6\nautopyfactory-wms-2.4.6-4.osg34.el6\nblahp-1.18.29.bosco-3.osg34.el6\nblahp-debuginfo-1.18.29.bosco-3.osg34.el6\nbwctl-1.4-7.osg34.el6\nbwctl-client-1.4-7.osg34.el6\nbwctl-debuginfo-1.4-7.osg34.el6\nbwctl-devel-1.4-7.osg34.el6\nbwctl-server-1.4-7.osg34.el6\ncctools-4.4.3-1.osg34.el6\ncctools-chirp-4.4.3-1.osg34.el6\ncctools-debuginfo-4.4.3-1.osg34.el6\ncctools-doc-4.4.3-1.osg34.el6\ncctools-dttools-4.4.3-1.osg34.el6\ncctools-makeflow-4.4.3-1.osg34.el6\ncctools-parrot-4.4.3-1.osg34.el6\ncctools-resource_monitor-4.4.3-1.osg34.el6\ncctools-sand-4.4.3-1.osg34.el6\ncctools-wavefront-4.4.3-1.osg34.el6\ncctools-weaver-4.4.3-1.osg34.el6\ncctools-work_queue-4.4.3-1.osg34.el6\ncondor-8.6.3-1.1.osg34.el6\ncondor-all-8.6.3-1.1.osg34.el6\ncondor-bosco-8.6.3-1.1.osg34.el6\ncondor-classads-8.6.3-1.1.osg34.el6\ncondor-classads-devel-8.6.3-1.1.osg34.el6\ncondor-cream-gahp-8.6.3-1.1.osg34.el6\ncondor-cron-1.1.1-2.osg34.el6\ncondor-debuginfo-8.6.3-1.1.osg34.el6\ncondor-kbdd-8.6.3-1.1.osg34.el6\ncondor-procd-8.6.3-1.1.osg34.el6\ncondor-python-8.6.3-1.1.osg34.el6\ncondor-std-universe-8.6.3-1.1.osg34.el6\ncondor-test-8.6.3-1.1.osg34.el6\ncondor-vm-gahp-8.6.3-1.1.osg34.el6\ncvmfs-2.3.5-1.osg34.el6\ncvmfs-config-osg-2.0-2.osg34.el6\ncvmfs-devel-2.3.5-1.osg34.el6\ncvmfs-server-2.3.5-1.osg34.el6\ncvmfs-unittests-2.3.5-1.osg34.el6\ncvmfs-x509-helper-1.0-1.osg34.el6\ncvmfs-x509-helper-debuginfo-1.0-1.osg34.el6\nfrontier-squid-3.5.24-3.1.osg34.el6\nfrontier-squid-debuginfo-3.5.24-3.1.osg34.el6\nglideinwms-3.2.19-2.osg34.el6\nglideinwms-common-tools-3.2.19-2.osg34.el6\nglideinwms-condor-common-config-3.2.19-2.osg34.el6\nglideinwms-factory-3.2.19-2.osg34.el6\nglideinwms-factory-condor-3.2.19-2.osg34.el6\nglideinwms-glidecondor-tools-3.2.19-2.osg34.el6\nglideinwms-libs-3.2.19-2.osg34.el6\nglideinwms-minimal-condor-3.2.19-2.osg34.el6\nglideinwms-usercollector-3.2.19-2.osg34.el6\nglideinwms-userschedd-3.2.19-2.osg34.el6\nglideinwms-vofrontend-3.2.19-2.osg34.el6\nglideinwms-vofrontend-standalone-3.2.19-2.osg34.el6\nglite-build-common-cpp-3.3.0.2-1.osg34.el6\nglite-ce-cream-client-api-c-1.15.4-2.3.osg34.el6\nglite-ce-cream-client-devel-1.15.4-2.3.osg34.el6\nglite-ce-wsdl-1.15.1-1.1.osg34.el6\nglite-lbjp-common-gsoap-plugin-3.2.12-1.1.osg34.el6\nglite-lbjp-common-gsoap-plugin-debuginfo-3.2.12-1.1.osg34.el6\nglite-lbjp-common-gsoap-plugin-devel-3.2.12-1.1.osg34.el6\nglobus-ftp-client-8.29-1.1.osg34.el6\nglobus-ftp-client-debuginfo-8.29-1.1.osg34.el6\nglobus-ftp-client-devel-8.29-1.1.osg34.el6\nglobus-ftp-client-doc-8.29-1.1.osg34.el6\nglobus-gridftp-osg-extensions-0.3-2.osg34.el6\nglobus-gridftp-osg-extensions-debuginfo-0.3-2.osg34.el6\nglobus-gridftp-server-11.8-1.1.osg34.el6\nglobus-gridftp-server-control-4.1-1.3.osg34.el6\nglobus-gridftp-server-control-debuginfo-4.1-1.3.osg34.el6\nglobus-gridftp-server-control-devel-4.1-1.3.osg34.el6\nglobus-gridftp-server-debuginfo-11.8-1.1.osg34.el6\nglobus-gridftp-server-devel-11.8-1.1.osg34.el6\nglobus-gridftp-server-progs-11.8-1.1.osg34.el6\ngratia-probe-1.17.5-1.osg34.el6\ngratia-probe-bdii-status-1.17.5-1.osg34.el6\ngratia-probe-common-1.17.5-1.osg34.el6\ngratia-probe-condor-1.17.5-1.osg34.el6\ngratia-probe-condor-events-1.17.5-1.osg34.el6\ngratia-probe-dcache-storage-1.17.5-1.osg34.el6\ngratia-probe-dcache-storagegroup-1.17.5-1.osg34.el6\ngratia-probe-dcache-transfer-1.17.5-1.osg34.el6\ngratia-probe-debuginfo-1.17.5-1.osg34.el6\ngratia-probe-enstore-storage-1.17.5-1.osg34.el6\ngratia-probe-enstore-tapedrive-1.17.5-1.osg34.el6\ngratia-probe-enstore-transfer-1.17.5-1.osg34.el6\ngratia-probe-glexec-1.17.5-1.osg34.el6\ngratia-probe-glideinwms-1.17.5-1.osg34.el6\ngratia-probe-gram-1.17.5-1.osg34.el6\ngratia-probe-gridftp-transfer-1.17.5-1.osg34.el6\ngratia-probe-hadoop-storage-1.17.5-1.osg34.el6\ngratia-probe-htcondor-ce-1.17.5-1.osg34.el6\ngratia-probe-lsf-1.17.5-1.osg34.el6\ngratia-probe-metric-1.17.5-1.osg34.el6\ngratia-probe-onevm-1.17.5-1.osg34.el6\ngratia-probe-pbs-lsf-1.17.5-1.osg34.el6\ngratia-probe-services-1.17.5-1.osg34.el6\ngratia-probe-sge-1.17.5-1.osg34.el6\ngratia-probe-slurm-1.17.5-1.osg34.el6\ngratia-probe-xrootd-storage-1.17.5-1.osg34.el6\ngratia-probe-xrootd-transfer-1.17.5-1.osg34.el6\ngsi-openssh-7.1p2f-1.2.osg34.el6\ngsi-openssh-clients-7.1p2f-1.2.osg34.el6\ngsi-openssh-debuginfo-7.1p2f-1.2.osg34.el6\ngsi-openssh-server-7.1p2f-1.2.osg34.el6\nhtcondor-ce-2.2.0-1.osg34.el6\nhtcondor-ce-bosco-2.2.0-1.osg34.el6\nhtcondor-ce-client-2.2.0-1.osg34.el6\nhtcondor-ce-collector-2.2.0-1.osg34.el6\nhtcondor-ce-condor-2.2.0-1.osg34.el6\nhtcondor-ce-lsf-2.2.0-1.osg34.el6\nhtcondor-ce-pbs-2.2.0-1.osg34.el6\nhtcondor-ce-sge-2.2.0-1.osg34.el6\nhtcondor-ce-slurm-2.2.0-1.osg34.el6\nhtcondor-ce-view-2.2.0-1.osg34.el6\nigtf-ca-certs-1.82-1.osg34.el6\njavascriptrrd-1.1.1-1.osg34.el6\nkoji-1.11.0-1.5.osg34.el6\nkoji-builder-1.11.0-1.5.osg34.el6\nkoji-hub-1.11.0-1.5.osg34.el6\nkoji-hub-plugins-1.11.0-1.5.osg34.el6\nkoji-utils-1.11.0-1.5.osg34.el6\nkoji-vm-1.11.0-1.5.osg34.el6\nkoji-web-1.11.0-1.5.osg34.el6\nlcas-lcmaps-gt4-interface-0.3.1-1.2.osg34.el6\nlcas-lcmaps-gt4-interface-debuginfo-0.3.1-1.2.osg34.el6\nlcmaps-1.6.6-1.6.osg34.el6\nlcmaps-common-devel-1.6.6-1.6.osg34.el6\nlcmaps-db-templates-1.6.6-1.6.osg34.el6\nlcmaps-debuginfo-1.6.6-1.6.osg34.el6\nlcmaps-devel-1.6.6-1.6.osg34.el6\nlcmaps-plugins-basic-1.7.0-2.osg34.el6\nlcmaps-plugins-basic-debuginfo-1.7.0-2.osg34.el6\nlcmaps-plugins-basic-ldap-1.7.0-2.osg34.el6\nlcmaps-plugins-scas-client-0.5.6-1.osg34.el6\nlcmaps-plugins-scas-client-debuginfo-0.5.6-1.osg34.el6\nlcmaps-plugins-verify-proxy-1.5.9-1.1.osg34.el6\nlcmaps-plugins-verify-proxy-debuginfo-1.5.9-1.1.osg34.el6\nlcmaps-plugins-voms-1.7.1-1.4.osg34.el6\nlcmaps-plugins-voms-debuginfo-1.7.1-1.4.osg34.el6\nlcmaps-without-gsi-1.6.6-1.6.osg34.el6\nlcmaps-without-gsi-devel-1.6.6-1.6.osg34.el6\nllrun-0.1.3-1.3.osg34.el6\nllrun-debuginfo-0.1.3-1.3.osg34.el6\nmash-0.5.22-3.osg34.el6\nmyproxy-6.1.18-1.4.osg34.el6\nmyproxy-admin-6.1.18-1.4.osg34.el6\nmyproxy-debuginfo-6.1.18-1.4.osg34.el6\nmyproxy-devel-6.1.18-1.4.osg34.el6\nmyproxy-doc-6.1.18-1.4.osg34.el6\nmyproxy-libs-6.1.18-1.4.osg34.el6\nmyproxy-server-6.1.18-1.4.osg34.el6\nmyproxy-voms-6.1.18-1.4.osg34.el6\nnuttcp-6.1.2-1.osg34.el6\nnuttcp-debuginfo-6.1.2-1.osg34.el6\nosg-base-ce-3.4-2.osg34.el6\nosg-base-ce-bosco-3.4-2.osg34.el6\nosg-base-ce-condor-3.4-2.osg34.el6\nosg-base-ce-lsf-3.4-2.osg34.el6\nosg-base-ce-pbs-3.4-2.osg34.el6\nosg-base-ce-sge-3.4-2.osg34.el6\nosg-base-ce-slurm-3.4-2.osg34.el6\nosg-build-1.10.0-1.osg34.el6\nosg-build-base-1.10.0-1.osg34.el6\nosg-build-koji-1.10.0-1.osg34.el6\nosg-build-mock-1.10.0-1.osg34.el6\nosg-build-tests-1.10.0-1.osg34.el6\nosg-ca-certs-1.62-1.osg34.el6\nosg-ca-certs-updater-1.4-1.osg34.el6\nosg-ca-generator-1.2.0-1.osg34.el6\nosg-ca-scripts-1.1.6-1.osg34.el6\nosg-ce-3.4-2.osg34.el6\nosg-ce-bosco-3.4-2.osg34.el6\nosg-ce-condor-3.4-2.osg34.el6\nosg-ce-lsf-3.4-2.osg34.el6\nosg-ce-pbs-3.4-2.osg34.el6\nosg-ce-sge-3.4-2.osg34.el6\nosg-ce-slurm-3.4-2.osg34.el6\nosg-configure-2.0.0-3.osg34.el6\nosg-configure-bosco-2.0.0-3.osg34.el6\nosg-configure-ce-2.0.0-3.osg34.el6\nosg-configure-condor-2.0.0-3.osg34.el6\nosg-configure-gateway-2.0.0-3.osg34.el6\nosg-configure-gip-2.0.0-3.osg34.el6\nosg-configure-gratia-2.0.0-3.osg34.el6\nosg-configure-infoservices-2.0.0-3.osg34.el6\nosg-configure-lsf-2.0.0-3.osg34.el6\nosg-configure-managedfork-2.0.0-3.osg34.el6\nosg-configure-misc-2.0.0-3.osg34.el6\nosg-configure-network-2.0.0-3.osg34.el6\nosg-configure-pbs-2.0.0-3.osg34.el6\nosg-configure-rsv-2.0.0-3.osg34.el6\nosg-configure-sge-2.0.0-3.osg34.el6\nosg-configure-slurm-2.0.0-3.osg34.el6\nosg-configure-squid-2.0.0-3.osg34.el6\nosg-configure-tests-2.0.0-3.osg34.el6\nosg-control-1.1.0-1.osg34.el6\nosg-gridftp-3.4-2.osg34.el6\nosg-gridftp-xrootd-3.4-1.osg34.el6\nosg-gums-config-73-1.osg34.el6\nosg-htcondor-ce-3.4-2.osg34.el6\nosg-htcondor-ce-bosco-3.4-2.osg34.el6\nosg-htcondor-ce-condor-3.4-2.osg34.el6\nosg-htcondor-ce-lsf-3.4-2.osg34.el6\nosg-htcondor-ce-pbs-3.4-2.osg34.el6\nosg-htcondor-ce-sge-3.4-2.osg34.el6\nosg-htcondor-ce-slurm-3.4-2.osg34.el6\nosg-oasis-7-9.osg34.el6\nosg-pki-tools-1.2.20-1.osg34.el6\nosg-pki-tools-tests-1.2.20-1.osg34.el6\nosg-system-profiler-1.4.0-1.osg34.el6\nosg-system-profiler-viewer-1.4.0-1.osg34.el6\nosg-test-1.10.1-1.osg34.el6\nosg-tested-internal-3.4-2.osg34.el6\nosg-test-log-viewer-1.10.1-1.osg34.el6\nosg-update-data-1.4.0-1.osg34.el6\nosg-update-vos-1.4.0-1.osg34.el6\nosg-version-3.4.0-1.osg34.el6\nosg-vo-map-0.0.2-1.osg34.el6\nosg-wn-client-3.4-1.osg34.el6\nowamp-3.2rc4-2.osg34.el6\nowamp-client-3.2rc4-2.osg34.el6\nowamp-debuginfo-3.2rc4-2.osg34.el6\nowamp-server-3.2rc4-2.osg34.el6\npegasus-4.7.4-1.1.osg34.el6\npegasus-debuginfo-4.7.4-1.1.osg34.el6\nrsv-3.14.0-2.osg34.el6\nrsv-consumers-3.14.0-2.osg34.el6\nrsv-core-3.14.0-2.osg34.el6\nrsv-gwms-tester-1.1.2-1.osg34.el6\nrsv-metrics-3.14.0-2.osg34.el6\nuberftp-2.8-2.1.osg34.el6\nuberftp-debuginfo-2.8-2.1.osg34.el6\nvo-client-73-1.osg34.el6\nvo-client-edgmkgridmap-73-1.osg34.el6\nvo-client-lcmaps-voms-73-1.osg34.el6\nvoms-2.0.14-1.3.osg34.el6\nvoms-clients-cpp-2.0.14-1.3.osg34.el6\nvoms-debuginfo-2.0.14-1.3.osg34.el6\nvoms-devel-2.0.14-1.3.osg34.el6\nvoms-doc-2.0.14-1.3.osg34.el6\nvoms-server-2.0.14-1.3.osg34.el6\nxacml-1.5.0-1.osg34.el6\nxacml-debuginfo-1.5.0-1.osg34.el6\nxacml-devel-1.5.0-1.osg34.el6\nxrootd-4.6.1-1.osg34.el6\nxrootd-client-4.6.1-1.osg34.el6\nxrootd-client-devel-4.6.1-1.osg34.el6\nxrootd-client-libs-4.6.1-1.osg34.el6\nxrootd-debuginfo-4.6.1-1.osg34.el6\nxrootd-devel-4.6.1-1.osg34.el6\nxrootd-doc-4.6.1-1.osg34.el6\nxrootd-dsi-3.0.4-22.osg34.el6\nxrootd-dsi-debuginfo-3.0.4-22.osg34.el6\nxrootd-fuse-4.6.1-1.osg34.el6\nxrootd-lcmaps-1.2.1-2.osg34.el6\nxrootd-lcmaps-debuginfo-1.2.1-2.osg34.el6\nxrootd-libs-4.6.1-1.osg34.el6\nxrootd-private-devel-4.6.1-1.osg34.el6\nxrootd-python-4.6.1-1.osg34.el6\nxrootd-selinux-4.6.1-1.osg34.el6\nxrootd-server-4.6.1-1.osg34.el6\nxrootd-server-devel-4.6.1-1.osg34.el6\nxrootd-server-libs-4.6.1-1.osg34.el6\nxrootd-voms-plugin-0.4.0-1.osg34.el6\nxrootd-voms-plugin-debuginfo-0.4.0-1.osg34.el6\nxrootd-voms-plugin-devel-0.4.0-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#enterprise-linux-7_2", 
            "text": "autopyfactory-2.4.6-4.osg34.el7\nautopyfactory-cloud-2.4.6-4.osg34.el7\nautopyfactory-common-2.4.6-4.osg34.el7\nautopyfactory-panda-2.4.6-4.osg34.el7\nautopyfactory-plugins-cloud-2.4.6-4.osg34.el7\nautopyfactory-plugins-local-2.4.6-4.osg34.el7\nautopyfactory-plugins-monitor-2.4.6-4.osg34.el7\nautopyfactory-plugins-panda-2.4.6-4.osg34.el7\nautopyfactory-plugins-remote-2.4.6-4.osg34.el7\nautopyfactory-plugins-scheds-2.4.6-4.osg34.el7\nautopyfactory-proxymanager-2.4.6-4.osg34.el7\nautopyfactory-remote-2.4.6-4.osg34.el7\nautopyfactory-wms-2.4.6-4.osg34.el7\nblahp-1.18.29.bosco-3.osg34.el7\nblahp-debuginfo-1.18.29.bosco-3.osg34.el7\nbwctl-1.4-7.osg34.el7\nbwctl-client-1.4-7.osg34.el7\nbwctl-debuginfo-1.4-7.osg34.el7\nbwctl-devel-1.4-7.osg34.el7\nbwctl-server-1.4-7.osg34.el7\ncctools-4.4.3-1.osg34.el7\ncctools-chirp-4.4.3-1.osg34.el7\ncctools-debuginfo-4.4.3-1.osg34.el7\ncctools-doc-4.4.3-1.osg34.el7\ncctools-dttools-4.4.3-1.osg34.el7\ncctools-makeflow-4.4.3-1.osg34.el7\ncctools-parrot-4.4.3-1.osg34.el7\ncctools-resource_monitor-4.4.3-1.osg34.el7\ncctools-sand-4.4.3-1.osg34.el7\ncctools-wavefront-4.4.3-1.osg34.el7\ncctools-weaver-4.4.3-1.osg34.el7\ncctools-work_queue-4.4.3-1.osg34.el7\ncondor-8.6.3-1.1.osg34.el7\ncondor-all-8.6.3-1.1.osg34.el7\ncondor-bosco-8.6.3-1.1.osg34.el7\ncondor-classads-8.6.3-1.1.osg34.el7\ncondor-classads-devel-8.6.3-1.1.osg34.el7\ncondor-cream-gahp-8.6.3-1.1.osg34.el7\ncondor-cron-1.1.1-2.osg34.el7\ncondor-debuginfo-8.6.3-1.1.osg34.el7\ncondor-kbdd-8.6.3-1.1.osg34.el7\ncondor-procd-8.6.3-1.1.osg34.el7\ncondor-python-8.6.3-1.1.osg34.el7\ncondor-test-8.6.3-1.1.osg34.el7\ncondor-vm-gahp-8.6.3-1.1.osg34.el7\ncvmfs-2.3.5-1.osg34.el7\ncvmfs-config-osg-2.0-2.osg34.el7\ncvmfs-devel-2.3.5-1.osg34.el7\ncvmfs-server-2.3.5-1.osg34.el7\ncvmfs-unittests-2.3.5-1.osg34.el7\ncvmfs-x509-helper-1.0-1.osg34.el7\ncvmfs-x509-helper-debuginfo-1.0-1.osg34.el7\nfrontier-squid-3.5.24-3.1.osg34.el7\nfrontier-squid-debuginfo-3.5.24-3.1.osg34.el7\nglideinwms-3.2.19-2.osg34.el7\nglideinwms-common-tools-3.2.19-2.osg34.el7\nglideinwms-condor-common-config-3.2.19-2.osg34.el7\nglideinwms-factory-3.2.19-2.osg34.el7\nglideinwms-factory-condor-3.2.19-2.osg34.el7\nglideinwms-glidecondor-tools-3.2.19-2.osg34.el7\nglideinwms-libs-3.2.19-2.osg34.el7\nglideinwms-minimal-condor-3.2.19-2.osg34.el7\nglideinwms-usercollector-3.2.19-2.osg34.el7\nglideinwms-userschedd-3.2.19-2.osg34.el7\nglideinwms-vofrontend-3.2.19-2.osg34.el7\nglideinwms-vofrontend-standalone-3.2.19-2.osg34.el7\nglite-build-common-cpp-3.3.0.2-1.osg34.el7\nglite-ce-cream-client-api-c-1.15.4-2.3.osg34.el7\nglite-ce-cream-client-devel-1.15.4-2.3.osg34.el7\nglite-ce-wsdl-1.15.1-1.1.osg34.el7\nglite-lbjp-common-gsoap-plugin-3.2.12-1.1.osg34.el7\nglite-lbjp-common-gsoap-plugin-debuginfo-3.2.12-1.1.osg34.el7\nglite-lbjp-common-gsoap-plugin-devel-3.2.12-1.1.osg34.el7\nglobus-ftp-client-8.29-1.1.osg34.el7\nglobus-ftp-client-debuginfo-8.29-1.1.osg34.el7\nglobus-ftp-client-devel-8.29-1.1.osg34.el7\nglobus-ftp-client-doc-8.29-1.1.osg34.el7\nglobus-gridftp-osg-extensions-0.3-2.osg34.el7\nglobus-gridftp-osg-extensions-debuginfo-0.3-2.osg34.el7\nglobus-gridftp-server-11.8-1.1.osg34.el7\nglobus-gridftp-server-control-4.1-1.3.osg34.el7\nglobus-gridftp-server-control-debuginfo-4.1-1.3.osg34.el7\nglobus-gridftp-server-control-devel-4.1-1.3.osg34.el7\nglobus-gridftp-server-debuginfo-11.8-1.1.osg34.el7\nglobus-gridftp-server-devel-11.8-1.1.osg34.el7\nglobus-gridftp-server-progs-11.8-1.1.osg34.el7\ngratia-probe-1.17.5-1.osg34.el7\ngratia-probe-bdii-status-1.17.5-1.osg34.el7\ngratia-probe-common-1.17.5-1.osg34.el7\ngratia-probe-condor-1.17.5-1.osg34.el7\ngratia-probe-condor-events-1.17.5-1.osg34.el7\ngratia-probe-dcache-storage-1.17.5-1.osg34.el7\ngratia-probe-dcache-storagegroup-1.17.5-1.osg34.el7\ngratia-probe-dcache-transfer-1.17.5-1.osg34.el7\ngratia-probe-debuginfo-1.17.5-1.osg34.el7\ngratia-probe-enstore-storage-1.17.5-1.osg34.el7\ngratia-probe-enstore-tapedrive-1.17.5-1.osg34.el7\ngratia-probe-enstore-transfer-1.17.5-1.osg34.el7\ngratia-probe-glexec-1.17.5-1.osg34.el7\ngratia-probe-glideinwms-1.17.5-1.osg34.el7\ngratia-probe-gram-1.17.5-1.osg34.el7\ngratia-probe-gridftp-transfer-1.17.5-1.osg34.el7\ngratia-probe-hadoop-storage-1.17.5-1.osg34.el7\ngratia-probe-htcondor-ce-1.17.5-1.osg34.el7\ngratia-probe-lsf-1.17.5-1.osg34.el7\ngratia-probe-metric-1.17.5-1.osg34.el7\ngratia-probe-onevm-1.17.5-1.osg34.el7\ngratia-probe-pbs-lsf-1.17.5-1.osg34.el7\ngratia-probe-services-1.17.5-1.osg34.el7\ngratia-probe-sge-1.17.5-1.osg34.el7\ngratia-probe-slurm-1.17.5-1.osg34.el7\ngratia-probe-xrootd-storage-1.17.5-1.osg34.el7\ngratia-probe-xrootd-transfer-1.17.5-1.osg34.el7\ngsi-openssh-7.1p2f-1.2.osg34.el7\ngsi-openssh-clients-7.1p2f-1.2.osg34.el7\ngsi-openssh-debuginfo-7.1p2f-1.2.osg34.el7\ngsi-openssh-server-7.1p2f-1.2.osg34.el7\nhtcondor-ce-2.2.0-1.osg34.el7\nhtcondor-ce-bosco-2.2.0-1.osg34.el7\nhtcondor-ce-client-2.2.0-1.osg34.el7\nhtcondor-ce-collector-2.2.0-1.osg34.el7\nhtcondor-ce-condor-2.2.0-1.osg34.el7\nhtcondor-ce-lsf-2.2.0-1.osg34.el7\nhtcondor-ce-pbs-2.2.0-1.osg34.el7\nhtcondor-ce-sge-2.2.0-1.osg34.el7\nhtcondor-ce-slurm-2.2.0-1.osg34.el7\nhtcondor-ce-view-2.2.0-1.osg34.el7\nigtf-ca-certs-1.82-1.osg34.el7\njavascriptrrd-1.1.1-1.osg34.el7\nkoji-1.11.0-1.5.osg34.el7\nkoji-builder-1.11.0-1.5.osg34.el7\nkoji-hub-1.11.0-1.5.osg34.el7\nkoji-hub-plugins-1.11.0-1.5.osg34.el7\nkoji-utils-1.11.0-1.5.osg34.el7\nkoji-vm-1.11.0-1.5.osg34.el7\nkoji-web-1.11.0-1.5.osg34.el7\nlcas-lcmaps-gt4-interface-0.3.1-1.2.osg34.el7\nlcas-lcmaps-gt4-interface-debuginfo-0.3.1-1.2.osg34.el7\nlcmaps-1.6.6-1.6.osg34.el7\nlcmaps-common-devel-1.6.6-1.6.osg34.el7\nlcmaps-db-templates-1.6.6-1.6.osg34.el7\nlcmaps-debuginfo-1.6.6-1.6.osg34.el7\nlcmaps-devel-1.6.6-1.6.osg34.el7\nlcmaps-plugins-basic-1.7.0-2.osg34.el7\nlcmaps-plugins-basic-debuginfo-1.7.0-2.osg34.el7\nlcmaps-plugins-basic-ldap-1.7.0-2.osg34.el7\nlcmaps-plugins-scas-client-0.5.6-1.osg34.el7\nlcmaps-plugins-scas-client-debuginfo-0.5.6-1.osg34.el7\nlcmaps-plugins-verify-proxy-1.5.9-1.1.osg34.el7\nlcmaps-plugins-verify-proxy-debuginfo-1.5.9-1.1.osg34.el7\nlcmaps-plugins-voms-1.7.1-1.4.osg34.el7\nlcmaps-plugins-voms-debuginfo-1.7.1-1.4.osg34.el7\nlcmaps-without-gsi-1.6.6-1.6.osg34.el7\nlcmaps-without-gsi-devel-1.6.6-1.6.osg34.el7\nllrun-0.1.3-1.3.osg34.el7\nllrun-debuginfo-0.1.3-1.3.osg34.el7\nmash-0.5.22-3.osg34.el7\nmyproxy-6.1.18-1.4.osg34.el7\nmyproxy-admin-6.1.18-1.4.osg34.el7\nmyproxy-debuginfo-6.1.18-1.4.osg34.el7\nmyproxy-devel-6.1.18-1.4.osg34.el7\nmyproxy-doc-6.1.18-1.4.osg34.el7\nmyproxy-libs-6.1.18-1.4.osg34.el7\nmyproxy-server-6.1.18-1.4.osg34.el7\nmyproxy-voms-6.1.18-1.4.osg34.el7\nnuttcp-6.1.2-1.osg34.el7\nnuttcp-debuginfo-6.1.2-1.osg34.el7\nosg-base-ce-3.4-2.osg34.el7\nosg-base-ce-bosco-3.4-2.osg34.el7\nosg-base-ce-condor-3.4-2.osg34.el7\nosg-base-ce-lsf-3.4-2.osg34.el7\nosg-base-ce-pbs-3.4-2.osg34.el7\nosg-base-ce-sge-3.4-2.osg34.el7\nosg-base-ce-slurm-3.4-2.osg34.el7\nosg-build-1.10.0-1.osg34.el7\nosg-build-base-1.10.0-1.osg34.el7\nosg-build-koji-1.10.0-1.osg34.el7\nosg-build-mock-1.10.0-1.osg34.el7\nosg-build-tests-1.10.0-1.osg34.el7\nosg-ca-certs-1.62-1.osg34.el7\nosg-ca-certs-updater-1.4-1.osg34.el7\nosg-ca-generator-1.2.0-1.osg34.el7\nosg-ca-scripts-1.1.6-1.osg34.el7\nosg-ce-3.4-2.osg34.el7\nosg-ce-bosco-3.4-2.osg34.el7\nosg-ce-condor-3.4-2.osg34.el7\nosg-ce-lsf-3.4-2.osg34.el7\nosg-ce-pbs-3.4-2.osg34.el7\nosg-ce-sge-3.4-2.osg34.el7\nosg-ce-slurm-3.4-2.osg34.el7\nosg-configure-2.0.0-3.osg34.el7\nosg-configure-bosco-2.0.0-3.osg34.el7\nosg-configure-ce-2.0.0-3.osg34.el7\nosg-configure-condor-2.0.0-3.osg34.el7\nosg-configure-gateway-2.0.0-3.osg34.el7\nosg-configure-gip-2.0.0-3.osg34.el7\nosg-configure-gratia-2.0.0-3.osg34.el7\nosg-configure-infoservices-2.0.0-3.osg34.el7\nosg-configure-lsf-2.0.0-3.osg34.el7\nosg-configure-managedfork-2.0.0-3.osg34.el7\nosg-configure-misc-2.0.0-3.osg34.el7\nosg-configure-network-2.0.0-3.osg34.el7\nosg-configure-pbs-2.0.0-3.osg34.el7\nosg-configure-rsv-2.0.0-3.osg34.el7\nosg-configure-sge-2.0.0-3.osg34.el7\nosg-configure-slurm-2.0.0-3.osg34.el7\nosg-configure-squid-2.0.0-3.osg34.el7\nosg-configure-tests-2.0.0-3.osg34.el7\nosg-control-1.1.0-1.osg34.el7\nosg-gridftp-3.4-2.osg34.el7\nosg-gridftp-xrootd-3.4-1.osg34.el7\nosg-gums-config-73-1.osg34.el7\nosg-htcondor-ce-3.4-2.osg34.el7\nosg-htcondor-ce-bosco-3.4-2.osg34.el7\nosg-htcondor-ce-condor-3.4-2.osg34.el7\nosg-htcondor-ce-lsf-3.4-2.osg34.el7\nosg-htcondor-ce-pbs-3.4-2.osg34.el7\nosg-htcondor-ce-sge-3.4-2.osg34.el7\nosg-htcondor-ce-slurm-3.4-2.osg34.el7\nosg-oasis-7-9.osg34.el7\nosg-pki-tools-1.2.20-1.osg34.el7\nosg-pki-tools-tests-1.2.20-1.osg34.el7\nosg-system-profiler-1.4.0-1.osg34.el7\nosg-system-profiler-viewer-1.4.0-1.osg34.el7\nosg-test-1.10.1-1.osg34.el7\nosg-tested-internal-3.4-2.osg34.el7\nosg-test-log-viewer-1.10.1-1.osg34.el7\nosg-update-data-1.4.0-1.osg34.el7\nosg-update-vos-1.4.0-1.osg34.el7\nosg-version-3.4.0-1.osg34.el7\nosg-vo-map-0.0.2-1.osg34.el7\nosg-wn-client-3.4-1.osg34.el7\nowamp-3.2rc4-2.osg34.el7\nowamp-client-3.2rc4-2.osg34.el7\nowamp-debuginfo-3.2rc4-2.osg34.el7\nowamp-server-3.2rc4-2.osg34.el7\npegasus-4.7.4-1.1.osg34.el7\npegasus-debuginfo-4.7.4-1.1.osg34.el7\nrsv-3.14.0-2.osg34.el7\nrsv-consumers-3.14.0-2.osg34.el7\nrsv-core-3.14.0-2.osg34.el7\nrsv-gwms-tester-1.1.2-1.osg34.el7\nrsv-metrics-3.14.0-2.osg34.el7\nstashcache-0.7-2.osg34.el7\nstashcache-cache-server-0.7-2.osg34.el7\nstashcache-daemon-0.7-2.osg34.el7\nstashcache-origin-server-0.7-2.osg34.el7\nuberftp-2.8-2.1.osg34.el7\nuberftp-debuginfo-2.8-2.1.osg34.el7\nvo-client-73-1.osg34.el7\nvo-client-edgmkgridmap-73-1.osg34.el7\nvo-client-lcmaps-voms-73-1.osg34.el7\nvoms-2.0.14-1.3.osg34.el7\nvoms-clients-cpp-2.0.14-1.3.osg34.el7\nvoms-debuginfo-2.0.14-1.3.osg34.el7\nvoms-devel-2.0.14-1.3.osg34.el7\nvoms-doc-2.0.14-1.3.osg34.el7\nvoms-server-2.0.14-1.3.osg34.el7\nxacml-1.5.0-1.osg34.el7\nxacml-debuginfo-1.5.0-1.osg34.el7\nxacml-devel-1.5.0-1.osg34.el7\nxrootd-4.6.1-1.osg34.el7\nxrootd-client-4.6.1-1.osg34.el7\nxrootd-client-devel-4.6.1-1.osg34.el7\nxrootd-client-libs-4.6.1-1.osg34.el7\nxrootd-debuginfo-4.6.1-1.osg34.el7\nxrootd-devel-4.6.1-1.osg34.el7\nxrootd-doc-4.6.1-1.osg34.el7\nxrootd-dsi-3.0.4-22.osg34.el7\nxrootd-dsi-debuginfo-3.0.4-22.osg34.el7\nxrootd-fuse-4.6.1-1.osg34.el7\nxrootd-lcmaps-1.3.3-3.osg34.el7\nxrootd-lcmaps-debuginfo-1.3.3-3.osg34.el7\nxrootd-libs-4.6.1-1.osg34.el7\nxrootd-private-devel-4.6.1-1.osg34.el7\nxrootd-python-4.6.1-1.osg34.el7\nxrootd-selinux-4.6.1-1.osg34.el7\nxrootd-server-4.6.1-1.osg34.el7\nxrootd-server-devel-4.6.1-1.osg34.el7\nxrootd-server-libs-4.6.1-1.osg34.el7\nxrootd-voms-plugin-0.4.0-1.osg34.el7\nxrootd-voms-plugin-debuginfo-0.4.0-1.osg34.el7\nxrootd-voms-plugin-devel-0.4.0-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#enterprise-linux-6_3", 
            "text": "glideinwms-3.3.2-2.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#enterprise-linux-7_3", 
            "text": "glideinwms-3.3.2-2.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  glideinwms glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#enterprise-linux-6_4", 
            "text": "glideinwms-3.3.2-2.osgup.el6\nglideinwms-common-tools-3.3.2-2.osgup.el6\nglideinwms-condor-common-config-3.3.2-2.osgup.el6\nglideinwms-factory-3.3.2-2.osgup.el6\nglideinwms-factory-condor-3.3.2-2.osgup.el6\nglideinwms-glidecondor-tools-3.3.2-2.osgup.el6\nglideinwms-libs-3.3.2-2.osgup.el6\nglideinwms-minimal-condor-3.3.2-2.osgup.el6\nglideinwms-usercollector-3.3.2-2.osgup.el6\nglideinwms-userschedd-3.3.2-2.osgup.el6\nglideinwms-vofrontend-3.3.2-2.osgup.el6\nglideinwms-vofrontend-standalone-3.3.2-2.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#enterprise-linux-7_4", 
            "text": "glideinwms-3.3.2-2.osgup.el7\nglideinwms-common-tools-3.3.2-2.osgup.el7\nglideinwms-condor-common-config-3.3.2-2.osgup.el7\nglideinwms-factory-3.3.2-2.osgup.el7\nglideinwms-factory-condor-3.3.2-2.osgup.el7\nglideinwms-glidecondor-tools-3.3.2-2.osgup.el7\nglideinwms-libs-3.3.2-2.osgup.el7\nglideinwms-minimal-condor-3.3.2-2.osgup.el7\nglideinwms-usercollector-3.3.2-2.osgup.el7\nglideinwms-userschedd-3.3.2-2.osgup.el7\nglideinwms-vofrontend-3.3.2-2.osgup.el7\nglideinwms-vofrontend-standalone-3.3.2-2.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-27/", 
            "text": "OSG Software Release 3.3.27\n\n\nRelease Date\n: 2017-08-08\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nHTCondor 8.4.12\n\n\nUpdated SELinux profile which is required on Red Hat 7.4\n\n\n\n\n\n\nHTCondor-CE\n\n\nDo not hold running jobs with an expired proxy\n\n\nInitialize \n$SPOOL/ceview/vos\n directory at installation time so that the VO tab functions in CEView before any pilots are received\n\n\nDon't warn about not running osg-configure, if osg-configure is not installed\n\n\n\n\n\n\nDefault configuration improvements for condor-cron\n\n\nClarified how to override the \nCONDOR_IDS\n\n\nDisable the unused GSI authentication that was producing spurious log messages.\n\n\n\n\n\n\nSeveral improvements to osg-configure\n\n\nEnsure that GUMS is configured before trying to query the GUMS server\n\n\nProgress updates (such as informing when an operation is expected to take a while) during the user VO file validation are presented to the user rather than being sent to the log file.\n\n\nosg-configure will issue same warnings and errors with -v option as it does with the -c option.\n\n\n\n\n\n\nAdded blahp configuration option for PBS Pro. This option must be used when the disambiguation code does not correctly determine which PBS is in use.\n\n\nRelaxed proxy validation in jGlobus to be RFC-3820 compliant in order for proxies to work properly with BeStMan\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n StashCache is supported on EL7 only. \n xrootd-lcmaps will remain at 1.2.1-1 on EL6.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nKnown Issues\n\n\n\n\nBecause the Gratia system has been turned off, RSV emits the following warning: \nWARNING: Gratia server gratia-osg-prod.opensciencegrid.org:80 not responding to obtain last contact details.\n. This warning can safely be ignored.\n\n\nUsing the LCMAPS VOMS will result in a failing \"supported VO\" RSV test (\nSOFTWARE-2763\n). This can be ignored and a fix is targeted for the September release.\n\n\nUpdates to VOMS admin server require the updated emi-trustmanager-tomcat and re-running the configure script:\\ \nroot@host # /var/lib/trustmanager-tomcat/configure.sh\n\n\nVOMS admin server shows an error when modifying/adding/signing AUPs, but all the actions still work.\n\n\nAfter updating OSG-CE to version 3.3-12, please disable and remove OSG Info Services via the following procedure:\n\n\n\n\nroot@host #\n service osg-info-services stop\n\nroot@host #\n yum erase gip osg-info-services\n\n\n\n\n\n\n\nThe Koji client config has changed in the new version of Koji: `pkgurl\nhttp://koji.chtc.wisc.edu/packages` has been replaced by `topurl=http://koji.chtc.wisc.edu` and the Koji client will give a harmless but annoying warning when it finds `pkgurl`. To get rid of the warning, update to osg-build \n 1.8.0, rerun `osg-koji setup`, and say 'yes' when asked to replace the Koji configuration file; or, you may make the above change manually.\n\n\nA previous version (\n1.17.0-2.5\n) of the Gratia probes required changes in the HTCondor configuration to operate properly. Now, these changes need to be reverted to use verison \n1.17.0-2.6\n and later of the probes. The lines below are likely to be found in \ncondor_config.local\n or a new file in \n/etc/condor/config.d\n directory. Delete these lines and run \ncondor_reconfig\n.\n\n\n\n\n# GlideinWMS Gratia commands\n\n\nPER_JOB_HISTORY_DIR\n \n=\n /var/lib/gratia/data\n\nJOBGLIDEIN_ResourceName\n=\n$$\n([IfThenElse(IsUndefined(TARGET.GLIDEIN_ResourceName), IfThenElse(IsUndefined(TARGET.GLIDEIN_Site), IfThenElse(IsUndefined(TARGET.FileSystemDomain), \\\nLocal Job\\\n, TARGET.FileSystemDomain), TARGET.GLIDEIN_Site), TARGET.GLIDEIN_ResourceName)])\n\n\nSUBMIT_EXPRS\n \n=\n \n$(\nSUBMIT_EXPRS\n)\n JOBGLIDEIN_ResourceName   \n\n\n\n\n\n\n\nOn EL7, your version of voms-proxy-init may come from the voms-clients-java package; this version will continue to make legacy proxies by default. If you want the new behavior, install the voms-clients-cpp package and uninstall the voms-clients-java package.\n\n\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.32.bosco-1.osg33.el6\n\n\ncondor-8.4.12-2.1.osg33.el6\n\n\ncondor-cron-1.1.3-1.osg33.el6\n\n\nhtcondor-ce-2.2.2-1.osg33.el6\n\n\njglobus-2.1.0-9.osg33.el6\n\n\nosg-configure-1.9.1-1.osg33.el6\n\n\nosg-test-1.11.1-1.osg33.el6\n\n\nosg-tested-internal-3.3-18.osg33.el6\n\n\nosg-version-3.3.27-1.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.32.bosco-1.osg33.el7\n\n\ncondor-8.4.12-2.1.osg33.el7\n\n\ncondor-cron-1.1.3-1.osg33.el7\n\n\nhtcondor-ce-2.2.2-1.osg33.el7\n\n\njglobus-2.1.0-9.osg33.el7\n\n\nosg-configure-1.9.1-1.osg33.el7\n\n\nosg-test-1.11.1-1.osg33.el7\n\n\nosg-tested-internal-3.3-18.osg33.el7\n\n\nosg-version-3.3.27-1.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-cron condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-slurm htcondor-ce-view igtf-ca-certs jglobus osg-ca-certs osg-configure osg-configure-bosco osg-configure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-test osg-tested-internal osg-tested-internal-gram osg-test-log-viewer osg-version\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp-1.18.32.bosco-1.osg33.el6\nblahp-debuginfo-1.18.32.bosco-1.osg33.el6\ncondor-8.4.12-2.1.osg33.el6\ncondor-all-8.4.12-2.1.osg33.el6\ncondor-bosco-8.4.12-2.1.osg33.el6\ncondor-classads-8.4.12-2.1.osg33.el6\ncondor-classads-devel-8.4.12-2.1.osg33.el6\ncondor-cream-gahp-8.4.12-2.1.osg33.el6\ncondor-cron-1.1.3-1.osg33.el6\ncondor-debuginfo-8.4.12-2.1.osg33.el6\ncondor-kbdd-8.4.12-2.1.osg33.el6\ncondor-procd-8.4.12-2.1.osg33.el6\ncondor-python-8.4.12-2.1.osg33.el6\ncondor-std-universe-8.4.12-2.1.osg33.el6\ncondor-test-8.4.12-2.1.osg33.el6\ncondor-vm-gahp-8.4.12-2.1.osg33.el6\nhtcondor-ce-2.2.2-1.osg33.el6\nhtcondor-ce-bosco-2.2.2-1.osg33.el6\nhtcondor-ce-client-2.2.2-1.osg33.el6\nhtcondor-ce-collector-2.2.2-1.osg33.el6\nhtcondor-ce-condor-2.2.2-1.osg33.el6\nhtcondor-ce-lsf-2.2.2-1.osg33.el6\nhtcondor-ce-pbs-2.2.2-1.osg33.el6\nhtcondor-ce-sge-2.2.2-1.osg33.el6\nhtcondor-ce-slurm-2.2.2-1.osg33.el6\nhtcondor-ce-view-2.2.2-1.osg33.el6\njglobus-2.1.0-9.osg33.el6\nosg-configure-1.9.1-1.osg33.el6\nosg-configure-bosco-1.9.1-1.osg33.el6\nosg-configure-ce-1.9.1-1.osg33.el6\nosg-configure-cemon-1.9.1-1.osg33.el6\nosg-configure-condor-1.9.1-1.osg33.el6\nosg-configure-gateway-1.9.1-1.osg33.el6\nosg-configure-gip-1.9.1-1.osg33.el6\nosg-configure-gratia-1.9.1-1.osg33.el6\nosg-configure-infoservices-1.9.1-1.osg33.el6\nosg-configure-lsf-1.9.1-1.osg33.el6\nosg-configure-managedfork-1.9.1-1.osg33.el6\nosg-configure-misc-1.9.1-1.osg33.el6\nosg-configure-monalisa-1.9.1-1.osg33.el6\nosg-configure-network-1.9.1-1.osg33.el6\nosg-configure-pbs-1.9.1-1.osg33.el6\nosg-configure-rsv-1.9.1-1.osg33.el6\nosg-configure-sge-1.9.1-1.osg33.el6\nosg-configure-slurm-1.9.1-1.osg33.el6\nosg-configure-squid-1.9.1-1.osg33.el6\nosg-configure-tests-1.9.1-1.osg33.el6\nosg-test-1.11.1-1.osg33.el6\nosg-tested-internal-3.3-18.osg33.el6\nosg-tested-internal-gram-3.3-18.osg33.el6\nosg-test-log-viewer-1.11.1-1.osg33.el6\nosg-version-3.3.27-1.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp-1.18.32.bosco-1.osg33.el7\nblahp-debuginfo-1.18.32.bosco-1.osg33.el7\ncondor-8.4.12-2.1.osg33.el7\ncondor-all-8.4.12-2.1.osg33.el7\ncondor-bosco-8.4.12-2.1.osg33.el7\ncondor-classads-8.4.12-2.1.osg33.el7\ncondor-classads-devel-8.4.12-2.1.osg33.el7\ncondor-cream-gahp-8.4.12-2.1.osg33.el7\ncondor-cron-1.1.3-1.osg33.el7\ncondor-debuginfo-8.4.12-2.1.osg33.el7\ncondor-kbdd-8.4.12-2.1.osg33.el7\ncondor-procd-8.4.12-2.1.osg33.el7\ncondor-python-8.4.12-2.1.osg33.el7\ncondor-test-8.4.12-2.1.osg33.el7\ncondor-vm-gahp-8.4.12-2.1.osg33.el7\nhtcondor-ce-2.2.2-1.osg33.el7\nhtcondor-ce-bosco-2.2.2-1.osg33.el7\nhtcondor-ce-client-2.2.2-1.osg33.el7\nhtcondor-ce-collector-2.2.2-1.osg33.el7\nhtcondor-ce-condor-2.2.2-1.osg33.el7\nhtcondor-ce-lsf-2.2.2-1.osg33.el7\nhtcondor-ce-pbs-2.2.2-1.osg33.el7\nhtcondor-ce-sge-2.2.2-1.osg33.el7\nhtcondor-ce-slurm-2.2.2-1.osg33.el7\nhtcondor-ce-view-2.2.2-1.osg33.el7\njglobus-2.1.0-9.osg33.el7\nosg-configure-1.9.1-1.osg33.el7\nosg-configure-bosco-1.9.1-1.osg33.el7\nosg-configure-ce-1.9.1-1.osg33.el7\nosg-configure-cemon-1.9.1-1.osg33.el7\nosg-configure-condor-1.9.1-1.osg33.el7\nosg-configure-gateway-1.9.1-1.osg33.el7\nosg-configure-gip-1.9.1-1.osg33.el7\nosg-configure-gratia-1.9.1-1.osg33.el7\nosg-configure-infoservices-1.9.1-1.osg33.el7\nosg-configure-lsf-1.9.1-1.osg33.el7\nosg-configure-managedfork-1.9.1-1.osg33.el7\nosg-configure-misc-1.9.1-1.osg33.el7\nosg-configure-monalisa-1.9.1-1.osg33.el7\nosg-configure-network-1.9.1-1.osg33.el7\nosg-configure-pbs-1.9.1-1.osg33.el7\nosg-configure-rsv-1.9.1-1.osg33.el7\nosg-configure-sge-1.9.1-1.osg33.el7\nosg-configure-slurm-1.9.1-1.osg33.el7\nosg-configure-squid-1.9.1-1.osg33.el7\nosg-configure-tests-1.9.1-1.osg33.el7\nosg-test-1.11.1-1.osg33.el7\nosg-tested-internal-3.3-18.osg33.el7\nosg-tested-internal-gram-3.3-18.osg33.el7\nosg-test-log-viewer-1.11.1-1.osg33.el7\nosg-version-3.3.27-1.osg33.el7", 
            "title": "OSG Release 3.3.27"
        }, 
        {
            "location": "/release/3.3/release-3-3-27/#osg-software-release-3327", 
            "text": "Release Date : 2017-08-08", 
            "title": "OSG Software Release 3.3.27"
        }, 
        {
            "location": "/release/3.3/release-3-3-27/#summary-of-changes", 
            "text": "This release contains:   HTCondor 8.4.12  Updated SELinux profile which is required on Red Hat 7.4    HTCondor-CE  Do not hold running jobs with an expired proxy  Initialize  $SPOOL/ceview/vos  directory at installation time so that the VO tab functions in CEView before any pilots are received  Don't warn about not running osg-configure, if osg-configure is not installed    Default configuration improvements for condor-cron  Clarified how to override the  CONDOR_IDS  Disable the unused GSI authentication that was producing spurious log messages.    Several improvements to osg-configure  Ensure that GUMS is configured before trying to query the GUMS server  Progress updates (such as informing when an operation is expected to take a while) during the user VO file validation are presented to the user rather than being sent to the log file.  osg-configure will issue same warnings and errors with -v option as it does with the -c option.    Added blahp configuration option for PBS Pro. This option must be used when the disambiguation code does not correctly determine which PBS is in use.  Relaxed proxy validation in jGlobus to be RFC-3820 compliant in order for proxies to work properly with BeStMan   These  JIRA tickets  were addressed in this release.   StashCache is supported on EL7 only.   xrootd-lcmaps will remain at 1.2.1-1 on EL6.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-27/#known-issues", 
            "text": "Because the Gratia system has been turned off, RSV emits the following warning:  WARNING: Gratia server gratia-osg-prod.opensciencegrid.org:80 not responding to obtain last contact details. . This warning can safely be ignored.  Using the LCMAPS VOMS will result in a failing \"supported VO\" RSV test ( SOFTWARE-2763 ). This can be ignored and a fix is targeted for the September release.  Updates to VOMS admin server require the updated emi-trustmanager-tomcat and re-running the configure script:\\  root@host # /var/lib/trustmanager-tomcat/configure.sh  VOMS admin server shows an error when modifying/adding/signing AUPs, but all the actions still work.  After updating OSG-CE to version 3.3-12, please disable and remove OSG Info Services via the following procedure:   root@host #  service osg-info-services stop root@host #  yum erase gip osg-info-services   The Koji client config has changed in the new version of Koji: `pkgurl http://koji.chtc.wisc.edu/packages` has been replaced by `topurl=http://koji.chtc.wisc.edu` and the Koji client will give a harmless but annoying warning when it finds `pkgurl`. To get rid of the warning, update to osg-build   1.8.0, rerun `osg-koji setup`, and say 'yes' when asked to replace the Koji configuration file; or, you may make the above change manually.  A previous version ( 1.17.0-2.5 ) of the Gratia probes required changes in the HTCondor configuration to operate properly. Now, these changes need to be reverted to use verison  1.17.0-2.6  and later of the probes. The lines below are likely to be found in  condor_config.local  or a new file in  /etc/condor/config.d  directory. Delete these lines and run  condor_reconfig .   # GlideinWMS Gratia commands  PER_JOB_HISTORY_DIR   =  /var/lib/gratia/data JOBGLIDEIN_ResourceName = $$ ([IfThenElse(IsUndefined(TARGET.GLIDEIN_ResourceName), IfThenElse(IsUndefined(TARGET.GLIDEIN_Site), IfThenElse(IsUndefined(TARGET.FileSystemDomain), \\ Local Job\\ , TARGET.FileSystemDomain), TARGET.GLIDEIN_Site), TARGET.GLIDEIN_ResourceName)])  SUBMIT_EXPRS   =   $( SUBMIT_EXPRS )  JOBGLIDEIN_ResourceName      On EL7, your version of voms-proxy-init may come from the voms-clients-java package; this version will continue to make legacy proxies by default. If you want the new behavior, install the voms-clients-cpp package and uninstall the voms-clients-java package.", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.3/release-3-3-27/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-27/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-27/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-27/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-27/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-27/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-27/#enterprise-linux-6", 
            "text": "blahp-1.18.32.bosco-1.osg33.el6  condor-8.4.12-2.1.osg33.el6  condor-cron-1.1.3-1.osg33.el6  htcondor-ce-2.2.2-1.osg33.el6  jglobus-2.1.0-9.osg33.el6  osg-configure-1.9.1-1.osg33.el6  osg-test-1.11.1-1.osg33.el6  osg-tested-internal-3.3-18.osg33.el6  osg-version-3.3.27-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-27/#enterprise-linux-7", 
            "text": "blahp-1.18.32.bosco-1.osg33.el7  condor-8.4.12-2.1.osg33.el7  condor-cron-1.1.3-1.osg33.el7  htcondor-ce-2.2.2-1.osg33.el7  jglobus-2.1.0-9.osg33.el7  osg-configure-1.9.1-1.osg33.el7  osg-test-1.11.1-1.osg33.el7  osg-tested-internal-3.3-18.osg33.el7  osg-version-3.3.27-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-27/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-cron condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-slurm htcondor-ce-view igtf-ca-certs jglobus osg-ca-certs osg-configure osg-configure-bosco osg-configure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-test osg-tested-internal osg-tested-internal-gram osg-test-log-viewer osg-version  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-27/#enterprise-linux-6_1", 
            "text": "blahp-1.18.32.bosco-1.osg33.el6\nblahp-debuginfo-1.18.32.bosco-1.osg33.el6\ncondor-8.4.12-2.1.osg33.el6\ncondor-all-8.4.12-2.1.osg33.el6\ncondor-bosco-8.4.12-2.1.osg33.el6\ncondor-classads-8.4.12-2.1.osg33.el6\ncondor-classads-devel-8.4.12-2.1.osg33.el6\ncondor-cream-gahp-8.4.12-2.1.osg33.el6\ncondor-cron-1.1.3-1.osg33.el6\ncondor-debuginfo-8.4.12-2.1.osg33.el6\ncondor-kbdd-8.4.12-2.1.osg33.el6\ncondor-procd-8.4.12-2.1.osg33.el6\ncondor-python-8.4.12-2.1.osg33.el6\ncondor-std-universe-8.4.12-2.1.osg33.el6\ncondor-test-8.4.12-2.1.osg33.el6\ncondor-vm-gahp-8.4.12-2.1.osg33.el6\nhtcondor-ce-2.2.2-1.osg33.el6\nhtcondor-ce-bosco-2.2.2-1.osg33.el6\nhtcondor-ce-client-2.2.2-1.osg33.el6\nhtcondor-ce-collector-2.2.2-1.osg33.el6\nhtcondor-ce-condor-2.2.2-1.osg33.el6\nhtcondor-ce-lsf-2.2.2-1.osg33.el6\nhtcondor-ce-pbs-2.2.2-1.osg33.el6\nhtcondor-ce-sge-2.2.2-1.osg33.el6\nhtcondor-ce-slurm-2.2.2-1.osg33.el6\nhtcondor-ce-view-2.2.2-1.osg33.el6\njglobus-2.1.0-9.osg33.el6\nosg-configure-1.9.1-1.osg33.el6\nosg-configure-bosco-1.9.1-1.osg33.el6\nosg-configure-ce-1.9.1-1.osg33.el6\nosg-configure-cemon-1.9.1-1.osg33.el6\nosg-configure-condor-1.9.1-1.osg33.el6\nosg-configure-gateway-1.9.1-1.osg33.el6\nosg-configure-gip-1.9.1-1.osg33.el6\nosg-configure-gratia-1.9.1-1.osg33.el6\nosg-configure-infoservices-1.9.1-1.osg33.el6\nosg-configure-lsf-1.9.1-1.osg33.el6\nosg-configure-managedfork-1.9.1-1.osg33.el6\nosg-configure-misc-1.9.1-1.osg33.el6\nosg-configure-monalisa-1.9.1-1.osg33.el6\nosg-configure-network-1.9.1-1.osg33.el6\nosg-configure-pbs-1.9.1-1.osg33.el6\nosg-configure-rsv-1.9.1-1.osg33.el6\nosg-configure-sge-1.9.1-1.osg33.el6\nosg-configure-slurm-1.9.1-1.osg33.el6\nosg-configure-squid-1.9.1-1.osg33.el6\nosg-configure-tests-1.9.1-1.osg33.el6\nosg-test-1.11.1-1.osg33.el6\nosg-tested-internal-3.3-18.osg33.el6\nosg-tested-internal-gram-3.3-18.osg33.el6\nosg-test-log-viewer-1.11.1-1.osg33.el6\nosg-version-3.3.27-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-27/#enterprise-linux-7_1", 
            "text": "blahp-1.18.32.bosco-1.osg33.el7\nblahp-debuginfo-1.18.32.bosco-1.osg33.el7\ncondor-8.4.12-2.1.osg33.el7\ncondor-all-8.4.12-2.1.osg33.el7\ncondor-bosco-8.4.12-2.1.osg33.el7\ncondor-classads-8.4.12-2.1.osg33.el7\ncondor-classads-devel-8.4.12-2.1.osg33.el7\ncondor-cream-gahp-8.4.12-2.1.osg33.el7\ncondor-cron-1.1.3-1.osg33.el7\ncondor-debuginfo-8.4.12-2.1.osg33.el7\ncondor-kbdd-8.4.12-2.1.osg33.el7\ncondor-procd-8.4.12-2.1.osg33.el7\ncondor-python-8.4.12-2.1.osg33.el7\ncondor-test-8.4.12-2.1.osg33.el7\ncondor-vm-gahp-8.4.12-2.1.osg33.el7\nhtcondor-ce-2.2.2-1.osg33.el7\nhtcondor-ce-bosco-2.2.2-1.osg33.el7\nhtcondor-ce-client-2.2.2-1.osg33.el7\nhtcondor-ce-collector-2.2.2-1.osg33.el7\nhtcondor-ce-condor-2.2.2-1.osg33.el7\nhtcondor-ce-lsf-2.2.2-1.osg33.el7\nhtcondor-ce-pbs-2.2.2-1.osg33.el7\nhtcondor-ce-sge-2.2.2-1.osg33.el7\nhtcondor-ce-slurm-2.2.2-1.osg33.el7\nhtcondor-ce-view-2.2.2-1.osg33.el7\njglobus-2.1.0-9.osg33.el7\nosg-configure-1.9.1-1.osg33.el7\nosg-configure-bosco-1.9.1-1.osg33.el7\nosg-configure-ce-1.9.1-1.osg33.el7\nosg-configure-cemon-1.9.1-1.osg33.el7\nosg-configure-condor-1.9.1-1.osg33.el7\nosg-configure-gateway-1.9.1-1.osg33.el7\nosg-configure-gip-1.9.1-1.osg33.el7\nosg-configure-gratia-1.9.1-1.osg33.el7\nosg-configure-infoservices-1.9.1-1.osg33.el7\nosg-configure-lsf-1.9.1-1.osg33.el7\nosg-configure-managedfork-1.9.1-1.osg33.el7\nosg-configure-misc-1.9.1-1.osg33.el7\nosg-configure-monalisa-1.9.1-1.osg33.el7\nosg-configure-network-1.9.1-1.osg33.el7\nosg-configure-pbs-1.9.1-1.osg33.el7\nosg-configure-rsv-1.9.1-1.osg33.el7\nosg-configure-sge-1.9.1-1.osg33.el7\nosg-configure-slurm-1.9.1-1.osg33.el7\nosg-configure-squid-1.9.1-1.osg33.el7\nosg-configure-tests-1.9.1-1.osg33.el7\nosg-test-1.11.1-1.osg33.el7\nosg-tested-internal-3.3-18.osg33.el7\nosg-tested-internal-gram-3.3-18.osg33.el7\nosg-test-log-viewer-1.11.1-1.osg33.el7\nosg-version-3.3.27-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-26/", 
            "text": "OSG Software Release 3.3.26\n\n\nRelease Date\n: 2017-07-12\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nBug fix in LCMAPS plugin that could cause the HTCondor-CE schedd to crash\n\n\nosg-configure uses the new GUMS JSON interface\n\n\nThe BLAHP properly requests multi-core resources for Slurm batch systems\n\n\nHTCondor-CE 2.2.1\n\n\nFixed memory requirement requests to non-HTCondor batch systems\n\n\nCorrect CPU allocation for whole node jobs\n\n\n\n\n\n\nGratia probes\n\n\nsupport whole node jobs\n\n\ncan include arbitrary ClassAd attributes in Gratia usage records\n\n\n\n\n\n\nBug fix to CVMFS client to able to mount when large groups exist\n\n\nGridFTP server now uses correct configuration with a dsi plugin\n\n\ngridftp-dsi-posix replaces the xrootd-dsi plugin\n\n\nany local changes made to \n/etc/sysconfig/xrootd-dsi\n should be transferred over to \n/etc/sysconfig/gridftp-dsi-posix\n\n\n\n\n\n\nEnhanced gridftp-dsi-posix\n\n\nAdded MD5 checksum\n\n\nAdded GRIDFTP_APPEND_XROOTD_CGI hook to support XRootD space tokens\n\n\n\n\n\n\nBug fix for HDFS NameNode LeaseManager to prevent endless loop\n\n\nRSV - replace software.grid.iu.edu with repo.grid.iu.edu\n\n\nosg-gridftp now pulls in osg-configure-misc\n\n\ncondor_cron: eliminate email on restart\n\n\nInternal tools\n\n\nosg-build update\n\n\nDrop unused tests from osg-test\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n StashCache is supported on EL7 only. \n xrootd-lcmaps will remain at 1.2.1-1 on EL6.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nKnown Issues\n\n\n\n\nBecause the Gratia system has been turned off, RSV emits the following warning: \nWARNING: Gratia server gratia-osg-prod.opensciencegrid.org:80 not responding to obtain last contact details.\n. This warning can safely be ignored.\n\n\nUsing the LCMAPS VOMS will result in a failing \"supported VO\" RSV test (\nSOFTWARE-2763\n). This can be ignored and a fix is targeted for the August release.\n\n\nUpdates to VOMS admin server require the updated emi-trustmanager-tomcat and re-running the configure script:\\ \nroot@host # /var/lib/trustmanager-tomcat/configure.sh\n\n\nVOMS admin server shows an error when modifying/adding/signing AUPs, but all the actions still work.\n\n\nAfter updating OSG-CE to version 3.3-12, please disable and remove OSG Info Services via the following procedure:\n\n\n\n\nroot@host #\n service osg-info-services stop\n\nroot@host #\n yum erase gip osg-info-services\n\n\n\n\n\n\n\nThe Koji client config has changed in the new version of Koji: `pkgurl\nhttp://koji.chtc.wisc.edu/packages` has been replaced by `topurl=http://koji.chtc.wisc.edu` and the Koji client will give a harmless but annoying warning when it finds `pkgurl`. To get rid of the warning, update to osg-build \n 1.8.0, rerun `osg-koji setup`, and say 'yes' when asked to replace the Koji configuration file; or, you may make the above change manually.\n\n\nA previous version (\n1.17.0-2.5\n) of the Gratia probes required changes in the HTCondor configuration to operate properly. Now, these changes need to be reverted to use verison \n1.17.0-2.6\n and later of the probes. The lines below are likely to be found in \ncondor_config.local\n or a new file in \n/etc/condor/config.d\n directory. Delete these lines and run \ncondor_reconfig\n.\n\n\n\n\n# GlideinWMS Gratia commands\n\n\nPER_JOB_HISTORY_DIR\n \n=\n /var/lib/gratia/data\n\nJOBGLIDEIN_ResourceName\n=\n$$\n([IfThenElse(IsUndefined(TARGET.GLIDEIN_ResourceName), IfThenElse(IsUndefined(TARGET.GLIDEIN_Site), IfThenElse(IsUndefined(TARGET.FileSystemDomain), \\\nLocal Job\\\n, TARGET.FileSystemDomain), TARGET.GLIDEIN_Site), TARGET.GLIDEIN_ResourceName)])\n\n\nSUBMIT_EXPRS\n \n=\n \n$(\nSUBMIT_EXPRS\n)\n JOBGLIDEIN_ResourceName   \n\n\n\n\n\n\n\nOn EL7, your version of voms-proxy-init may come from the voms-clients-java package; this version will continue to make legacy proxies by default. If you want the new behavior, install the voms-clients-cpp package and uninstall the voms-clients-java package.\n\n\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.30.bosco-1.osg33.el6\n\n\ncondor-cron-1.1.2-1.osg33.el6\n\n\ncvmfs-2.3.5-1.1.osg33.el6\n\n\nglobus-gridftp-server-11.8-1.2.osg33.el6\n\n\ngratia-probe-1.18.1-1.osg33.el6\n\n\ngridftp-dsi-posix-1.4-2.osg33.el6\n\n\nhadoop-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\n\n\nhtcondor-ce-2.2.1-1.osg33.el6\n\n\nlcmaps-plugins-verify-proxy-1.5.9-1.2.osg33.el6\n\n\nosg-build-1.10.1-1.osg33.el6\n\n\nosg-configure-1.9.0-1.osg33.el6\n\n\nosg-gridftp-3.3-5.osg33.el6\n\n\nosg-test-1.11.0-1.osg33.el6\n\n\nosg-version-3.3.26-1.osg33.el6\n\n\nrsv-3.14.2-1.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.30.bosco-1.osg33.el7\n\n\ncondor-cron-1.1.2-1.osg33.el7\n\n\ncvmfs-2.3.5-1.1.osg33.el7\n\n\nglobus-gridftp-server-11.8-1.2.osg33.el7\n\n\ngratia-probe-1.18.1-1.osg33.el7\n\n\ngridftp-dsi-posix-1.4-2.osg33.el7\n\n\nhadoop-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\n\n\nhtcondor-ce-2.2.1-1.osg33.el7\n\n\nlcmaps-plugins-verify-proxy-1.5.9-1.2.osg33.el7\n\n\nosg-build-1.10.1-1.osg33.el7\n\n\nosg-configure-1.9.0-1.osg33.el7\n\n\nosg-gridftp-3.3-5.osg33.el7\n\n\nosg-test-1.11.0-1.osg33.el7\n\n\nosg-version-3.3.26-1.osg33.el7\n\n\nrsv-3.14.2-1.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp blahp-debuginfo condor-cron cvmfs cvmfs-devel cvmfs-server cvmfs-unittests globus-gridftp-server globus-gridftp-server-debuginfo globus-gridftp-server-devel globus-gridftp-server-progs gratia-probe-bdii-status gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glexec gratia-probe-glideinwms gratia-probe-gram gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer gridftp-dsi-posix gridftp-dsi-posix-debuginfo hadoop hadoop-0.20-conf-pseudo hadoop-0.20-mapreduce hadoop-client hadoop-conf-pseudo hadoop-debuginfo hadoop-doc hadoop-hdfs hadoop-hdfs-datanode hadoop-hdfs-fuse hadoop-hdfs-fuse-selinux hadoop-hdfs-journalnode hadoop-hdfs-namenode hadoop-hdfs-secondarynamenode hadoop-hdfs-zkfc hadoop-httpfs hadoop-libhdfs hadoop-mapreduce hadoop-yarn htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-slurm htcondor-ce-view igtf-ca-certs lcmaps-plugins-verify-proxy lcmaps-plugins-verify-proxy-debuginfo osg-build osg-build-base osg-build-koji osg-build-mock osg-build-tests osg-ca-certs osg-configure osg-configure-bosco osg-configure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-gridftp osg-test osg-test-log-viewer osg-version rsv rsv-consumers rsv-core rsv-metrics\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp-1.18.30.bosco-1.osg33.el6\nblahp-debuginfo-1.18.30.bosco-1.osg33.el6\ncondor-cron-1.1.2-1.osg33.el6\ncvmfs-2.3.5-1.1.osg33.el6\ncvmfs-devel-2.3.5-1.1.osg33.el6\ncvmfs-server-2.3.5-1.1.osg33.el6\ncvmfs-unittests-2.3.5-1.1.osg33.el6\nglobus-gridftp-server-11.8-1.2.osg33.el6\nglobus-gridftp-server-debuginfo-11.8-1.2.osg33.el6\nglobus-gridftp-server-devel-11.8-1.2.osg33.el6\nglobus-gridftp-server-progs-11.8-1.2.osg33.el6\ngratia-probe-1.18.1-1.osg33.el6\ngratia-probe-bdii-status-1.18.1-1.osg33.el6\ngratia-probe-common-1.18.1-1.osg33.el6\ngratia-probe-condor-1.18.1-1.osg33.el6\ngratia-probe-condor-events-1.18.1-1.osg33.el6\ngratia-probe-dcache-storage-1.18.1-1.osg33.el6\ngratia-probe-dcache-storagegroup-1.18.1-1.osg33.el6\ngratia-probe-dcache-transfer-1.18.1-1.osg33.el6\ngratia-probe-debuginfo-1.18.1-1.osg33.el6\ngratia-probe-enstore-storage-1.18.1-1.osg33.el6\ngratia-probe-enstore-tapedrive-1.18.1-1.osg33.el6\ngratia-probe-enstore-transfer-1.18.1-1.osg33.el6\ngratia-probe-glexec-1.18.1-1.osg33.el6\ngratia-probe-glideinwms-1.18.1-1.osg33.el6\ngratia-probe-gram-1.18.1-1.osg33.el6\ngratia-probe-gridftp-transfer-1.18.1-1.osg33.el6\ngratia-probe-hadoop-storage-1.18.1-1.osg33.el6\ngratia-probe-htcondor-ce-1.18.1-1.osg33.el6\ngratia-probe-lsf-1.18.1-1.osg33.el6\ngratia-probe-metric-1.18.1-1.osg33.el6\ngratia-probe-onevm-1.18.1-1.osg33.el6\ngratia-probe-pbs-lsf-1.18.1-1.osg33.el6\ngratia-probe-services-1.18.1-1.osg33.el6\ngratia-probe-sge-1.18.1-1.osg33.el6\ngratia-probe-slurm-1.18.1-1.osg33.el6\ngratia-probe-xrootd-storage-1.18.1-1.osg33.el6\ngratia-probe-xrootd-transfer-1.18.1-1.osg33.el6\ngridftp-dsi-posix-1.4-2.osg33.el6\ngridftp-dsi-posix-debuginfo-1.4-2.osg33.el6\nhadoop-0.20-conf-pseudo-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-0.20-mapreduce-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-client-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-conf-pseudo-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-debuginfo-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-doc-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-hdfs-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-hdfs-datanode-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-hdfs-fuse-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-hdfs-fuse-selinux-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-hdfs-journalnode-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-hdfs-namenode-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-hdfs-secondarynamenode-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-hdfs-zkfc-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-httpfs-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-libhdfs-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-mapreduce-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-yarn-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhtcondor-ce-2.2.1-1.osg33.el6\nhtcondor-ce-bosco-2.2.1-1.osg33.el6\nhtcondor-ce-client-2.2.1-1.osg33.el6\nhtcondor-ce-collector-2.2.1-1.osg33.el6\nhtcondor-ce-condor-2.2.1-1.osg33.el6\nhtcondor-ce-lsf-2.2.1-1.osg33.el6\nhtcondor-ce-pbs-2.2.1-1.osg33.el6\nhtcondor-ce-sge-2.2.1-1.osg33.el6\nhtcondor-ce-slurm-2.2.1-1.osg33.el6\nhtcondor-ce-view-2.2.1-1.osg33.el6\nlcmaps-plugins-verify-proxy-1.5.9-1.2.osg33.el6\nlcmaps-plugins-verify-proxy-debuginfo-1.5.9-1.2.osg33.el6\nosg-build-1.10.1-1.osg33.el6\nosg-build-base-1.10.1-1.osg33.el6\nosg-build-koji-1.10.1-1.osg33.el6\nosg-build-mock-1.10.1-1.osg33.el6\nosg-build-tests-1.10.1-1.osg33.el6\nosg-configure-1.9.0-1.osg33.el6\nosg-configure-bosco-1.9.0-1.osg33.el6\nosg-configure-ce-1.9.0-1.osg33.el6\nosg-configure-cemon-1.9.0-1.osg33.el6\nosg-configure-condor-1.9.0-1.osg33.el6\nosg-configure-gateway-1.9.0-1.osg33.el6\nosg-configure-gip-1.9.0-1.osg33.el6\nosg-configure-gratia-1.9.0-1.osg33.el6\nosg-configure-infoservices-1.9.0-1.osg33.el6\nosg-configure-lsf-1.9.0-1.osg33.el6\nosg-configure-managedfork-1.9.0-1.osg33.el6\nosg-configure-misc-1.9.0-1.osg33.el6\nosg-configure-monalisa-1.9.0-1.osg33.el6\nosg-configure-network-1.9.0-1.osg33.el6\nosg-configure-pbs-1.9.0-1.osg33.el6\nosg-configure-rsv-1.9.0-1.osg33.el6\nosg-configure-sge-1.9.0-1.osg33.el6\nosg-configure-slurm-1.9.0-1.osg33.el6\nosg-configure-squid-1.9.0-1.osg33.el6\nosg-configure-tests-1.9.0-1.osg33.el6\nosg-gridftp-3.3-5.osg33.el6\nosg-test-1.11.0-1.osg33.el6\nosg-test-log-viewer-1.11.0-1.osg33.el6\nosg-version-3.3.26-1.osg33.el6\nrsv-3.14.2-1.osg33.el6\nrsv-consumers-3.14.2-1.osg33.el6\nrsv-core-3.14.2-1.osg33.el6\nrsv-metrics-3.14.2-1.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp-1.18.30.bosco-1.osg33.el7\nblahp-debuginfo-1.18.30.bosco-1.osg33.el7\ncondor-cron-1.1.2-1.osg33.el7\ncvmfs-2.3.5-1.1.osg33.el7\ncvmfs-devel-2.3.5-1.1.osg33.el7\ncvmfs-server-2.3.5-1.1.osg33.el7\ncvmfs-unittests-2.3.5-1.1.osg33.el7\nglobus-gridftp-server-11.8-1.2.osg33.el7\nglobus-gridftp-server-debuginfo-11.8-1.2.osg33.el7\nglobus-gridftp-server-devel-11.8-1.2.osg33.el7\nglobus-gridftp-server-progs-11.8-1.2.osg33.el7\ngratia-probe-1.18.1-1.osg33.el7\ngratia-probe-bdii-status-1.18.1-1.osg33.el7\ngratia-probe-common-1.18.1-1.osg33.el7\ngratia-probe-condor-1.18.1-1.osg33.el7\ngratia-probe-condor-events-1.18.1-1.osg33.el7\ngratia-probe-dcache-storage-1.18.1-1.osg33.el7\ngratia-probe-dcache-storagegroup-1.18.1-1.osg33.el7\ngratia-probe-dcache-transfer-1.18.1-1.osg33.el7\ngratia-probe-debuginfo-1.18.1-1.osg33.el7\ngratia-probe-enstore-storage-1.18.1-1.osg33.el7\ngratia-probe-enstore-tapedrive-1.18.1-1.osg33.el7\ngratia-probe-enstore-transfer-1.18.1-1.osg33.el7\ngratia-probe-glexec-1.18.1-1.osg33.el7\ngratia-probe-glideinwms-1.18.1-1.osg33.el7\ngratia-probe-gram-1.18.1-1.osg33.el7\ngratia-probe-gridftp-transfer-1.18.1-1.osg33.el7\ngratia-probe-hadoop-storage-1.18.1-1.osg33.el7\ngratia-probe-htcondor-ce-1.18.1-1.osg33.el7\ngratia-probe-lsf-1.18.1-1.osg33.el7\ngratia-probe-metric-1.18.1-1.osg33.el7\ngratia-probe-onevm-1.18.1-1.osg33.el7\ngratia-probe-pbs-lsf-1.18.1-1.osg33.el7\ngratia-probe-services-1.18.1-1.osg33.el7\ngratia-probe-sge-1.18.1-1.osg33.el7\ngratia-probe-slurm-1.18.1-1.osg33.el7\ngratia-probe-xrootd-storage-1.18.1-1.osg33.el7\ngratia-probe-xrootd-transfer-1.18.1-1.osg33.el7\ngridftp-dsi-posix-1.4-2.osg33.el7\ngridftp-dsi-posix-debuginfo-1.4-2.osg33.el7\nhadoop-0.20-conf-pseudo-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-0.20-mapreduce-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-client-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-conf-pseudo-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-debuginfo-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-doc-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-hdfs-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-hdfs-datanode-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-hdfs-fuse-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-hdfs-fuse-selinux-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-hdfs-journalnode-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-hdfs-namenode-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-hdfs-secondarynamenode-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-hdfs-zkfc-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-httpfs-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-libhdfs-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-mapreduce-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-yarn-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhtcondor-ce-2.2.1-1.osg33.el7\nhtcondor-ce-bosco-2.2.1-1.osg33.el7\nhtcondor-ce-client-2.2.1-1.osg33.el7\nhtcondor-ce-collector-2.2.1-1.osg33.el7\nhtcondor-ce-condor-2.2.1-1.osg33.el7\nhtcondor-ce-lsf-2.2.1-1.osg33.el7\nhtcondor-ce-pbs-2.2.1-1.osg33.el7\nhtcondor-ce-sge-2.2.1-1.osg33.el7\nhtcondor-ce-slurm-2.2.1-1.osg33.el7\nhtcondor-ce-view-2.2.1-1.osg33.el7\nlcmaps-plugins-verify-proxy-1.5.9-1.2.osg33.el7\nlcmaps-plugins-verify-proxy-debuginfo-1.5.9-1.2.osg33.el7\nosg-build-1.10.1-1.osg33.el7\nosg-build-base-1.10.1-1.osg33.el7\nosg-build-koji-1.10.1-1.osg33.el7\nosg-build-mock-1.10.1-1.osg33.el7\nosg-build-tests-1.10.1-1.osg33.el7\nosg-configure-1.9.0-1.osg33.el7\nosg-configure-bosco-1.9.0-1.osg33.el7\nosg-configure-ce-1.9.0-1.osg33.el7\nosg-configure-cemon-1.9.0-1.osg33.el7\nosg-configure-condor-1.9.0-1.osg33.el7\nosg-configure-gateway-1.9.0-1.osg33.el7\nosg-configure-gip-1.9.0-1.osg33.el7\nosg-configure-gratia-1.9.0-1.osg33.el7\nosg-configure-infoservices-1.9.0-1.osg33.el7\nosg-configure-lsf-1.9.0-1.osg33.el7\nosg-configure-managedfork-1.9.0-1.osg33.el7\nosg-configure-misc-1.9.0-1.osg33.el7\nosg-configure-monalisa-1.9.0-1.osg33.el7\nosg-configure-network-1.9.0-1.osg33.el7\nosg-configure-pbs-1.9.0-1.osg33.el7\nosg-configure-rsv-1.9.0-1.osg33.el7\nosg-configure-sge-1.9.0-1.osg33.el7\nosg-configure-slurm-1.9.0-1.osg33.el7\nosg-configure-squid-1.9.0-1.osg33.el7\nosg-configure-tests-1.9.0-1.osg33.el7\nosg-gridftp-3.3-5.osg33.el7\nosg-test-1.11.0-1.osg33.el7\nosg-test-log-viewer-1.11.0-1.osg33.el7\nosg-version-3.3.26-1.osg33.el7\nrsv-3.14.2-1.osg33.el7\nrsv-consumers-3.14.2-1.osg33.el7\nrsv-core-3.14.2-1.osg33.el7\nrsv-metrics-3.14.2-1.osg33.el7", 
            "title": "OSG Release 3.3.26"
        }, 
        {
            "location": "/release/3.3/release-3-3-26/#osg-software-release-3326", 
            "text": "Release Date : 2017-07-12", 
            "title": "OSG Software Release 3.3.26"
        }, 
        {
            "location": "/release/3.3/release-3-3-26/#summary-of-changes", 
            "text": "This release contains:   Bug fix in LCMAPS plugin that could cause the HTCondor-CE schedd to crash  osg-configure uses the new GUMS JSON interface  The BLAHP properly requests multi-core resources for Slurm batch systems  HTCondor-CE 2.2.1  Fixed memory requirement requests to non-HTCondor batch systems  Correct CPU allocation for whole node jobs    Gratia probes  support whole node jobs  can include arbitrary ClassAd attributes in Gratia usage records    Bug fix to CVMFS client to able to mount when large groups exist  GridFTP server now uses correct configuration with a dsi plugin  gridftp-dsi-posix replaces the xrootd-dsi plugin  any local changes made to  /etc/sysconfig/xrootd-dsi  should be transferred over to  /etc/sysconfig/gridftp-dsi-posix    Enhanced gridftp-dsi-posix  Added MD5 checksum  Added GRIDFTP_APPEND_XROOTD_CGI hook to support XRootD space tokens    Bug fix for HDFS NameNode LeaseManager to prevent endless loop  RSV - replace software.grid.iu.edu with repo.grid.iu.edu  osg-gridftp now pulls in osg-configure-misc  condor_cron: eliminate email on restart  Internal tools  osg-build update  Drop unused tests from osg-test     These  JIRA tickets  were addressed in this release.   StashCache is supported on EL7 only.   xrootd-lcmaps will remain at 1.2.1-1 on EL6.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-26/#known-issues", 
            "text": "Because the Gratia system has been turned off, RSV emits the following warning:  WARNING: Gratia server gratia-osg-prod.opensciencegrid.org:80 not responding to obtain last contact details. . This warning can safely be ignored.  Using the LCMAPS VOMS will result in a failing \"supported VO\" RSV test ( SOFTWARE-2763 ). This can be ignored and a fix is targeted for the August release.  Updates to VOMS admin server require the updated emi-trustmanager-tomcat and re-running the configure script:\\  root@host # /var/lib/trustmanager-tomcat/configure.sh  VOMS admin server shows an error when modifying/adding/signing AUPs, but all the actions still work.  After updating OSG-CE to version 3.3-12, please disable and remove OSG Info Services via the following procedure:   root@host #  service osg-info-services stop root@host #  yum erase gip osg-info-services   The Koji client config has changed in the new version of Koji: `pkgurl http://koji.chtc.wisc.edu/packages` has been replaced by `topurl=http://koji.chtc.wisc.edu` and the Koji client will give a harmless but annoying warning when it finds `pkgurl`. To get rid of the warning, update to osg-build   1.8.0, rerun `osg-koji setup`, and say 'yes' when asked to replace the Koji configuration file; or, you may make the above change manually.  A previous version ( 1.17.0-2.5 ) of the Gratia probes required changes in the HTCondor configuration to operate properly. Now, these changes need to be reverted to use verison  1.17.0-2.6  and later of the probes. The lines below are likely to be found in  condor_config.local  or a new file in  /etc/condor/config.d  directory. Delete these lines and run  condor_reconfig .   # GlideinWMS Gratia commands  PER_JOB_HISTORY_DIR   =  /var/lib/gratia/data JOBGLIDEIN_ResourceName = $$ ([IfThenElse(IsUndefined(TARGET.GLIDEIN_ResourceName), IfThenElse(IsUndefined(TARGET.GLIDEIN_Site), IfThenElse(IsUndefined(TARGET.FileSystemDomain), \\ Local Job\\ , TARGET.FileSystemDomain), TARGET.GLIDEIN_Site), TARGET.GLIDEIN_ResourceName)])  SUBMIT_EXPRS   =   $( SUBMIT_EXPRS )  JOBGLIDEIN_ResourceName      On EL7, your version of voms-proxy-init may come from the voms-clients-java package; this version will continue to make legacy proxies by default. If you want the new behavior, install the voms-clients-cpp package and uninstall the voms-clients-java package.", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.3/release-3-3-26/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-26/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-26/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-26/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-26/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-26/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-26/#enterprise-linux-6", 
            "text": "blahp-1.18.30.bosco-1.osg33.el6  condor-cron-1.1.2-1.osg33.el6  cvmfs-2.3.5-1.1.osg33.el6  globus-gridftp-server-11.8-1.2.osg33.el6  gratia-probe-1.18.1-1.osg33.el6  gridftp-dsi-posix-1.4-2.osg33.el6  hadoop-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6  htcondor-ce-2.2.1-1.osg33.el6  lcmaps-plugins-verify-proxy-1.5.9-1.2.osg33.el6  osg-build-1.10.1-1.osg33.el6  osg-configure-1.9.0-1.osg33.el6  osg-gridftp-3.3-5.osg33.el6  osg-test-1.11.0-1.osg33.el6  osg-version-3.3.26-1.osg33.el6  rsv-3.14.2-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-26/#enterprise-linux-7", 
            "text": "blahp-1.18.30.bosco-1.osg33.el7  condor-cron-1.1.2-1.osg33.el7  cvmfs-2.3.5-1.1.osg33.el7  globus-gridftp-server-11.8-1.2.osg33.el7  gratia-probe-1.18.1-1.osg33.el7  gridftp-dsi-posix-1.4-2.osg33.el7  hadoop-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7  htcondor-ce-2.2.1-1.osg33.el7  lcmaps-plugins-verify-proxy-1.5.9-1.2.osg33.el7  osg-build-1.10.1-1.osg33.el7  osg-configure-1.9.0-1.osg33.el7  osg-gridftp-3.3-5.osg33.el7  osg-test-1.11.0-1.osg33.el7  osg-version-3.3.26-1.osg33.el7  rsv-3.14.2-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-26/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp blahp-debuginfo condor-cron cvmfs cvmfs-devel cvmfs-server cvmfs-unittests globus-gridftp-server globus-gridftp-server-debuginfo globus-gridftp-server-devel globus-gridftp-server-progs gratia-probe-bdii-status gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glexec gratia-probe-glideinwms gratia-probe-gram gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer gridftp-dsi-posix gridftp-dsi-posix-debuginfo hadoop hadoop-0.20-conf-pseudo hadoop-0.20-mapreduce hadoop-client hadoop-conf-pseudo hadoop-debuginfo hadoop-doc hadoop-hdfs hadoop-hdfs-datanode hadoop-hdfs-fuse hadoop-hdfs-fuse-selinux hadoop-hdfs-journalnode hadoop-hdfs-namenode hadoop-hdfs-secondarynamenode hadoop-hdfs-zkfc hadoop-httpfs hadoop-libhdfs hadoop-mapreduce hadoop-yarn htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-slurm htcondor-ce-view igtf-ca-certs lcmaps-plugins-verify-proxy lcmaps-plugins-verify-proxy-debuginfo osg-build osg-build-base osg-build-koji osg-build-mock osg-build-tests osg-ca-certs osg-configure osg-configure-bosco osg-configure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-gridftp osg-test osg-test-log-viewer osg-version rsv rsv-consumers rsv-core rsv-metrics  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-26/#enterprise-linux-6_1", 
            "text": "blahp-1.18.30.bosco-1.osg33.el6\nblahp-debuginfo-1.18.30.bosco-1.osg33.el6\ncondor-cron-1.1.2-1.osg33.el6\ncvmfs-2.3.5-1.1.osg33.el6\ncvmfs-devel-2.3.5-1.1.osg33.el6\ncvmfs-server-2.3.5-1.1.osg33.el6\ncvmfs-unittests-2.3.5-1.1.osg33.el6\nglobus-gridftp-server-11.8-1.2.osg33.el6\nglobus-gridftp-server-debuginfo-11.8-1.2.osg33.el6\nglobus-gridftp-server-devel-11.8-1.2.osg33.el6\nglobus-gridftp-server-progs-11.8-1.2.osg33.el6\ngratia-probe-1.18.1-1.osg33.el6\ngratia-probe-bdii-status-1.18.1-1.osg33.el6\ngratia-probe-common-1.18.1-1.osg33.el6\ngratia-probe-condor-1.18.1-1.osg33.el6\ngratia-probe-condor-events-1.18.1-1.osg33.el6\ngratia-probe-dcache-storage-1.18.1-1.osg33.el6\ngratia-probe-dcache-storagegroup-1.18.1-1.osg33.el6\ngratia-probe-dcache-transfer-1.18.1-1.osg33.el6\ngratia-probe-debuginfo-1.18.1-1.osg33.el6\ngratia-probe-enstore-storage-1.18.1-1.osg33.el6\ngratia-probe-enstore-tapedrive-1.18.1-1.osg33.el6\ngratia-probe-enstore-transfer-1.18.1-1.osg33.el6\ngratia-probe-glexec-1.18.1-1.osg33.el6\ngratia-probe-glideinwms-1.18.1-1.osg33.el6\ngratia-probe-gram-1.18.1-1.osg33.el6\ngratia-probe-gridftp-transfer-1.18.1-1.osg33.el6\ngratia-probe-hadoop-storage-1.18.1-1.osg33.el6\ngratia-probe-htcondor-ce-1.18.1-1.osg33.el6\ngratia-probe-lsf-1.18.1-1.osg33.el6\ngratia-probe-metric-1.18.1-1.osg33.el6\ngratia-probe-onevm-1.18.1-1.osg33.el6\ngratia-probe-pbs-lsf-1.18.1-1.osg33.el6\ngratia-probe-services-1.18.1-1.osg33.el6\ngratia-probe-sge-1.18.1-1.osg33.el6\ngratia-probe-slurm-1.18.1-1.osg33.el6\ngratia-probe-xrootd-storage-1.18.1-1.osg33.el6\ngratia-probe-xrootd-transfer-1.18.1-1.osg33.el6\ngridftp-dsi-posix-1.4-2.osg33.el6\ngridftp-dsi-posix-debuginfo-1.4-2.osg33.el6\nhadoop-0.20-conf-pseudo-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-0.20-mapreduce-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-client-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-conf-pseudo-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-debuginfo-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-doc-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-hdfs-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-hdfs-datanode-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-hdfs-fuse-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-hdfs-fuse-selinux-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-hdfs-journalnode-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-hdfs-namenode-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-hdfs-secondarynamenode-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-hdfs-zkfc-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-httpfs-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-libhdfs-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-mapreduce-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhadoop-yarn-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el6\nhtcondor-ce-2.2.1-1.osg33.el6\nhtcondor-ce-bosco-2.2.1-1.osg33.el6\nhtcondor-ce-client-2.2.1-1.osg33.el6\nhtcondor-ce-collector-2.2.1-1.osg33.el6\nhtcondor-ce-condor-2.2.1-1.osg33.el6\nhtcondor-ce-lsf-2.2.1-1.osg33.el6\nhtcondor-ce-pbs-2.2.1-1.osg33.el6\nhtcondor-ce-sge-2.2.1-1.osg33.el6\nhtcondor-ce-slurm-2.2.1-1.osg33.el6\nhtcondor-ce-view-2.2.1-1.osg33.el6\nlcmaps-plugins-verify-proxy-1.5.9-1.2.osg33.el6\nlcmaps-plugins-verify-proxy-debuginfo-1.5.9-1.2.osg33.el6\nosg-build-1.10.1-1.osg33.el6\nosg-build-base-1.10.1-1.osg33.el6\nosg-build-koji-1.10.1-1.osg33.el6\nosg-build-mock-1.10.1-1.osg33.el6\nosg-build-tests-1.10.1-1.osg33.el6\nosg-configure-1.9.0-1.osg33.el6\nosg-configure-bosco-1.9.0-1.osg33.el6\nosg-configure-ce-1.9.0-1.osg33.el6\nosg-configure-cemon-1.9.0-1.osg33.el6\nosg-configure-condor-1.9.0-1.osg33.el6\nosg-configure-gateway-1.9.0-1.osg33.el6\nosg-configure-gip-1.9.0-1.osg33.el6\nosg-configure-gratia-1.9.0-1.osg33.el6\nosg-configure-infoservices-1.9.0-1.osg33.el6\nosg-configure-lsf-1.9.0-1.osg33.el6\nosg-configure-managedfork-1.9.0-1.osg33.el6\nosg-configure-misc-1.9.0-1.osg33.el6\nosg-configure-monalisa-1.9.0-1.osg33.el6\nosg-configure-network-1.9.0-1.osg33.el6\nosg-configure-pbs-1.9.0-1.osg33.el6\nosg-configure-rsv-1.9.0-1.osg33.el6\nosg-configure-sge-1.9.0-1.osg33.el6\nosg-configure-slurm-1.9.0-1.osg33.el6\nosg-configure-squid-1.9.0-1.osg33.el6\nosg-configure-tests-1.9.0-1.osg33.el6\nosg-gridftp-3.3-5.osg33.el6\nosg-test-1.11.0-1.osg33.el6\nosg-test-log-viewer-1.11.0-1.osg33.el6\nosg-version-3.3.26-1.osg33.el6\nrsv-3.14.2-1.osg33.el6\nrsv-consumers-3.14.2-1.osg33.el6\nrsv-core-3.14.2-1.osg33.el6\nrsv-metrics-3.14.2-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-26/#enterprise-linux-7_1", 
            "text": "blahp-1.18.30.bosco-1.osg33.el7\nblahp-debuginfo-1.18.30.bosco-1.osg33.el7\ncondor-cron-1.1.2-1.osg33.el7\ncvmfs-2.3.5-1.1.osg33.el7\ncvmfs-devel-2.3.5-1.1.osg33.el7\ncvmfs-server-2.3.5-1.1.osg33.el7\ncvmfs-unittests-2.3.5-1.1.osg33.el7\nglobus-gridftp-server-11.8-1.2.osg33.el7\nglobus-gridftp-server-debuginfo-11.8-1.2.osg33.el7\nglobus-gridftp-server-devel-11.8-1.2.osg33.el7\nglobus-gridftp-server-progs-11.8-1.2.osg33.el7\ngratia-probe-1.18.1-1.osg33.el7\ngratia-probe-bdii-status-1.18.1-1.osg33.el7\ngratia-probe-common-1.18.1-1.osg33.el7\ngratia-probe-condor-1.18.1-1.osg33.el7\ngratia-probe-condor-events-1.18.1-1.osg33.el7\ngratia-probe-dcache-storage-1.18.1-1.osg33.el7\ngratia-probe-dcache-storagegroup-1.18.1-1.osg33.el7\ngratia-probe-dcache-transfer-1.18.1-1.osg33.el7\ngratia-probe-debuginfo-1.18.1-1.osg33.el7\ngratia-probe-enstore-storage-1.18.1-1.osg33.el7\ngratia-probe-enstore-tapedrive-1.18.1-1.osg33.el7\ngratia-probe-enstore-transfer-1.18.1-1.osg33.el7\ngratia-probe-glexec-1.18.1-1.osg33.el7\ngratia-probe-glideinwms-1.18.1-1.osg33.el7\ngratia-probe-gram-1.18.1-1.osg33.el7\ngratia-probe-gridftp-transfer-1.18.1-1.osg33.el7\ngratia-probe-hadoop-storage-1.18.1-1.osg33.el7\ngratia-probe-htcondor-ce-1.18.1-1.osg33.el7\ngratia-probe-lsf-1.18.1-1.osg33.el7\ngratia-probe-metric-1.18.1-1.osg33.el7\ngratia-probe-onevm-1.18.1-1.osg33.el7\ngratia-probe-pbs-lsf-1.18.1-1.osg33.el7\ngratia-probe-services-1.18.1-1.osg33.el7\ngratia-probe-sge-1.18.1-1.osg33.el7\ngratia-probe-slurm-1.18.1-1.osg33.el7\ngratia-probe-xrootd-storage-1.18.1-1.osg33.el7\ngratia-probe-xrootd-transfer-1.18.1-1.osg33.el7\ngridftp-dsi-posix-1.4-2.osg33.el7\ngridftp-dsi-posix-debuginfo-1.4-2.osg33.el7\nhadoop-0.20-conf-pseudo-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-0.20-mapreduce-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-client-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-conf-pseudo-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-debuginfo-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-doc-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-hdfs-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-hdfs-datanode-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-hdfs-fuse-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-hdfs-fuse-selinux-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-hdfs-journalnode-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-hdfs-namenode-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-hdfs-secondarynamenode-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-hdfs-zkfc-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-httpfs-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-libhdfs-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-mapreduce-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhadoop-yarn-2.0.0+1612-1.cdh4.7.1.p0.12.7.osg33.el7\nhtcondor-ce-2.2.1-1.osg33.el7\nhtcondor-ce-bosco-2.2.1-1.osg33.el7\nhtcondor-ce-client-2.2.1-1.osg33.el7\nhtcondor-ce-collector-2.2.1-1.osg33.el7\nhtcondor-ce-condor-2.2.1-1.osg33.el7\nhtcondor-ce-lsf-2.2.1-1.osg33.el7\nhtcondor-ce-pbs-2.2.1-1.osg33.el7\nhtcondor-ce-sge-2.2.1-1.osg33.el7\nhtcondor-ce-slurm-2.2.1-1.osg33.el7\nhtcondor-ce-view-2.2.1-1.osg33.el7\nlcmaps-plugins-verify-proxy-1.5.9-1.2.osg33.el7\nlcmaps-plugins-verify-proxy-debuginfo-1.5.9-1.2.osg33.el7\nosg-build-1.10.1-1.osg33.el7\nosg-build-base-1.10.1-1.osg33.el7\nosg-build-koji-1.10.1-1.osg33.el7\nosg-build-mock-1.10.1-1.osg33.el7\nosg-build-tests-1.10.1-1.osg33.el7\nosg-configure-1.9.0-1.osg33.el7\nosg-configure-bosco-1.9.0-1.osg33.el7\nosg-configure-ce-1.9.0-1.osg33.el7\nosg-configure-cemon-1.9.0-1.osg33.el7\nosg-configure-condor-1.9.0-1.osg33.el7\nosg-configure-gateway-1.9.0-1.osg33.el7\nosg-configure-gip-1.9.0-1.osg33.el7\nosg-configure-gratia-1.9.0-1.osg33.el7\nosg-configure-infoservices-1.9.0-1.osg33.el7\nosg-configure-lsf-1.9.0-1.osg33.el7\nosg-configure-managedfork-1.9.0-1.osg33.el7\nosg-configure-misc-1.9.0-1.osg33.el7\nosg-configure-monalisa-1.9.0-1.osg33.el7\nosg-configure-network-1.9.0-1.osg33.el7\nosg-configure-pbs-1.9.0-1.osg33.el7\nosg-configure-rsv-1.9.0-1.osg33.el7\nosg-configure-sge-1.9.0-1.osg33.el7\nosg-configure-slurm-1.9.0-1.osg33.el7\nosg-configure-squid-1.9.0-1.osg33.el7\nosg-configure-tests-1.9.0-1.osg33.el7\nosg-gridftp-3.3-5.osg33.el7\nosg-test-1.11.0-1.osg33.el7\nosg-test-log-viewer-1.11.0-1.osg33.el7\nosg-version-3.3.26-1.osg33.el7\nrsv-3.14.2-1.osg33.el7\nrsv-consumers-3.14.2-1.osg33.el7\nrsv-core-3.14.2-1.osg33.el7\nrsv-metrics-3.14.2-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-25/", 
            "text": "OSG Software Release 3.3.25\n\n\nRelease Date\n: 2017-06-14\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nOSG 3.3.25\n\n\nMake the LCMAPS VOMS plugin consider only the first FQAN to be consistent with GUMS\n\n\nUpdate to \nXRootD 4.6.1\n\n\nUpdate to xrootd-lcmaps 1.3.3 for EL7\n\n\n\n\n\n\nUpdate StashCache meta-packages to require XRootD 4.6.1\n\n\nUpdate to \nGlideinWMS 3.2.19\n\n\nHTCondor-CE: Add WholeNodeWanted ClassAd expression so jobs can request a whole node from the batch system\n\n\nAdd vo-client-lcmaps-voms dependency to osg-gridftp and osg-ce\n\n\nUpdated to voms-admin-server 2.7.0-1.22 (security update)\n\n\nFix osg-update-vos script to clean yum cache in order pick up the latest vo-client RPM\n\n\nosg-configure 1.8.1\n\n\nreject empty \nallowed_vos\n in subclusters\n\n\nget default \nallowed_vos\n from LCMAPS VOMS plugin\n\n\nissue warning (rather than error out) if OSG_APP or OSG_DATA directories are not present\n\n\n\n\n\n\nosg-ca-scripts now refers to repo.grid.iu.edu (rather than the retired software.grid.iu.edu)\n\n\nNo patch to globus-xio, drop the unneeded one\n\n\nosg-build 1.10.0\n\n\ndrop vdt-build alias\n\n\ndrop ~/.osg-build.ini configuration file\n\n\n\n\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n StashCache is supported on EL7 only. \n xrootd-lcmaps will remain at 1.2.1-1 on EL6.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nKnown Issues\n\n\n\n\nUpdates to VOMS admin server require the updated emi-trustmanager-tomcat and re-running the configure script:\\ \nroot@host # /var/lib/trustmanager-tomcat/configure.sh\n\n\nUsing the LCMAPS VOMS will result in a failing \"supported VO\" RSV test (\nSOFTWARE-2763\n). This can be ignored and a fix is targeted for the July release.\n\n\nVOMS admin server shows an error when modifying/adding/signing AUPs, but all the actions still work.\n\n\nAfter updating OSG-CE to version 3.3-12, please disable and remove OSG Info Services via the following procedure:\n\n\n\n\nroot@host #\n service osg-info-services stop\n\nroot@host #\n yum erase gip osg-info-services\n\n\n\n\n\n\n\nThe Koji client config has changed in the new version of Koji: `pkgurl\nhttp://koji.chtc.wisc.edu/packages` has been replaced by `topurl=http://koji.chtc.wisc.edu` and the Koji client will give a harmless but annoying warning when it finds `pkgurl`. To get rid of the warning, update to osg-build \n 1.8.0, rerun `osg-koji setup`, and say 'yes' when asked to replace the Koji configuration file; or, you may make the above change manually.\n\n\nA previous version (\n1.17.0-2.5\n) of the Gratia probes required changes in the HTCondor configuration to operate properly. Now, these changes need to be reverted to use verison \n1.17.0-2.6\n and later of the probes. The lines below are likely to be found in \ncondor_config.local\n or a new file in \n/etc/condor/config.d\n directory. Delete these lines and run \ncondor_reconfig\n.\n\n\n\n\n# GlideinWMS Gratia commands\n\n\nPER_JOB_HISTORY_DIR\n \n=\n /var/lib/gratia/data\n\nJOBGLIDEIN_ResourceName\n=\n$$\n([IfThenElse(IsUndefined(TARGET.GLIDEIN_ResourceName), IfThenElse(IsUndefined(TARGET.GLIDEIN_Site), IfThenElse(IsUndefined(TARGET.FileSystemDomain), \\\nLocal Job\\\n, TARGET.FileSystemDomain), TARGET.GLIDEIN_Site), TARGET.GLIDEIN_ResourceName)])\n\n\nSUBMIT_EXPRS\n \n=\n \n$(\nSUBMIT_EXPRS\n)\n JOBGLIDEIN_ResourceName   \n\n\n\n\n\n\n\nOn EL7, your version of voms-proxy-init may come from the voms-clients-java package; this version will continue to make legacy proxies by default. If you want the new behavior, install the voms-clients-cpp package and uninstall the voms-clients-java package.\n\n\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nemi-trustmanager-tomcat-3.0.0-15.osg33.el6\n\n\nglideinwms-3.2.19-1.osg33.el6\n\n\nglobus-xio-5.12-1.2.osg33.el6\n\n\nhtcondor-ce-2.2.0-1.osg33.el6\n\n\nlcmaps-plugins-voms-1.7.1-1.4.osg33.el6\n\n\nosg-build-1.10.0-1.osg33.el6\n\n\nosg-ca-scripts-1.1.6-1.osg33.el6\n\n\nosg-ce-3.3-13.osg33.el6\n\n\nosg-configure-1.8.1-2.osg33.el6\n\n\nosg-gridftp-3.3-4.osg33.el6\n\n\nosg-update-vos-1.4.0-1.osg33.el6\n\n\nosg-version-3.3.25-1.osg33.el6\n\n\nvoms-admin-server-2.7.0-1.22.osg33.el6\n\n\nxrootd-4.6.1-1.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nemi-trustmanager-tomcat-3.0.0-15.osg33.el7\n\n\nglideinwms-3.2.19-1.osg33.el7\n\n\nglobus-xio-5.12-1.2.osg33.el7\n\n\nhtcondor-ce-2.2.0-1.osg33.el7\n\n\nlcmaps-plugins-voms-1.7.1-1.4.osg33.el7\n\n\nosg-build-1.10.0-1.osg33.el7\n\n\nosg-ca-scripts-1.1.6-1.osg33.el7\n\n\nosg-ce-3.3-13.osg33.el7\n\n\nosg-configure-1.8.1-2.osg33.el7\n\n\nosg-gridftp-3.3-4.osg33.el7\n\n\nosg-update-vos-1.4.0-1.osg33.el7\n\n\nosg-version-3.3.25-1.osg33.el7\n\n\nstashcache-0.7-2.osg33.el7\n\n\nxrootd-4.6.1-1.osg33.el7\n\n\nxrootd-lcmaps-1.3.3-3.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nemi-trustmanager-tomcat glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone globus-xio globus-xio-debuginfo globus-xio-devel globus-xio-doc htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-slurm htcondor-ce-view lcmaps-plugins-voms lcmaps-plugins-voms-debuginfo osg-base-ce osg-base-ce-bosco osg-base-ce-condor osg-base-ce-lsf osg-base-ce-pbs osg-base-ce-sge osg-base-ce-slurm osg-build osg-build-base osg-build-koji osg-build-mock osg-build-tests osg-ca-scripts osg-ce osg-ce-bosco osg-ce-condor osg-ce-lsf osg-ce-pbs osg-ce-sge osg-ce-slurm osg-configure osg-configure-bosco osg-configure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-gridftp osg-htcondor-ce osg-htcondor-ce-bosco osg-htcondor-ce-condor osg-htcondor-ce-lsf osg-htcondor-ce-pbs osg-htcondor-ce-sge osg-htcondor-ce-slurm osg-update-data osg-update-vos osg-version voms-admin-server xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-libs xrootd-private-devel xrootd-python xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nemi-trustmanager-tomcat-3.0.0-15.osg33.el6\nglideinwms-3.2.19-1.osg33.el6\nglideinwms-common-tools-3.2.19-1.osg33.el6\nglideinwms-condor-common-config-3.2.19-1.osg33.el6\nglideinwms-factory-3.2.19-1.osg33.el6\nglideinwms-factory-condor-3.2.19-1.osg33.el6\nglideinwms-glidecondor-tools-3.2.19-1.osg33.el6\nglideinwms-libs-3.2.19-1.osg33.el6\nglideinwms-minimal-condor-3.2.19-1.osg33.el6\nglideinwms-usercollector-3.2.19-1.osg33.el6\nglideinwms-userschedd-3.2.19-1.osg33.el6\nglideinwms-vofrontend-3.2.19-1.osg33.el6\nglideinwms-vofrontend-standalone-3.2.19-1.osg33.el6\nglobus-xio-5.12-1.2.osg33.el6\nglobus-xio-debuginfo-5.12-1.2.osg33.el6\nglobus-xio-devel-5.12-1.2.osg33.el6\nglobus-xio-doc-5.12-1.2.osg33.el6\nhtcondor-ce-2.2.0-1.osg33.el6\nhtcondor-ce-bosco-2.2.0-1.osg33.el6\nhtcondor-ce-client-2.2.0-1.osg33.el6\nhtcondor-ce-collector-2.2.0-1.osg33.el6\nhtcondor-ce-condor-2.2.0-1.osg33.el6\nhtcondor-ce-lsf-2.2.0-1.osg33.el6\nhtcondor-ce-pbs-2.2.0-1.osg33.el6\nhtcondor-ce-sge-2.2.0-1.osg33.el6\nhtcondor-ce-slurm-2.2.0-1.osg33.el6\nhtcondor-ce-view-2.2.0-1.osg33.el6\nlcmaps-plugins-voms-1.7.1-1.4.osg33.el6\nlcmaps-plugins-voms-debuginfo-1.7.1-1.4.osg33.el6\nosg-base-ce-3.3-13.osg33.el6\nosg-base-ce-bosco-3.3-13.osg33.el6\nosg-base-ce-condor-3.3-13.osg33.el6\nosg-base-ce-lsf-3.3-13.osg33.el6\nosg-base-ce-pbs-3.3-13.osg33.el6\nosg-base-ce-sge-3.3-13.osg33.el6\nosg-base-ce-slurm-3.3-13.osg33.el6\nosg-build-1.10.0-1.osg33.el6\nosg-build-base-1.10.0-1.osg33.el6\nosg-build-koji-1.10.0-1.osg33.el6\nosg-build-mock-1.10.0-1.osg33.el6\nosg-build-tests-1.10.0-1.osg33.el6\nosg-ca-scripts-1.1.6-1.osg33.el6\nosg-ce-3.3-13.osg33.el6\nosg-ce-bosco-3.3-13.osg33.el6\nosg-ce-condor-3.3-13.osg33.el6\nosg-ce-lsf-3.3-13.osg33.el6\nosg-ce-pbs-3.3-13.osg33.el6\nosg-ce-sge-3.3-13.osg33.el6\nosg-ce-slurm-3.3-13.osg33.el6\nosg-configure-1.8.1-2.osg33.el6\nosg-configure-bosco-1.8.1-2.osg33.el6\nosg-configure-ce-1.8.1-2.osg33.el6\nosg-configure-cemon-1.8.1-2.osg33.el6\nosg-configure-condor-1.8.1-2.osg33.el6\nosg-configure-gateway-1.8.1-2.osg33.el6\nosg-configure-gip-1.8.1-2.osg33.el6\nosg-configure-gratia-1.8.1-2.osg33.el6\nosg-configure-infoservices-1.8.1-2.osg33.el6\nosg-configure-lsf-1.8.1-2.osg33.el6\nosg-configure-managedfork-1.8.1-2.osg33.el6\nosg-configure-misc-1.8.1-2.osg33.el6\nosg-configure-monalisa-1.8.1-2.osg33.el6\nosg-configure-network-1.8.1-2.osg33.el6\nosg-configure-pbs-1.8.1-2.osg33.el6\nosg-configure-rsv-1.8.1-2.osg33.el6\nosg-configure-sge-1.8.1-2.osg33.el6\nosg-configure-slurm-1.8.1-2.osg33.el6\nosg-configure-squid-1.8.1-2.osg33.el6\nosg-configure-tests-1.8.1-2.osg33.el6\nosg-gridftp-3.3-4.osg33.el6\nosg-htcondor-ce-3.3-13.osg33.el6\nosg-htcondor-ce-bosco-3.3-13.osg33.el6\nosg-htcondor-ce-condor-3.3-13.osg33.el6\nosg-htcondor-ce-lsf-3.3-13.osg33.el6\nosg-htcondor-ce-pbs-3.3-13.osg33.el6\nosg-htcondor-ce-sge-3.3-13.osg33.el6\nosg-htcondor-ce-slurm-3.3-13.osg33.el6\nosg-update-data-1.4.0-1.osg33.el6\nosg-update-vos-1.4.0-1.osg33.el6\nosg-version-3.3.25-1.osg33.el6\nvoms-admin-server-2.7.0-1.22.osg33.el6\nxrootd-4.6.1-1.osg33.el6\nxrootd-client-4.6.1-1.osg33.el6\nxrootd-client-devel-4.6.1-1.osg33.el6\nxrootd-client-libs-4.6.1-1.osg33.el6\nxrootd-debuginfo-4.6.1-1.osg33.el6\nxrootd-devel-4.6.1-1.osg33.el6\nxrootd-doc-4.6.1-1.osg33.el6\nxrootd-fuse-4.6.1-1.osg33.el6\nxrootd-libs-4.6.1-1.osg33.el6\nxrootd-private-devel-4.6.1-1.osg33.el6\nxrootd-python-4.6.1-1.osg33.el6\nxrootd-selinux-4.6.1-1.osg33.el6\nxrootd-server-4.6.1-1.osg33.el6\nxrootd-server-devel-4.6.1-1.osg33.el6\nxrootd-server-libs-4.6.1-1.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nemi-trustmanager-tomcat-3.0.0-15.osg33.el7\nglideinwms-3.2.19-1.osg33.el7\nglideinwms-common-tools-3.2.19-1.osg33.el7\nglideinwms-condor-common-config-3.2.19-1.osg33.el7\nglideinwms-factory-3.2.19-1.osg33.el7\nglideinwms-factory-condor-3.2.19-1.osg33.el7\nglideinwms-glidecondor-tools-3.2.19-1.osg33.el7\nglideinwms-libs-3.2.19-1.osg33.el7\nglideinwms-minimal-condor-3.2.19-1.osg33.el7\nglideinwms-usercollector-3.2.19-1.osg33.el7\nglideinwms-userschedd-3.2.19-1.osg33.el7\nglideinwms-vofrontend-3.2.19-1.osg33.el7\nglideinwms-vofrontend-standalone-3.2.19-1.osg33.el7\nglobus-xio-5.12-1.2.osg33.el7\nglobus-xio-debuginfo-5.12-1.2.osg33.el7\nglobus-xio-devel-5.12-1.2.osg33.el7\nglobus-xio-doc-5.12-1.2.osg33.el7\nhtcondor-ce-2.2.0-1.osg33.el7\nhtcondor-ce-bosco-2.2.0-1.osg33.el7\nhtcondor-ce-client-2.2.0-1.osg33.el7\nhtcondor-ce-collector-2.2.0-1.osg33.el7\nhtcondor-ce-condor-2.2.0-1.osg33.el7\nhtcondor-ce-lsf-2.2.0-1.osg33.el7\nhtcondor-ce-pbs-2.2.0-1.osg33.el7\nhtcondor-ce-sge-2.2.0-1.osg33.el7\nhtcondor-ce-slurm-2.2.0-1.osg33.el7\nhtcondor-ce-view-2.2.0-1.osg33.el7\nlcmaps-plugins-voms-1.7.1-1.4.osg33.el7\nlcmaps-plugins-voms-debuginfo-1.7.1-1.4.osg33.el7\nosg-base-ce-3.3-13.osg33.el7\nosg-base-ce-bosco-3.3-13.osg33.el7\nosg-base-ce-condor-3.3-13.osg33.el7\nosg-base-ce-lsf-3.3-13.osg33.el7\nosg-base-ce-pbs-3.3-13.osg33.el7\nosg-base-ce-sge-3.3-13.osg33.el7\nosg-base-ce-slurm-3.3-13.osg33.el7\nosg-build-1.10.0-1.osg33.el7\nosg-build-base-1.10.0-1.osg33.el7\nosg-build-koji-1.10.0-1.osg33.el7\nosg-build-mock-1.10.0-1.osg33.el7\nosg-build-tests-1.10.0-1.osg33.el7\nosg-ca-scripts-1.1.6-1.osg33.el7\nosg-ce-3.3-13.osg33.el7\nosg-ce-bosco-3.3-13.osg33.el7\nosg-ce-condor-3.3-13.osg33.el7\nosg-ce-lsf-3.3-13.osg33.el7\nosg-ce-pbs-3.3-13.osg33.el7\nosg-ce-sge-3.3-13.osg33.el7\nosg-ce-slurm-3.3-13.osg33.el7\nosg-configure-1.8.1-2.osg33.el7\nosg-configure-bosco-1.8.1-2.osg33.el7\nosg-configure-ce-1.8.1-2.osg33.el7\nosg-configure-cemon-1.8.1-2.osg33.el7\nosg-configure-condor-1.8.1-2.osg33.el7\nosg-configure-gateway-1.8.1-2.osg33.el7\nosg-configure-gip-1.8.1-2.osg33.el7\nosg-configure-gratia-1.8.1-2.osg33.el7\nosg-configure-infoservices-1.8.1-2.osg33.el7\nosg-configure-lsf-1.8.1-2.osg33.el7\nosg-configure-managedfork-1.8.1-2.osg33.el7\nosg-configure-misc-1.8.1-2.osg33.el7\nosg-configure-monalisa-1.8.1-2.osg33.el7\nosg-configure-network-1.8.1-2.osg33.el7\nosg-configure-pbs-1.8.1-2.osg33.el7\nosg-configure-rsv-1.8.1-2.osg33.el7\nosg-configure-sge-1.8.1-2.osg33.el7\nosg-configure-slurm-1.8.1-2.osg33.el7\nosg-configure-squid-1.8.1-2.osg33.el7\nosg-configure-tests-1.8.1-2.osg33.el7\nosg-gridftp-3.3-4.osg33.el7\nosg-htcondor-ce-3.3-13.osg33.el7\nosg-htcondor-ce-bosco-3.3-13.osg33.el7\nosg-htcondor-ce-condor-3.3-13.osg33.el7\nosg-htcondor-ce-lsf-3.3-13.osg33.el7\nosg-htcondor-ce-pbs-3.3-13.osg33.el7\nosg-htcondor-ce-sge-3.3-13.osg33.el7\nosg-htcondor-ce-slurm-3.3-13.osg33.el7\nosg-update-data-1.4.0-1.osg33.el7\nosg-update-vos-1.4.0-1.osg33.el7\nosg-version-3.3.25-1.osg33.el7\nstashcache-0.7-2.osg33.el7\nstashcache-cache-server-0.7-2.osg33.el7\nstashcache-daemon-0.7-2.osg33.el7\nstashcache-origin-server-0.7-2.osg33.el7\nxrootd-4.6.1-1.osg33.el7\nxrootd-client-4.6.1-1.osg33.el7\nxrootd-client-devel-4.6.1-1.osg33.el7\nxrootd-client-libs-4.6.1-1.osg33.el7\nxrootd-debuginfo-4.6.1-1.osg33.el7\nxrootd-devel-4.6.1-1.osg33.el7\nxrootd-doc-4.6.1-1.osg33.el7\nxrootd-fuse-4.6.1-1.osg33.el7\nxrootd-lcmaps-1.3.3-3.osg33.el7\nxrootd-lcmaps-debuginfo-1.3.3-3.osg33.el7\nxrootd-libs-4.6.1-1.osg33.el7\nxrootd-private-devel-4.6.1-1.osg33.el7\nxrootd-python-4.6.1-1.osg33.el7\nxrootd-selinux-4.6.1-1.osg33.el7\nxrootd-server-4.6.1-1.osg33.el7\nxrootd-server-devel-4.6.1-1.osg33.el7\nxrootd-server-libs-4.6.1-1.osg33.el7", 
            "title": "OSG Release 3.3.25"
        }, 
        {
            "location": "/release/3.3/release-3-3-25/#osg-software-release-3325", 
            "text": "Release Date : 2017-06-14", 
            "title": "OSG Software Release 3.3.25"
        }, 
        {
            "location": "/release/3.3/release-3-3-25/#summary-of-changes", 
            "text": "This release contains:   OSG 3.3.25  Make the LCMAPS VOMS plugin consider only the first FQAN to be consistent with GUMS  Update to  XRootD 4.6.1  Update to xrootd-lcmaps 1.3.3 for EL7    Update StashCache meta-packages to require XRootD 4.6.1  Update to  GlideinWMS 3.2.19  HTCondor-CE: Add WholeNodeWanted ClassAd expression so jobs can request a whole node from the batch system  Add vo-client-lcmaps-voms dependency to osg-gridftp and osg-ce  Updated to voms-admin-server 2.7.0-1.22 (security update)  Fix osg-update-vos script to clean yum cache in order pick up the latest vo-client RPM  osg-configure 1.8.1  reject empty  allowed_vos  in subclusters  get default  allowed_vos  from LCMAPS VOMS plugin  issue warning (rather than error out) if OSG_APP or OSG_DATA directories are not present    osg-ca-scripts now refers to repo.grid.iu.edu (rather than the retired software.grid.iu.edu)  No patch to globus-xio, drop the unneeded one  osg-build 1.10.0  drop vdt-build alias  drop ~/.osg-build.ini configuration file       These  JIRA tickets  were addressed in this release.   StashCache is supported on EL7 only.   xrootd-lcmaps will remain at 1.2.1-1 on EL6.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-25/#known-issues", 
            "text": "Updates to VOMS admin server require the updated emi-trustmanager-tomcat and re-running the configure script:\\  root@host # /var/lib/trustmanager-tomcat/configure.sh  Using the LCMAPS VOMS will result in a failing \"supported VO\" RSV test ( SOFTWARE-2763 ). This can be ignored and a fix is targeted for the July release.  VOMS admin server shows an error when modifying/adding/signing AUPs, but all the actions still work.  After updating OSG-CE to version 3.3-12, please disable and remove OSG Info Services via the following procedure:   root@host #  service osg-info-services stop root@host #  yum erase gip osg-info-services   The Koji client config has changed in the new version of Koji: `pkgurl http://koji.chtc.wisc.edu/packages` has been replaced by `topurl=http://koji.chtc.wisc.edu` and the Koji client will give a harmless but annoying warning when it finds `pkgurl`. To get rid of the warning, update to osg-build   1.8.0, rerun `osg-koji setup`, and say 'yes' when asked to replace the Koji configuration file; or, you may make the above change manually.  A previous version ( 1.17.0-2.5 ) of the Gratia probes required changes in the HTCondor configuration to operate properly. Now, these changes need to be reverted to use verison  1.17.0-2.6  and later of the probes. The lines below are likely to be found in  condor_config.local  or a new file in  /etc/condor/config.d  directory. Delete these lines and run  condor_reconfig .   # GlideinWMS Gratia commands  PER_JOB_HISTORY_DIR   =  /var/lib/gratia/data JOBGLIDEIN_ResourceName = $$ ([IfThenElse(IsUndefined(TARGET.GLIDEIN_ResourceName), IfThenElse(IsUndefined(TARGET.GLIDEIN_Site), IfThenElse(IsUndefined(TARGET.FileSystemDomain), \\ Local Job\\ , TARGET.FileSystemDomain), TARGET.GLIDEIN_Site), TARGET.GLIDEIN_ResourceName)])  SUBMIT_EXPRS   =   $( SUBMIT_EXPRS )  JOBGLIDEIN_ResourceName      On EL7, your version of voms-proxy-init may come from the voms-clients-java package; this version will continue to make legacy proxies by default. If you want the new behavior, install the voms-clients-cpp package and uninstall the voms-clients-java package.", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.3/release-3-3-25/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-25/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-25/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-25/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-25/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-25/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-25/#enterprise-linux-6", 
            "text": "emi-trustmanager-tomcat-3.0.0-15.osg33.el6  glideinwms-3.2.19-1.osg33.el6  globus-xio-5.12-1.2.osg33.el6  htcondor-ce-2.2.0-1.osg33.el6  lcmaps-plugins-voms-1.7.1-1.4.osg33.el6  osg-build-1.10.0-1.osg33.el6  osg-ca-scripts-1.1.6-1.osg33.el6  osg-ce-3.3-13.osg33.el6  osg-configure-1.8.1-2.osg33.el6  osg-gridftp-3.3-4.osg33.el6  osg-update-vos-1.4.0-1.osg33.el6  osg-version-3.3.25-1.osg33.el6  voms-admin-server-2.7.0-1.22.osg33.el6  xrootd-4.6.1-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-25/#enterprise-linux-7", 
            "text": "emi-trustmanager-tomcat-3.0.0-15.osg33.el7  glideinwms-3.2.19-1.osg33.el7  globus-xio-5.12-1.2.osg33.el7  htcondor-ce-2.2.0-1.osg33.el7  lcmaps-plugins-voms-1.7.1-1.4.osg33.el7  osg-build-1.10.0-1.osg33.el7  osg-ca-scripts-1.1.6-1.osg33.el7  osg-ce-3.3-13.osg33.el7  osg-configure-1.8.1-2.osg33.el7  osg-gridftp-3.3-4.osg33.el7  osg-update-vos-1.4.0-1.osg33.el7  osg-version-3.3.25-1.osg33.el7  stashcache-0.7-2.osg33.el7  xrootd-4.6.1-1.osg33.el7  xrootd-lcmaps-1.3.3-3.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-25/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  emi-trustmanager-tomcat glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone globus-xio globus-xio-debuginfo globus-xio-devel globus-xio-doc htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-slurm htcondor-ce-view lcmaps-plugins-voms lcmaps-plugins-voms-debuginfo osg-base-ce osg-base-ce-bosco osg-base-ce-condor osg-base-ce-lsf osg-base-ce-pbs osg-base-ce-sge osg-base-ce-slurm osg-build osg-build-base osg-build-koji osg-build-mock osg-build-tests osg-ca-scripts osg-ce osg-ce-bosco osg-ce-condor osg-ce-lsf osg-ce-pbs osg-ce-sge osg-ce-slurm osg-configure osg-configure-bosco osg-configure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-gridftp osg-htcondor-ce osg-htcondor-ce-bosco osg-htcondor-ce-condor osg-htcondor-ce-lsf osg-htcondor-ce-pbs osg-htcondor-ce-sge osg-htcondor-ce-slurm osg-update-data osg-update-vos osg-version voms-admin-server xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-libs xrootd-private-devel xrootd-python xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-25/#enterprise-linux-6_1", 
            "text": "emi-trustmanager-tomcat-3.0.0-15.osg33.el6\nglideinwms-3.2.19-1.osg33.el6\nglideinwms-common-tools-3.2.19-1.osg33.el6\nglideinwms-condor-common-config-3.2.19-1.osg33.el6\nglideinwms-factory-3.2.19-1.osg33.el6\nglideinwms-factory-condor-3.2.19-1.osg33.el6\nglideinwms-glidecondor-tools-3.2.19-1.osg33.el6\nglideinwms-libs-3.2.19-1.osg33.el6\nglideinwms-minimal-condor-3.2.19-1.osg33.el6\nglideinwms-usercollector-3.2.19-1.osg33.el6\nglideinwms-userschedd-3.2.19-1.osg33.el6\nglideinwms-vofrontend-3.2.19-1.osg33.el6\nglideinwms-vofrontend-standalone-3.2.19-1.osg33.el6\nglobus-xio-5.12-1.2.osg33.el6\nglobus-xio-debuginfo-5.12-1.2.osg33.el6\nglobus-xio-devel-5.12-1.2.osg33.el6\nglobus-xio-doc-5.12-1.2.osg33.el6\nhtcondor-ce-2.2.0-1.osg33.el6\nhtcondor-ce-bosco-2.2.0-1.osg33.el6\nhtcondor-ce-client-2.2.0-1.osg33.el6\nhtcondor-ce-collector-2.2.0-1.osg33.el6\nhtcondor-ce-condor-2.2.0-1.osg33.el6\nhtcondor-ce-lsf-2.2.0-1.osg33.el6\nhtcondor-ce-pbs-2.2.0-1.osg33.el6\nhtcondor-ce-sge-2.2.0-1.osg33.el6\nhtcondor-ce-slurm-2.2.0-1.osg33.el6\nhtcondor-ce-view-2.2.0-1.osg33.el6\nlcmaps-plugins-voms-1.7.1-1.4.osg33.el6\nlcmaps-plugins-voms-debuginfo-1.7.1-1.4.osg33.el6\nosg-base-ce-3.3-13.osg33.el6\nosg-base-ce-bosco-3.3-13.osg33.el6\nosg-base-ce-condor-3.3-13.osg33.el6\nosg-base-ce-lsf-3.3-13.osg33.el6\nosg-base-ce-pbs-3.3-13.osg33.el6\nosg-base-ce-sge-3.3-13.osg33.el6\nosg-base-ce-slurm-3.3-13.osg33.el6\nosg-build-1.10.0-1.osg33.el6\nosg-build-base-1.10.0-1.osg33.el6\nosg-build-koji-1.10.0-1.osg33.el6\nosg-build-mock-1.10.0-1.osg33.el6\nosg-build-tests-1.10.0-1.osg33.el6\nosg-ca-scripts-1.1.6-1.osg33.el6\nosg-ce-3.3-13.osg33.el6\nosg-ce-bosco-3.3-13.osg33.el6\nosg-ce-condor-3.3-13.osg33.el6\nosg-ce-lsf-3.3-13.osg33.el6\nosg-ce-pbs-3.3-13.osg33.el6\nosg-ce-sge-3.3-13.osg33.el6\nosg-ce-slurm-3.3-13.osg33.el6\nosg-configure-1.8.1-2.osg33.el6\nosg-configure-bosco-1.8.1-2.osg33.el6\nosg-configure-ce-1.8.1-2.osg33.el6\nosg-configure-cemon-1.8.1-2.osg33.el6\nosg-configure-condor-1.8.1-2.osg33.el6\nosg-configure-gateway-1.8.1-2.osg33.el6\nosg-configure-gip-1.8.1-2.osg33.el6\nosg-configure-gratia-1.8.1-2.osg33.el6\nosg-configure-infoservices-1.8.1-2.osg33.el6\nosg-configure-lsf-1.8.1-2.osg33.el6\nosg-configure-managedfork-1.8.1-2.osg33.el6\nosg-configure-misc-1.8.1-2.osg33.el6\nosg-configure-monalisa-1.8.1-2.osg33.el6\nosg-configure-network-1.8.1-2.osg33.el6\nosg-configure-pbs-1.8.1-2.osg33.el6\nosg-configure-rsv-1.8.1-2.osg33.el6\nosg-configure-sge-1.8.1-2.osg33.el6\nosg-configure-slurm-1.8.1-2.osg33.el6\nosg-configure-squid-1.8.1-2.osg33.el6\nosg-configure-tests-1.8.1-2.osg33.el6\nosg-gridftp-3.3-4.osg33.el6\nosg-htcondor-ce-3.3-13.osg33.el6\nosg-htcondor-ce-bosco-3.3-13.osg33.el6\nosg-htcondor-ce-condor-3.3-13.osg33.el6\nosg-htcondor-ce-lsf-3.3-13.osg33.el6\nosg-htcondor-ce-pbs-3.3-13.osg33.el6\nosg-htcondor-ce-sge-3.3-13.osg33.el6\nosg-htcondor-ce-slurm-3.3-13.osg33.el6\nosg-update-data-1.4.0-1.osg33.el6\nosg-update-vos-1.4.0-1.osg33.el6\nosg-version-3.3.25-1.osg33.el6\nvoms-admin-server-2.7.0-1.22.osg33.el6\nxrootd-4.6.1-1.osg33.el6\nxrootd-client-4.6.1-1.osg33.el6\nxrootd-client-devel-4.6.1-1.osg33.el6\nxrootd-client-libs-4.6.1-1.osg33.el6\nxrootd-debuginfo-4.6.1-1.osg33.el6\nxrootd-devel-4.6.1-1.osg33.el6\nxrootd-doc-4.6.1-1.osg33.el6\nxrootd-fuse-4.6.1-1.osg33.el6\nxrootd-libs-4.6.1-1.osg33.el6\nxrootd-private-devel-4.6.1-1.osg33.el6\nxrootd-python-4.6.1-1.osg33.el6\nxrootd-selinux-4.6.1-1.osg33.el6\nxrootd-server-4.6.1-1.osg33.el6\nxrootd-server-devel-4.6.1-1.osg33.el6\nxrootd-server-libs-4.6.1-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-25/#enterprise-linux-7_1", 
            "text": "emi-trustmanager-tomcat-3.0.0-15.osg33.el7\nglideinwms-3.2.19-1.osg33.el7\nglideinwms-common-tools-3.2.19-1.osg33.el7\nglideinwms-condor-common-config-3.2.19-1.osg33.el7\nglideinwms-factory-3.2.19-1.osg33.el7\nglideinwms-factory-condor-3.2.19-1.osg33.el7\nglideinwms-glidecondor-tools-3.2.19-1.osg33.el7\nglideinwms-libs-3.2.19-1.osg33.el7\nglideinwms-minimal-condor-3.2.19-1.osg33.el7\nglideinwms-usercollector-3.2.19-1.osg33.el7\nglideinwms-userschedd-3.2.19-1.osg33.el7\nglideinwms-vofrontend-3.2.19-1.osg33.el7\nglideinwms-vofrontend-standalone-3.2.19-1.osg33.el7\nglobus-xio-5.12-1.2.osg33.el7\nglobus-xio-debuginfo-5.12-1.2.osg33.el7\nglobus-xio-devel-5.12-1.2.osg33.el7\nglobus-xio-doc-5.12-1.2.osg33.el7\nhtcondor-ce-2.2.0-1.osg33.el7\nhtcondor-ce-bosco-2.2.0-1.osg33.el7\nhtcondor-ce-client-2.2.0-1.osg33.el7\nhtcondor-ce-collector-2.2.0-1.osg33.el7\nhtcondor-ce-condor-2.2.0-1.osg33.el7\nhtcondor-ce-lsf-2.2.0-1.osg33.el7\nhtcondor-ce-pbs-2.2.0-1.osg33.el7\nhtcondor-ce-sge-2.2.0-1.osg33.el7\nhtcondor-ce-slurm-2.2.0-1.osg33.el7\nhtcondor-ce-view-2.2.0-1.osg33.el7\nlcmaps-plugins-voms-1.7.1-1.4.osg33.el7\nlcmaps-plugins-voms-debuginfo-1.7.1-1.4.osg33.el7\nosg-base-ce-3.3-13.osg33.el7\nosg-base-ce-bosco-3.3-13.osg33.el7\nosg-base-ce-condor-3.3-13.osg33.el7\nosg-base-ce-lsf-3.3-13.osg33.el7\nosg-base-ce-pbs-3.3-13.osg33.el7\nosg-base-ce-sge-3.3-13.osg33.el7\nosg-base-ce-slurm-3.3-13.osg33.el7\nosg-build-1.10.0-1.osg33.el7\nosg-build-base-1.10.0-1.osg33.el7\nosg-build-koji-1.10.0-1.osg33.el7\nosg-build-mock-1.10.0-1.osg33.el7\nosg-build-tests-1.10.0-1.osg33.el7\nosg-ca-scripts-1.1.6-1.osg33.el7\nosg-ce-3.3-13.osg33.el7\nosg-ce-bosco-3.3-13.osg33.el7\nosg-ce-condor-3.3-13.osg33.el7\nosg-ce-lsf-3.3-13.osg33.el7\nosg-ce-pbs-3.3-13.osg33.el7\nosg-ce-sge-3.3-13.osg33.el7\nosg-ce-slurm-3.3-13.osg33.el7\nosg-configure-1.8.1-2.osg33.el7\nosg-configure-bosco-1.8.1-2.osg33.el7\nosg-configure-ce-1.8.1-2.osg33.el7\nosg-configure-cemon-1.8.1-2.osg33.el7\nosg-configure-condor-1.8.1-2.osg33.el7\nosg-configure-gateway-1.8.1-2.osg33.el7\nosg-configure-gip-1.8.1-2.osg33.el7\nosg-configure-gratia-1.8.1-2.osg33.el7\nosg-configure-infoservices-1.8.1-2.osg33.el7\nosg-configure-lsf-1.8.1-2.osg33.el7\nosg-configure-managedfork-1.8.1-2.osg33.el7\nosg-configure-misc-1.8.1-2.osg33.el7\nosg-configure-monalisa-1.8.1-2.osg33.el7\nosg-configure-network-1.8.1-2.osg33.el7\nosg-configure-pbs-1.8.1-2.osg33.el7\nosg-configure-rsv-1.8.1-2.osg33.el7\nosg-configure-sge-1.8.1-2.osg33.el7\nosg-configure-slurm-1.8.1-2.osg33.el7\nosg-configure-squid-1.8.1-2.osg33.el7\nosg-configure-tests-1.8.1-2.osg33.el7\nosg-gridftp-3.3-4.osg33.el7\nosg-htcondor-ce-3.3-13.osg33.el7\nosg-htcondor-ce-bosco-3.3-13.osg33.el7\nosg-htcondor-ce-condor-3.3-13.osg33.el7\nosg-htcondor-ce-lsf-3.3-13.osg33.el7\nosg-htcondor-ce-pbs-3.3-13.osg33.el7\nosg-htcondor-ce-sge-3.3-13.osg33.el7\nosg-htcondor-ce-slurm-3.3-13.osg33.el7\nosg-update-data-1.4.0-1.osg33.el7\nosg-update-vos-1.4.0-1.osg33.el7\nosg-version-3.3.25-1.osg33.el7\nstashcache-0.7-2.osg33.el7\nstashcache-cache-server-0.7-2.osg33.el7\nstashcache-daemon-0.7-2.osg33.el7\nstashcache-origin-server-0.7-2.osg33.el7\nxrootd-4.6.1-1.osg33.el7\nxrootd-client-4.6.1-1.osg33.el7\nxrootd-client-devel-4.6.1-1.osg33.el7\nxrootd-client-libs-4.6.1-1.osg33.el7\nxrootd-debuginfo-4.6.1-1.osg33.el7\nxrootd-devel-4.6.1-1.osg33.el7\nxrootd-doc-4.6.1-1.osg33.el7\nxrootd-fuse-4.6.1-1.osg33.el7\nxrootd-lcmaps-1.3.3-3.osg33.el7\nxrootd-lcmaps-debuginfo-1.3.3-3.osg33.el7\nxrootd-libs-4.6.1-1.osg33.el7\nxrootd-private-devel-4.6.1-1.osg33.el7\nxrootd-python-4.6.1-1.osg33.el7\nxrootd-selinux-4.6.1-1.osg33.el7\nxrootd-server-4.6.1-1.osg33.el7\nxrootd-server-devel-4.6.1-1.osg33.el7\nxrootd-server-libs-4.6.1-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-24-2/", 
            "text": "OSG Software Stack -- Data Release -- 3.3.24-2\n\n\nRelease Date\n: 2017-05-16\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nVO Package v73\n\n\nUpdate edg-mkgridmap CDF entry to match vomses\n\n\nRemove LIGO server from GUMS template\n\n\nRemove unused CDF glidecaf settings\n\n\nDrop /production role for DES vomsUserGroup\n\n\nUpdate default CMS mappings\n\n\nUpdate default Fermilab / FIFE mappings\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nvo-client-73-1.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nvo-client-73-1.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nosg-gums-config vo-client vo-client-edgmkgridmap vo-client-lcmaps-voms\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nosg-gums-config-73-1.osg33.el6\nvo-client-73-1.osg33.el6\nvo-client-edgmkgridmap-73-1.osg33.el6\nvo-client-lcmaps-voms-73-1.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nosg-gums-config-73-1.osg33.el7\nvo-client-73-1.osg33.el7\nvo-client-edgmkgridmap-73-1.osg33.el7\nvo-client-lcmaps-voms-73-1.osg33.el7", 
            "title": "OSG Release 3.3.24-2"
        }, 
        {
            "location": "/release/3.3/release-3-3-24-2/#osg-software-stack-data-release-3324-2", 
            "text": "Release Date : 2017-05-16", 
            "title": "OSG Software Stack -- Data Release -- 3.3.24-2"
        }, 
        {
            "location": "/release/3.3/release-3-3-24-2/#summary-of-changes", 
            "text": "This release contains:   VO Package v73  Update edg-mkgridmap CDF entry to match vomses  Remove LIGO server from GUMS template  Remove unused CDF glidecaf settings  Drop /production role for DES vomsUserGroup  Update default CMS mappings  Update default Fermilab / FIFE mappings     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-24-2/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-24-2/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-24-2/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-24-2/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-24-2/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-24-2/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-24-2/#enterprise-linux-6", 
            "text": "vo-client-73-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-24-2/#enterprise-linux-7", 
            "text": "vo-client-73-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-24-2/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  osg-gums-config vo-client vo-client-edgmkgridmap vo-client-lcmaps-voms  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-24-2/#enterprise-linux-6_1", 
            "text": "osg-gums-config-73-1.osg33.el6\nvo-client-73-1.osg33.el6\nvo-client-edgmkgridmap-73-1.osg33.el6\nvo-client-lcmaps-voms-73-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-24-2/#enterprise-linux-7_1", 
            "text": "osg-gums-config-73-1.osg33.el7\nvo-client-73-1.osg33.el7\nvo-client-edgmkgridmap-73-1.osg33.el7\nvo-client-lcmaps-voms-73-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-24/", 
            "text": "OSG Software Release 3.3.24\n\n\nRelease Date\n: 2017-05-09\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nOSG 3.3.24\n\n\nosg-configure 1.7.0\n\n\nEdit lcmaps.db to use the VOMS plugin\n\n\nAdd template lcmaps.db files\n\n\n\n\n\n\nMake all attributes relating to the defunct BDII service optional\n\n\nDon't error out if user-vo-map missing, issue warning with suggestion\n\n\n\n\n\n\nCVMFS X.509 helper - fix for running inside a container\n\n\ngsissh in tarball installations\n\n\nFix HTCondor Gratia probe to not call .eval() if not present when doing DebugPrint logging\n\n\nosg-build 1.9.0\n\n\nSplit osg-build into subpackages\n\n\nadd supprt for git repos in .source files (for HCC)\n\n\nosg-build notes default options\n\n\nadd support for 3.4\n\n\n\n\n\n\n\n\n\n\nUpcoming repository\n\n\nHTCondor 8.6.2\n\n\nGlideinWMS 3.3.2\n\n\nAllow multiple remote directories for BOSCO submissions\n\n\nBug fix: Submit attributes in entry configuration are now transmitted to AWS VM\n\n\n\n\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nKnown Issues\n\n\n\n\nAfter updating OSG-CE to version 3.3-12, please disable and remove OSG Info Services via the following procedure:\n\n\n\n\nroot@host #\n service osg-info-services stop\n\nroot@host #\n yum erase gip osg-info-services\n\n\n\n\n\n\n\nThe Koji client config has changed in the new version of Koji: `pkgurl\nhttp://koji.chtc.wisc.edu/packages` has been replaced by `topurl=http://koji.chtc.wisc.edu` and the Koji client will give a harmless but annoying warning when it finds `pkgurl`. To get rid of the warning, update to osg-build \n 1.8.0, rerun `osg-koji setup`, and say 'yes' when asked to replace the Koji configuration file; or, you may make the above change manually.\n\n\nA previous version (\n1.17.0-2.5\n) of the Gratia probes required changes in the HTCondor configuration to operate properly. Now, these changes need to be reverted to use verison \n1.17.0-2.6\n and later of the probes. The lines below are likely to be found in \ncondor_config.local\n or a new file in \n/etc/condor/config.d\n directory. Delete these lines and run \ncondor_reconfig\n.\n\n\n\n\n# GlideinWMS Gratia commands\n\n\nPER_JOB_HISTORY_DIR\n \n=\n /var/lib/gratia/data\n\nJOBGLIDEIN_ResourceName\n=\n$$\n([IfThenElse(IsUndefined(TARGET.GLIDEIN_ResourceName), IfThenElse(IsUndefined(TARGET.GLIDEIN_Site), IfThenElse(IsUndefined(TARGET.FileSystemDomain), \\\nLocal Job\\\n, TARGET.FileSystemDomain), TARGET.GLIDEIN_Site), TARGET.GLIDEIN_ResourceName)])\n\n\nSUBMIT_EXPRS\n \n=\n \n$(\nSUBMIT_EXPRS\n)\n JOBGLIDEIN_ResourceName   \n\n\n\n\n\n\n\nOn EL7, your version of voms-proxy-init may come from the voms-clients-java package; this version will continue to make legacy proxies by default. If you want the new behavior, install the voms-clients-cpp package and uninstall the voms-clients-java package.\n\n\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\ncvmfs-x509-helper-1.0-1.osg33.el6\n\n\ngratia-probe-1.17.5-1.osg33.el6\n\n\nlcmaps-1.6.6-1.3.osg33.el6\n\n\nosg-build-1.9.0-2.osg33.el6\n\n\nosg-configure-1.7.0-1.osg33.el6\n\n\nosg-version-3.3.24-1.osg33.el6\n\n\nosg-wn-client-3.3-7.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\ncvmfs-x509-helper-1.0-1.osg33.el7\n\n\ngratia-probe-1.17.5-1.osg33.el7\n\n\nlcmaps-1.6.6-1.3.osg33.el7\n\n\nosg-build-1.9.0-2.osg33.el7\n\n\nosg-configure-1.7.0-1.osg33.el7\n\n\nosg-version-3.3.24-1.osg33.el7\n\n\nosg-wn-client-3.3-7.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\ncvmfs-x509-helper cvmfs-x509-helper-debuginfo gratia-probe-bdii-status gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glexec gratia-probe-glideinwms gratia-probe-gram gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer lcmaps lcmaps-common-devel lcmaps-db-templates lcmaps-debuginfo lcmaps-devel lcmaps-without-gsi lcmaps-without-gsi-devel osg-build osg-build-base osg-build-koji osg-build-mock osg-build-tests osg-configure osg-configure-bosco osg-configure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-version osg-wn-client osg-wn-client-glexec\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\ncvmfs-x509-helper-1.0-1.osg33.el6\ncvmfs-x509-helper-debuginfo-1.0-1.osg33.el6\ngratia-probe-1.17.5-1.osg33.el6\ngratia-probe-bdii-status-1.17.5-1.osg33.el6\ngratia-probe-common-1.17.5-1.osg33.el6\ngratia-probe-condor-1.17.5-1.osg33.el6\ngratia-probe-condor-events-1.17.5-1.osg33.el6\ngratia-probe-dcache-storage-1.17.5-1.osg33.el6\ngratia-probe-dcache-storagegroup-1.17.5-1.osg33.el6\ngratia-probe-dcache-transfer-1.17.5-1.osg33.el6\ngratia-probe-debuginfo-1.17.5-1.osg33.el6\ngratia-probe-enstore-storage-1.17.5-1.osg33.el6\ngratia-probe-enstore-tapedrive-1.17.5-1.osg33.el6\ngratia-probe-enstore-transfer-1.17.5-1.osg33.el6\ngratia-probe-glexec-1.17.5-1.osg33.el6\ngratia-probe-glideinwms-1.17.5-1.osg33.el6\ngratia-probe-gram-1.17.5-1.osg33.el6\ngratia-probe-gridftp-transfer-1.17.5-1.osg33.el6\ngratia-probe-hadoop-storage-1.17.5-1.osg33.el6\ngratia-probe-htcondor-ce-1.17.5-1.osg33.el6\ngratia-probe-lsf-1.17.5-1.osg33.el6\ngratia-probe-metric-1.17.5-1.osg33.el6\ngratia-probe-onevm-1.17.5-1.osg33.el6\ngratia-probe-pbs-lsf-1.17.5-1.osg33.el6\ngratia-probe-services-1.17.5-1.osg33.el6\ngratia-probe-sge-1.17.5-1.osg33.el6\ngratia-probe-slurm-1.17.5-1.osg33.el6\ngratia-probe-xrootd-storage-1.17.5-1.osg33.el6\ngratia-probe-xrootd-transfer-1.17.5-1.osg33.el6\nlcmaps-1.6.6-1.3.osg33.el6\nlcmaps-common-devel-1.6.6-1.3.osg33.el6\nlcmaps-db-templates-1.6.6-1.3.osg33.el6\nlcmaps-debuginfo-1.6.6-1.3.osg33.el6\nlcmaps-devel-1.6.6-1.3.osg33.el6\nlcmaps-without-gsi-1.6.6-1.3.osg33.el6\nlcmaps-without-gsi-devel-1.6.6-1.3.osg33.el6\nosg-build-1.9.0-2.osg33.el6\nosg-build-base-1.9.0-2.osg33.el6\nosg-build-koji-1.9.0-2.osg33.el6\nosg-build-mock-1.9.0-2.osg33.el6\nosg-build-tests-1.9.0-2.osg33.el6\nosg-configure-1.7.0-1.osg33.el6\nosg-configure-bosco-1.7.0-1.osg33.el6\nosg-configure-ce-1.7.0-1.osg33.el6\nosg-configure-cemon-1.7.0-1.osg33.el6\nosg-configure-condor-1.7.0-1.osg33.el6\nosg-configure-gateway-1.7.0-1.osg33.el6\nosg-configure-gip-1.7.0-1.osg33.el6\nosg-configure-gratia-1.7.0-1.osg33.el6\nosg-configure-infoservices-1.7.0-1.osg33.el6\nosg-configure-lsf-1.7.0-1.osg33.el6\nosg-configure-managedfork-1.7.0-1.osg33.el6\nosg-configure-misc-1.7.0-1.osg33.el6\nosg-configure-monalisa-1.7.0-1.osg33.el6\nosg-configure-network-1.7.0-1.osg33.el6\nosg-configure-pbs-1.7.0-1.osg33.el6\nosg-configure-rsv-1.7.0-1.osg33.el6\nosg-configure-sge-1.7.0-1.osg33.el6\nosg-configure-slurm-1.7.0-1.osg33.el6\nosg-configure-squid-1.7.0-1.osg33.el6\nosg-configure-tests-1.7.0-1.osg33.el6\nosg-version-3.3.24-1.osg33.el6\nosg-wn-client-3.3-7.osg33.el6\nosg-wn-client-glexec-3.3-7.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\ncvmfs-x509-helper-1.0-1.osg33.el7\ncvmfs-x509-helper-debuginfo-1.0-1.osg33.el7\ngratia-probe-1.17.5-1.osg33.el7\ngratia-probe-bdii-status-1.17.5-1.osg33.el7\ngratia-probe-common-1.17.5-1.osg33.el7\ngratia-probe-condor-1.17.5-1.osg33.el7\ngratia-probe-condor-events-1.17.5-1.osg33.el7\ngratia-probe-dcache-storage-1.17.5-1.osg33.el7\ngratia-probe-dcache-storagegroup-1.17.5-1.osg33.el7\ngratia-probe-dcache-transfer-1.17.5-1.osg33.el7\ngratia-probe-debuginfo-1.17.5-1.osg33.el7\ngratia-probe-enstore-storage-1.17.5-1.osg33.el7\ngratia-probe-enstore-tapedrive-1.17.5-1.osg33.el7\ngratia-probe-enstore-transfer-1.17.5-1.osg33.el7\ngratia-probe-glexec-1.17.5-1.osg33.el7\ngratia-probe-glideinwms-1.17.5-1.osg33.el7\ngratia-probe-gram-1.17.5-1.osg33.el7\ngratia-probe-gridftp-transfer-1.17.5-1.osg33.el7\ngratia-probe-hadoop-storage-1.17.5-1.osg33.el7\ngratia-probe-htcondor-ce-1.17.5-1.osg33.el7\ngratia-probe-lsf-1.17.5-1.osg33.el7\ngratia-probe-metric-1.17.5-1.osg33.el7\ngratia-probe-onevm-1.17.5-1.osg33.el7\ngratia-probe-pbs-lsf-1.17.5-1.osg33.el7\ngratia-probe-services-1.17.5-1.osg33.el7\ngratia-probe-sge-1.17.5-1.osg33.el7\ngratia-probe-slurm-1.17.5-1.osg33.el7\ngratia-probe-xrootd-storage-1.17.5-1.osg33.el7\ngratia-probe-xrootd-transfer-1.17.5-1.osg33.el7\nlcmaps-1.6.6-1.3.osg33.el7\nlcmaps-common-devel-1.6.6-1.3.osg33.el7\nlcmaps-db-templates-1.6.6-1.3.osg33.el7\nlcmaps-debuginfo-1.6.6-1.3.osg33.el7\nlcmaps-devel-1.6.6-1.3.osg33.el7\nlcmaps-without-gsi-1.6.6-1.3.osg33.el7\nlcmaps-without-gsi-devel-1.6.6-1.3.osg33.el7\nosg-build-1.9.0-2.osg33.el7\nosg-build-base-1.9.0-2.osg33.el7\nosg-build-koji-1.9.0-2.osg33.el7\nosg-build-mock-1.9.0-2.osg33.el7\nosg-build-tests-1.9.0-2.osg33.el7\nosg-configure-1.7.0-1.osg33.el7\nosg-configure-bosco-1.7.0-1.osg33.el7\nosg-configure-ce-1.7.0-1.osg33.el7\nosg-configure-cemon-1.7.0-1.osg33.el7\nosg-configure-condor-1.7.0-1.osg33.el7\nosg-configure-gateway-1.7.0-1.osg33.el7\nosg-configure-gip-1.7.0-1.osg33.el7\nosg-configure-gratia-1.7.0-1.osg33.el7\nosg-configure-infoservices-1.7.0-1.osg33.el7\nosg-configure-lsf-1.7.0-1.osg33.el7\nosg-configure-managedfork-1.7.0-1.osg33.el7\nosg-configure-misc-1.7.0-1.osg33.el7\nosg-configure-monalisa-1.7.0-1.osg33.el7\nosg-configure-network-1.7.0-1.osg33.el7\nosg-configure-pbs-1.7.0-1.osg33.el7\nosg-configure-rsv-1.7.0-1.osg33.el7\nosg-configure-sge-1.7.0-1.osg33.el7\nosg-configure-slurm-1.7.0-1.osg33.el7\nosg-configure-squid-1.7.0-1.osg33.el7\nosg-configure-tests-1.7.0-1.osg33.el7\nosg-version-3.3.24-1.osg33.el7\nosg-wn-client-3.3-7.osg33.el7\nosg-wn-client-glexec-3.3-7.osg33.el7\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\ncondor-8.6.2-1.osgup.el6\n\n\nglideinwms-3.3.2-2.osgup.el6\n\n\nlcmaps-1.6.6-1.4.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\ncondor-8.6.2-1.osgup.el7\n\n\nglideinwms-3.3.2-2.osgup.el7\n\n\nlcmaps-1.6.6-1.4.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\ncondor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone lcmaps lcmaps-common-devel lcmaps-db-templates lcmaps-debuginfo lcmaps-devel lcmaps-without-gsi lcmaps-without-gsi-devel\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\ncondor-8.6.2-1.osgup.el6\ncondor-all-8.6.2-1.osgup.el6\ncondor-bosco-8.6.2-1.osgup.el6\ncondor-classads-8.6.2-1.osgup.el6\ncondor-classads-devel-8.6.2-1.osgup.el6\ncondor-cream-gahp-8.6.2-1.osgup.el6\ncondor-debuginfo-8.6.2-1.osgup.el6\ncondor-kbdd-8.6.2-1.osgup.el6\ncondor-procd-8.6.2-1.osgup.el6\ncondor-python-8.6.2-1.osgup.el6\ncondor-std-universe-8.6.2-1.osgup.el6\ncondor-test-8.6.2-1.osgup.el6\ncondor-vm-gahp-8.6.2-1.osgup.el6\nglideinwms-3.3.2-2.osgup.el6\nglideinwms-common-tools-3.3.2-2.osgup.el6\nglideinwms-condor-common-config-3.3.2-2.osgup.el6\nglideinwms-factory-3.3.2-2.osgup.el6\nglideinwms-factory-condor-3.3.2-2.osgup.el6\nglideinwms-glidecondor-tools-3.3.2-2.osgup.el6\nglideinwms-libs-3.3.2-2.osgup.el6\nglideinwms-minimal-condor-3.3.2-2.osgup.el6\nglideinwms-usercollector-3.3.2-2.osgup.el6\nglideinwms-userschedd-3.3.2-2.osgup.el6\nglideinwms-vofrontend-3.3.2-2.osgup.el6\nglideinwms-vofrontend-standalone-3.3.2-2.osgup.el6\nlcmaps-1.6.6-1.4.osgup.el6\nlcmaps-common-devel-1.6.6-1.4.osgup.el6\nlcmaps-db-templates-1.6.6-1.4.osgup.el6\nlcmaps-debuginfo-1.6.6-1.4.osgup.el6\nlcmaps-devel-1.6.6-1.4.osgup.el6\nlcmaps-without-gsi-1.6.6-1.4.osgup.el6\nlcmaps-without-gsi-devel-1.6.6-1.4.osgup.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\ncondor-8.6.2-1.osgup.el7\ncondor-all-8.6.2-1.osgup.el7\ncondor-bosco-8.6.2-1.osgup.el7\ncondor-classads-8.6.2-1.osgup.el7\ncondor-classads-devel-8.6.2-1.osgup.el7\ncondor-cream-gahp-8.6.2-1.osgup.el7\ncondor-debuginfo-8.6.2-1.osgup.el7\ncondor-kbdd-8.6.2-1.osgup.el7\ncondor-procd-8.6.2-1.osgup.el7\ncondor-python-8.6.2-1.osgup.el7\ncondor-test-8.6.2-1.osgup.el7\ncondor-vm-gahp-8.6.2-1.osgup.el7\nglideinwms-3.3.2-2.osgup.el7\nglideinwms-common-tools-3.3.2-2.osgup.el7\nglideinwms-condor-common-config-3.3.2-2.osgup.el7\nglideinwms-factory-3.3.2-2.osgup.el7\nglideinwms-factory-condor-3.3.2-2.osgup.el7\nglideinwms-glidecondor-tools-3.3.2-2.osgup.el7\nglideinwms-libs-3.3.2-2.osgup.el7\nglideinwms-minimal-condor-3.3.2-2.osgup.el7\nglideinwms-usercollector-3.3.2-2.osgup.el7\nglideinwms-userschedd-3.3.2-2.osgup.el7\nglideinwms-vofrontend-3.3.2-2.osgup.el7\nglideinwms-vofrontend-standalone-3.3.2-2.osgup.el7\nlcmaps-1.6.6-1.4.osgup.el7\nlcmaps-common-devel-1.6.6-1.4.osgup.el7\nlcmaps-db-templates-1.6.6-1.4.osgup.el7\nlcmaps-debuginfo-1.6.6-1.4.osgup.el7\nlcmaps-devel-1.6.6-1.4.osgup.el7\nlcmaps-without-gsi-1.6.6-1.4.osgup.el7\nlcmaps-without-gsi-devel-1.6.6-1.4.osgup.el7", 
            "title": "OSG Release 3.3.24"
        }, 
        {
            "location": "/release/3.3/release-3-3-24/#osg-software-release-3324", 
            "text": "Release Date : 2017-05-09", 
            "title": "OSG Software Release 3.3.24"
        }, 
        {
            "location": "/release/3.3/release-3-3-24/#summary-of-changes", 
            "text": "This release contains:   OSG 3.3.24  osg-configure 1.7.0  Edit lcmaps.db to use the VOMS plugin  Add template lcmaps.db files    Make all attributes relating to the defunct BDII service optional  Don't error out if user-vo-map missing, issue warning with suggestion    CVMFS X.509 helper - fix for running inside a container  gsissh in tarball installations  Fix HTCondor Gratia probe to not call .eval() if not present when doing DebugPrint logging  osg-build 1.9.0  Split osg-build into subpackages  add supprt for git repos in .source files (for HCC)  osg-build notes default options  add support for 3.4      Upcoming repository  HTCondor 8.6.2  GlideinWMS 3.3.2  Allow multiple remote directories for BOSCO submissions  Bug fix: Submit attributes in entry configuration are now transmitted to AWS VM       These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-24/#known-issues", 
            "text": "After updating OSG-CE to version 3.3-12, please disable and remove OSG Info Services via the following procedure:   root@host #  service osg-info-services stop root@host #  yum erase gip osg-info-services   The Koji client config has changed in the new version of Koji: `pkgurl http://koji.chtc.wisc.edu/packages` has been replaced by `topurl=http://koji.chtc.wisc.edu` and the Koji client will give a harmless but annoying warning when it finds `pkgurl`. To get rid of the warning, update to osg-build   1.8.0, rerun `osg-koji setup`, and say 'yes' when asked to replace the Koji configuration file; or, you may make the above change manually.  A previous version ( 1.17.0-2.5 ) of the Gratia probes required changes in the HTCondor configuration to operate properly. Now, these changes need to be reverted to use verison  1.17.0-2.6  and later of the probes. The lines below are likely to be found in  condor_config.local  or a new file in  /etc/condor/config.d  directory. Delete these lines and run  condor_reconfig .   # GlideinWMS Gratia commands  PER_JOB_HISTORY_DIR   =  /var/lib/gratia/data JOBGLIDEIN_ResourceName = $$ ([IfThenElse(IsUndefined(TARGET.GLIDEIN_ResourceName), IfThenElse(IsUndefined(TARGET.GLIDEIN_Site), IfThenElse(IsUndefined(TARGET.FileSystemDomain), \\ Local Job\\ , TARGET.FileSystemDomain), TARGET.GLIDEIN_Site), TARGET.GLIDEIN_ResourceName)])  SUBMIT_EXPRS   =   $( SUBMIT_EXPRS )  JOBGLIDEIN_ResourceName      On EL7, your version of voms-proxy-init may come from the voms-clients-java package; this version will continue to make legacy proxies by default. If you want the new behavior, install the voms-clients-cpp package and uninstall the voms-clients-java package.", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.3/release-3-3-24/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-24/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-24/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-24/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-24/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-24/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-24/#enterprise-linux-6", 
            "text": "cvmfs-x509-helper-1.0-1.osg33.el6  gratia-probe-1.17.5-1.osg33.el6  lcmaps-1.6.6-1.3.osg33.el6  osg-build-1.9.0-2.osg33.el6  osg-configure-1.7.0-1.osg33.el6  osg-version-3.3.24-1.osg33.el6  osg-wn-client-3.3-7.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-24/#enterprise-linux-7", 
            "text": "cvmfs-x509-helper-1.0-1.osg33.el7  gratia-probe-1.17.5-1.osg33.el7  lcmaps-1.6.6-1.3.osg33.el7  osg-build-1.9.0-2.osg33.el7  osg-configure-1.7.0-1.osg33.el7  osg-version-3.3.24-1.osg33.el7  osg-wn-client-3.3-7.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-24/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  cvmfs-x509-helper cvmfs-x509-helper-debuginfo gratia-probe-bdii-status gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glexec gratia-probe-glideinwms gratia-probe-gram gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer lcmaps lcmaps-common-devel lcmaps-db-templates lcmaps-debuginfo lcmaps-devel lcmaps-without-gsi lcmaps-without-gsi-devel osg-build osg-build-base osg-build-koji osg-build-mock osg-build-tests osg-configure osg-configure-bosco osg-configure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-version osg-wn-client osg-wn-client-glexec  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-24/#enterprise-linux-6_1", 
            "text": "cvmfs-x509-helper-1.0-1.osg33.el6\ncvmfs-x509-helper-debuginfo-1.0-1.osg33.el6\ngratia-probe-1.17.5-1.osg33.el6\ngratia-probe-bdii-status-1.17.5-1.osg33.el6\ngratia-probe-common-1.17.5-1.osg33.el6\ngratia-probe-condor-1.17.5-1.osg33.el6\ngratia-probe-condor-events-1.17.5-1.osg33.el6\ngratia-probe-dcache-storage-1.17.5-1.osg33.el6\ngratia-probe-dcache-storagegroup-1.17.5-1.osg33.el6\ngratia-probe-dcache-transfer-1.17.5-1.osg33.el6\ngratia-probe-debuginfo-1.17.5-1.osg33.el6\ngratia-probe-enstore-storage-1.17.5-1.osg33.el6\ngratia-probe-enstore-tapedrive-1.17.5-1.osg33.el6\ngratia-probe-enstore-transfer-1.17.5-1.osg33.el6\ngratia-probe-glexec-1.17.5-1.osg33.el6\ngratia-probe-glideinwms-1.17.5-1.osg33.el6\ngratia-probe-gram-1.17.5-1.osg33.el6\ngratia-probe-gridftp-transfer-1.17.5-1.osg33.el6\ngratia-probe-hadoop-storage-1.17.5-1.osg33.el6\ngratia-probe-htcondor-ce-1.17.5-1.osg33.el6\ngratia-probe-lsf-1.17.5-1.osg33.el6\ngratia-probe-metric-1.17.5-1.osg33.el6\ngratia-probe-onevm-1.17.5-1.osg33.el6\ngratia-probe-pbs-lsf-1.17.5-1.osg33.el6\ngratia-probe-services-1.17.5-1.osg33.el6\ngratia-probe-sge-1.17.5-1.osg33.el6\ngratia-probe-slurm-1.17.5-1.osg33.el6\ngratia-probe-xrootd-storage-1.17.5-1.osg33.el6\ngratia-probe-xrootd-transfer-1.17.5-1.osg33.el6\nlcmaps-1.6.6-1.3.osg33.el6\nlcmaps-common-devel-1.6.6-1.3.osg33.el6\nlcmaps-db-templates-1.6.6-1.3.osg33.el6\nlcmaps-debuginfo-1.6.6-1.3.osg33.el6\nlcmaps-devel-1.6.6-1.3.osg33.el6\nlcmaps-without-gsi-1.6.6-1.3.osg33.el6\nlcmaps-without-gsi-devel-1.6.6-1.3.osg33.el6\nosg-build-1.9.0-2.osg33.el6\nosg-build-base-1.9.0-2.osg33.el6\nosg-build-koji-1.9.0-2.osg33.el6\nosg-build-mock-1.9.0-2.osg33.el6\nosg-build-tests-1.9.0-2.osg33.el6\nosg-configure-1.7.0-1.osg33.el6\nosg-configure-bosco-1.7.0-1.osg33.el6\nosg-configure-ce-1.7.0-1.osg33.el6\nosg-configure-cemon-1.7.0-1.osg33.el6\nosg-configure-condor-1.7.0-1.osg33.el6\nosg-configure-gateway-1.7.0-1.osg33.el6\nosg-configure-gip-1.7.0-1.osg33.el6\nosg-configure-gratia-1.7.0-1.osg33.el6\nosg-configure-infoservices-1.7.0-1.osg33.el6\nosg-configure-lsf-1.7.0-1.osg33.el6\nosg-configure-managedfork-1.7.0-1.osg33.el6\nosg-configure-misc-1.7.0-1.osg33.el6\nosg-configure-monalisa-1.7.0-1.osg33.el6\nosg-configure-network-1.7.0-1.osg33.el6\nosg-configure-pbs-1.7.0-1.osg33.el6\nosg-configure-rsv-1.7.0-1.osg33.el6\nosg-configure-sge-1.7.0-1.osg33.el6\nosg-configure-slurm-1.7.0-1.osg33.el6\nosg-configure-squid-1.7.0-1.osg33.el6\nosg-configure-tests-1.7.0-1.osg33.el6\nosg-version-3.3.24-1.osg33.el6\nosg-wn-client-3.3-7.osg33.el6\nosg-wn-client-glexec-3.3-7.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-24/#enterprise-linux-7_1", 
            "text": "cvmfs-x509-helper-1.0-1.osg33.el7\ncvmfs-x509-helper-debuginfo-1.0-1.osg33.el7\ngratia-probe-1.17.5-1.osg33.el7\ngratia-probe-bdii-status-1.17.5-1.osg33.el7\ngratia-probe-common-1.17.5-1.osg33.el7\ngratia-probe-condor-1.17.5-1.osg33.el7\ngratia-probe-condor-events-1.17.5-1.osg33.el7\ngratia-probe-dcache-storage-1.17.5-1.osg33.el7\ngratia-probe-dcache-storagegroup-1.17.5-1.osg33.el7\ngratia-probe-dcache-transfer-1.17.5-1.osg33.el7\ngratia-probe-debuginfo-1.17.5-1.osg33.el7\ngratia-probe-enstore-storage-1.17.5-1.osg33.el7\ngratia-probe-enstore-tapedrive-1.17.5-1.osg33.el7\ngratia-probe-enstore-transfer-1.17.5-1.osg33.el7\ngratia-probe-glexec-1.17.5-1.osg33.el7\ngratia-probe-glideinwms-1.17.5-1.osg33.el7\ngratia-probe-gram-1.17.5-1.osg33.el7\ngratia-probe-gridftp-transfer-1.17.5-1.osg33.el7\ngratia-probe-hadoop-storage-1.17.5-1.osg33.el7\ngratia-probe-htcondor-ce-1.17.5-1.osg33.el7\ngratia-probe-lsf-1.17.5-1.osg33.el7\ngratia-probe-metric-1.17.5-1.osg33.el7\ngratia-probe-onevm-1.17.5-1.osg33.el7\ngratia-probe-pbs-lsf-1.17.5-1.osg33.el7\ngratia-probe-services-1.17.5-1.osg33.el7\ngratia-probe-sge-1.17.5-1.osg33.el7\ngratia-probe-slurm-1.17.5-1.osg33.el7\ngratia-probe-xrootd-storage-1.17.5-1.osg33.el7\ngratia-probe-xrootd-transfer-1.17.5-1.osg33.el7\nlcmaps-1.6.6-1.3.osg33.el7\nlcmaps-common-devel-1.6.6-1.3.osg33.el7\nlcmaps-db-templates-1.6.6-1.3.osg33.el7\nlcmaps-debuginfo-1.6.6-1.3.osg33.el7\nlcmaps-devel-1.6.6-1.3.osg33.el7\nlcmaps-without-gsi-1.6.6-1.3.osg33.el7\nlcmaps-without-gsi-devel-1.6.6-1.3.osg33.el7\nosg-build-1.9.0-2.osg33.el7\nosg-build-base-1.9.0-2.osg33.el7\nosg-build-koji-1.9.0-2.osg33.el7\nosg-build-mock-1.9.0-2.osg33.el7\nosg-build-tests-1.9.0-2.osg33.el7\nosg-configure-1.7.0-1.osg33.el7\nosg-configure-bosco-1.7.0-1.osg33.el7\nosg-configure-ce-1.7.0-1.osg33.el7\nosg-configure-cemon-1.7.0-1.osg33.el7\nosg-configure-condor-1.7.0-1.osg33.el7\nosg-configure-gateway-1.7.0-1.osg33.el7\nosg-configure-gip-1.7.0-1.osg33.el7\nosg-configure-gratia-1.7.0-1.osg33.el7\nosg-configure-infoservices-1.7.0-1.osg33.el7\nosg-configure-lsf-1.7.0-1.osg33.el7\nosg-configure-managedfork-1.7.0-1.osg33.el7\nosg-configure-misc-1.7.0-1.osg33.el7\nosg-configure-monalisa-1.7.0-1.osg33.el7\nosg-configure-network-1.7.0-1.osg33.el7\nosg-configure-pbs-1.7.0-1.osg33.el7\nosg-configure-rsv-1.7.0-1.osg33.el7\nosg-configure-sge-1.7.0-1.osg33.el7\nosg-configure-slurm-1.7.0-1.osg33.el7\nosg-configure-squid-1.7.0-1.osg33.el7\nosg-configure-tests-1.7.0-1.osg33.el7\nosg-version-3.3.24-1.osg33.el7\nosg-wn-client-3.3-7.osg33.el7\nosg-wn-client-glexec-3.3-7.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-24/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-24/#enterprise-linux-6_2", 
            "text": "condor-8.6.2-1.osgup.el6  glideinwms-3.3.2-2.osgup.el6  lcmaps-1.6.6-1.4.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-24/#enterprise-linux-7_2", 
            "text": "condor-8.6.2-1.osgup.el7  glideinwms-3.3.2-2.osgup.el7  lcmaps-1.6.6-1.4.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-24/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone lcmaps lcmaps-common-devel lcmaps-db-templates lcmaps-debuginfo lcmaps-devel lcmaps-without-gsi lcmaps-without-gsi-devel  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-24/#enterprise-linux-6_3", 
            "text": "condor-8.6.2-1.osgup.el6\ncondor-all-8.6.2-1.osgup.el6\ncondor-bosco-8.6.2-1.osgup.el6\ncondor-classads-8.6.2-1.osgup.el6\ncondor-classads-devel-8.6.2-1.osgup.el6\ncondor-cream-gahp-8.6.2-1.osgup.el6\ncondor-debuginfo-8.6.2-1.osgup.el6\ncondor-kbdd-8.6.2-1.osgup.el6\ncondor-procd-8.6.2-1.osgup.el6\ncondor-python-8.6.2-1.osgup.el6\ncondor-std-universe-8.6.2-1.osgup.el6\ncondor-test-8.6.2-1.osgup.el6\ncondor-vm-gahp-8.6.2-1.osgup.el6\nglideinwms-3.3.2-2.osgup.el6\nglideinwms-common-tools-3.3.2-2.osgup.el6\nglideinwms-condor-common-config-3.3.2-2.osgup.el6\nglideinwms-factory-3.3.2-2.osgup.el6\nglideinwms-factory-condor-3.3.2-2.osgup.el6\nglideinwms-glidecondor-tools-3.3.2-2.osgup.el6\nglideinwms-libs-3.3.2-2.osgup.el6\nglideinwms-minimal-condor-3.3.2-2.osgup.el6\nglideinwms-usercollector-3.3.2-2.osgup.el6\nglideinwms-userschedd-3.3.2-2.osgup.el6\nglideinwms-vofrontend-3.3.2-2.osgup.el6\nglideinwms-vofrontend-standalone-3.3.2-2.osgup.el6\nlcmaps-1.6.6-1.4.osgup.el6\nlcmaps-common-devel-1.6.6-1.4.osgup.el6\nlcmaps-db-templates-1.6.6-1.4.osgup.el6\nlcmaps-debuginfo-1.6.6-1.4.osgup.el6\nlcmaps-devel-1.6.6-1.4.osgup.el6\nlcmaps-without-gsi-1.6.6-1.4.osgup.el6\nlcmaps-without-gsi-devel-1.6.6-1.4.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-24/#enterprise-linux-7_3", 
            "text": "condor-8.6.2-1.osgup.el7\ncondor-all-8.6.2-1.osgup.el7\ncondor-bosco-8.6.2-1.osgup.el7\ncondor-classads-8.6.2-1.osgup.el7\ncondor-classads-devel-8.6.2-1.osgup.el7\ncondor-cream-gahp-8.6.2-1.osgup.el7\ncondor-debuginfo-8.6.2-1.osgup.el7\ncondor-kbdd-8.6.2-1.osgup.el7\ncondor-procd-8.6.2-1.osgup.el7\ncondor-python-8.6.2-1.osgup.el7\ncondor-test-8.6.2-1.osgup.el7\ncondor-vm-gahp-8.6.2-1.osgup.el7\nglideinwms-3.3.2-2.osgup.el7\nglideinwms-common-tools-3.3.2-2.osgup.el7\nglideinwms-condor-common-config-3.3.2-2.osgup.el7\nglideinwms-factory-3.3.2-2.osgup.el7\nglideinwms-factory-condor-3.3.2-2.osgup.el7\nglideinwms-glidecondor-tools-3.3.2-2.osgup.el7\nglideinwms-libs-3.3.2-2.osgup.el7\nglideinwms-minimal-condor-3.3.2-2.osgup.el7\nglideinwms-usercollector-3.3.2-2.osgup.el7\nglideinwms-userschedd-3.3.2-2.osgup.el7\nglideinwms-vofrontend-3.3.2-2.osgup.el7\nglideinwms-vofrontend-standalone-3.3.2-2.osgup.el7\nlcmaps-1.6.6-1.4.osgup.el7\nlcmaps-common-devel-1.6.6-1.4.osgup.el7\nlcmaps-db-templates-1.6.6-1.4.osgup.el7\nlcmaps-debuginfo-1.6.6-1.4.osgup.el7\nlcmaps-devel-1.6.6-1.4.osgup.el7\nlcmaps-without-gsi-1.6.6-1.4.osgup.el7\nlcmaps-without-gsi-devel-1.6.6-1.4.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-23/", 
            "text": "OSG Software Release 3.3.23\n\n\nRelease Date\n: 2017-04-11\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nOSG 3.3.23\n\n\nLCMAPS VOMS plugin: Use VOMS attributes to map users\n\n\nHTCondor-CE 2.1.5\n: LCMAPS VOMS integration, package Slurm configuration\n\n\nCVMFS 2.3.5\n: Fixes, including automount fix when autofs restarts on EL7\n\n\nPegasus 4.7.4\n: \nUpdate from version 4.6.1\n\n\nOSG-CE 3.3-12: Removed gip and osg-info-services, see note below\n\n\n\n\n\n\nUpcoming repository\n\n\nLCMAPS 1.6.6-1.3: Enable VOMS attribute checking by default\n\n\nFrontier squid 3.5.24-3.1\n: Fix for some crashes under heavy load\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n After updating OSG-CE to version 3.3-12, please disable and remove OSG Info Services via the following procedure:\n\n\nroot@host #\n service osg-info-services stop\n\nroot@host #\n yum erase gip osg-info-services\n\n\n\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nKnown Issues\n\n\n\n\nThe Koji client config has changed in the new version of Koji: `pkgurl\nhttp://koji.chtc.wisc.edu/packages` has been replaced by `topurl=http://koji.chtc.wisc.edu` and the Koji client will give a harmless but annoying warning when it finds `pkgurl`. To get rid of the warning, update to osg-build \n 1.8.0, rerun `osg-koji setup`, and say 'yes' when asked to replace the Koji configuration file; or, you may make the above change manually.\n\n\nA previous version (\n1.17.0-2.5\n) of the Gratia probes required changes in the HTCondor configuration to operate properly. Now, these changes need to be reverted to use verison \n1.17.0-2.6\n and later of the probes. The lines below are likely to be found in \ncondor_config.local\n or a new file in \n/etc/condor/config.d\n directory. Delete these lines and run \ncondor_reconfig\n.\n\n\n\n\n# GlideinWMS Gratia commands\n\n\nPER_JOB_HISTORY_DIR\n \n=\n /var/lib/gratia/data\n\nJOBGLIDEIN_ResourceName\n=\n$$\n([IfThenElse(IsUndefined(TARGET.GLIDEIN_ResourceName), IfThenElse(IsUndefined(TARGET.GLIDEIN_Site), IfThenElse(IsUndefined(TARGET.FileSystemDomain), \\\nLocal Job\\\n, TARGET.FileSystemDomain), TARGET.GLIDEIN_Site), TARGET.GLIDEIN_ResourceName)])\n\n\nSUBMIT_EXPRS\n \n=\n \n$(\nSUBMIT_EXPRS\n)\n JOBGLIDEIN_ResourceName   \n\n\n\n\n\n\n\nOn EL7, your version of voms-proxy-init may come from the voms-clients-java package; this version will continue to make legacy proxies by default. If you want the new behavior, install the voms-clients-cpp package and uninstall the voms-clients-java package.\n\n\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\ncvmfs-2.3.5-1.osg33.el6\n\n\nhtcondor-ce-2.1.5-1.osg33.el6\n\n\nlcmaps-1.6.6-1.2.osg33.el6\n\n\nlcmaps-plugins-verify-proxy-1.5.9-1.1.osg33.el6\n\n\nlcmaps-plugins-voms-1.7.1-1.2.osg33.el6\n\n\nosg-build-1.8.1-1.osg33.el6\n\n\nosg-ce-3.3-12.osg33.el6\n\n\nosg-configure-1.6.2-1.osg33.el6\n\n\nosg-oasis-7-9.osg33.el6\n\n\nosg-version-3.3.23-1.osg33.el6\n\n\npegasus-4.7.4-1.1.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\ncvmfs-2.3.5-1.osg33.el7\n\n\nhtcondor-ce-2.1.5-1.osg33.el7\n\n\nlcmaps-1.6.6-1.2.osg33.el7\n\n\nlcmaps-plugins-verify-proxy-1.5.9-1.1.osg33.el7\n\n\nlcmaps-plugins-voms-1.7.1-1.2.osg33.el7\n\n\nosg-build-1.8.1-1.osg33.el7\n\n\nosg-ce-3.3-12.osg33.el7\n\n\nosg-configure-1.6.2-1.osg33.el7\n\n\nosg-oasis-7-9.osg33.el7\n\n\nosg-version-3.3.23-1.osg33.el7\n\n\npegasus-4.7.4-1.1.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\ncvmfs cvmfs-devel cvmfs-server cvmfs-unittests htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-slurm htcondor-ce-view igtf-ca-certs lcmaps lcmaps-common-devel lcmaps-debuginfo lcmaps-devel lcmaps-plugins-verify-proxy lcmaps-plugins-verify-proxy-debuginfo lcmaps-plugins-voms lcmaps-plugins-voms-debuginfo lcmaps-without-gsi lcmaps-without-gsi-devel osg-base-ce osg-base-ce-bosco osg-base-ce-condor osg-base-ce-lsf osg-base-ce-pbs osg-base-ce-sge osg-base-ce-slurm osg-build osg-ca-certs osg-ce osg-ce-bosco osg-ce-condor osg-ce-lsf osg-ce-pbs osg-ce-sge osg-ce-slurm osg-configure osg-configure-bosco osg-configure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-gums-config osg-htcondor-ce osg-htcondor-ce-bosco osg-htcondor-ce-condor osg-htcondor-ce-lsf osg-htcondor-ce-pbs osg-htcondor-ce-sge osg-htcondor-ce-slurm osg-oasis osg-version pegasus pegasus-debuginfo vo-client vo-client-edgmkgridmap vo-client-lcmaps-voms xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-libs xrootd-private-devel xrootd-python xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\ncvmfs-2.3.5-1.osg33.el7\ncvmfs-devel-2.3.5-1.osg33.el7\ncvmfs-server-2.3.5-1.osg33.el7\ncvmfs-unittests-2.3.5-1.osg33.el7\nhtcondor-ce-2.1.5-1.osg33.el7\nhtcondor-ce-bosco-2.1.5-1.osg33.el7\nhtcondor-ce-client-2.1.5-1.osg33.el7\nhtcondor-ce-collector-2.1.5-1.osg33.el7\nhtcondor-ce-condor-2.1.5-1.osg33.el7\nhtcondor-ce-lsf-2.1.5-1.osg33.el7\nhtcondor-ce-pbs-2.1.5-1.osg33.el7\nhtcondor-ce-sge-2.1.5-1.osg33.el7\nhtcondor-ce-slurm-2.1.5-1.osg33.el7\nhtcondor-ce-view-2.1.5-1.osg33.el7\nlcmaps-1.6.6-1.2.osg33.el7\nlcmaps-common-devel-1.6.6-1.2.osg33.el7\nlcmaps-debuginfo-1.6.6-1.2.osg33.el7\nlcmaps-devel-1.6.6-1.2.osg33.el7\nlcmaps-plugins-verify-proxy-1.5.9-1.1.osg33.el7\nlcmaps-plugins-verify-proxy-debuginfo-1.5.9-1.1.osg33.el7\nlcmaps-plugins-voms-1.7.1-1.2.osg33.el7\nlcmaps-plugins-voms-debuginfo-1.7.1-1.2.osg33.el7\nlcmaps-without-gsi-1.6.6-1.2.osg33.el7\nlcmaps-without-gsi-devel-1.6.6-1.2.osg33.el7\nosg-base-ce-3.3-12.osg33.el7\nosg-base-ce-bosco-3.3-12.osg33.el7\nosg-base-ce-condor-3.3-12.osg33.el7\nosg-base-ce-lsf-3.3-12.osg33.el7\nosg-base-ce-pbs-3.3-12.osg33.el7\nosg-base-ce-sge-3.3-12.osg33.el7\nosg-base-ce-slurm-3.3-12.osg33.el7\nosg-build-1.8.1-1.osg33.el7\nosg-ce-3.3-12.osg33.el7\nosg-ce-bosco-3.3-12.osg33.el7\nosg-ce-condor-3.3-12.osg33.el7\nosg-ce-lsf-3.3-12.osg33.el7\nosg-ce-pbs-3.3-12.osg33.el7\nosg-ce-sge-3.3-12.osg33.el7\nosg-ce-slurm-3.3-12.osg33.el7\nosg-configure-1.6.2-1.osg33.el7\nosg-configure-bosco-1.6.2-1.osg33.el7\nosg-configure-ce-1.6.2-1.osg33.el7\nosg-configure-cemon-1.6.2-1.osg33.el7\nosg-configure-condor-1.6.2-1.osg33.el7\nosg-configure-gateway-1.6.2-1.osg33.el7\nosg-configure-gip-1.6.2-1.osg33.el7\nosg-configure-gratia-1.6.2-1.osg33.el7\nosg-configure-infoservices-1.6.2-1.osg33.el7\nosg-configure-lsf-1.6.2-1.osg33.el7\nosg-configure-managedfork-1.6.2-1.osg33.el7\nosg-configure-misc-1.6.2-1.osg33.el7\nosg-configure-monalisa-1.6.2-1.osg33.el7\nosg-configure-network-1.6.2-1.osg33.el7\nosg-configure-pbs-1.6.2-1.osg33.el7\nosg-configure-rsv-1.6.2-1.osg33.el7\nosg-configure-sge-1.6.2-1.osg33.el7\nosg-configure-slurm-1.6.2-1.osg33.el7\nosg-configure-squid-1.6.2-1.osg33.el7\nosg-configure-tests-1.6.2-1.osg33.el7\nosg-htcondor-ce-3.3-12.osg33.el7\nosg-htcondor-ce-bosco-3.3-12.osg33.el7\nosg-htcondor-ce-condor-3.3-12.osg33.el7\nosg-htcondor-ce-lsf-3.3-12.osg33.el7\nosg-htcondor-ce-pbs-3.3-12.osg33.el7\nosg-htcondor-ce-sge-3.3-12.osg33.el7\nosg-htcondor-ce-slurm-3.3-12.osg33.el7\nosg-oasis-7-9.osg33.el7\nosg-version-3.3.23-1.osg33.el7\npegasus-4.7.4-1.1.osg33.el7\npegasus-debuginfo-4.7.4-1.1.osg33.el7\n\n\n\n\n\nEnterprise Linux 7\n\n\ncvmfs-2.3.5-1.osg33.el7\ncvmfs-devel-2.3.5-1.osg33.el7\ncvmfs-server-2.3.5-1.osg33.el7\ncvmfs-unittests-2.3.5-1.osg33.el7\nhtcondor-ce-2.1.5-1.osg33.el7\nhtcondor-ce-bosco-2.1.5-1.osg33.el7\nhtcondor-ce-client-2.1.5-1.osg33.el7\nhtcondor-ce-collector-2.1.5-1.osg33.el7\nhtcondor-ce-condor-2.1.5-1.osg33.el7\nhtcondor-ce-lsf-2.1.5-1.osg33.el7\nhtcondor-ce-pbs-2.1.5-1.osg33.el7\nhtcondor-ce-sge-2.1.5-1.osg33.el7\nhtcondor-ce-slurm-2.1.5-1.osg33.el7\nhtcondor-ce-view-2.1.5-1.osg33.el7\nlcmaps-1.6.6-1.2.osg33.el7\nlcmaps-common-devel-1.6.6-1.2.osg33.el7\nlcmaps-debuginfo-1.6.6-1.2.osg33.el7\nlcmaps-devel-1.6.6-1.2.osg33.el7\nlcmaps-plugins-verify-proxy-1.5.9-1.1.osg33.el7\nlcmaps-plugins-verify-proxy-debuginfo-1.5.9-1.1.osg33.el7\nlcmaps-plugins-voms-1.7.1-1.2.osg33.el7\nlcmaps-plugins-voms-debuginfo-1.7.1-1.2.osg33.el7\nlcmaps-without-gsi-1.6.6-1.2.osg33.el7\nlcmaps-without-gsi-devel-1.6.6-1.2.osg33.el7\nosg-base-ce-3.3-12.osg33.el7\nosg-base-ce-bosco-3.3-12.osg33.el7\nosg-base-ce-condor-3.3-12.osg33.el7\nosg-base-ce-lsf-3.3-12.osg33.el7\nosg-base-ce-pbs-3.3-12.osg33.el7\nosg-base-ce-sge-3.3-12.osg33.el7\nosg-base-ce-slurm-3.3-12.osg33.el7\nosg-build-1.8.1-1.osg33.el7\nosg-ce-3.3-12.osg33.el7\nosg-ce-bosco-3.3-12.osg33.el7\nosg-ce-condor-3.3-12.osg33.el7\nosg-ce-lsf-3.3-12.osg33.el7\nosg-ce-pbs-3.3-12.osg33.el7\nosg-ce-sge-3.3-12.osg33.el7\nosg-ce-slurm-3.3-12.osg33.el7\nosg-configure-1.6.2-1.osg33.el7\nosg-configure-bosco-1.6.2-1.osg33.el7\nosg-configure-ce-1.6.2-1.osg33.el7\nosg-configure-cemon-1.6.2-1.osg33.el7\nosg-configure-condor-1.6.2-1.osg33.el7\nosg-configure-gateway-1.6.2-1.osg33.el7\nosg-configure-gip-1.6.2-1.osg33.el7\nosg-configure-gratia-1.6.2-1.osg33.el7\nosg-configure-infoservices-1.6.2-1.osg33.el7\nosg-configure-lsf-1.6.2-1.osg33.el7\nosg-configure-managedfork-1.6.2-1.osg33.el7\nosg-configure-misc-1.6.2-1.osg33.el7\nosg-configure-monalisa-1.6.2-1.osg33.el7\nosg-configure-network-1.6.2-1.osg33.el7\nosg-configure-pbs-1.6.2-1.osg33.el7\nosg-configure-rsv-1.6.2-1.osg33.el7\nosg-configure-sge-1.6.2-1.osg33.el7\nosg-configure-slurm-1.6.2-1.osg33.el7\nosg-configure-squid-1.6.2-1.osg33.el7\nosg-configure-tests-1.6.2-1.osg33.el7\nosg-htcondor-ce-3.3-12.osg33.el7\nosg-htcondor-ce-bosco-3.3-12.osg33.el7\nosg-htcondor-ce-condor-3.3-12.osg33.el7\nosg-htcondor-ce-lsf-3.3-12.osg33.el7\nosg-htcondor-ce-pbs-3.3-12.osg33.el7\nosg-htcondor-ce-sge-3.3-12.osg33.el7\nosg-htcondor-ce-slurm-3.3-12.osg33.el7\nosg-oasis-7-9.osg33.el7\nosg-version-3.3.23-1.osg33.el7\npegasus-4.7.4-1.1.osg33.el7\npegasus-debuginfo-4.7.4-1.1.osg33.el7\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nfrontier-squid-3.5.24-3.1.osgup.el6\n\n\nlcmaps-1.6.6-1.3.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nfrontier-squid-3.5.24-3.1.osgup.el7\n\n\nlcmaps-1.6.6-1.3.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nfrontier-squid frontier-squid-debuginfo lcmaps lcmaps-common-devel lcmaps-debuginfo lcmaps-devel lcmaps-without-gsi lcmaps-without-gsi-devel\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nfrontier-squid-3.5.24-3.1.osgup.el6\nfrontier-squid-debuginfo-3.5.24-3.1.osgup.el6\nlcmaps-1.6.6-1.3.osgup.el6\nlcmaps-common-devel-1.6.6-1.3.osgup.el6\nlcmaps-debuginfo-1.6.6-1.3.osgup.el6\nlcmaps-devel-1.6.6-1.3.osgup.el6\nlcmaps-without-gsi-1.6.6-1.3.osgup.el6\nlcmaps-without-gsi-devel-1.6.6-1.3.osgup.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nfrontier-squid-3.5.24-3.1.osgup.el7\nfrontier-squid-debuginfo-3.5.24-3.1.osgup.el7\nlcmaps-1.6.6-1.3.osgup.el7\nlcmaps-common-devel-1.6.6-1.3.osgup.el7\nlcmaps-debuginfo-1.6.6-1.3.osgup.el7\nlcmaps-devel-1.6.6-1.3.osgup.el7\nlcmaps-without-gsi-1.6.6-1.3.osgup.el7\nlcmaps-without-gsi-devel-1.6.6-1.3.osgup.el7", 
            "title": "OSG Release 3.3.23"
        }, 
        {
            "location": "/release/3.3/release-3-3-23/#osg-software-release-3323", 
            "text": "Release Date : 2017-04-11", 
            "title": "OSG Software Release 3.3.23"
        }, 
        {
            "location": "/release/3.3/release-3-3-23/#summary-of-changes", 
            "text": "This release contains:   OSG 3.3.23  LCMAPS VOMS plugin: Use VOMS attributes to map users  HTCondor-CE 2.1.5 : LCMAPS VOMS integration, package Slurm configuration  CVMFS 2.3.5 : Fixes, including automount fix when autofs restarts on EL7  Pegasus 4.7.4 :  Update from version 4.6.1  OSG-CE 3.3-12: Removed gip and osg-info-services, see note below    Upcoming repository  LCMAPS 1.6.6-1.3: Enable VOMS attribute checking by default  Frontier squid 3.5.24-3.1 : Fix for some crashes under heavy load     These  JIRA tickets  were addressed in this release.   After updating OSG-CE to version 3.3-12, please disable and remove OSG Info Services via the following procedure:  root@host #  service osg-info-services stop root@host #  yum erase gip osg-info-services  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-23/#known-issues", 
            "text": "The Koji client config has changed in the new version of Koji: `pkgurl http://koji.chtc.wisc.edu/packages` has been replaced by `topurl=http://koji.chtc.wisc.edu` and the Koji client will give a harmless but annoying warning when it finds `pkgurl`. To get rid of the warning, update to osg-build   1.8.0, rerun `osg-koji setup`, and say 'yes' when asked to replace the Koji configuration file; or, you may make the above change manually.  A previous version ( 1.17.0-2.5 ) of the Gratia probes required changes in the HTCondor configuration to operate properly. Now, these changes need to be reverted to use verison  1.17.0-2.6  and later of the probes. The lines below are likely to be found in  condor_config.local  or a new file in  /etc/condor/config.d  directory. Delete these lines and run  condor_reconfig .   # GlideinWMS Gratia commands  PER_JOB_HISTORY_DIR   =  /var/lib/gratia/data JOBGLIDEIN_ResourceName = $$ ([IfThenElse(IsUndefined(TARGET.GLIDEIN_ResourceName), IfThenElse(IsUndefined(TARGET.GLIDEIN_Site), IfThenElse(IsUndefined(TARGET.FileSystemDomain), \\ Local Job\\ , TARGET.FileSystemDomain), TARGET.GLIDEIN_Site), TARGET.GLIDEIN_ResourceName)])  SUBMIT_EXPRS   =   $( SUBMIT_EXPRS )  JOBGLIDEIN_ResourceName      On EL7, your version of voms-proxy-init may come from the voms-clients-java package; this version will continue to make legacy proxies by default. If you want the new behavior, install the voms-clients-cpp package and uninstall the voms-clients-java package.", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.3/release-3-3-23/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-23/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-23/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-23/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-23/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-23/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-23/#enterprise-linux-6", 
            "text": "cvmfs-2.3.5-1.osg33.el6  htcondor-ce-2.1.5-1.osg33.el6  lcmaps-1.6.6-1.2.osg33.el6  lcmaps-plugins-verify-proxy-1.5.9-1.1.osg33.el6  lcmaps-plugins-voms-1.7.1-1.2.osg33.el6  osg-build-1.8.1-1.osg33.el6  osg-ce-3.3-12.osg33.el6  osg-configure-1.6.2-1.osg33.el6  osg-oasis-7-9.osg33.el6  osg-version-3.3.23-1.osg33.el6  pegasus-4.7.4-1.1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-23/#enterprise-linux-7", 
            "text": "cvmfs-2.3.5-1.osg33.el7  htcondor-ce-2.1.5-1.osg33.el7  lcmaps-1.6.6-1.2.osg33.el7  lcmaps-plugins-verify-proxy-1.5.9-1.1.osg33.el7  lcmaps-plugins-voms-1.7.1-1.2.osg33.el7  osg-build-1.8.1-1.osg33.el7  osg-ce-3.3-12.osg33.el7  osg-configure-1.6.2-1.osg33.el7  osg-oasis-7-9.osg33.el7  osg-version-3.3.23-1.osg33.el7  pegasus-4.7.4-1.1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-23/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  cvmfs cvmfs-devel cvmfs-server cvmfs-unittests htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-slurm htcondor-ce-view igtf-ca-certs lcmaps lcmaps-common-devel lcmaps-debuginfo lcmaps-devel lcmaps-plugins-verify-proxy lcmaps-plugins-verify-proxy-debuginfo lcmaps-plugins-voms lcmaps-plugins-voms-debuginfo lcmaps-without-gsi lcmaps-without-gsi-devel osg-base-ce osg-base-ce-bosco osg-base-ce-condor osg-base-ce-lsf osg-base-ce-pbs osg-base-ce-sge osg-base-ce-slurm osg-build osg-ca-certs osg-ce osg-ce-bosco osg-ce-condor osg-ce-lsf osg-ce-pbs osg-ce-sge osg-ce-slurm osg-configure osg-configure-bosco osg-configure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-gums-config osg-htcondor-ce osg-htcondor-ce-bosco osg-htcondor-ce-condor osg-htcondor-ce-lsf osg-htcondor-ce-pbs osg-htcondor-ce-sge osg-htcondor-ce-slurm osg-oasis osg-version pegasus pegasus-debuginfo vo-client vo-client-edgmkgridmap vo-client-lcmaps-voms xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-libs xrootd-private-devel xrootd-python xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-23/#enterprise-linux-6_1", 
            "text": "cvmfs-2.3.5-1.osg33.el7\ncvmfs-devel-2.3.5-1.osg33.el7\ncvmfs-server-2.3.5-1.osg33.el7\ncvmfs-unittests-2.3.5-1.osg33.el7\nhtcondor-ce-2.1.5-1.osg33.el7\nhtcondor-ce-bosco-2.1.5-1.osg33.el7\nhtcondor-ce-client-2.1.5-1.osg33.el7\nhtcondor-ce-collector-2.1.5-1.osg33.el7\nhtcondor-ce-condor-2.1.5-1.osg33.el7\nhtcondor-ce-lsf-2.1.5-1.osg33.el7\nhtcondor-ce-pbs-2.1.5-1.osg33.el7\nhtcondor-ce-sge-2.1.5-1.osg33.el7\nhtcondor-ce-slurm-2.1.5-1.osg33.el7\nhtcondor-ce-view-2.1.5-1.osg33.el7\nlcmaps-1.6.6-1.2.osg33.el7\nlcmaps-common-devel-1.6.6-1.2.osg33.el7\nlcmaps-debuginfo-1.6.6-1.2.osg33.el7\nlcmaps-devel-1.6.6-1.2.osg33.el7\nlcmaps-plugins-verify-proxy-1.5.9-1.1.osg33.el7\nlcmaps-plugins-verify-proxy-debuginfo-1.5.9-1.1.osg33.el7\nlcmaps-plugins-voms-1.7.1-1.2.osg33.el7\nlcmaps-plugins-voms-debuginfo-1.7.1-1.2.osg33.el7\nlcmaps-without-gsi-1.6.6-1.2.osg33.el7\nlcmaps-without-gsi-devel-1.6.6-1.2.osg33.el7\nosg-base-ce-3.3-12.osg33.el7\nosg-base-ce-bosco-3.3-12.osg33.el7\nosg-base-ce-condor-3.3-12.osg33.el7\nosg-base-ce-lsf-3.3-12.osg33.el7\nosg-base-ce-pbs-3.3-12.osg33.el7\nosg-base-ce-sge-3.3-12.osg33.el7\nosg-base-ce-slurm-3.3-12.osg33.el7\nosg-build-1.8.1-1.osg33.el7\nosg-ce-3.3-12.osg33.el7\nosg-ce-bosco-3.3-12.osg33.el7\nosg-ce-condor-3.3-12.osg33.el7\nosg-ce-lsf-3.3-12.osg33.el7\nosg-ce-pbs-3.3-12.osg33.el7\nosg-ce-sge-3.3-12.osg33.el7\nosg-ce-slurm-3.3-12.osg33.el7\nosg-configure-1.6.2-1.osg33.el7\nosg-configure-bosco-1.6.2-1.osg33.el7\nosg-configure-ce-1.6.2-1.osg33.el7\nosg-configure-cemon-1.6.2-1.osg33.el7\nosg-configure-condor-1.6.2-1.osg33.el7\nosg-configure-gateway-1.6.2-1.osg33.el7\nosg-configure-gip-1.6.2-1.osg33.el7\nosg-configure-gratia-1.6.2-1.osg33.el7\nosg-configure-infoservices-1.6.2-1.osg33.el7\nosg-configure-lsf-1.6.2-1.osg33.el7\nosg-configure-managedfork-1.6.2-1.osg33.el7\nosg-configure-misc-1.6.2-1.osg33.el7\nosg-configure-monalisa-1.6.2-1.osg33.el7\nosg-configure-network-1.6.2-1.osg33.el7\nosg-configure-pbs-1.6.2-1.osg33.el7\nosg-configure-rsv-1.6.2-1.osg33.el7\nosg-configure-sge-1.6.2-1.osg33.el7\nosg-configure-slurm-1.6.2-1.osg33.el7\nosg-configure-squid-1.6.2-1.osg33.el7\nosg-configure-tests-1.6.2-1.osg33.el7\nosg-htcondor-ce-3.3-12.osg33.el7\nosg-htcondor-ce-bosco-3.3-12.osg33.el7\nosg-htcondor-ce-condor-3.3-12.osg33.el7\nosg-htcondor-ce-lsf-3.3-12.osg33.el7\nosg-htcondor-ce-pbs-3.3-12.osg33.el7\nosg-htcondor-ce-sge-3.3-12.osg33.el7\nosg-htcondor-ce-slurm-3.3-12.osg33.el7\nosg-oasis-7-9.osg33.el7\nosg-version-3.3.23-1.osg33.el7\npegasus-4.7.4-1.1.osg33.el7\npegasus-debuginfo-4.7.4-1.1.osg33.el7", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-23/#enterprise-linux-7_1", 
            "text": "cvmfs-2.3.5-1.osg33.el7\ncvmfs-devel-2.3.5-1.osg33.el7\ncvmfs-server-2.3.5-1.osg33.el7\ncvmfs-unittests-2.3.5-1.osg33.el7\nhtcondor-ce-2.1.5-1.osg33.el7\nhtcondor-ce-bosco-2.1.5-1.osg33.el7\nhtcondor-ce-client-2.1.5-1.osg33.el7\nhtcondor-ce-collector-2.1.5-1.osg33.el7\nhtcondor-ce-condor-2.1.5-1.osg33.el7\nhtcondor-ce-lsf-2.1.5-1.osg33.el7\nhtcondor-ce-pbs-2.1.5-1.osg33.el7\nhtcondor-ce-sge-2.1.5-1.osg33.el7\nhtcondor-ce-slurm-2.1.5-1.osg33.el7\nhtcondor-ce-view-2.1.5-1.osg33.el7\nlcmaps-1.6.6-1.2.osg33.el7\nlcmaps-common-devel-1.6.6-1.2.osg33.el7\nlcmaps-debuginfo-1.6.6-1.2.osg33.el7\nlcmaps-devel-1.6.6-1.2.osg33.el7\nlcmaps-plugins-verify-proxy-1.5.9-1.1.osg33.el7\nlcmaps-plugins-verify-proxy-debuginfo-1.5.9-1.1.osg33.el7\nlcmaps-plugins-voms-1.7.1-1.2.osg33.el7\nlcmaps-plugins-voms-debuginfo-1.7.1-1.2.osg33.el7\nlcmaps-without-gsi-1.6.6-1.2.osg33.el7\nlcmaps-without-gsi-devel-1.6.6-1.2.osg33.el7\nosg-base-ce-3.3-12.osg33.el7\nosg-base-ce-bosco-3.3-12.osg33.el7\nosg-base-ce-condor-3.3-12.osg33.el7\nosg-base-ce-lsf-3.3-12.osg33.el7\nosg-base-ce-pbs-3.3-12.osg33.el7\nosg-base-ce-sge-3.3-12.osg33.el7\nosg-base-ce-slurm-3.3-12.osg33.el7\nosg-build-1.8.1-1.osg33.el7\nosg-ce-3.3-12.osg33.el7\nosg-ce-bosco-3.3-12.osg33.el7\nosg-ce-condor-3.3-12.osg33.el7\nosg-ce-lsf-3.3-12.osg33.el7\nosg-ce-pbs-3.3-12.osg33.el7\nosg-ce-sge-3.3-12.osg33.el7\nosg-ce-slurm-3.3-12.osg33.el7\nosg-configure-1.6.2-1.osg33.el7\nosg-configure-bosco-1.6.2-1.osg33.el7\nosg-configure-ce-1.6.2-1.osg33.el7\nosg-configure-cemon-1.6.2-1.osg33.el7\nosg-configure-condor-1.6.2-1.osg33.el7\nosg-configure-gateway-1.6.2-1.osg33.el7\nosg-configure-gip-1.6.2-1.osg33.el7\nosg-configure-gratia-1.6.2-1.osg33.el7\nosg-configure-infoservices-1.6.2-1.osg33.el7\nosg-configure-lsf-1.6.2-1.osg33.el7\nosg-configure-managedfork-1.6.2-1.osg33.el7\nosg-configure-misc-1.6.2-1.osg33.el7\nosg-configure-monalisa-1.6.2-1.osg33.el7\nosg-configure-network-1.6.2-1.osg33.el7\nosg-configure-pbs-1.6.2-1.osg33.el7\nosg-configure-rsv-1.6.2-1.osg33.el7\nosg-configure-sge-1.6.2-1.osg33.el7\nosg-configure-slurm-1.6.2-1.osg33.el7\nosg-configure-squid-1.6.2-1.osg33.el7\nosg-configure-tests-1.6.2-1.osg33.el7\nosg-htcondor-ce-3.3-12.osg33.el7\nosg-htcondor-ce-bosco-3.3-12.osg33.el7\nosg-htcondor-ce-condor-3.3-12.osg33.el7\nosg-htcondor-ce-lsf-3.3-12.osg33.el7\nosg-htcondor-ce-pbs-3.3-12.osg33.el7\nosg-htcondor-ce-sge-3.3-12.osg33.el7\nosg-htcondor-ce-slurm-3.3-12.osg33.el7\nosg-oasis-7-9.osg33.el7\nosg-version-3.3.23-1.osg33.el7\npegasus-4.7.4-1.1.osg33.el7\npegasus-debuginfo-4.7.4-1.1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-23/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-23/#enterprise-linux-6_2", 
            "text": "frontier-squid-3.5.24-3.1.osgup.el6  lcmaps-1.6.6-1.3.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-23/#enterprise-linux-7_2", 
            "text": "frontier-squid-3.5.24-3.1.osgup.el7  lcmaps-1.6.6-1.3.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-23/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  frontier-squid frontier-squid-debuginfo lcmaps lcmaps-common-devel lcmaps-debuginfo lcmaps-devel lcmaps-without-gsi lcmaps-without-gsi-devel  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-23/#enterprise-linux-6_3", 
            "text": "frontier-squid-3.5.24-3.1.osgup.el6\nfrontier-squid-debuginfo-3.5.24-3.1.osgup.el6\nlcmaps-1.6.6-1.3.osgup.el6\nlcmaps-common-devel-1.6.6-1.3.osgup.el6\nlcmaps-debuginfo-1.6.6-1.3.osgup.el6\nlcmaps-devel-1.6.6-1.3.osgup.el6\nlcmaps-without-gsi-1.6.6-1.3.osgup.el6\nlcmaps-without-gsi-devel-1.6.6-1.3.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-23/#enterprise-linux-7_3", 
            "text": "frontier-squid-3.5.24-3.1.osgup.el7\nfrontier-squid-debuginfo-3.5.24-3.1.osgup.el7\nlcmaps-1.6.6-1.3.osgup.el7\nlcmaps-common-devel-1.6.6-1.3.osgup.el7\nlcmaps-debuginfo-1.6.6-1.3.osgup.el7\nlcmaps-devel-1.6.6-1.3.osgup.el7\nlcmaps-without-gsi-1.6.6-1.3.osgup.el7\nlcmaps-without-gsi-devel-1.6.6-1.3.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-22-3/", 
            "text": "OSG Software Stack -- Data Release -- 3.3.22-3\n\n\nRelease Date\n: 2017-04-10\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nCA Certificates based on \nIGTF 1.82\n\n\nAdded new G2 UGrid trust anchor (UA)\n\n\nExtended validity for AEGIS CA (RS)\n\n\nWithdrawn discontinued FNAL KCA (US)\n\n\nExtended validity for REUNA CA (CL)\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nigtf-ca-certs-1.82-1.osg33.el6\n\n\nosg-ca-certs-1.62-1.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nigtf-ca-certs-1.82-1.osg33.el7\n\n\nosg-ca-certs-1.62-1.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nigtf-ca-certs osg-ca-certs\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nigtf-ca-certs-1.82-1.osg33.el6\nosg-ca-certs-1.62-1.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nigtf-ca-certs-1.82-1.osg33.el7\nosg-ca-certs-1.62-1.osg33.el7", 
            "title": "OSG Release 3.3.22-3"
        }, 
        {
            "location": "/release/3.3/release-3-3-22-3/#osg-software-stack-data-release-3322-3", 
            "text": "Release Date : 2017-04-10", 
            "title": "OSG Software Stack -- Data Release -- 3.3.22-3"
        }, 
        {
            "location": "/release/3.3/release-3-3-22-3/#summary-of-changes", 
            "text": "This release contains:   CA Certificates based on  IGTF 1.82  Added new G2 UGrid trust anchor (UA)  Extended validity for AEGIS CA (RS)  Withdrawn discontinued FNAL KCA (US)  Extended validity for REUNA CA (CL)     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-22-3/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-22-3/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-22-3/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-22-3/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-22-3/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-22-3/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-22-3/#enterprise-linux-6", 
            "text": "igtf-ca-certs-1.82-1.osg33.el6  osg-ca-certs-1.62-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-22-3/#enterprise-linux-7", 
            "text": "igtf-ca-certs-1.82-1.osg33.el7  osg-ca-certs-1.62-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-22-3/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  igtf-ca-certs osg-ca-certs  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-22-3/#enterprise-linux-6_1", 
            "text": "igtf-ca-certs-1.82-1.osg33.el6\nosg-ca-certs-1.62-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-22-3/#enterprise-linux-7_1", 
            "text": "igtf-ca-certs-1.82-1.osg33.el7\nosg-ca-certs-1.62-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-22-2/", 
            "text": "OSG Software Stack -- Data Release -- 3.3.22-2\n\n\nRelease Date\n: 2017-03-28\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nVO Package v72\n\n\nAdded VOMS server to OSG VO: voms.grid.iu.edu\n\n\nRemoved CSIU, NYSGRID, and OSGEDU VOs\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nvo-client-72-2.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nvo-client-72-2.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nosg-gums-config vo-client vo-client-edgmkgridmap vo-client-lcmaps-voms xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-libs xrootd-private-devel xrootd-python xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nosg-gums-config-72-2.osg33.el6\nvo-client-72-2.osg33.el6\nvo-client-edgmkgridmap-72-2.osg33.el6\nvo-client-lcmaps-voms-72-2.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nosg-gums-config-72-2.osg33.el7\nvo-client-72-2.osg33.el7\nvo-client-edgmkgridmap-72-2.osg33.el7\nvo-client-lcmaps-voms-72-2.osg33.el7", 
            "title": "OSG Release 3.3.22-2"
        }, 
        {
            "location": "/release/3.3/release-3-3-22-2/#osg-software-stack-data-release-3322-2", 
            "text": "Release Date : 2017-03-28", 
            "title": "OSG Software Stack -- Data Release -- 3.3.22-2"
        }, 
        {
            "location": "/release/3.3/release-3-3-22-2/#summary-of-changes", 
            "text": "This release contains:   VO Package v72  Added VOMS server to OSG VO: voms.grid.iu.edu  Removed CSIU, NYSGRID, and OSGEDU VOs     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-22-2/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-22-2/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-22-2/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-22-2/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-22-2/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-22-2/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-22-2/#enterprise-linux-6", 
            "text": "vo-client-72-2.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-22-2/#enterprise-linux-7", 
            "text": "vo-client-72-2.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-22-2/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  osg-gums-config vo-client vo-client-edgmkgridmap vo-client-lcmaps-voms xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-libs xrootd-private-devel xrootd-python xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-22-2/#enterprise-linux-6_1", 
            "text": "osg-gums-config-72-2.osg33.el6\nvo-client-72-2.osg33.el6\nvo-client-edgmkgridmap-72-2.osg33.el6\nvo-client-lcmaps-voms-72-2.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-22-2/#enterprise-linux-7_1", 
            "text": "osg-gums-config-72-2.osg33.el7\nvo-client-72-2.osg33.el7\nvo-client-edgmkgridmap-72-2.osg33.el7\nvo-client-lcmaps-voms-72-2.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-22/", 
            "text": "OSG Software Release 3.3.22\n\n\nRelease Date\n: 2017-03-14\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nOSG 3.3.22\n\n\nHTCondor 8.4.11: Backport fix to avoid crash in the Job Router when job transform or submit requirements fail. (\nSOFTWARE-2615\n)\n\n\nUpdate default emi-trustmanager configuration to have GUMS use proper TLS protocols when contacting VOMS admin. (\nSOFTWARE-2523\n)\n\n\nBLAHP 1.18.29: Better Slurm integration, fixed problem with proxy refresh\n\n\nHTCondor-CE 2.1.4\n: Respect \nRequestCpus\n, Added JSON attibutes for AGIS (This OSG release contains changes from \nHTCondor-CE 2.1.3\n for the first time)\n\n\nGratia probe 1.17.4: Now picks up \nRequestCpus\n with HTCondor-CE\n\n\nUpdate to \nCVMFS 2.3.3\n\n\nUpdate to \nGlideinWMS 3.2.18\n\n\nUpdate to \nXRootD 4.6.0\n\n\nHDFS: GridFTP prints proper error message when HDFS quota is exhausted\n\n\nVOMS 2.0.14-1.3: Now validates top-level group of proxy\n\n\n\n\n\n\nUpcoming repository\n\n\nHTCondor 8.6.1\n: New stable series of HTCondor in Upcoming (This OSG release contains changes from \nHTCondor-8.6.0\n for the first time)\n\n\nUpdate to \nfrontier-squid 3.5.24-1\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nKnown Issues\n\n\n\n\n On Friday, March 17th, \nXRootD 4.6.0\n was pulled from the release repository. \n Most of the issues origin from improper CRL verification bug introduced in the recent code. Criticality of the issue:\n\n\nhigh criticality running XRootD in a \nserver\n mode you may have issues since reading the final file usually does require authentication (mostly GSI auth which relates to CRL verification code)\n\n\nlow criticality running XRootD in a \nmanager\n mode you may NOT experience any issues assuming your redirector does not require authentication\n\n\nto check if you have the affected components installed, run \nroot@host # rpm -qa | grep xrootd\n to display the versions of your xrootd packages. If any of them are version 4.6.0, run \nroot@host # yum downgrade \n on those packages.\n\n\nas pre-caution to avoid this bug \nXRootD 4.6.0\n was pulled from the release repository and we do not recommend using it until further notice\n\n\ngiven the number of recent improvements and fixes 4.6.1 is available in EPEL testing repo and once it gets to production release we push it into OSG release as well\n\n\n\n\n\n\nThe Koji client config has changed in the new version of Koji: `pkgurl\nhttp://koji.chtc.wisc.edu/packages` has been replaced by `topurl=http://koji.chtc.wisc.edu` and the Koji client will give a harmless but annoying warning when it finds `pkgurl`. To get rid of the warning, update to osg-build \n 1.8.0, rerun `osg-koji setup`, and say 'yes' when asked to replace the Koji configuration file; or, you may make the above change manually.\n\n\nA previous version (\n1.17.0-2.5\n) of the Gratia probes required changes in the HTCondor configuration to operate properly. Now, these changes need to be reverted to use verison \n1.17.0-2.6\n and later of the probes. The lines below are likely to be found in \ncondor_config.local\n or a new file in \n/etc/condor/config.d\n directory. Delete these lines and run \ncondor_reconfig\n.\n\n\n\n\n# GlideinWMS Gratia commands\n\n\nPER_JOB_HISTORY_DIR\n \n=\n /var/lib/gratia/data\n\nJOBGLIDEIN_ResourceName\n=\n$$\n([IfThenElse(IsUndefined(TARGET.GLIDEIN_ResourceName), IfThenElse(IsUndefined(TARGET.GLIDEIN_Site), IfThenElse(IsUndefined(TARGET.FileSystemDomain), \\\nLocal Job\\\n, TARGET.FileSystemDomain), TARGET.GLIDEIN_Site), TARGET.GLIDEIN_ResourceName)])\n\n\nSUBMIT_EXPRS\n \n=\n \n$(\nSUBMIT_EXPRS\n)\n JOBGLIDEIN_ResourceName   \n\n\n\n\n\n\n\nOn EL7, your version of voms-proxy-init may come from the voms-clients-java package; this version will continue to make legacy proxies by default. If you want the new behavior, install the voms-clients-cpp package and uninstall the voms-clients-java package.\n\n\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.29.bosco-1.osg33.el6\n\n\ncondor-8.4.11-1.1.osg33.el6\n\n\ncvmfs-2.3.3-1.osg33.el6\n\n\ncvmfs-config-osg-2.0-1.osg33.el6\n\n\nemi-trustmanager-3.0.3-14.osg33.el6\n\n\nglideinwms-3.2.18-1.osg33.el6\n\n\ngratia-probe-1.17.4-1.osg33.el6\n\n\nhadoop-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\n\n\nhtcondor-ce-2.1.4-1.osg33.el6\n\n\nosg-oasis-7-7.osg33.el6\n\n\nosg-version-3.3.22-1.osg33.el6\n\n\nvoms-2.0.14-1.3.osg33.el6\n\n\n \nxrootd-4.6.0-1.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.29.bosco-1.osg33.el7\n\n\ncondor-8.4.11-1.1.osg33.el7\n\n\ncvmfs-2.3.3-1.osg33.el7\n\n\ncvmfs-config-osg-2.0-1.osg33.el7\n\n\nemi-trustmanager-3.0.3-14.osg33.el7\n\n\nglideinwms-3.2.18-1.osg33.el7\n\n\ngratia-probe-1.17.4-1.osg33.el7\n\n\nhadoop-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\n\n\nhtcondor-ce-2.1.4-1.osg33.el7\n\n\nosg-oasis-7-7.osg33.el7\n\n\nosg-version-3.3.22-1.osg33.el7\n\n\nvoms-2.0.14-1.3.osg33.el7\n\n\nxrootd-4.6.0-1.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp cvmfs cvmfs-config-osg cvmfs-devel cvmfs-server cvmfs-unittests emi-trustmanager glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone gratia-probe-bdii-status gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glexec gratia-probe-glideinwms gratia-probe-gram gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer hadoop hadoop-0.20-conf-pseudo hadoop-0.20-mapreduce hadoop-client hadoop-conf-pseudo hadoop-debuginfo hadoop-doc hadoop-hdfs hadoop-hdfs-datanode hadoop-hdfs-fuse hadoop-hdfs-fuse-selinux hadoop-hdfs-journalnode hadoop-hdfs-namenode hadoop-hdfs-secondarynamenode hadoop-hdfs-zkfc hadoop-httpfs hadoop-libhdfs hadoop-mapreduce hadoop-yarn htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-view igtf-ca-certs osg-ca-certs osg-gums-config osg-oasis osg-version vo-client vo-client-edgmkgridmap vo-client-lcmaps-voms voms voms-clients-cpp voms-debuginfo voms-devel voms-doc voms-server \nstrike\nxrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-libs xrootd-private-devel xrootd-python xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs\n/strike\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp-1.18.29.bosco-1.osg33.el6\nblahp-debuginfo-1.18.29.bosco-1.osg33.el6\ncondor-8.4.11-1.1.osg33.el6\ncondor-all-8.4.11-1.1.osg33.el6\ncondor-bosco-8.4.11-1.1.osg33.el6\ncondor-classads-8.4.11-1.1.osg33.el6\ncondor-classads-devel-8.4.11-1.1.osg33.el6\ncondor-cream-gahp-8.4.11-1.1.osg33.el6\ncondor-debuginfo-8.4.11-1.1.osg33.el6\ncondor-kbdd-8.4.11-1.1.osg33.el6\ncondor-procd-8.4.11-1.1.osg33.el6\ncondor-python-8.4.11-1.1.osg33.el6\ncondor-std-universe-8.4.11-1.1.osg33.el6\ncondor-test-8.4.11-1.1.osg33.el6\ncondor-vm-gahp-8.4.11-1.1.osg33.el6\ncvmfs-2.3.3-1.osg33.el6\ncvmfs-config-osg-2.0-1.osg33.el6\ncvmfs-devel-2.3.3-1.osg33.el6\ncvmfs-server-2.3.3-1.osg33.el6\ncvmfs-unittests-2.3.3-1.osg33.el6\nemi-trustmanager-3.0.3-14.osg33.el6\nglideinwms-3.2.18-1.osg33.el6\nglideinwms-common-tools-3.2.18-1.osg33.el6\nglideinwms-condor-common-config-3.2.18-1.osg33.el6\nglideinwms-factory-3.2.18-1.osg33.el6\nglideinwms-factory-condor-3.2.18-1.osg33.el6\nglideinwms-glidecondor-tools-3.2.18-1.osg33.el6\nglideinwms-libs-3.2.18-1.osg33.el6\nglideinwms-minimal-condor-3.2.18-1.osg33.el6\nglideinwms-usercollector-3.2.18-1.osg33.el6\nglideinwms-userschedd-3.2.18-1.osg33.el6\nglideinwms-vofrontend-3.2.18-1.osg33.el6\nglideinwms-vofrontend-standalone-3.2.18-1.osg33.el6\ngratia-probe-1.17.4-1.osg33.el6\ngratia-probe-bdii-status-1.17.4-1.osg33.el6\ngratia-probe-common-1.17.4-1.osg33.el6\ngratia-probe-condor-1.17.4-1.osg33.el6\ngratia-probe-condor-events-1.17.4-1.osg33.el6\ngratia-probe-dcache-storage-1.17.4-1.osg33.el6\ngratia-probe-dcache-storagegroup-1.17.4-1.osg33.el6\ngratia-probe-dcache-transfer-1.17.4-1.osg33.el6\ngratia-probe-debuginfo-1.17.4-1.osg33.el6\ngratia-probe-enstore-storage-1.17.4-1.osg33.el6\ngratia-probe-enstore-tapedrive-1.17.4-1.osg33.el6\ngratia-probe-enstore-transfer-1.17.4-1.osg33.el6\ngratia-probe-glexec-1.17.4-1.osg33.el6\ngratia-probe-glideinwms-1.17.4-1.osg33.el6\ngratia-probe-gram-1.17.4-1.osg33.el6\ngratia-probe-gridftp-transfer-1.17.4-1.osg33.el6\ngratia-probe-hadoop-storage-1.17.4-1.osg33.el6\ngratia-probe-htcondor-ce-1.17.4-1.osg33.el6\ngratia-probe-lsf-1.17.4-1.osg33.el6\ngratia-probe-metric-1.17.4-1.osg33.el6\ngratia-probe-onevm-1.17.4-1.osg33.el6\ngratia-probe-pbs-lsf-1.17.4-1.osg33.el6\ngratia-probe-services-1.17.4-1.osg33.el6\ngratia-probe-sge-1.17.4-1.osg33.el6\ngratia-probe-slurm-1.17.4-1.osg33.el6\ngratia-probe-xrootd-storage-1.17.4-1.osg33.el6\ngratia-probe-xrootd-transfer-1.17.4-1.osg33.el6\nhadoop-0.20-conf-pseudo-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-0.20-mapreduce-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-client-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-conf-pseudo-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-debuginfo-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-doc-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-hdfs-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-hdfs-datanode-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-hdfs-fuse-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-hdfs-fuse-selinux-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-hdfs-journalnode-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-hdfs-namenode-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-hdfs-secondarynamenode-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-hdfs-zkfc-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-httpfs-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-libhdfs-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-mapreduce-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-yarn-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhtcondor-ce-2.1.4-1.osg33.el6\nhtcondor-ce-bosco-2.1.4-1.osg33.el6\nhtcondor-ce-client-2.1.4-1.osg33.el6\nhtcondor-ce-collector-2.1.4-1.osg33.el6\nhtcondor-ce-condor-2.1.4-1.osg33.el6\nhtcondor-ce-lsf-2.1.4-1.osg33.el6\nhtcondor-ce-pbs-2.1.4-1.osg33.el6\nhtcondor-ce-sge-2.1.4-1.osg33.el6\nhtcondor-ce-view-2.1.4-1.osg33.el6\nosg-oasis-7-7.osg33.el6\nosg-version-3.3.22-1.osg33.el6\nvoms-2.0.14-1.3.osg33.el6\nvoms-clients-cpp-2.0.14-1.3.osg33.el6\nvoms-debuginfo-2.0.14-1.3.osg33.el6\nvoms-devel-2.0.14-1.3.osg33.el6\nvoms-doc-2.0.14-1.3.osg33.el6\nvoms-server-2.0.14-1.3.osg33.el6\n\nstrike\nxrootd-4.6.0-1.osg33.el6\nxrootd-client-4.6.0-1.osg33.el6\nxrootd-client-devel-4.6.0-1.osg33.el6\nxrootd-client-libs-4.6.0-1.osg33.el6\nxrootd-debuginfo-4.6.0-1.osg33.el6\nxrootd-devel-4.6.0-1.osg33.el6\nxrootd-doc-4.6.0-1.osg33.el6\nxrootd-fuse-4.6.0-1.osg33.el6\nxrootd-libs-4.6.0-1.osg33.el6\nxrootd-private-devel-4.6.0-1.osg33.el6\nxrootd-python-4.6.0-1.osg33.el6\nxrootd-selinux-4.6.0-1.osg33.el6\nxrootd-server-4.6.0-1.osg33.el6\nxrootd-server-devel-4.6.0-1.osg33.el6\nxrootd-server-libs-4.6.0-1.osg33.el6\n/strike\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp-1.18.29.bosco-1.osg33.el7\nblahp-debuginfo-1.18.29.bosco-1.osg33.el7\ncondor-8.4.11-1.1.osg33.el7\ncondor-all-8.4.11-1.1.osg33.el7\ncondor-bosco-8.4.11-1.1.osg33.el7\ncondor-classads-8.4.11-1.1.osg33.el7\ncondor-classads-devel-8.4.11-1.1.osg33.el7\ncondor-cream-gahp-8.4.11-1.1.osg33.el7\ncondor-debuginfo-8.4.11-1.1.osg33.el7\ncondor-kbdd-8.4.11-1.1.osg33.el7\ncondor-procd-8.4.11-1.1.osg33.el7\ncondor-python-8.4.11-1.1.osg33.el7\ncondor-test-8.4.11-1.1.osg33.el7\ncondor-vm-gahp-8.4.11-1.1.osg33.el7\ncvmfs-2.3.3-1.osg33.el7\ncvmfs-config-osg-2.0-1.osg33.el7\ncvmfs-devel-2.3.3-1.osg33.el7\ncvmfs-server-2.3.3-1.osg33.el7\ncvmfs-unittests-2.3.3-1.osg33.el7\nemi-trustmanager-3.0.3-14.osg33.el7\nglideinwms-3.2.18-1.osg33.el7\nglideinwms-common-tools-3.2.18-1.osg33.el7\nglideinwms-condor-common-config-3.2.18-1.osg33.el7\nglideinwms-factory-3.2.18-1.osg33.el7\nglideinwms-factory-condor-3.2.18-1.osg33.el7\nglideinwms-glidecondor-tools-3.2.18-1.osg33.el7\nglideinwms-libs-3.2.18-1.osg33.el7\nglideinwms-minimal-condor-3.2.18-1.osg33.el7\nglideinwms-usercollector-3.2.18-1.osg33.el7\nglideinwms-userschedd-3.2.18-1.osg33.el7\nglideinwms-vofrontend-3.2.18-1.osg33.el7\nglideinwms-vofrontend-standalone-3.2.18-1.osg33.el7\ngratia-probe-1.17.4-1.osg33.el7\ngratia-probe-bdii-status-1.17.4-1.osg33.el7\ngratia-probe-common-1.17.4-1.osg33.el7\ngratia-probe-condor-1.17.4-1.osg33.el7\ngratia-probe-condor-events-1.17.4-1.osg33.el7\ngratia-probe-dcache-storage-1.17.4-1.osg33.el7\ngratia-probe-dcache-storagegroup-1.17.4-1.osg33.el7\ngratia-probe-dcache-transfer-1.17.4-1.osg33.el7\ngratia-probe-debuginfo-1.17.4-1.osg33.el7\ngratia-probe-enstore-storage-1.17.4-1.osg33.el7\ngratia-probe-enstore-tapedrive-1.17.4-1.osg33.el7\ngratia-probe-enstore-transfer-1.17.4-1.osg33.el7\ngratia-probe-glexec-1.17.4-1.osg33.el7\ngratia-probe-glideinwms-1.17.4-1.osg33.el7\ngratia-probe-gram-1.17.4-1.osg33.el7\ngratia-probe-gridftp-transfer-1.17.4-1.osg33.el7\ngratia-probe-hadoop-storage-1.17.4-1.osg33.el7\ngratia-probe-htcondor-ce-1.17.4-1.osg33.el7\ngratia-probe-lsf-1.17.4-1.osg33.el7\ngratia-probe-metric-1.17.4-1.osg33.el7\ngratia-probe-onevm-1.17.4-1.osg33.el7\ngratia-probe-pbs-lsf-1.17.4-1.osg33.el7\ngratia-probe-services-1.17.4-1.osg33.el7\ngratia-probe-sge-1.17.4-1.osg33.el7\ngratia-probe-slurm-1.17.4-1.osg33.el7\ngratia-probe-xrootd-storage-1.17.4-1.osg33.el7\ngratia-probe-xrootd-transfer-1.17.4-1.osg33.el7\nhadoop-0.20-conf-pseudo-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-0.20-mapreduce-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-client-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-conf-pseudo-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-debuginfo-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-doc-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-hdfs-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-hdfs-datanode-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-hdfs-fuse-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-hdfs-fuse-selinux-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-hdfs-journalnode-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-hdfs-namenode-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-hdfs-secondarynamenode-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-hdfs-zkfc-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-httpfs-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-libhdfs-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-mapreduce-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-yarn-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhtcondor-ce-2.1.4-1.osg33.el7\nhtcondor-ce-bosco-2.1.4-1.osg33.el7\nhtcondor-ce-client-2.1.4-1.osg33.el7\nhtcondor-ce-collector-2.1.4-1.osg33.el7\nhtcondor-ce-condor-2.1.4-1.osg33.el7\nhtcondor-ce-lsf-2.1.4-1.osg33.el7\nhtcondor-ce-pbs-2.1.4-1.osg33.el7\nhtcondor-ce-sge-2.1.4-1.osg33.el7\nhtcondor-ce-view-2.1.4-1.osg33.el7\nosg-oasis-7-7.osg33.el7\nosg-version-3.3.22-1.osg33.el7\nvoms-2.0.14-1.3.osg33.el7\nvoms-clients-cpp-2.0.14-1.3.osg33.el7\nvoms-debuginfo-2.0.14-1.3.osg33.el7\nvoms-devel-2.0.14-1.3.osg33.el7\nvoms-doc-2.0.14-1.3.osg33.el7\nvoms-server-2.0.14-1.3.osg33.el7\n\nstrike\nxrootd-4.6.0-1.osg33.el7\nxrootd-client-4.6.0-1.osg33.el7\nxrootd-client-devel-4.6.0-1.osg33.el7\nxrootd-client-libs-4.6.0-1.osg33.el7\nxrootd-debuginfo-4.6.0-1.osg33.el7\nxrootd-devel-4.6.0-1.osg33.el7\nxrootd-doc-4.6.0-1.osg33.el7\nxrootd-fuse-4.6.0-1.osg33.el7\nxrootd-libs-4.6.0-1.osg33.el7\nxrootd-private-devel-4.6.0-1.osg33.el7\nxrootd-python-4.6.0-1.osg33.el7\nxrootd-selinux-4.6.0-1.osg33.el7\nxrootd-server-4.6.0-1.osg33.el7\nxrootd-server-devel-4.6.0-1.osg33.el7\nxrootd-server-libs-4.6.0-1.osg33.el7\n/strike\n\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.29.bosco-1.osgup.el6\n\n\ncondor-8.6.1-1.osgup.el6\n\n\nfrontier-squid-3.5.24-1.1.osgup.el6\n\n\nglite-ce-cream-client-api-c-1.15.4-2.3.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.29.bosco-1.osgup.el7\n\n\ncondor-8.6.1-1.osgup.el7\n\n\nfrontier-squid-3.5.24-1.1.osgup.el7\n\n\nglite-ce-cream-client-api-c-1.15.4-2.3.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp frontier-squid frontier-squid-debuginfo glite-ce-cream-client-api-c glite-ce-cream-client-devel\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp-1.18.29.bosco-1.osgup.el6\nblahp-debuginfo-1.18.29.bosco-1.osgup.el6\ncondor-8.6.1-1.osgup.el6\ncondor-all-8.6.1-1.osgup.el6\ncondor-bosco-8.6.1-1.osgup.el6\ncondor-classads-8.6.1-1.osgup.el6\ncondor-classads-devel-8.6.1-1.osgup.el6\ncondor-cream-gahp-8.6.1-1.osgup.el6\ncondor-debuginfo-8.6.1-1.osgup.el6\ncondor-kbdd-8.6.1-1.osgup.el6\ncondor-procd-8.6.1-1.osgup.el6\ncondor-python-8.6.1-1.osgup.el6\ncondor-std-universe-8.6.1-1.osgup.el6\ncondor-test-8.6.1-1.osgup.el6\ncondor-vm-gahp-8.6.1-1.osgup.el6\nfrontier-squid-3.5.24-1.1.osgup.el6\nfrontier-squid-debuginfo-3.5.24-1.1.osgup.el6\nglite-ce-cream-client-api-c-1.15.4-2.3.osgup.el6\nglite-ce-cream-client-devel-1.15.4-2.3.osgup.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp-1.18.29.bosco-1.osgup.el7\nblahp-debuginfo-1.18.29.bosco-1.osgup.el7\ncondor-8.6.1-1.osgup.el7\ncondor-all-8.6.1-1.osgup.el7\ncondor-bosco-8.6.1-1.osgup.el7\ncondor-classads-8.6.1-1.osgup.el7\ncondor-classads-devel-8.6.1-1.osgup.el7\ncondor-cream-gahp-8.6.1-1.osgup.el7\ncondor-debuginfo-8.6.1-1.osgup.el7\ncondor-kbdd-8.6.1-1.osgup.el7\ncondor-procd-8.6.1-1.osgup.el7\ncondor-python-8.6.1-1.osgup.el7\ncondor-test-8.6.1-1.osgup.el7\ncondor-vm-gahp-8.6.1-1.osgup.el7\nfrontier-squid-3.5.24-1.1.osgup.el7\nfrontier-squid-debuginfo-3.5.24-1.1.osgup.el7\nglite-ce-cream-client-api-c-1.15.4-2.3.osgup.el7\nglite-ce-cream-client-devel-1.15.4-2.3.osgup.el7", 
            "title": "OSG Release 3.3.22"
        }, 
        {
            "location": "/release/3.3/release-3-3-22/#osg-software-release-3322", 
            "text": "Release Date : 2017-03-14", 
            "title": "OSG Software Release 3.3.22"
        }, 
        {
            "location": "/release/3.3/release-3-3-22/#summary-of-changes", 
            "text": "This release contains:   OSG 3.3.22  HTCondor 8.4.11: Backport fix to avoid crash in the Job Router when job transform or submit requirements fail. ( SOFTWARE-2615 )  Update default emi-trustmanager configuration to have GUMS use proper TLS protocols when contacting VOMS admin. ( SOFTWARE-2523 )  BLAHP 1.18.29: Better Slurm integration, fixed problem with proxy refresh  HTCondor-CE 2.1.4 : Respect  RequestCpus , Added JSON attibutes for AGIS (This OSG release contains changes from  HTCondor-CE 2.1.3  for the first time)  Gratia probe 1.17.4: Now picks up  RequestCpus  with HTCondor-CE  Update to  CVMFS 2.3.3  Update to  GlideinWMS 3.2.18  Update to  XRootD 4.6.0  HDFS: GridFTP prints proper error message when HDFS quota is exhausted  VOMS 2.0.14-1.3: Now validates top-level group of proxy    Upcoming repository  HTCondor 8.6.1 : New stable series of HTCondor in Upcoming (This OSG release contains changes from  HTCondor-8.6.0  for the first time)  Update to  frontier-squid 3.5.24-1     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-22/#known-issues", 
            "text": "On Friday, March 17th,  XRootD 4.6.0  was pulled from the release repository.   Most of the issues origin from improper CRL verification bug introduced in the recent code. Criticality of the issue:  high criticality running XRootD in a  server  mode you may have issues since reading the final file usually does require authentication (mostly GSI auth which relates to CRL verification code)  low criticality running XRootD in a  manager  mode you may NOT experience any issues assuming your redirector does not require authentication  to check if you have the affected components installed, run  root@host # rpm -qa | grep xrootd  to display the versions of your xrootd packages. If any of them are version 4.6.0, run  root@host # yum downgrade   on those packages.  as pre-caution to avoid this bug  XRootD 4.6.0  was pulled from the release repository and we do not recommend using it until further notice  given the number of recent improvements and fixes 4.6.1 is available in EPEL testing repo and once it gets to production release we push it into OSG release as well    The Koji client config has changed in the new version of Koji: `pkgurl http://koji.chtc.wisc.edu/packages` has been replaced by `topurl=http://koji.chtc.wisc.edu` and the Koji client will give a harmless but annoying warning when it finds `pkgurl`. To get rid of the warning, update to osg-build   1.8.0, rerun `osg-koji setup`, and say 'yes' when asked to replace the Koji configuration file; or, you may make the above change manually.  A previous version ( 1.17.0-2.5 ) of the Gratia probes required changes in the HTCondor configuration to operate properly. Now, these changes need to be reverted to use verison  1.17.0-2.6  and later of the probes. The lines below are likely to be found in  condor_config.local  or a new file in  /etc/condor/config.d  directory. Delete these lines and run  condor_reconfig .   # GlideinWMS Gratia commands  PER_JOB_HISTORY_DIR   =  /var/lib/gratia/data JOBGLIDEIN_ResourceName = $$ ([IfThenElse(IsUndefined(TARGET.GLIDEIN_ResourceName), IfThenElse(IsUndefined(TARGET.GLIDEIN_Site), IfThenElse(IsUndefined(TARGET.FileSystemDomain), \\ Local Job\\ , TARGET.FileSystemDomain), TARGET.GLIDEIN_Site), TARGET.GLIDEIN_ResourceName)])  SUBMIT_EXPRS   =   $( SUBMIT_EXPRS )  JOBGLIDEIN_ResourceName      On EL7, your version of voms-proxy-init may come from the voms-clients-java package; this version will continue to make legacy proxies by default. If you want the new behavior, install the voms-clients-cpp package and uninstall the voms-clients-java package.", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.3/release-3-3-22/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-22/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-22/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-22/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-22/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-22/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-22/#enterprise-linux-6", 
            "text": "blahp-1.18.29.bosco-1.osg33.el6  condor-8.4.11-1.1.osg33.el6  cvmfs-2.3.3-1.osg33.el6  cvmfs-config-osg-2.0-1.osg33.el6  emi-trustmanager-3.0.3-14.osg33.el6  glideinwms-3.2.18-1.osg33.el6  gratia-probe-1.17.4-1.osg33.el6  hadoop-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6  htcondor-ce-2.1.4-1.osg33.el6  osg-oasis-7-7.osg33.el6  osg-version-3.3.22-1.osg33.el6  voms-2.0.14-1.3.osg33.el6    xrootd-4.6.0-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-22/#enterprise-linux-7", 
            "text": "blahp-1.18.29.bosco-1.osg33.el7  condor-8.4.11-1.1.osg33.el7  cvmfs-2.3.3-1.osg33.el7  cvmfs-config-osg-2.0-1.osg33.el7  emi-trustmanager-3.0.3-14.osg33.el7  glideinwms-3.2.18-1.osg33.el7  gratia-probe-1.17.4-1.osg33.el7  hadoop-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7  htcondor-ce-2.1.4-1.osg33.el7  osg-oasis-7-7.osg33.el7  osg-version-3.3.22-1.osg33.el7  voms-2.0.14-1.3.osg33.el7  xrootd-4.6.0-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-22/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp cvmfs cvmfs-config-osg cvmfs-devel cvmfs-server cvmfs-unittests emi-trustmanager glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone gratia-probe-bdii-status gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glexec gratia-probe-glideinwms gratia-probe-gram gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer hadoop hadoop-0.20-conf-pseudo hadoop-0.20-mapreduce hadoop-client hadoop-conf-pseudo hadoop-debuginfo hadoop-doc hadoop-hdfs hadoop-hdfs-datanode hadoop-hdfs-fuse hadoop-hdfs-fuse-selinux hadoop-hdfs-journalnode hadoop-hdfs-namenode hadoop-hdfs-secondarynamenode hadoop-hdfs-zkfc hadoop-httpfs hadoop-libhdfs hadoop-mapreduce hadoop-yarn htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-view igtf-ca-certs osg-ca-certs osg-gums-config osg-oasis osg-version vo-client vo-client-edgmkgridmap vo-client-lcmaps-voms voms voms-clients-cpp voms-debuginfo voms-devel voms-doc voms-server  strike xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-libs xrootd-private-devel xrootd-python xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs /strike   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-22/#enterprise-linux-6_1", 
            "text": "blahp-1.18.29.bosco-1.osg33.el6\nblahp-debuginfo-1.18.29.bosco-1.osg33.el6\ncondor-8.4.11-1.1.osg33.el6\ncondor-all-8.4.11-1.1.osg33.el6\ncondor-bosco-8.4.11-1.1.osg33.el6\ncondor-classads-8.4.11-1.1.osg33.el6\ncondor-classads-devel-8.4.11-1.1.osg33.el6\ncondor-cream-gahp-8.4.11-1.1.osg33.el6\ncondor-debuginfo-8.4.11-1.1.osg33.el6\ncondor-kbdd-8.4.11-1.1.osg33.el6\ncondor-procd-8.4.11-1.1.osg33.el6\ncondor-python-8.4.11-1.1.osg33.el6\ncondor-std-universe-8.4.11-1.1.osg33.el6\ncondor-test-8.4.11-1.1.osg33.el6\ncondor-vm-gahp-8.4.11-1.1.osg33.el6\ncvmfs-2.3.3-1.osg33.el6\ncvmfs-config-osg-2.0-1.osg33.el6\ncvmfs-devel-2.3.3-1.osg33.el6\ncvmfs-server-2.3.3-1.osg33.el6\ncvmfs-unittests-2.3.3-1.osg33.el6\nemi-trustmanager-3.0.3-14.osg33.el6\nglideinwms-3.2.18-1.osg33.el6\nglideinwms-common-tools-3.2.18-1.osg33.el6\nglideinwms-condor-common-config-3.2.18-1.osg33.el6\nglideinwms-factory-3.2.18-1.osg33.el6\nglideinwms-factory-condor-3.2.18-1.osg33.el6\nglideinwms-glidecondor-tools-3.2.18-1.osg33.el6\nglideinwms-libs-3.2.18-1.osg33.el6\nglideinwms-minimal-condor-3.2.18-1.osg33.el6\nglideinwms-usercollector-3.2.18-1.osg33.el6\nglideinwms-userschedd-3.2.18-1.osg33.el6\nglideinwms-vofrontend-3.2.18-1.osg33.el6\nglideinwms-vofrontend-standalone-3.2.18-1.osg33.el6\ngratia-probe-1.17.4-1.osg33.el6\ngratia-probe-bdii-status-1.17.4-1.osg33.el6\ngratia-probe-common-1.17.4-1.osg33.el6\ngratia-probe-condor-1.17.4-1.osg33.el6\ngratia-probe-condor-events-1.17.4-1.osg33.el6\ngratia-probe-dcache-storage-1.17.4-1.osg33.el6\ngratia-probe-dcache-storagegroup-1.17.4-1.osg33.el6\ngratia-probe-dcache-transfer-1.17.4-1.osg33.el6\ngratia-probe-debuginfo-1.17.4-1.osg33.el6\ngratia-probe-enstore-storage-1.17.4-1.osg33.el6\ngratia-probe-enstore-tapedrive-1.17.4-1.osg33.el6\ngratia-probe-enstore-transfer-1.17.4-1.osg33.el6\ngratia-probe-glexec-1.17.4-1.osg33.el6\ngratia-probe-glideinwms-1.17.4-1.osg33.el6\ngratia-probe-gram-1.17.4-1.osg33.el6\ngratia-probe-gridftp-transfer-1.17.4-1.osg33.el6\ngratia-probe-hadoop-storage-1.17.4-1.osg33.el6\ngratia-probe-htcondor-ce-1.17.4-1.osg33.el6\ngratia-probe-lsf-1.17.4-1.osg33.el6\ngratia-probe-metric-1.17.4-1.osg33.el6\ngratia-probe-onevm-1.17.4-1.osg33.el6\ngratia-probe-pbs-lsf-1.17.4-1.osg33.el6\ngratia-probe-services-1.17.4-1.osg33.el6\ngratia-probe-sge-1.17.4-1.osg33.el6\ngratia-probe-slurm-1.17.4-1.osg33.el6\ngratia-probe-xrootd-storage-1.17.4-1.osg33.el6\ngratia-probe-xrootd-transfer-1.17.4-1.osg33.el6\nhadoop-0.20-conf-pseudo-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-0.20-mapreduce-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-client-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-conf-pseudo-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-debuginfo-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-doc-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-hdfs-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-hdfs-datanode-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-hdfs-fuse-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-hdfs-fuse-selinux-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-hdfs-journalnode-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-hdfs-namenode-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-hdfs-secondarynamenode-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-hdfs-zkfc-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-httpfs-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-libhdfs-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-mapreduce-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhadoop-yarn-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el6\nhtcondor-ce-2.1.4-1.osg33.el6\nhtcondor-ce-bosco-2.1.4-1.osg33.el6\nhtcondor-ce-client-2.1.4-1.osg33.el6\nhtcondor-ce-collector-2.1.4-1.osg33.el6\nhtcondor-ce-condor-2.1.4-1.osg33.el6\nhtcondor-ce-lsf-2.1.4-1.osg33.el6\nhtcondor-ce-pbs-2.1.4-1.osg33.el6\nhtcondor-ce-sge-2.1.4-1.osg33.el6\nhtcondor-ce-view-2.1.4-1.osg33.el6\nosg-oasis-7-7.osg33.el6\nosg-version-3.3.22-1.osg33.el6\nvoms-2.0.14-1.3.osg33.el6\nvoms-clients-cpp-2.0.14-1.3.osg33.el6\nvoms-debuginfo-2.0.14-1.3.osg33.el6\nvoms-devel-2.0.14-1.3.osg33.el6\nvoms-doc-2.0.14-1.3.osg33.el6\nvoms-server-2.0.14-1.3.osg33.el6 strike xrootd-4.6.0-1.osg33.el6\nxrootd-client-4.6.0-1.osg33.el6\nxrootd-client-devel-4.6.0-1.osg33.el6\nxrootd-client-libs-4.6.0-1.osg33.el6\nxrootd-debuginfo-4.6.0-1.osg33.el6\nxrootd-devel-4.6.0-1.osg33.el6\nxrootd-doc-4.6.0-1.osg33.el6\nxrootd-fuse-4.6.0-1.osg33.el6\nxrootd-libs-4.6.0-1.osg33.el6\nxrootd-private-devel-4.6.0-1.osg33.el6\nxrootd-python-4.6.0-1.osg33.el6\nxrootd-selinux-4.6.0-1.osg33.el6\nxrootd-server-4.6.0-1.osg33.el6\nxrootd-server-devel-4.6.0-1.osg33.el6\nxrootd-server-libs-4.6.0-1.osg33.el6 /strike", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-22/#enterprise-linux-7_1", 
            "text": "blahp-1.18.29.bosco-1.osg33.el7\nblahp-debuginfo-1.18.29.bosco-1.osg33.el7\ncondor-8.4.11-1.1.osg33.el7\ncondor-all-8.4.11-1.1.osg33.el7\ncondor-bosco-8.4.11-1.1.osg33.el7\ncondor-classads-8.4.11-1.1.osg33.el7\ncondor-classads-devel-8.4.11-1.1.osg33.el7\ncondor-cream-gahp-8.4.11-1.1.osg33.el7\ncondor-debuginfo-8.4.11-1.1.osg33.el7\ncondor-kbdd-8.4.11-1.1.osg33.el7\ncondor-procd-8.4.11-1.1.osg33.el7\ncondor-python-8.4.11-1.1.osg33.el7\ncondor-test-8.4.11-1.1.osg33.el7\ncondor-vm-gahp-8.4.11-1.1.osg33.el7\ncvmfs-2.3.3-1.osg33.el7\ncvmfs-config-osg-2.0-1.osg33.el7\ncvmfs-devel-2.3.3-1.osg33.el7\ncvmfs-server-2.3.3-1.osg33.el7\ncvmfs-unittests-2.3.3-1.osg33.el7\nemi-trustmanager-3.0.3-14.osg33.el7\nglideinwms-3.2.18-1.osg33.el7\nglideinwms-common-tools-3.2.18-1.osg33.el7\nglideinwms-condor-common-config-3.2.18-1.osg33.el7\nglideinwms-factory-3.2.18-1.osg33.el7\nglideinwms-factory-condor-3.2.18-1.osg33.el7\nglideinwms-glidecondor-tools-3.2.18-1.osg33.el7\nglideinwms-libs-3.2.18-1.osg33.el7\nglideinwms-minimal-condor-3.2.18-1.osg33.el7\nglideinwms-usercollector-3.2.18-1.osg33.el7\nglideinwms-userschedd-3.2.18-1.osg33.el7\nglideinwms-vofrontend-3.2.18-1.osg33.el7\nglideinwms-vofrontend-standalone-3.2.18-1.osg33.el7\ngratia-probe-1.17.4-1.osg33.el7\ngratia-probe-bdii-status-1.17.4-1.osg33.el7\ngratia-probe-common-1.17.4-1.osg33.el7\ngratia-probe-condor-1.17.4-1.osg33.el7\ngratia-probe-condor-events-1.17.4-1.osg33.el7\ngratia-probe-dcache-storage-1.17.4-1.osg33.el7\ngratia-probe-dcache-storagegroup-1.17.4-1.osg33.el7\ngratia-probe-dcache-transfer-1.17.4-1.osg33.el7\ngratia-probe-debuginfo-1.17.4-1.osg33.el7\ngratia-probe-enstore-storage-1.17.4-1.osg33.el7\ngratia-probe-enstore-tapedrive-1.17.4-1.osg33.el7\ngratia-probe-enstore-transfer-1.17.4-1.osg33.el7\ngratia-probe-glexec-1.17.4-1.osg33.el7\ngratia-probe-glideinwms-1.17.4-1.osg33.el7\ngratia-probe-gram-1.17.4-1.osg33.el7\ngratia-probe-gridftp-transfer-1.17.4-1.osg33.el7\ngratia-probe-hadoop-storage-1.17.4-1.osg33.el7\ngratia-probe-htcondor-ce-1.17.4-1.osg33.el7\ngratia-probe-lsf-1.17.4-1.osg33.el7\ngratia-probe-metric-1.17.4-1.osg33.el7\ngratia-probe-onevm-1.17.4-1.osg33.el7\ngratia-probe-pbs-lsf-1.17.4-1.osg33.el7\ngratia-probe-services-1.17.4-1.osg33.el7\ngratia-probe-sge-1.17.4-1.osg33.el7\ngratia-probe-slurm-1.17.4-1.osg33.el7\ngratia-probe-xrootd-storage-1.17.4-1.osg33.el7\ngratia-probe-xrootd-transfer-1.17.4-1.osg33.el7\nhadoop-0.20-conf-pseudo-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-0.20-mapreduce-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-client-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-conf-pseudo-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-debuginfo-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-doc-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-hdfs-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-hdfs-datanode-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-hdfs-fuse-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-hdfs-fuse-selinux-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-hdfs-journalnode-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-hdfs-namenode-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-hdfs-secondarynamenode-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-hdfs-zkfc-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-httpfs-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-libhdfs-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-mapreduce-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhadoop-yarn-2.0.0+1612-1.cdh4.7.1.p0.12.6.osg33.el7\nhtcondor-ce-2.1.4-1.osg33.el7\nhtcondor-ce-bosco-2.1.4-1.osg33.el7\nhtcondor-ce-client-2.1.4-1.osg33.el7\nhtcondor-ce-collector-2.1.4-1.osg33.el7\nhtcondor-ce-condor-2.1.4-1.osg33.el7\nhtcondor-ce-lsf-2.1.4-1.osg33.el7\nhtcondor-ce-pbs-2.1.4-1.osg33.el7\nhtcondor-ce-sge-2.1.4-1.osg33.el7\nhtcondor-ce-view-2.1.4-1.osg33.el7\nosg-oasis-7-7.osg33.el7\nosg-version-3.3.22-1.osg33.el7\nvoms-2.0.14-1.3.osg33.el7\nvoms-clients-cpp-2.0.14-1.3.osg33.el7\nvoms-debuginfo-2.0.14-1.3.osg33.el7\nvoms-devel-2.0.14-1.3.osg33.el7\nvoms-doc-2.0.14-1.3.osg33.el7\nvoms-server-2.0.14-1.3.osg33.el7 strike xrootd-4.6.0-1.osg33.el7\nxrootd-client-4.6.0-1.osg33.el7\nxrootd-client-devel-4.6.0-1.osg33.el7\nxrootd-client-libs-4.6.0-1.osg33.el7\nxrootd-debuginfo-4.6.0-1.osg33.el7\nxrootd-devel-4.6.0-1.osg33.el7\nxrootd-doc-4.6.0-1.osg33.el7\nxrootd-fuse-4.6.0-1.osg33.el7\nxrootd-libs-4.6.0-1.osg33.el7\nxrootd-private-devel-4.6.0-1.osg33.el7\nxrootd-python-4.6.0-1.osg33.el7\nxrootd-selinux-4.6.0-1.osg33.el7\nxrootd-server-4.6.0-1.osg33.el7\nxrootd-server-devel-4.6.0-1.osg33.el7\nxrootd-server-libs-4.6.0-1.osg33.el7 /strike", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-22/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-22/#enterprise-linux-6_2", 
            "text": "blahp-1.18.29.bosco-1.osgup.el6  condor-8.6.1-1.osgup.el6  frontier-squid-3.5.24-1.1.osgup.el6  glite-ce-cream-client-api-c-1.15.4-2.3.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-22/#enterprise-linux-7_2", 
            "text": "blahp-1.18.29.bosco-1.osgup.el7  condor-8.6.1-1.osgup.el7  frontier-squid-3.5.24-1.1.osgup.el7  glite-ce-cream-client-api-c-1.15.4-2.3.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-22/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp frontier-squid frontier-squid-debuginfo glite-ce-cream-client-api-c glite-ce-cream-client-devel  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-22/#enterprise-linux-6_3", 
            "text": "blahp-1.18.29.bosco-1.osgup.el6\nblahp-debuginfo-1.18.29.bosco-1.osgup.el6\ncondor-8.6.1-1.osgup.el6\ncondor-all-8.6.1-1.osgup.el6\ncondor-bosco-8.6.1-1.osgup.el6\ncondor-classads-8.6.1-1.osgup.el6\ncondor-classads-devel-8.6.1-1.osgup.el6\ncondor-cream-gahp-8.6.1-1.osgup.el6\ncondor-debuginfo-8.6.1-1.osgup.el6\ncondor-kbdd-8.6.1-1.osgup.el6\ncondor-procd-8.6.1-1.osgup.el6\ncondor-python-8.6.1-1.osgup.el6\ncondor-std-universe-8.6.1-1.osgup.el6\ncondor-test-8.6.1-1.osgup.el6\ncondor-vm-gahp-8.6.1-1.osgup.el6\nfrontier-squid-3.5.24-1.1.osgup.el6\nfrontier-squid-debuginfo-3.5.24-1.1.osgup.el6\nglite-ce-cream-client-api-c-1.15.4-2.3.osgup.el6\nglite-ce-cream-client-devel-1.15.4-2.3.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-22/#enterprise-linux-7_3", 
            "text": "blahp-1.18.29.bosco-1.osgup.el7\nblahp-debuginfo-1.18.29.bosco-1.osgup.el7\ncondor-8.6.1-1.osgup.el7\ncondor-all-8.6.1-1.osgup.el7\ncondor-bosco-8.6.1-1.osgup.el7\ncondor-classads-8.6.1-1.osgup.el7\ncondor-classads-devel-8.6.1-1.osgup.el7\ncondor-cream-gahp-8.6.1-1.osgup.el7\ncondor-debuginfo-8.6.1-1.osgup.el7\ncondor-kbdd-8.6.1-1.osgup.el7\ncondor-procd-8.6.1-1.osgup.el7\ncondor-python-8.6.1-1.osgup.el7\ncondor-test-8.6.1-1.osgup.el7\ncondor-vm-gahp-8.6.1-1.osgup.el7\nfrontier-squid-3.5.24-1.1.osgup.el7\nfrontier-squid-debuginfo-3.5.24-1.1.osgup.el7\nglite-ce-cream-client-api-c-1.15.4-2.3.osgup.el7\nglite-ce-cream-client-devel-1.15.4-2.3.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-21-3/", 
            "text": "OSG Software Stack -- Data Release -- 3.3.21-3\n\n\nRelease Date\n: 2017-03-02\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nCA Certificates based on \nIGTF 1.81\n\n\nAdded accredited DarkMatter classic QV-intermediate ICAs (AE) including QuoVadis Root CA 2 G3 and Root CA 3 G3 higher level CAs (BM)\n\n\nUpdated contact information for EUN EG-GRID CA (EG)\n\n\nWithdrawn classic UKeScienceCA-2A in advance of repurposing (UK)\n\n\n\n\n\n\nVO Package v71\n\n\nRemoved INFN CDF VOMS servers\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nigtf-ca-certs-1.81-1.osg33.el6\n\n\nosg-ca-certs-1.61-1.osg33.el6\n\n\nvo-client-71-4.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nigtf-ca-certs-1.81-1.osg33.el7\n\n\nosg-ca-certs-1.61-1.osg33.el7\n\n\nvo-client-71-4.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nigtf-ca-certs osg-ca-certs osg-gums-config vo-client vo-client-edgmkgridmap vo-client-lcmaps-voms\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nigtf-ca-certs-1.81-1.osg33.el6\nosg-ca-certs-1.61-1.osg33.el6\nosg-gums-config-71-4.osg33.el6\nvo-client-71-4.osg33.el6\nvo-client-edgmkgridmap-71-4.osg33.el6\nvo-client-lcmaps-voms-71-4.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nigtf-ca-certs-1.81-1.osg33.el7\nosg-ca-certs-1.61-1.osg33.el7\nosg-gums-config-71-4.osg33.el7\nvo-client-71-4.osg33.el7\nvo-client-edgmkgridmap-71-4.osg33.el7\nvo-client-lcmaps-voms-71-4.osg33.el7", 
            "title": "OSG Release 3.3.21-3"
        }, 
        {
            "location": "/release/3.3/release-3-3-21-3/#osg-software-stack-data-release-3321-3", 
            "text": "Release Date : 2017-03-02", 
            "title": "OSG Software Stack -- Data Release -- 3.3.21-3"
        }, 
        {
            "location": "/release/3.3/release-3-3-21-3/#summary-of-changes", 
            "text": "This release contains:   CA Certificates based on  IGTF 1.81  Added accredited DarkMatter classic QV-intermediate ICAs (AE) including QuoVadis Root CA 2 G3 and Root CA 3 G3 higher level CAs (BM)  Updated contact information for EUN EG-GRID CA (EG)  Withdrawn classic UKeScienceCA-2A in advance of repurposing (UK)    VO Package v71  Removed INFN CDF VOMS servers     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-21-3/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-21-3/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-21-3/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-21-3/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-21-3/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-21-3/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-21-3/#enterprise-linux-6", 
            "text": "igtf-ca-certs-1.81-1.osg33.el6  osg-ca-certs-1.61-1.osg33.el6  vo-client-71-4.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-21-3/#enterprise-linux-7", 
            "text": "igtf-ca-certs-1.81-1.osg33.el7  osg-ca-certs-1.61-1.osg33.el7  vo-client-71-4.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-21-3/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  igtf-ca-certs osg-ca-certs osg-gums-config vo-client vo-client-edgmkgridmap vo-client-lcmaps-voms  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-21-3/#enterprise-linux-6_1", 
            "text": "igtf-ca-certs-1.81-1.osg33.el6\nosg-ca-certs-1.61-1.osg33.el6\nosg-gums-config-71-4.osg33.el6\nvo-client-71-4.osg33.el6\nvo-client-edgmkgridmap-71-4.osg33.el6\nvo-client-lcmaps-voms-71-4.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-21-3/#enterprise-linux-7_1", 
            "text": "igtf-ca-certs-1.81-1.osg33.el7\nosg-ca-certs-1.61-1.osg33.el7\nosg-gums-config-71-4.osg33.el7\nvo-client-71-4.osg33.el7\nvo-client-edgmkgridmap-71-4.osg33.el7\nvo-client-lcmaps-voms-71-4.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-21-2/", 
            "text": "OSG Software Stack -- Data Release -- 3.3.21-2\n\n\nRelease Date\n: 2017-02-16\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nCA Certificates based on \nIGTF 1.80\n\n\nDiscontinued BEGrid2008 (BELNET) classic authority (BE)\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nigtf-ca-certs-1.80-1.osg33.el6\n\n\nosg-ca-certs-1.60-1.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nigtf-ca-certs-1.80-1.osg33.el7\n\n\nosg-ca-certs-1.60-1.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nigtf-ca-certs osg-ca-certs\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nigtf-ca-certs-1.80-1.osg33.el6\nosg-ca-certs-1.60-1.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nigtf-ca-certs-1.80-1.osg33.el7\nosg-ca-certs-1.60-1.osg33.el7", 
            "title": "OSG Release 3.3.21-2"
        }, 
        {
            "location": "/release/3.3/release-3-3-21-2/#osg-software-stack-data-release-3321-2", 
            "text": "Release Date : 2017-02-16", 
            "title": "OSG Software Stack -- Data Release -- 3.3.21-2"
        }, 
        {
            "location": "/release/3.3/release-3-3-21-2/#summary-of-changes", 
            "text": "This release contains:   CA Certificates based on  IGTF 1.80  Discontinued BEGrid2008 (BELNET) classic authority (BE)     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-21-2/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-21-2/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-21-2/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-21-2/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-21-2/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-21-2/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-21-2/#enterprise-linux-6", 
            "text": "igtf-ca-certs-1.80-1.osg33.el6  osg-ca-certs-1.60-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-21-2/#enterprise-linux-7", 
            "text": "igtf-ca-certs-1.80-1.osg33.el7  osg-ca-certs-1.60-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-21-2/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  igtf-ca-certs osg-ca-certs  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-21-2/#enterprise-linux-6_1", 
            "text": "igtf-ca-certs-1.80-1.osg33.el6\nosg-ca-certs-1.60-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-21-2/#enterprise-linux-7_1", 
            "text": "igtf-ca-certs-1.80-1.osg33.el7\nosg-ca-certs-1.60-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-21/", 
            "text": "OSG Software Release 3.3.21\n\n\nRelease Date\n: 2017-02-14\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nOSG 3.3.21\n\n\nosg-configure 1.6.1: \nAdditional support\n for ATLAS AGIS in osg-configure\n\n\nglideinWMS 3.2.17\n\n\nSee known issues below\n\n\n\n\n\n\nImportant bug fixes for CVMFS server\n\n\nCVM-1165\n - swissknife hang during publish\n\n\nCVM-1108\n - Prevent garbage collection from running at the same time as snapshot\n\n\nCVM-1153\n - cvmfs build fails on centos7.3, in externals/build_c-ares\n\n\n\n\n\n\nHTCondor 8.4.11\n: Final bug fix release of the 8.4 series\n\n\nHTCondor-CE 2.1.2\n\n\nAccept Russian Data Intensive Grid certificates\n\n\nAvoid crash in client tools (e.g. \ncondor-ce-info-status\n) by accepting \nCPUs\n or \nCpus\n from the collector\n\n\n\n\n\n\nAdded two new scripts to help maintain tarball installations\n\n\nosg-update-vos: update VO client data\n\n\nosg-update-data: update VO client data and CA certificates\n\n\n\n\n\n\nrsv-perfsonar 1.2.1: Control over message send to the MQ\n\n\nInternal tools: koji, osg-build, osg-koji setup\n\n\nSee known issues below\n\n\n\n\n\n\nInternal automated tests: Use default cache location for CMVFS to address test failures with SELinux on EL6\n\n\n\n\n\n\nUpcoming repository\n\n\nSingularity 2.2.1\n Security Release\n\n\nSecurity Information\n\n\nIn versions of Singularity previous to 2.2.1, it was possible for a malicious user to create and manipulate specifically crafted raw devices within containers they own. Utilizing MS_NODEV as a container image mount option mitigates this potential vector of attack. As a result, this update should be implemented with high urgency.\n\n\n\n\n\n\nOther Improvements\n\n\nFixed some leaky file descriptors\n\n\nCleaned up *printf() usage\n\n\nCatch if user's group is not properly defined\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nKnown Issues\n\n\n\n\nThe glideinWMS 3.2.17 factory has a bug when you attempt to restart the service. You must first remove a lock file (\n/var/lib/gwms-factory/work-dir/lock/glideinWMS.lock\n) that isn't properly cleaned up when the service is stopped.\n\n\nThe Koji client config has changed in the new version of Koji: `pkgurl\nhttp://koji.chtc.wisc.edu/packages` has been replaced by `topurl=http://koji.chtc.wisc.edu` and the Koji client will give a harmless but annoying warning when it finds `pkgurl`. To get rid of the warning, update to osg-build \n 1.8.0, rerun `osg-koji setup`, and say 'yes' when asked to replace the Koji configuration file; or, you may make the above change manually.\n\n\nA previous version (\n1.17.0-2.5\n) of the Gratia probes required changes in the HTCondor configuration to operate properly. Now, these changes need to be reverted to use verison \n1.17.0-2.6\n and later of the probes. The lines below are likely to be found in \ncondor_config.local\n or a new file in \n/etc/condor/config.d\n directory. Delete these lines and run \ncondor_reconfig\n.\n\n\n\n\n# GlideinWMS Gratia commands\n\n\nPER_JOB_HISTORY_DIR\n \n=\n /var/lib/gratia/data\n\nJOBGLIDEIN_ResourceName\n=\n$$\n([IfThenElse(IsUndefined(TARGET.GLIDEIN_ResourceName), IfThenElse(IsUndefined(TARGET.GLIDEIN_Site), IfThenElse(IsUndefined(TARGET.FileSystemDomain), \\\nLocal Job\\\n, TARGET.FileSystemDomain), TARGET.GLIDEIN_Site), TARGET.GLIDEIN_ResourceName)])\n\n\nSUBMIT_EXPRS\n \n=\n \n$(\nSUBMIT_EXPRS\n)\n JOBGLIDEIN_ResourceName   \n\n\n\n\n\n\n\nOn EL7, your version of voms-proxy-init may come from the voms-clients-java package; this version will continue to make legacy proxies by default. If you want the new behavior, install the voms-clients-cpp package and uninstall the voms-clients-java package.\n\n\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\ncondor-8.4.11-1.osg33.el6\n\n\ncvmfs-2.3.2-1.1.osg33.el6\n\n\nglideinwms-3.2.17-1.osg33.el6\n\n\nhtcondor-ce-2.1.2-1.osg33.el6\n\n\nkoji-1.11.0-1.5.osg33.el6\n\n\nosg-build-1.8.0-1.osg33.el6\n\n\nosg-configure-1.6.1-1.osg33.el6\n\n\nosg-test-1.10.1-1.osg33.el6\n\n\nosg-update-vos-1.3.0-1.osg33.el6\n\n\nosg-version-3.3.21-1.osg33.el6\n\n\nrsv-perfsonar-1.2.1-1.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\ncondor-8.4.11-1.osg33.el7\n\n\ncvmfs-2.3.2-1.1.osg33.el7\n\n\nglideinwms-3.2.17-1.osg33.el7\n\n\nhtcondor-ce-2.1.2-1.osg33.el7\n\n\nkoji-1.11.0-1.5.osg33.el7\n\n\nosg-build-1.8.0-1.osg33.el7\n\n\nosg-configure-1.6.1-1.osg33.el7\n\n\nosg-test-1.10.1-1.osg33.el7\n\n\nosg-update-vos-1.3.0-1.osg33.el7\n\n\nosg-version-3.3.21-1.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\ncondor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp cvmfs cvmfs-devel cvmfs-server cvmfs-unittests glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-view igtf-ca-certs koji koji-builder koji-hub koji-hub-plugins koji-utils koji-vm koji-web osg-build osg-ca-certs osg-configure osg-configure-bosco osg-configure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-gums-config osg-test osg-test-log-viewer osg-update-data osg-update-vos osg-version rsv-perfsonar vo-client vo-client-edgmkgridmap\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\ncondor-8.4.11-1.osg33.el6\ncondor-all-8.4.11-1.osg33.el6\ncondor-bosco-8.4.11-1.osg33.el6\ncondor-classads-8.4.11-1.osg33.el6\ncondor-classads-devel-8.4.11-1.osg33.el6\ncondor-cream-gahp-8.4.11-1.osg33.el6\ncondor-debuginfo-8.4.11-1.osg33.el6\ncondor-kbdd-8.4.11-1.osg33.el6\ncondor-procd-8.4.11-1.osg33.el6\ncondor-python-8.4.11-1.osg33.el6\ncondor-std-universe-8.4.11-1.osg33.el6\ncondor-test-8.4.11-1.osg33.el6\ncondor-vm-gahp-8.4.11-1.osg33.el6\ncvmfs-2.3.2-1.1.osg33.el6\ncvmfs-devel-2.3.2-1.1.osg33.el6\ncvmfs-server-2.3.2-1.1.osg33.el6\ncvmfs-unittests-2.3.2-1.1.osg33.el6\nglideinwms-3.2.17-1.osg33.el6\nglideinwms-common-tools-3.2.17-1.osg33.el6\nglideinwms-condor-common-config-3.2.17-1.osg33.el6\nglideinwms-factory-3.2.17-1.osg33.el6\nglideinwms-factory-condor-3.2.17-1.osg33.el6\nglideinwms-glidecondor-tools-3.2.17-1.osg33.el6\nglideinwms-libs-3.2.17-1.osg33.el6\nglideinwms-minimal-condor-3.2.17-1.osg33.el6\nglideinwms-usercollector-3.2.17-1.osg33.el6\nglideinwms-userschedd-3.2.17-1.osg33.el6\nglideinwms-vofrontend-3.2.17-1.osg33.el6\nglideinwms-vofrontend-standalone-3.2.17-1.osg33.el6\nhtcondor-ce-2.1.2-1.osg33.el6\nhtcondor-ce-bosco-2.1.2-1.osg33.el6\nhtcondor-ce-client-2.1.2-1.osg33.el6\nhtcondor-ce-collector-2.1.2-1.osg33.el6\nhtcondor-ce-condor-2.1.2-1.osg33.el6\nhtcondor-ce-lsf-2.1.2-1.osg33.el6\nhtcondor-ce-pbs-2.1.2-1.osg33.el6\nhtcondor-ce-sge-2.1.2-1.osg33.el6\nhtcondor-ce-view-2.1.2-1.osg33.el6\nkoji-1.11.0-1.5.osg33.el6\nkoji-builder-1.11.0-1.5.osg33.el6\nkoji-hub-1.11.0-1.5.osg33.el6\nkoji-hub-plugins-1.11.0-1.5.osg33.el6\nkoji-utils-1.11.0-1.5.osg33.el6\nkoji-vm-1.11.0-1.5.osg33.el6\nkoji-web-1.11.0-1.5.osg33.el6\nosg-build-1.8.0-1.osg33.el6\nosg-configure-1.6.1-1.osg33.el6\nosg-configure-bosco-1.6.1-1.osg33.el6\nosg-configure-ce-1.6.1-1.osg33.el6\nosg-configure-cemon-1.6.1-1.osg33.el6\nosg-configure-condor-1.6.1-1.osg33.el6\nosg-configure-gateway-1.6.1-1.osg33.el6\nosg-configure-gip-1.6.1-1.osg33.el6\nosg-configure-gratia-1.6.1-1.osg33.el6\nosg-configure-infoservices-1.6.1-1.osg33.el6\nosg-configure-lsf-1.6.1-1.osg33.el6\nosg-configure-managedfork-1.6.1-1.osg33.el6\nosg-configure-misc-1.6.1-1.osg33.el6\nosg-configure-monalisa-1.6.1-1.osg33.el6\nosg-configure-network-1.6.1-1.osg33.el6\nosg-configure-pbs-1.6.1-1.osg33.el6\nosg-configure-rsv-1.6.1-1.osg33.el6\nosg-configure-sge-1.6.1-1.osg33.el6\nosg-configure-slurm-1.6.1-1.osg33.el6\nosg-configure-squid-1.6.1-1.osg33.el6\nosg-configure-tests-1.6.1-1.osg33.el6\nosg-test-1.10.1-1.osg33.el6\nosg-test-log-viewer-1.10.1-1.osg33.el6\nosg-update-data-1.3.0-1.osg33.el6\nosg-update-vos-1.3.0-1.osg33.el6\nosg-version-3.3.21-1.osg33.el6\nrsv-perfsonar-1.2.1-1.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\ncondor-8.4.11-1.osg33.el7\ncondor-all-8.4.11-1.osg33.el7\ncondor-bosco-8.4.11-1.osg33.el7\ncondor-classads-8.4.11-1.osg33.el7\ncondor-classads-devel-8.4.11-1.osg33.el7\ncondor-cream-gahp-8.4.11-1.osg33.el7\ncondor-debuginfo-8.4.11-1.osg33.el7\ncondor-kbdd-8.4.11-1.osg33.el7\ncondor-procd-8.4.11-1.osg33.el7\ncondor-python-8.4.11-1.osg33.el7\ncondor-test-8.4.11-1.osg33.el7\ncondor-vm-gahp-8.4.11-1.osg33.el7\ncvmfs-2.3.2-1.1.osg33.el7\ncvmfs-devel-2.3.2-1.1.osg33.el7\ncvmfs-server-2.3.2-1.1.osg33.el7\ncvmfs-unittests-2.3.2-1.1.osg33.el7\nglideinwms-3.2.17-1.osg33.el7\nglideinwms-common-tools-3.2.17-1.osg33.el7\nglideinwms-condor-common-config-3.2.17-1.osg33.el7\nglideinwms-factory-3.2.17-1.osg33.el7\nglideinwms-factory-condor-3.2.17-1.osg33.el7\nglideinwms-glidecondor-tools-3.2.17-1.osg33.el7\nglideinwms-libs-3.2.17-1.osg33.el7\nglideinwms-minimal-condor-3.2.17-1.osg33.el7\nglideinwms-usercollector-3.2.17-1.osg33.el7\nglideinwms-userschedd-3.2.17-1.osg33.el7\nglideinwms-vofrontend-3.2.17-1.osg33.el7\nglideinwms-vofrontend-standalone-3.2.17-1.osg33.el7\nhtcondor-ce-2.1.2-1.osg33.el7\nhtcondor-ce-bosco-2.1.2-1.osg33.el7\nhtcondor-ce-client-2.1.2-1.osg33.el7\nhtcondor-ce-collector-2.1.2-1.osg33.el7\nhtcondor-ce-condor-2.1.2-1.osg33.el7\nhtcondor-ce-lsf-2.1.2-1.osg33.el7\nhtcondor-ce-pbs-2.1.2-1.osg33.el7\nhtcondor-ce-sge-2.1.2-1.osg33.el7\nhtcondor-ce-view-2.1.2-1.osg33.el7\nkoji-1.11.0-1.5.osg33.el7\nkoji-builder-1.11.0-1.5.osg33.el7\nkoji-hub-1.11.0-1.5.osg33.el7\nkoji-hub-plugins-1.11.0-1.5.osg33.el7\nkoji-utils-1.11.0-1.5.osg33.el7\nkoji-vm-1.11.0-1.5.osg33.el7\nkoji-web-1.11.0-1.5.osg33.el7\nosg-build-1.8.0-1.osg33.el7\nosg-configure-1.6.1-1.osg33.el7\nosg-configure-bosco-1.6.1-1.osg33.el7\nosg-configure-ce-1.6.1-1.osg33.el7\nosg-configure-cemon-1.6.1-1.osg33.el7\nosg-configure-condor-1.6.1-1.osg33.el7\nosg-configure-gateway-1.6.1-1.osg33.el7\nosg-configure-gip-1.6.1-1.osg33.el7\nosg-configure-gratia-1.6.1-1.osg33.el7\nosg-configure-infoservices-1.6.1-1.osg33.el7\nosg-configure-lsf-1.6.1-1.osg33.el7\nosg-configure-managedfork-1.6.1-1.osg33.el7\nosg-configure-misc-1.6.1-1.osg33.el7\nosg-configure-monalisa-1.6.1-1.osg33.el7\nosg-configure-network-1.6.1-1.osg33.el7\nosg-configure-pbs-1.6.1-1.osg33.el7\nosg-configure-rsv-1.6.1-1.osg33.el7\nosg-configure-sge-1.6.1-1.osg33.el7\nosg-configure-slurm-1.6.1-1.osg33.el7\nosg-configure-squid-1.6.1-1.osg33.el7\nosg-configure-tests-1.6.1-1.osg33.el7\nosg-test-1.10.1-1.osg33.el7\nosg-test-log-viewer-1.10.1-1.osg33.el7\nosg-update-data-1.3.0-1.osg33.el7\nosg-update-vos-1.3.0-1.osg33.el7\nosg-version-3.3.21-1.osg33.el7\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nsingularity-2.2.1-1.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nsingularity-2.2.1-1.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nsingularity singularity-debuginfo singularity-devel\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nsingularity-2.2.1-1.osgup.el6\nsingularity-debuginfo-2.2.1-1.osgup.el6\nsingularity-devel-2.2.1-1.osgup.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nsingularity-2.2.1-1.osgup.el7\nsingularity-debuginfo-2.2.1-1.osgup.el7\nsingularity-devel-2.2.1-1.osgup.el7", 
            "title": "OSG Release 3.3.21"
        }, 
        {
            "location": "/release/3.3/release-3-3-21/#osg-software-release-3321", 
            "text": "Release Date : 2017-02-14", 
            "title": "OSG Software Release 3.3.21"
        }, 
        {
            "location": "/release/3.3/release-3-3-21/#summary-of-changes", 
            "text": "This release contains:   OSG 3.3.21  osg-configure 1.6.1:  Additional support  for ATLAS AGIS in osg-configure  glideinWMS 3.2.17  See known issues below    Important bug fixes for CVMFS server  CVM-1165  - swissknife hang during publish  CVM-1108  - Prevent garbage collection from running at the same time as snapshot  CVM-1153  - cvmfs build fails on centos7.3, in externals/build_c-ares    HTCondor 8.4.11 : Final bug fix release of the 8.4 series  HTCondor-CE 2.1.2  Accept Russian Data Intensive Grid certificates  Avoid crash in client tools (e.g.  condor-ce-info-status ) by accepting  CPUs  or  Cpus  from the collector    Added two new scripts to help maintain tarball installations  osg-update-vos: update VO client data  osg-update-data: update VO client data and CA certificates    rsv-perfsonar 1.2.1: Control over message send to the MQ  Internal tools: koji, osg-build, osg-koji setup  See known issues below    Internal automated tests: Use default cache location for CMVFS to address test failures with SELinux on EL6    Upcoming repository  Singularity 2.2.1  Security Release  Security Information  In versions of Singularity previous to 2.2.1, it was possible for a malicious user to create and manipulate specifically crafted raw devices within containers they own. Utilizing MS_NODEV as a container image mount option mitigates this potential vector of attack. As a result, this update should be implemented with high urgency.    Other Improvements  Fixed some leaky file descriptors  Cleaned up *printf() usage  Catch if user's group is not properly defined         These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-21/#known-issues", 
            "text": "The glideinWMS 3.2.17 factory has a bug when you attempt to restart the service. You must first remove a lock file ( /var/lib/gwms-factory/work-dir/lock/glideinWMS.lock ) that isn't properly cleaned up when the service is stopped.  The Koji client config has changed in the new version of Koji: `pkgurl http://koji.chtc.wisc.edu/packages` has been replaced by `topurl=http://koji.chtc.wisc.edu` and the Koji client will give a harmless but annoying warning when it finds `pkgurl`. To get rid of the warning, update to osg-build   1.8.0, rerun `osg-koji setup`, and say 'yes' when asked to replace the Koji configuration file; or, you may make the above change manually.  A previous version ( 1.17.0-2.5 ) of the Gratia probes required changes in the HTCondor configuration to operate properly. Now, these changes need to be reverted to use verison  1.17.0-2.6  and later of the probes. The lines below are likely to be found in  condor_config.local  or a new file in  /etc/condor/config.d  directory. Delete these lines and run  condor_reconfig .   # GlideinWMS Gratia commands  PER_JOB_HISTORY_DIR   =  /var/lib/gratia/data JOBGLIDEIN_ResourceName = $$ ([IfThenElse(IsUndefined(TARGET.GLIDEIN_ResourceName), IfThenElse(IsUndefined(TARGET.GLIDEIN_Site), IfThenElse(IsUndefined(TARGET.FileSystemDomain), \\ Local Job\\ , TARGET.FileSystemDomain), TARGET.GLIDEIN_Site), TARGET.GLIDEIN_ResourceName)])  SUBMIT_EXPRS   =   $( SUBMIT_EXPRS )  JOBGLIDEIN_ResourceName      On EL7, your version of voms-proxy-init may come from the voms-clients-java package; this version will continue to make legacy proxies by default. If you want the new behavior, install the voms-clients-cpp package and uninstall the voms-clients-java package.", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.3/release-3-3-21/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-21/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-21/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-21/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-21/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-21/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-21/#enterprise-linux-6", 
            "text": "condor-8.4.11-1.osg33.el6  cvmfs-2.3.2-1.1.osg33.el6  glideinwms-3.2.17-1.osg33.el6  htcondor-ce-2.1.2-1.osg33.el6  koji-1.11.0-1.5.osg33.el6  osg-build-1.8.0-1.osg33.el6  osg-configure-1.6.1-1.osg33.el6  osg-test-1.10.1-1.osg33.el6  osg-update-vos-1.3.0-1.osg33.el6  osg-version-3.3.21-1.osg33.el6  rsv-perfsonar-1.2.1-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-21/#enterprise-linux-7", 
            "text": "condor-8.4.11-1.osg33.el7  cvmfs-2.3.2-1.1.osg33.el7  glideinwms-3.2.17-1.osg33.el7  htcondor-ce-2.1.2-1.osg33.el7  koji-1.11.0-1.5.osg33.el7  osg-build-1.8.0-1.osg33.el7  osg-configure-1.6.1-1.osg33.el7  osg-test-1.10.1-1.osg33.el7  osg-update-vos-1.3.0-1.osg33.el7  osg-version-3.3.21-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-21/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp cvmfs cvmfs-devel cvmfs-server cvmfs-unittests glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-view igtf-ca-certs koji koji-builder koji-hub koji-hub-plugins koji-utils koji-vm koji-web osg-build osg-ca-certs osg-configure osg-configure-bosco osg-configure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-gums-config osg-test osg-test-log-viewer osg-update-data osg-update-vos osg-version rsv-perfsonar vo-client vo-client-edgmkgridmap  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-21/#enterprise-linux-6_1", 
            "text": "condor-8.4.11-1.osg33.el6\ncondor-all-8.4.11-1.osg33.el6\ncondor-bosco-8.4.11-1.osg33.el6\ncondor-classads-8.4.11-1.osg33.el6\ncondor-classads-devel-8.4.11-1.osg33.el6\ncondor-cream-gahp-8.4.11-1.osg33.el6\ncondor-debuginfo-8.4.11-1.osg33.el6\ncondor-kbdd-8.4.11-1.osg33.el6\ncondor-procd-8.4.11-1.osg33.el6\ncondor-python-8.4.11-1.osg33.el6\ncondor-std-universe-8.4.11-1.osg33.el6\ncondor-test-8.4.11-1.osg33.el6\ncondor-vm-gahp-8.4.11-1.osg33.el6\ncvmfs-2.3.2-1.1.osg33.el6\ncvmfs-devel-2.3.2-1.1.osg33.el6\ncvmfs-server-2.3.2-1.1.osg33.el6\ncvmfs-unittests-2.3.2-1.1.osg33.el6\nglideinwms-3.2.17-1.osg33.el6\nglideinwms-common-tools-3.2.17-1.osg33.el6\nglideinwms-condor-common-config-3.2.17-1.osg33.el6\nglideinwms-factory-3.2.17-1.osg33.el6\nglideinwms-factory-condor-3.2.17-1.osg33.el6\nglideinwms-glidecondor-tools-3.2.17-1.osg33.el6\nglideinwms-libs-3.2.17-1.osg33.el6\nglideinwms-minimal-condor-3.2.17-1.osg33.el6\nglideinwms-usercollector-3.2.17-1.osg33.el6\nglideinwms-userschedd-3.2.17-1.osg33.el6\nglideinwms-vofrontend-3.2.17-1.osg33.el6\nglideinwms-vofrontend-standalone-3.2.17-1.osg33.el6\nhtcondor-ce-2.1.2-1.osg33.el6\nhtcondor-ce-bosco-2.1.2-1.osg33.el6\nhtcondor-ce-client-2.1.2-1.osg33.el6\nhtcondor-ce-collector-2.1.2-1.osg33.el6\nhtcondor-ce-condor-2.1.2-1.osg33.el6\nhtcondor-ce-lsf-2.1.2-1.osg33.el6\nhtcondor-ce-pbs-2.1.2-1.osg33.el6\nhtcondor-ce-sge-2.1.2-1.osg33.el6\nhtcondor-ce-view-2.1.2-1.osg33.el6\nkoji-1.11.0-1.5.osg33.el6\nkoji-builder-1.11.0-1.5.osg33.el6\nkoji-hub-1.11.0-1.5.osg33.el6\nkoji-hub-plugins-1.11.0-1.5.osg33.el6\nkoji-utils-1.11.0-1.5.osg33.el6\nkoji-vm-1.11.0-1.5.osg33.el6\nkoji-web-1.11.0-1.5.osg33.el6\nosg-build-1.8.0-1.osg33.el6\nosg-configure-1.6.1-1.osg33.el6\nosg-configure-bosco-1.6.1-1.osg33.el6\nosg-configure-ce-1.6.1-1.osg33.el6\nosg-configure-cemon-1.6.1-1.osg33.el6\nosg-configure-condor-1.6.1-1.osg33.el6\nosg-configure-gateway-1.6.1-1.osg33.el6\nosg-configure-gip-1.6.1-1.osg33.el6\nosg-configure-gratia-1.6.1-1.osg33.el6\nosg-configure-infoservices-1.6.1-1.osg33.el6\nosg-configure-lsf-1.6.1-1.osg33.el6\nosg-configure-managedfork-1.6.1-1.osg33.el6\nosg-configure-misc-1.6.1-1.osg33.el6\nosg-configure-monalisa-1.6.1-1.osg33.el6\nosg-configure-network-1.6.1-1.osg33.el6\nosg-configure-pbs-1.6.1-1.osg33.el6\nosg-configure-rsv-1.6.1-1.osg33.el6\nosg-configure-sge-1.6.1-1.osg33.el6\nosg-configure-slurm-1.6.1-1.osg33.el6\nosg-configure-squid-1.6.1-1.osg33.el6\nosg-configure-tests-1.6.1-1.osg33.el6\nosg-test-1.10.1-1.osg33.el6\nosg-test-log-viewer-1.10.1-1.osg33.el6\nosg-update-data-1.3.0-1.osg33.el6\nosg-update-vos-1.3.0-1.osg33.el6\nosg-version-3.3.21-1.osg33.el6\nrsv-perfsonar-1.2.1-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-21/#enterprise-linux-7_1", 
            "text": "condor-8.4.11-1.osg33.el7\ncondor-all-8.4.11-1.osg33.el7\ncondor-bosco-8.4.11-1.osg33.el7\ncondor-classads-8.4.11-1.osg33.el7\ncondor-classads-devel-8.4.11-1.osg33.el7\ncondor-cream-gahp-8.4.11-1.osg33.el7\ncondor-debuginfo-8.4.11-1.osg33.el7\ncondor-kbdd-8.4.11-1.osg33.el7\ncondor-procd-8.4.11-1.osg33.el7\ncondor-python-8.4.11-1.osg33.el7\ncondor-test-8.4.11-1.osg33.el7\ncondor-vm-gahp-8.4.11-1.osg33.el7\ncvmfs-2.3.2-1.1.osg33.el7\ncvmfs-devel-2.3.2-1.1.osg33.el7\ncvmfs-server-2.3.2-1.1.osg33.el7\ncvmfs-unittests-2.3.2-1.1.osg33.el7\nglideinwms-3.2.17-1.osg33.el7\nglideinwms-common-tools-3.2.17-1.osg33.el7\nglideinwms-condor-common-config-3.2.17-1.osg33.el7\nglideinwms-factory-3.2.17-1.osg33.el7\nglideinwms-factory-condor-3.2.17-1.osg33.el7\nglideinwms-glidecondor-tools-3.2.17-1.osg33.el7\nglideinwms-libs-3.2.17-1.osg33.el7\nglideinwms-minimal-condor-3.2.17-1.osg33.el7\nglideinwms-usercollector-3.2.17-1.osg33.el7\nglideinwms-userschedd-3.2.17-1.osg33.el7\nglideinwms-vofrontend-3.2.17-1.osg33.el7\nglideinwms-vofrontend-standalone-3.2.17-1.osg33.el7\nhtcondor-ce-2.1.2-1.osg33.el7\nhtcondor-ce-bosco-2.1.2-1.osg33.el7\nhtcondor-ce-client-2.1.2-1.osg33.el7\nhtcondor-ce-collector-2.1.2-1.osg33.el7\nhtcondor-ce-condor-2.1.2-1.osg33.el7\nhtcondor-ce-lsf-2.1.2-1.osg33.el7\nhtcondor-ce-pbs-2.1.2-1.osg33.el7\nhtcondor-ce-sge-2.1.2-1.osg33.el7\nhtcondor-ce-view-2.1.2-1.osg33.el7\nkoji-1.11.0-1.5.osg33.el7\nkoji-builder-1.11.0-1.5.osg33.el7\nkoji-hub-1.11.0-1.5.osg33.el7\nkoji-hub-plugins-1.11.0-1.5.osg33.el7\nkoji-utils-1.11.0-1.5.osg33.el7\nkoji-vm-1.11.0-1.5.osg33.el7\nkoji-web-1.11.0-1.5.osg33.el7\nosg-build-1.8.0-1.osg33.el7\nosg-configure-1.6.1-1.osg33.el7\nosg-configure-bosco-1.6.1-1.osg33.el7\nosg-configure-ce-1.6.1-1.osg33.el7\nosg-configure-cemon-1.6.1-1.osg33.el7\nosg-configure-condor-1.6.1-1.osg33.el7\nosg-configure-gateway-1.6.1-1.osg33.el7\nosg-configure-gip-1.6.1-1.osg33.el7\nosg-configure-gratia-1.6.1-1.osg33.el7\nosg-configure-infoservices-1.6.1-1.osg33.el7\nosg-configure-lsf-1.6.1-1.osg33.el7\nosg-configure-managedfork-1.6.1-1.osg33.el7\nosg-configure-misc-1.6.1-1.osg33.el7\nosg-configure-monalisa-1.6.1-1.osg33.el7\nosg-configure-network-1.6.1-1.osg33.el7\nosg-configure-pbs-1.6.1-1.osg33.el7\nosg-configure-rsv-1.6.1-1.osg33.el7\nosg-configure-sge-1.6.1-1.osg33.el7\nosg-configure-slurm-1.6.1-1.osg33.el7\nosg-configure-squid-1.6.1-1.osg33.el7\nosg-configure-tests-1.6.1-1.osg33.el7\nosg-test-1.10.1-1.osg33.el7\nosg-test-log-viewer-1.10.1-1.osg33.el7\nosg-update-data-1.3.0-1.osg33.el7\nosg-update-vos-1.3.0-1.osg33.el7\nosg-version-3.3.21-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-21/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-21/#enterprise-linux-6_2", 
            "text": "singularity-2.2.1-1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-21/#enterprise-linux-7_2", 
            "text": "singularity-2.2.1-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-21/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  singularity singularity-debuginfo singularity-devel  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-21/#enterprise-linux-6_3", 
            "text": "singularity-2.2.1-1.osgup.el6\nsingularity-debuginfo-2.2.1-1.osgup.el6\nsingularity-devel-2.2.1-1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-21/#enterprise-linux-7_3", 
            "text": "singularity-2.2.1-1.osgup.el7\nsingularity-debuginfo-2.2.1-1.osgup.el7\nsingularity-devel-2.2.1-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-20-2/", 
            "text": "OSG Software Stack -- Data Release -- 3.3.20-2\n\n\nRelease Date\n: 2017-01-26\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nCA Certificates based on \nIGTF 1.79\n\n\nUpdated UNLPGrid CA with extended validity period (AR)\n\n\nFixed regular expressions in CILogon and NCSA CA namespaces files (US)\n\n\nIncluded rollover CA IRAN-GRID-CGC-G2 (IR)\n\n\nCorrected an incorrect line in selected info files for DigiCert (US)\n\n\nDiscontinued expiring NECTEC CA (TH)\n\n\n\n\n\n\nVO Package v70\n\n\nDeleted MCDRD VO\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nigtf-ca-certs-1.79-1.osg33.el6\n\n\nosg-ca-certs-1.59-1.osg33.el6\n\n\nvo-client-70-1.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nigtf-ca-certs-1.79-1.osg33.el7\n\n\nosg-ca-certs-1.59-1.osg33.el7\n\n\nvo-client-70-1.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nigtf-ca-certs osg-ca-certs osg-gums-config vo-client vo-client-edgmkgridmap\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nigtf-ca-certs-1.79-1.osg33.el6\nosg-ca-certs-1.59-1.osg33.el6\nosg-gums-config-70-1.osg33.el6\nvo-client-70-1.osg33.el6\nvo-client-edgmkgridmap-70-1.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nigtf-ca-certs-1.79-1.osg33.el7\nosg-ca-certs-1.59-1.osg33.el7\nosg-gums-config-70-1.osg33.el7\nvo-client-70-1.osg33.el7\nvo-client-edgmkgridmap-70-1.osg33.el7", 
            "title": "OSG Release 3.3.20-2"
        }, 
        {
            "location": "/release/3.3/release-3-3-20-2/#osg-software-stack-data-release-3320-2", 
            "text": "Release Date : 2017-01-26", 
            "title": "OSG Software Stack -- Data Release -- 3.3.20-2"
        }, 
        {
            "location": "/release/3.3/release-3-3-20-2/#summary-of-changes", 
            "text": "This release contains:   CA Certificates based on  IGTF 1.79  Updated UNLPGrid CA with extended validity period (AR)  Fixed regular expressions in CILogon and NCSA CA namespaces files (US)  Included rollover CA IRAN-GRID-CGC-G2 (IR)  Corrected an incorrect line in selected info files for DigiCert (US)  Discontinued expiring NECTEC CA (TH)    VO Package v70  Deleted MCDRD VO     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-20-2/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-20-2/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-20-2/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-20-2/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-20-2/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-20-2/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-20-2/#enterprise-linux-6", 
            "text": "igtf-ca-certs-1.79-1.osg33.el6  osg-ca-certs-1.59-1.osg33.el6  vo-client-70-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-20-2/#enterprise-linux-7", 
            "text": "igtf-ca-certs-1.79-1.osg33.el7  osg-ca-certs-1.59-1.osg33.el7  vo-client-70-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-20-2/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  igtf-ca-certs osg-ca-certs osg-gums-config vo-client vo-client-edgmkgridmap  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-20-2/#enterprise-linux-6_1", 
            "text": "igtf-ca-certs-1.79-1.osg33.el6\nosg-ca-certs-1.59-1.osg33.el6\nosg-gums-config-70-1.osg33.el6\nvo-client-70-1.osg33.el6\nvo-client-edgmkgridmap-70-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-20-2/#enterprise-linux-7_1", 
            "text": "igtf-ca-certs-1.79-1.osg33.el7\nosg-ca-certs-1.59-1.osg33.el7\nosg-gums-config-70-1.osg33.el7\nvo-client-70-1.osg33.el7\nvo-client-edgmkgridmap-70-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-20/", 
            "text": "OSG Software Release 3.3.20\n\n\nRelease Date\n: 2017-01-10\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nOSG 3.3.20\n\n\nHTCondor 8.4.10\n: Running in SELinux should work now, other bug fixes\n\n\ngratia-probe 1.17.2: Improved ability to report local jobs to OSG or not\n\n\nUpdated to \nXRootD 4.5.0\n\n\nUpdated gridftp-hdfs to enable ordered data\n\n\nosg-configure 1.5.4: Further updates to support ATLAS AGIS \nSOFTWARE-2554\n\n\nEnsure HTCondor-CE gratia probe is installed when installing osg-ce-bosco\n\n\nUpdated to VOMS 2.0.14\n\n\nCompleted conversion of packages to use systemd-tmpfiles on EL7\n\n\nzookeeper: populates /var/run/zookeeper using systemd-tmpfiles\n\n\n\n\n\n\n\n\n\n\nUpcoming repository\n\n\nUpdated to \nHTCondor 8.5.8\n\n\nAdded \nSingularity 2.2\n as a new, preview technology\n\n\nUpdated to \nfrontier-squid 3.5.23-3.1\n, a technology preview of version 3\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nKnown Issues\n\n\n\n\nWhen updating or installing HTCondor on an EL 7 system with SELinux enabled, make sure that policycoreutils-python is installed before HTCondor. This dependency will be properly declared in the HTCondor RPM in the next release.\n\n\nA previous version (\n1.17.0-2.5\n) of the Gratia probes required changes in the HTCondor configuration to operate properly. Now, these changes need to be reverted to use verison (\n1.17.0-2.6\n) and later of the probes. The lines below are likely to be found in \ncondor_config.local\n or a new file in \n/etc/condor/config.d\n directory. Delete these lines and run \ncondor_reconfig\n.\n\n\n\n\n# GlideinWMS Gratia commands\n\n\nPER_JOB_HISTORY_DIR\n \n=\n /var/lib/gratia/data\n\nJOBGLIDEIN_ResourceName\n=\n$$\n([IfThenElse(IsUndefined(TARGET.GLIDEIN_ResourceName), IfThenElse(IsUndefined(TARGET.GLIDEIN_Site), IfThenElse(IsUndefined(TARGET.FileSystemDomain), \\\nLocal Job\\\n, TARGET.FileSystemDomain), TARGET.GLIDEIN_Site), TARGET.GLIDEIN_ResourceName)])\n\n\nSUBMIT_EXPRS\n \n=\n \n$(\nSUBMIT_EXPRS\n)\n JOBGLIDEIN_ResourceName   \n\n\n\n\n\n\n\nOn EL7, your version of voms-proxy-init may come from the voms-clients-java package; this version will continue to make legacy proxies by default. If you want the new behavior, install the voms-clients-cpp package and uninstall the voms-clients-java package.\n\n\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.28.bosco-2.osg33.el6\n\n\ncondor-8.4.10-1.osg33.el6\n\n\nglobus-ftp-control-7.7-1.osg33.el6\n\n\nglobus-gridftp-server-11.8-1.1.osg33.el6\n\n\ngratia-probe-1.17.2-1.osg33.el6\n\n\ngridftp-hdfs-0.5.4-25.5.osg33.el6\n\n\nosg-ce-3.3-10.osg33.el6\n\n\nosg-configure-1.5.4-1.osg33.el6\n\n\nosg-test-1.10.0-1.osg33.el6\n\n\nosg-version-3.3.20-1.osg33.el6\n\n\nvoms-2.0.14-1.2.osg33.el6\n\n\nxrootd-4.5.0-2.osg33.el6\n\n\nzookeeper-3.4.3+15-1.cdh4.0.1.p0.4.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.28.bosco-2.osg33.el7\n\n\ncondor-8.4.10-1.osg33.el7\n\n\nglobus-ftp-control-7.7-1.osg33.el7\n\n\nglobus-gridftp-server-11.8-1.1.osg33.el7\n\n\ngratia-probe-1.17.2-1.osg33.el7\n\n\ngridftp-hdfs-0.5.4-25.5.osg33.el7\n\n\nosg-ce-3.3-10.osg33.el7\n\n\nosg-configure-1.5.4-1.osg33.el7\n\n\nosg-test-1.10.0-1.osg33.el7\n\n\nosg-version-3.3.20-1.osg33.el7\n\n\nvoms-2.0.14-1.2.osg33.el7\n\n\nxrootd-4.5.0-2.osg33.el7\n\n\nzookeeper-3.4.3+15-1.cdh4.0.1.p0.4.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp globus-ftp-control globus-ftp-control-debuginfo globus-ftp-control-devel globus-ftp-control-doc globus-gridftp-server globus-gridftp-server-debuginfo globus-gridftp-server-devel globus-gridftp-server-progs gratia-probe-bdii-status gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glexec gratia-probe-glideinwms gratia-probe-gram gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer gridftp-hdfs gridftp-hdfs-debuginfo osg-base-ce osg-base-ce-bosco osg-base-ce-condor osg-base-ce-lsf osg-base-ce-pbs osg-base-ce-sge osg-base-ce-slurm osg-ce osg-ce-bosco osg-ce-condor osg-ce-lsf osg-ce-pbs osg-ce-sge osg-ce-slurm osg-configure osg-configure-bosco osg-configure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-htcondor-ce osg-htcondor-ce-bosco osg-htcondor-ce-condor osg-htcondor-ce-lsf osg-htcondor-ce-pbs osg-htcondor-ce-sge osg-htcondor-ce-slurm osg-test osg-test-log-viewer osg-version voms voms-clients-cpp voms-debuginfo voms-devel voms-doc voms-server xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-libs xrootd-private-devel xrootd-python xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs zookeeper zookeeper-server\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp-1.18.28.bosco-2.osg33.el6\nblahp-debuginfo-1.18.28.bosco-2.osg33.el6\ncondor-8.4.10-1.osg33.el6\ncondor-all-8.4.10-1.osg33.el6\ncondor-bosco-8.4.10-1.osg33.el6\ncondor-classads-8.4.10-1.osg33.el6\ncondor-classads-devel-8.4.10-1.osg33.el6\ncondor-cream-gahp-8.4.10-1.osg33.el6\ncondor-debuginfo-8.4.10-1.osg33.el6\ncondor-kbdd-8.4.10-1.osg33.el6\ncondor-procd-8.4.10-1.osg33.el6\ncondor-python-8.4.10-1.osg33.el6\ncondor-std-universe-8.4.10-1.osg33.el6\ncondor-test-8.4.10-1.osg33.el6\ncondor-vm-gahp-8.4.10-1.osg33.el6\nglobus-ftp-control-7.7-1.osg33.el6\nglobus-ftp-control-debuginfo-7.7-1.osg33.el6\nglobus-ftp-control-devel-7.7-1.osg33.el6\nglobus-ftp-control-doc-7.7-1.osg33.el6\nglobus-gridftp-server-11.8-1.1.osg33.el6\nglobus-gridftp-server-debuginfo-11.8-1.1.osg33.el6\nglobus-gridftp-server-devel-11.8-1.1.osg33.el6\nglobus-gridftp-server-progs-11.8-1.1.osg33.el6\ngratia-probe-1.17.2-1.osg33.el6\ngratia-probe-bdii-status-1.17.2-1.osg33.el6\ngratia-probe-common-1.17.2-1.osg33.el6\ngratia-probe-condor-1.17.2-1.osg33.el6\ngratia-probe-condor-events-1.17.2-1.osg33.el6\ngratia-probe-dcache-storage-1.17.2-1.osg33.el6\ngratia-probe-dcache-storagegroup-1.17.2-1.osg33.el6\ngratia-probe-dcache-transfer-1.17.2-1.osg33.el6\ngratia-probe-debuginfo-1.17.2-1.osg33.el6\ngratia-probe-enstore-storage-1.17.2-1.osg33.el6\ngratia-probe-enstore-tapedrive-1.17.2-1.osg33.el6\ngratia-probe-enstore-transfer-1.17.2-1.osg33.el6\ngratia-probe-glexec-1.17.2-1.osg33.el6\ngratia-probe-glideinwms-1.17.2-1.osg33.el6\ngratia-probe-gram-1.17.2-1.osg33.el6\ngratia-probe-gridftp-transfer-1.17.2-1.osg33.el6\ngratia-probe-hadoop-storage-1.17.2-1.osg33.el6\ngratia-probe-htcondor-ce-1.17.2-1.osg33.el6\ngratia-probe-lsf-1.17.2-1.osg33.el6\ngratia-probe-metric-1.17.2-1.osg33.el6\ngratia-probe-onevm-1.17.2-1.osg33.el6\ngratia-probe-pbs-lsf-1.17.2-1.osg33.el6\ngratia-probe-services-1.17.2-1.osg33.el6\ngratia-probe-sge-1.17.2-1.osg33.el6\ngratia-probe-slurm-1.17.2-1.osg33.el6\ngratia-probe-xrootd-storage-1.17.2-1.osg33.el6\ngratia-probe-xrootd-transfer-1.17.2-1.osg33.el6\ngridftp-hdfs-0.5.4-25.5.osg33.el6\ngridftp-hdfs-debuginfo-0.5.4-25.5.osg33.el6\nosg-base-ce-3.3-10.osg33.el6\nosg-base-ce-bosco-3.3-10.osg33.el6\nosg-base-ce-condor-3.3-10.osg33.el6\nosg-base-ce-lsf-3.3-10.osg33.el6\nosg-base-ce-pbs-3.3-10.osg33.el6\nosg-base-ce-sge-3.3-10.osg33.el6\nosg-base-ce-slurm-3.3-10.osg33.el6\nosg-ce-3.3-10.osg33.el6\nosg-ce-bosco-3.3-10.osg33.el6\nosg-ce-condor-3.3-10.osg33.el6\nosg-ce-lsf-3.3-10.osg33.el6\nosg-ce-pbs-3.3-10.osg33.el6\nosg-ce-sge-3.3-10.osg33.el6\nosg-ce-slurm-3.3-10.osg33.el6\nosg-configure-1.5.4-1.osg33.el6\nosg-configure-bosco-1.5.4-1.osg33.el6\nosg-configure-ce-1.5.4-1.osg33.el6\nosg-configure-cemon-1.5.4-1.osg33.el6\nosg-configure-condor-1.5.4-1.osg33.el6\nosg-configure-gateway-1.5.4-1.osg33.el6\nosg-configure-gip-1.5.4-1.osg33.el6\nosg-configure-gratia-1.5.4-1.osg33.el6\nosg-configure-infoservices-1.5.4-1.osg33.el6\nosg-configure-lsf-1.5.4-1.osg33.el6\nosg-configure-managedfork-1.5.4-1.osg33.el6\nosg-configure-misc-1.5.4-1.osg33.el6\nosg-configure-monalisa-1.5.4-1.osg33.el6\nosg-configure-network-1.5.4-1.osg33.el6\nosg-configure-pbs-1.5.4-1.osg33.el6\nosg-configure-rsv-1.5.4-1.osg33.el6\nosg-configure-sge-1.5.4-1.osg33.el6\nosg-configure-slurm-1.5.4-1.osg33.el6\nosg-configure-squid-1.5.4-1.osg33.el6\nosg-configure-tests-1.5.4-1.osg33.el6\nosg-htcondor-ce-3.3-10.osg33.el6\nosg-htcondor-ce-bosco-3.3-10.osg33.el6\nosg-htcondor-ce-condor-3.3-10.osg33.el6\nosg-htcondor-ce-lsf-3.3-10.osg33.el6\nosg-htcondor-ce-pbs-3.3-10.osg33.el6\nosg-htcondor-ce-sge-3.3-10.osg33.el6\nosg-htcondor-ce-slurm-3.3-10.osg33.el6\nosg-test-1.10.0-1.osg33.el6\nosg-test-log-viewer-1.10.0-1.osg33.el6\nosg-version-3.3.20-1.osg33.el6\nvoms-2.0.14-1.2.osg33.el6\nvoms-clients-cpp-2.0.14-1.2.osg33.el6\nvoms-debuginfo-2.0.14-1.2.osg33.el6\nvoms-devel-2.0.14-1.2.osg33.el6\nvoms-doc-2.0.14-1.2.osg33.el6\nvoms-server-2.0.14-1.2.osg33.el6\nxrootd-4.5.0-2.osg33.el6\nxrootd-client-4.5.0-2.osg33.el6\nxrootd-client-devel-4.5.0-2.osg33.el6\nxrootd-client-libs-4.5.0-2.osg33.el6\nxrootd-debuginfo-4.5.0-2.osg33.el6\nxrootd-devel-4.5.0-2.osg33.el6\nxrootd-doc-4.5.0-2.osg33.el6\nxrootd-fuse-4.5.0-2.osg33.el6\nxrootd-libs-4.5.0-2.osg33.el6\nxrootd-private-devel-4.5.0-2.osg33.el6\nxrootd-python-4.5.0-2.osg33.el6\nxrootd-selinux-4.5.0-2.osg33.el6\nxrootd-server-4.5.0-2.osg33.el6\nxrootd-server-devel-4.5.0-2.osg33.el6\nxrootd-server-libs-4.5.0-2.osg33.el6\nzookeeper-3.4.3+15-1.cdh4.0.1.p0.4.osg33.el6\nzookeeper-server-3.4.3+15-1.cdh4.0.1.p0.4.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp-1.18.28.bosco-2.osg33.el7\nblahp-debuginfo-1.18.28.bosco-2.osg33.el7\ncondor-8.4.10-1.osg33.el7\ncondor-all-8.4.10-1.osg33.el7\ncondor-bosco-8.4.10-1.osg33.el7\ncondor-classads-8.4.10-1.osg33.el7\ncondor-classads-devel-8.4.10-1.osg33.el7\ncondor-cream-gahp-8.4.10-1.osg33.el7\ncondor-debuginfo-8.4.10-1.osg33.el7\ncondor-kbdd-8.4.10-1.osg33.el7\ncondor-procd-8.4.10-1.osg33.el7\ncondor-python-8.4.10-1.osg33.el7\ncondor-test-8.4.10-1.osg33.el7\ncondor-vm-gahp-8.4.10-1.osg33.el7\nglobus-ftp-control-7.7-1.osg33.el7\nglobus-ftp-control-debuginfo-7.7-1.osg33.el7\nglobus-ftp-control-devel-7.7-1.osg33.el7\nglobus-ftp-control-doc-7.7-1.osg33.el7\nglobus-gridftp-server-11.8-1.1.osg33.el7\nglobus-gridftp-server-debuginfo-11.8-1.1.osg33.el7\nglobus-gridftp-server-devel-11.8-1.1.osg33.el7\nglobus-gridftp-server-progs-11.8-1.1.osg33.el7\ngratia-probe-1.17.2-1.osg33.el7\ngratia-probe-bdii-status-1.17.2-1.osg33.el7\ngratia-probe-common-1.17.2-1.osg33.el7\ngratia-probe-condor-1.17.2-1.osg33.el7\ngratia-probe-condor-events-1.17.2-1.osg33.el7\ngratia-probe-dcache-storage-1.17.2-1.osg33.el7\ngratia-probe-dcache-storagegroup-1.17.2-1.osg33.el7\ngratia-probe-dcache-transfer-1.17.2-1.osg33.el7\ngratia-probe-debuginfo-1.17.2-1.osg33.el7\ngratia-probe-enstore-storage-1.17.2-1.osg33.el7\ngratia-probe-enstore-tapedrive-1.17.2-1.osg33.el7\ngratia-probe-enstore-transfer-1.17.2-1.osg33.el7\ngratia-probe-glexec-1.17.2-1.osg33.el7\ngratia-probe-glideinwms-1.17.2-1.osg33.el7\ngratia-probe-gram-1.17.2-1.osg33.el7\ngratia-probe-gridftp-transfer-1.17.2-1.osg33.el7\ngratia-probe-hadoop-storage-1.17.2-1.osg33.el7\ngratia-probe-htcondor-ce-1.17.2-1.osg33.el7\ngratia-probe-lsf-1.17.2-1.osg33.el7\ngratia-probe-metric-1.17.2-1.osg33.el7\ngratia-probe-onevm-1.17.2-1.osg33.el7\ngratia-probe-pbs-lsf-1.17.2-1.osg33.el7\ngratia-probe-services-1.17.2-1.osg33.el7\ngratia-probe-sge-1.17.2-1.osg33.el7\ngratia-probe-slurm-1.17.2-1.osg33.el7\ngratia-probe-xrootd-storage-1.17.2-1.osg33.el7\ngratia-probe-xrootd-transfer-1.17.2-1.osg33.el7\ngridftp-hdfs-0.5.4-25.5.osg33.el7\ngridftp-hdfs-debuginfo-0.5.4-25.5.osg33.el7\nosg-base-ce-3.3-10.osg33.el7\nosg-base-ce-bosco-3.3-10.osg33.el7\nosg-base-ce-condor-3.3-10.osg33.el7\nosg-base-ce-lsf-3.3-10.osg33.el7\nosg-base-ce-pbs-3.3-10.osg33.el7\nosg-base-ce-sge-3.3-10.osg33.el7\nosg-base-ce-slurm-3.3-10.osg33.el7\nosg-ce-3.3-10.osg33.el7\nosg-ce-bosco-3.3-10.osg33.el7\nosg-ce-condor-3.3-10.osg33.el7\nosg-ce-lsf-3.3-10.osg33.el7\nosg-ce-pbs-3.3-10.osg33.el7\nosg-ce-sge-3.3-10.osg33.el7\nosg-ce-slurm-3.3-10.osg33.el7\nosg-configure-1.5.4-1.osg33.el7\nosg-configure-bosco-1.5.4-1.osg33.el7\nosg-configure-ce-1.5.4-1.osg33.el7\nosg-configure-cemon-1.5.4-1.osg33.el7\nosg-configure-condor-1.5.4-1.osg33.el7\nosg-configure-gateway-1.5.4-1.osg33.el7\nosg-configure-gip-1.5.4-1.osg33.el7\nosg-configure-gratia-1.5.4-1.osg33.el7\nosg-configure-infoservices-1.5.4-1.osg33.el7\nosg-configure-lsf-1.5.4-1.osg33.el7\nosg-configure-managedfork-1.5.4-1.osg33.el7\nosg-configure-misc-1.5.4-1.osg33.el7\nosg-configure-monalisa-1.5.4-1.osg33.el7\nosg-configure-network-1.5.4-1.osg33.el7\nosg-configure-pbs-1.5.4-1.osg33.el7\nosg-configure-rsv-1.5.4-1.osg33.el7\nosg-configure-sge-1.5.4-1.osg33.el7\nosg-configure-slurm-1.5.4-1.osg33.el7\nosg-configure-squid-1.5.4-1.osg33.el7\nosg-configure-tests-1.5.4-1.osg33.el7\nosg-htcondor-ce-3.3-10.osg33.el7\nosg-htcondor-ce-bosco-3.3-10.osg33.el7\nosg-htcondor-ce-condor-3.3-10.osg33.el7\nosg-htcondor-ce-lsf-3.3-10.osg33.el7\nosg-htcondor-ce-pbs-3.3-10.osg33.el7\nosg-htcondor-ce-sge-3.3-10.osg33.el7\nosg-htcondor-ce-slurm-3.3-10.osg33.el7\nosg-test-1.10.0-1.osg33.el7\nosg-test-log-viewer-1.10.0-1.osg33.el7\nosg-version-3.3.20-1.osg33.el7\nvoms-2.0.14-1.2.osg33.el7\nvoms-clients-cpp-2.0.14-1.2.osg33.el7\nvoms-debuginfo-2.0.14-1.2.osg33.el7\nvoms-devel-2.0.14-1.2.osg33.el7\nvoms-doc-2.0.14-1.2.osg33.el7\nvoms-server-2.0.14-1.2.osg33.el7\nxrootd-4.5.0-2.osg33.el7\nxrootd-client-4.5.0-2.osg33.el7\nxrootd-client-devel-4.5.0-2.osg33.el7\nxrootd-client-libs-4.5.0-2.osg33.el7\nxrootd-debuginfo-4.5.0-2.osg33.el7\nxrootd-devel-4.5.0-2.osg33.el7\nxrootd-doc-4.5.0-2.osg33.el7\nxrootd-fuse-4.5.0-2.osg33.el7\nxrootd-libs-4.5.0-2.osg33.el7\nxrootd-private-devel-4.5.0-2.osg33.el7\nxrootd-python-4.5.0-2.osg33.el7\nxrootd-selinux-4.5.0-2.osg33.el7\nxrootd-server-4.5.0-2.osg33.el7\nxrootd-server-devel-4.5.0-2.osg33.el7\nxrootd-server-libs-4.5.0-2.osg33.el7\nzookeeper-3.4.3+15-1.cdh4.0.1.p0.4.osg33.el7\nzookeeper-server-3.4.3+15-1.cdh4.0.1.p0.4.osg33.el7\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.28.bosco-2.osgup.el6\n\n\ncondor-8.5.8-1.osgup.el6\n\n\nfrontier-squid-3.5.23-3.1.osgup.el6\n\n\nsingularity-2.2-1.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.28.bosco-2.osgup.el7\n\n\ncondor-8.5.8-1.osgup.el7\n\n\nfrontier-squid-3.5.23-3.1.osgup.el7\n\n\nsingularity-2.2-1.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp frontier-squid frontier-squid-debuginfo singularity singularity-debuginfo singularity-devel\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp-1.18.28.bosco-2.osgup.el6\nblahp-debuginfo-1.18.28.bosco-2.osgup.el6\ncondor-8.5.8-1.osgup.el6\ncondor-all-8.5.8-1.osgup.el6\ncondor-bosco-8.5.8-1.osgup.el6\ncondor-classads-8.5.8-1.osgup.el6\ncondor-classads-devel-8.5.8-1.osgup.el6\ncondor-cream-gahp-8.5.8-1.osgup.el6\ncondor-debuginfo-8.5.8-1.osgup.el6\ncondor-kbdd-8.5.8-1.osgup.el6\ncondor-procd-8.5.8-1.osgup.el6\ncondor-python-8.5.8-1.osgup.el6\ncondor-std-universe-8.5.8-1.osgup.el6\ncondor-test-8.5.8-1.osgup.el6\ncondor-vm-gahp-8.5.8-1.osgup.el6\nfrontier-squid-3.5.23-3.1.osgup.el6\nfrontier-squid-debuginfo-3.5.23-3.1.osgup.el6\nsingularity-2.2-1.osgup.el6\nsingularity-debuginfo-2.2-1.osgup.el6\nsingularity-devel-2.2-1.osgup.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp-1.18.28.bosco-2.osgup.el7\nblahp-debuginfo-1.18.28.bosco-2.osgup.el7\ncondor-8.5.8-1.osgup.el7\ncondor-all-8.5.8-1.osgup.el7\ncondor-bosco-8.5.8-1.osgup.el7\ncondor-classads-8.5.8-1.osgup.el7\ncondor-classads-devel-8.5.8-1.osgup.el7\ncondor-cream-gahp-8.5.8-1.osgup.el7\ncondor-debuginfo-8.5.8-1.osgup.el7\ncondor-kbdd-8.5.8-1.osgup.el7\ncondor-procd-8.5.8-1.osgup.el7\ncondor-python-8.5.8-1.osgup.el7\ncondor-test-8.5.8-1.osgup.el7\ncondor-vm-gahp-8.5.8-1.osgup.el7\nfrontier-squid-3.5.23-3.1.osgup.el7\nfrontier-squid-debuginfo-3.5.23-3.1.osgup.el7\nsingularity-2.2-1.osgup.el7\nsingularity-debuginfo-2.2-1.osgup.el7\nsingularity-devel-2.2-1.osgup.el7", 
            "title": "OSG Release 3.3.20"
        }, 
        {
            "location": "/release/3.3/release-3-3-20/#osg-software-release-3320", 
            "text": "Release Date : 2017-01-10", 
            "title": "OSG Software Release 3.3.20"
        }, 
        {
            "location": "/release/3.3/release-3-3-20/#summary-of-changes", 
            "text": "This release contains:   OSG 3.3.20  HTCondor 8.4.10 : Running in SELinux should work now, other bug fixes  gratia-probe 1.17.2: Improved ability to report local jobs to OSG or not  Updated to  XRootD 4.5.0  Updated gridftp-hdfs to enable ordered data  osg-configure 1.5.4: Further updates to support ATLAS AGIS  SOFTWARE-2554  Ensure HTCondor-CE gratia probe is installed when installing osg-ce-bosco  Updated to VOMS 2.0.14  Completed conversion of packages to use systemd-tmpfiles on EL7  zookeeper: populates /var/run/zookeeper using systemd-tmpfiles      Upcoming repository  Updated to  HTCondor 8.5.8  Added  Singularity 2.2  as a new, preview technology  Updated to  frontier-squid 3.5.23-3.1 , a technology preview of version 3     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-20/#known-issues", 
            "text": "When updating or installing HTCondor on an EL 7 system with SELinux enabled, make sure that policycoreutils-python is installed before HTCondor. This dependency will be properly declared in the HTCondor RPM in the next release.  A previous version ( 1.17.0-2.5 ) of the Gratia probes required changes in the HTCondor configuration to operate properly. Now, these changes need to be reverted to use verison ( 1.17.0-2.6 ) and later of the probes. The lines below are likely to be found in  condor_config.local  or a new file in  /etc/condor/config.d  directory. Delete these lines and run  condor_reconfig .   # GlideinWMS Gratia commands  PER_JOB_HISTORY_DIR   =  /var/lib/gratia/data JOBGLIDEIN_ResourceName = $$ ([IfThenElse(IsUndefined(TARGET.GLIDEIN_ResourceName), IfThenElse(IsUndefined(TARGET.GLIDEIN_Site), IfThenElse(IsUndefined(TARGET.FileSystemDomain), \\ Local Job\\ , TARGET.FileSystemDomain), TARGET.GLIDEIN_Site), TARGET.GLIDEIN_ResourceName)])  SUBMIT_EXPRS   =   $( SUBMIT_EXPRS )  JOBGLIDEIN_ResourceName      On EL7, your version of voms-proxy-init may come from the voms-clients-java package; this version will continue to make legacy proxies by default. If you want the new behavior, install the voms-clients-cpp package and uninstall the voms-clients-java package.", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.3/release-3-3-20/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-20/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-20/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-20/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-20/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-20/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-20/#enterprise-linux-6", 
            "text": "blahp-1.18.28.bosco-2.osg33.el6  condor-8.4.10-1.osg33.el6  globus-ftp-control-7.7-1.osg33.el6  globus-gridftp-server-11.8-1.1.osg33.el6  gratia-probe-1.17.2-1.osg33.el6  gridftp-hdfs-0.5.4-25.5.osg33.el6  osg-ce-3.3-10.osg33.el6  osg-configure-1.5.4-1.osg33.el6  osg-test-1.10.0-1.osg33.el6  osg-version-3.3.20-1.osg33.el6  voms-2.0.14-1.2.osg33.el6  xrootd-4.5.0-2.osg33.el6  zookeeper-3.4.3+15-1.cdh4.0.1.p0.4.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-20/#enterprise-linux-7", 
            "text": "blahp-1.18.28.bosco-2.osg33.el7  condor-8.4.10-1.osg33.el7  globus-ftp-control-7.7-1.osg33.el7  globus-gridftp-server-11.8-1.1.osg33.el7  gratia-probe-1.17.2-1.osg33.el7  gridftp-hdfs-0.5.4-25.5.osg33.el7  osg-ce-3.3-10.osg33.el7  osg-configure-1.5.4-1.osg33.el7  osg-test-1.10.0-1.osg33.el7  osg-version-3.3.20-1.osg33.el7  voms-2.0.14-1.2.osg33.el7  xrootd-4.5.0-2.osg33.el7  zookeeper-3.4.3+15-1.cdh4.0.1.p0.4.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-20/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp globus-ftp-control globus-ftp-control-debuginfo globus-ftp-control-devel globus-ftp-control-doc globus-gridftp-server globus-gridftp-server-debuginfo globus-gridftp-server-devel globus-gridftp-server-progs gratia-probe-bdii-status gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glexec gratia-probe-glideinwms gratia-probe-gram gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer gridftp-hdfs gridftp-hdfs-debuginfo osg-base-ce osg-base-ce-bosco osg-base-ce-condor osg-base-ce-lsf osg-base-ce-pbs osg-base-ce-sge osg-base-ce-slurm osg-ce osg-ce-bosco osg-ce-condor osg-ce-lsf osg-ce-pbs osg-ce-sge osg-ce-slurm osg-configure osg-configure-bosco osg-configure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-htcondor-ce osg-htcondor-ce-bosco osg-htcondor-ce-condor osg-htcondor-ce-lsf osg-htcondor-ce-pbs osg-htcondor-ce-sge osg-htcondor-ce-slurm osg-test osg-test-log-viewer osg-version voms voms-clients-cpp voms-debuginfo voms-devel voms-doc voms-server xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-libs xrootd-private-devel xrootd-python xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs zookeeper zookeeper-server  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-20/#enterprise-linux-6_1", 
            "text": "blahp-1.18.28.bosco-2.osg33.el6\nblahp-debuginfo-1.18.28.bosco-2.osg33.el6\ncondor-8.4.10-1.osg33.el6\ncondor-all-8.4.10-1.osg33.el6\ncondor-bosco-8.4.10-1.osg33.el6\ncondor-classads-8.4.10-1.osg33.el6\ncondor-classads-devel-8.4.10-1.osg33.el6\ncondor-cream-gahp-8.4.10-1.osg33.el6\ncondor-debuginfo-8.4.10-1.osg33.el6\ncondor-kbdd-8.4.10-1.osg33.el6\ncondor-procd-8.4.10-1.osg33.el6\ncondor-python-8.4.10-1.osg33.el6\ncondor-std-universe-8.4.10-1.osg33.el6\ncondor-test-8.4.10-1.osg33.el6\ncondor-vm-gahp-8.4.10-1.osg33.el6\nglobus-ftp-control-7.7-1.osg33.el6\nglobus-ftp-control-debuginfo-7.7-1.osg33.el6\nglobus-ftp-control-devel-7.7-1.osg33.el6\nglobus-ftp-control-doc-7.7-1.osg33.el6\nglobus-gridftp-server-11.8-1.1.osg33.el6\nglobus-gridftp-server-debuginfo-11.8-1.1.osg33.el6\nglobus-gridftp-server-devel-11.8-1.1.osg33.el6\nglobus-gridftp-server-progs-11.8-1.1.osg33.el6\ngratia-probe-1.17.2-1.osg33.el6\ngratia-probe-bdii-status-1.17.2-1.osg33.el6\ngratia-probe-common-1.17.2-1.osg33.el6\ngratia-probe-condor-1.17.2-1.osg33.el6\ngratia-probe-condor-events-1.17.2-1.osg33.el6\ngratia-probe-dcache-storage-1.17.2-1.osg33.el6\ngratia-probe-dcache-storagegroup-1.17.2-1.osg33.el6\ngratia-probe-dcache-transfer-1.17.2-1.osg33.el6\ngratia-probe-debuginfo-1.17.2-1.osg33.el6\ngratia-probe-enstore-storage-1.17.2-1.osg33.el6\ngratia-probe-enstore-tapedrive-1.17.2-1.osg33.el6\ngratia-probe-enstore-transfer-1.17.2-1.osg33.el6\ngratia-probe-glexec-1.17.2-1.osg33.el6\ngratia-probe-glideinwms-1.17.2-1.osg33.el6\ngratia-probe-gram-1.17.2-1.osg33.el6\ngratia-probe-gridftp-transfer-1.17.2-1.osg33.el6\ngratia-probe-hadoop-storage-1.17.2-1.osg33.el6\ngratia-probe-htcondor-ce-1.17.2-1.osg33.el6\ngratia-probe-lsf-1.17.2-1.osg33.el6\ngratia-probe-metric-1.17.2-1.osg33.el6\ngratia-probe-onevm-1.17.2-1.osg33.el6\ngratia-probe-pbs-lsf-1.17.2-1.osg33.el6\ngratia-probe-services-1.17.2-1.osg33.el6\ngratia-probe-sge-1.17.2-1.osg33.el6\ngratia-probe-slurm-1.17.2-1.osg33.el6\ngratia-probe-xrootd-storage-1.17.2-1.osg33.el6\ngratia-probe-xrootd-transfer-1.17.2-1.osg33.el6\ngridftp-hdfs-0.5.4-25.5.osg33.el6\ngridftp-hdfs-debuginfo-0.5.4-25.5.osg33.el6\nosg-base-ce-3.3-10.osg33.el6\nosg-base-ce-bosco-3.3-10.osg33.el6\nosg-base-ce-condor-3.3-10.osg33.el6\nosg-base-ce-lsf-3.3-10.osg33.el6\nosg-base-ce-pbs-3.3-10.osg33.el6\nosg-base-ce-sge-3.3-10.osg33.el6\nosg-base-ce-slurm-3.3-10.osg33.el6\nosg-ce-3.3-10.osg33.el6\nosg-ce-bosco-3.3-10.osg33.el6\nosg-ce-condor-3.3-10.osg33.el6\nosg-ce-lsf-3.3-10.osg33.el6\nosg-ce-pbs-3.3-10.osg33.el6\nosg-ce-sge-3.3-10.osg33.el6\nosg-ce-slurm-3.3-10.osg33.el6\nosg-configure-1.5.4-1.osg33.el6\nosg-configure-bosco-1.5.4-1.osg33.el6\nosg-configure-ce-1.5.4-1.osg33.el6\nosg-configure-cemon-1.5.4-1.osg33.el6\nosg-configure-condor-1.5.4-1.osg33.el6\nosg-configure-gateway-1.5.4-1.osg33.el6\nosg-configure-gip-1.5.4-1.osg33.el6\nosg-configure-gratia-1.5.4-1.osg33.el6\nosg-configure-infoservices-1.5.4-1.osg33.el6\nosg-configure-lsf-1.5.4-1.osg33.el6\nosg-configure-managedfork-1.5.4-1.osg33.el6\nosg-configure-misc-1.5.4-1.osg33.el6\nosg-configure-monalisa-1.5.4-1.osg33.el6\nosg-configure-network-1.5.4-1.osg33.el6\nosg-configure-pbs-1.5.4-1.osg33.el6\nosg-configure-rsv-1.5.4-1.osg33.el6\nosg-configure-sge-1.5.4-1.osg33.el6\nosg-configure-slurm-1.5.4-1.osg33.el6\nosg-configure-squid-1.5.4-1.osg33.el6\nosg-configure-tests-1.5.4-1.osg33.el6\nosg-htcondor-ce-3.3-10.osg33.el6\nosg-htcondor-ce-bosco-3.3-10.osg33.el6\nosg-htcondor-ce-condor-3.3-10.osg33.el6\nosg-htcondor-ce-lsf-3.3-10.osg33.el6\nosg-htcondor-ce-pbs-3.3-10.osg33.el6\nosg-htcondor-ce-sge-3.3-10.osg33.el6\nosg-htcondor-ce-slurm-3.3-10.osg33.el6\nosg-test-1.10.0-1.osg33.el6\nosg-test-log-viewer-1.10.0-1.osg33.el6\nosg-version-3.3.20-1.osg33.el6\nvoms-2.0.14-1.2.osg33.el6\nvoms-clients-cpp-2.0.14-1.2.osg33.el6\nvoms-debuginfo-2.0.14-1.2.osg33.el6\nvoms-devel-2.0.14-1.2.osg33.el6\nvoms-doc-2.0.14-1.2.osg33.el6\nvoms-server-2.0.14-1.2.osg33.el6\nxrootd-4.5.0-2.osg33.el6\nxrootd-client-4.5.0-2.osg33.el6\nxrootd-client-devel-4.5.0-2.osg33.el6\nxrootd-client-libs-4.5.0-2.osg33.el6\nxrootd-debuginfo-4.5.0-2.osg33.el6\nxrootd-devel-4.5.0-2.osg33.el6\nxrootd-doc-4.5.0-2.osg33.el6\nxrootd-fuse-4.5.0-2.osg33.el6\nxrootd-libs-4.5.0-2.osg33.el6\nxrootd-private-devel-4.5.0-2.osg33.el6\nxrootd-python-4.5.0-2.osg33.el6\nxrootd-selinux-4.5.0-2.osg33.el6\nxrootd-server-4.5.0-2.osg33.el6\nxrootd-server-devel-4.5.0-2.osg33.el6\nxrootd-server-libs-4.5.0-2.osg33.el6\nzookeeper-3.4.3+15-1.cdh4.0.1.p0.4.osg33.el6\nzookeeper-server-3.4.3+15-1.cdh4.0.1.p0.4.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-20/#enterprise-linux-7_1", 
            "text": "blahp-1.18.28.bosco-2.osg33.el7\nblahp-debuginfo-1.18.28.bosco-2.osg33.el7\ncondor-8.4.10-1.osg33.el7\ncondor-all-8.4.10-1.osg33.el7\ncondor-bosco-8.4.10-1.osg33.el7\ncondor-classads-8.4.10-1.osg33.el7\ncondor-classads-devel-8.4.10-1.osg33.el7\ncondor-cream-gahp-8.4.10-1.osg33.el7\ncondor-debuginfo-8.4.10-1.osg33.el7\ncondor-kbdd-8.4.10-1.osg33.el7\ncondor-procd-8.4.10-1.osg33.el7\ncondor-python-8.4.10-1.osg33.el7\ncondor-test-8.4.10-1.osg33.el7\ncondor-vm-gahp-8.4.10-1.osg33.el7\nglobus-ftp-control-7.7-1.osg33.el7\nglobus-ftp-control-debuginfo-7.7-1.osg33.el7\nglobus-ftp-control-devel-7.7-1.osg33.el7\nglobus-ftp-control-doc-7.7-1.osg33.el7\nglobus-gridftp-server-11.8-1.1.osg33.el7\nglobus-gridftp-server-debuginfo-11.8-1.1.osg33.el7\nglobus-gridftp-server-devel-11.8-1.1.osg33.el7\nglobus-gridftp-server-progs-11.8-1.1.osg33.el7\ngratia-probe-1.17.2-1.osg33.el7\ngratia-probe-bdii-status-1.17.2-1.osg33.el7\ngratia-probe-common-1.17.2-1.osg33.el7\ngratia-probe-condor-1.17.2-1.osg33.el7\ngratia-probe-condor-events-1.17.2-1.osg33.el7\ngratia-probe-dcache-storage-1.17.2-1.osg33.el7\ngratia-probe-dcache-storagegroup-1.17.2-1.osg33.el7\ngratia-probe-dcache-transfer-1.17.2-1.osg33.el7\ngratia-probe-debuginfo-1.17.2-1.osg33.el7\ngratia-probe-enstore-storage-1.17.2-1.osg33.el7\ngratia-probe-enstore-tapedrive-1.17.2-1.osg33.el7\ngratia-probe-enstore-transfer-1.17.2-1.osg33.el7\ngratia-probe-glexec-1.17.2-1.osg33.el7\ngratia-probe-glideinwms-1.17.2-1.osg33.el7\ngratia-probe-gram-1.17.2-1.osg33.el7\ngratia-probe-gridftp-transfer-1.17.2-1.osg33.el7\ngratia-probe-hadoop-storage-1.17.2-1.osg33.el7\ngratia-probe-htcondor-ce-1.17.2-1.osg33.el7\ngratia-probe-lsf-1.17.2-1.osg33.el7\ngratia-probe-metric-1.17.2-1.osg33.el7\ngratia-probe-onevm-1.17.2-1.osg33.el7\ngratia-probe-pbs-lsf-1.17.2-1.osg33.el7\ngratia-probe-services-1.17.2-1.osg33.el7\ngratia-probe-sge-1.17.2-1.osg33.el7\ngratia-probe-slurm-1.17.2-1.osg33.el7\ngratia-probe-xrootd-storage-1.17.2-1.osg33.el7\ngratia-probe-xrootd-transfer-1.17.2-1.osg33.el7\ngridftp-hdfs-0.5.4-25.5.osg33.el7\ngridftp-hdfs-debuginfo-0.5.4-25.5.osg33.el7\nosg-base-ce-3.3-10.osg33.el7\nosg-base-ce-bosco-3.3-10.osg33.el7\nosg-base-ce-condor-3.3-10.osg33.el7\nosg-base-ce-lsf-3.3-10.osg33.el7\nosg-base-ce-pbs-3.3-10.osg33.el7\nosg-base-ce-sge-3.3-10.osg33.el7\nosg-base-ce-slurm-3.3-10.osg33.el7\nosg-ce-3.3-10.osg33.el7\nosg-ce-bosco-3.3-10.osg33.el7\nosg-ce-condor-3.3-10.osg33.el7\nosg-ce-lsf-3.3-10.osg33.el7\nosg-ce-pbs-3.3-10.osg33.el7\nosg-ce-sge-3.3-10.osg33.el7\nosg-ce-slurm-3.3-10.osg33.el7\nosg-configure-1.5.4-1.osg33.el7\nosg-configure-bosco-1.5.4-1.osg33.el7\nosg-configure-ce-1.5.4-1.osg33.el7\nosg-configure-cemon-1.5.4-1.osg33.el7\nosg-configure-condor-1.5.4-1.osg33.el7\nosg-configure-gateway-1.5.4-1.osg33.el7\nosg-configure-gip-1.5.4-1.osg33.el7\nosg-configure-gratia-1.5.4-1.osg33.el7\nosg-configure-infoservices-1.5.4-1.osg33.el7\nosg-configure-lsf-1.5.4-1.osg33.el7\nosg-configure-managedfork-1.5.4-1.osg33.el7\nosg-configure-misc-1.5.4-1.osg33.el7\nosg-configure-monalisa-1.5.4-1.osg33.el7\nosg-configure-network-1.5.4-1.osg33.el7\nosg-configure-pbs-1.5.4-1.osg33.el7\nosg-configure-rsv-1.5.4-1.osg33.el7\nosg-configure-sge-1.5.4-1.osg33.el7\nosg-configure-slurm-1.5.4-1.osg33.el7\nosg-configure-squid-1.5.4-1.osg33.el7\nosg-configure-tests-1.5.4-1.osg33.el7\nosg-htcondor-ce-3.3-10.osg33.el7\nosg-htcondor-ce-bosco-3.3-10.osg33.el7\nosg-htcondor-ce-condor-3.3-10.osg33.el7\nosg-htcondor-ce-lsf-3.3-10.osg33.el7\nosg-htcondor-ce-pbs-3.3-10.osg33.el7\nosg-htcondor-ce-sge-3.3-10.osg33.el7\nosg-htcondor-ce-slurm-3.3-10.osg33.el7\nosg-test-1.10.0-1.osg33.el7\nosg-test-log-viewer-1.10.0-1.osg33.el7\nosg-version-3.3.20-1.osg33.el7\nvoms-2.0.14-1.2.osg33.el7\nvoms-clients-cpp-2.0.14-1.2.osg33.el7\nvoms-debuginfo-2.0.14-1.2.osg33.el7\nvoms-devel-2.0.14-1.2.osg33.el7\nvoms-doc-2.0.14-1.2.osg33.el7\nvoms-server-2.0.14-1.2.osg33.el7\nxrootd-4.5.0-2.osg33.el7\nxrootd-client-4.5.0-2.osg33.el7\nxrootd-client-devel-4.5.0-2.osg33.el7\nxrootd-client-libs-4.5.0-2.osg33.el7\nxrootd-debuginfo-4.5.0-2.osg33.el7\nxrootd-devel-4.5.0-2.osg33.el7\nxrootd-doc-4.5.0-2.osg33.el7\nxrootd-fuse-4.5.0-2.osg33.el7\nxrootd-libs-4.5.0-2.osg33.el7\nxrootd-private-devel-4.5.0-2.osg33.el7\nxrootd-python-4.5.0-2.osg33.el7\nxrootd-selinux-4.5.0-2.osg33.el7\nxrootd-server-4.5.0-2.osg33.el7\nxrootd-server-devel-4.5.0-2.osg33.el7\nxrootd-server-libs-4.5.0-2.osg33.el7\nzookeeper-3.4.3+15-1.cdh4.0.1.p0.4.osg33.el7\nzookeeper-server-3.4.3+15-1.cdh4.0.1.p0.4.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-20/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-20/#enterprise-linux-6_2", 
            "text": "blahp-1.18.28.bosco-2.osgup.el6  condor-8.5.8-1.osgup.el6  frontier-squid-3.5.23-3.1.osgup.el6  singularity-2.2-1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-20/#enterprise-linux-7_2", 
            "text": "blahp-1.18.28.bosco-2.osgup.el7  condor-8.5.8-1.osgup.el7  frontier-squid-3.5.23-3.1.osgup.el7  singularity-2.2-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-20/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp frontier-squid frontier-squid-debuginfo singularity singularity-debuginfo singularity-devel  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-20/#enterprise-linux-6_3", 
            "text": "blahp-1.18.28.bosco-2.osgup.el6\nblahp-debuginfo-1.18.28.bosco-2.osgup.el6\ncondor-8.5.8-1.osgup.el6\ncondor-all-8.5.8-1.osgup.el6\ncondor-bosco-8.5.8-1.osgup.el6\ncondor-classads-8.5.8-1.osgup.el6\ncondor-classads-devel-8.5.8-1.osgup.el6\ncondor-cream-gahp-8.5.8-1.osgup.el6\ncondor-debuginfo-8.5.8-1.osgup.el6\ncondor-kbdd-8.5.8-1.osgup.el6\ncondor-procd-8.5.8-1.osgup.el6\ncondor-python-8.5.8-1.osgup.el6\ncondor-std-universe-8.5.8-1.osgup.el6\ncondor-test-8.5.8-1.osgup.el6\ncondor-vm-gahp-8.5.8-1.osgup.el6\nfrontier-squid-3.5.23-3.1.osgup.el6\nfrontier-squid-debuginfo-3.5.23-3.1.osgup.el6\nsingularity-2.2-1.osgup.el6\nsingularity-debuginfo-2.2-1.osgup.el6\nsingularity-devel-2.2-1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-20/#enterprise-linux-7_3", 
            "text": "blahp-1.18.28.bosco-2.osgup.el7\nblahp-debuginfo-1.18.28.bosco-2.osgup.el7\ncondor-8.5.8-1.osgup.el7\ncondor-all-8.5.8-1.osgup.el7\ncondor-bosco-8.5.8-1.osgup.el7\ncondor-classads-8.5.8-1.osgup.el7\ncondor-classads-devel-8.5.8-1.osgup.el7\ncondor-cream-gahp-8.5.8-1.osgup.el7\ncondor-debuginfo-8.5.8-1.osgup.el7\ncondor-kbdd-8.5.8-1.osgup.el7\ncondor-procd-8.5.8-1.osgup.el7\ncondor-python-8.5.8-1.osgup.el7\ncondor-test-8.5.8-1.osgup.el7\ncondor-vm-gahp-8.5.8-1.osgup.el7\nfrontier-squid-3.5.23-3.1.osgup.el7\nfrontier-squid-debuginfo-3.5.23-3.1.osgup.el7\nsingularity-2.2-1.osgup.el7\nsingularity-debuginfo-2.2-1.osgup.el7\nsingularity-devel-2.2-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-19/", 
            "text": "OSG Software Release 3.3.19\n\n\nRelease Date\n: 2016-12-13\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nHTCondor-CE 2.1.1\n\n\nUpdate HTCondor-CE to provide data needed by the ATLAS AGIS system\n\n\nProvide better hold messages when the Job Router does not route a job\n\n\n\n\n\n\nAugment osg-configure 1.5.2 to process \"Resource Entry\" sections needed to provide data to AGIS\n\n\nhandle double quotes in batch system configuration input\n\n\n\n\n\n\nProvide a way for Gratia to avoid reporting local, non-OSG jobs records\n\n\nSee known issues section below for configuration information\n\n\n\n\n\n\nSystemd service files\n\n\nRSV\n\n\nGlobus gridFTP server\n\n\n\n\n\n\nRemove inaccurate comments from wn-client/setup.sh\n\n\nosg-vo-map: Put the correct month in log timestamps\n\n\ntmpfiles.d configuration for hadoop packages\n\n\nCheck folder ownership of /var/run/myproxy\n\n\nUpdate to frontier-squid 3 in the Upcoming repository\n\n\nSee the \nupgrading section of the upstream documentation\n for the system administrators view of the changes\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nKnown Issues\n\n\n\n\nThe previous version (\n1.17.0-2.5\n) of the Gratia probes required changes in the HTCondor configuration to operate properly. Now, these changes need to be reverted to use this version (\n1.17.0-2.6\n) of the probes. The lines below are likely to be found in \ncondor_config.local\n or a new file in \n/etc/condor/config.d\n directory. Delete these lines and run \ncondor_reconfig\n.\n\n\n\n\n# GlideinWMS Gratia commands\n\n\nPER_JOB_HISTORY_DIR\n \n=\n /var/lib/gratia/data\n\nJOBGLIDEIN_ResourceName\n=\n$$\n([IfThenElse(IsUndefined(TARGET.GLIDEIN_ResourceName), IfThenElse(IsUndefined(TARGET.GLIDEIN_Site), IfThenElse(IsUndefined(TARGET.FileSystemDomain), \\\nLocal Job\\\n, TARGET.FileSystemDomain), TARGET.GLIDEIN_Site), TARGET.GLIDEIN_ResourceName)])\n\n\nSUBMIT_EXPRS\n \n=\n \n$(\nSUBMIT_EXPRS\n)\n JOBGLIDEIN_ResourceName   \n\n\n\n\n\n\n\nOn EL7, your version of voms-proxy-init may come from the voms-clients-java package; this version will continue to make legacy proxies by default. If you want the new behavior, install the voms-clients-cpp package and uninstall the voms-clients-java package.\n\n\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nglobus-gridftp-server-10.4-1.5.osg33.el6\n\n\ngratia-probe-1.17.0-2.6.osg33.el6\n\n\nhadoop-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\n\n\nhtcondor-ce-2.1.1-1.osg33.el6\n\n\nmyproxy-6.1.18-1.4.osg33.el6\n\n\nosg-configure-1.5.3-1.osg33.el6\n\n\nosg-version-3.3.19-1.osg33.el6\n\n\nosg-vo-map-0.0.2-1.osg33.el6\n\n\nosg-wn-client-3.3-6.osg33.el6\n\n\nrsv-3.14.0-1.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nglobus-gridftp-server-10.4-1.5.osg33.el7\n\n\ngratia-probe-1.17.0-2.6.osg33.el7\n\n\nhadoop-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\n\n\nhtcondor-ce-2.1.1-1.osg33.el7\n\n\nmyproxy-6.1.18-1.4.osg33.el7\n\n\nosg-configure-1.5.3-1.osg33.el7\n\n\nosg-version-3.3.19-1.osg33.el7\n\n\nosg-vo-map-0.0.2-1.osg33.el7\n\n\nosg-wn-client-3.3-6.osg33.el7\n\n\nrsv-3.14.0-1.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nglobus-gridftp-server globus-gridftp-server-debuginfo globus-gridftp-server-devel globus-gridftp-server-progs gratia-probe-bdii-status gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-p\nrobe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glexec gratia-probe-glideinwms gratia-probe-gram gratia-pr\nobe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage g\nratia-probe-xrootd-transfer hadoop hadoop-0.20-conf-pseudo hadoop-0.20-mapreduce hadoop-client hadoop-conf-pseudo hadoop-debuginfo hadoop-doc hadoop-hdfs hadoop-hdfs-datanode hadoop-hdfs-fuse hadoop-hdfs-fuse-selinux hadoop-hdfs-journalno\nde hadoop-hdfs-namenode hadoop-hdfs-secondarynamenode hadoop-hdfs-zkfc hadoop-httpfs hadoop-libhdfs hadoop-mapreduce hadoop-yarn htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htc\nondor-ce-pbs htcondor-ce-sge htcondor-ce-view myproxy myproxy-admin myproxy-debuginfo myproxy-devel myproxy-doc myproxy-libs myproxy-server myproxy-voms osg-configure osg-configure-bosco osg-configure-ce osg-configure-cemon osg-configure-\ncondor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv\n osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-version osg-vo-map osg-wn-client osg-wn-client-glexec rsv rsv-consumers rsv-core rsv-metrics\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nglobus-gridftp-server-10.4-1.5.osg33.el6\nglobus-gridftp-server-debuginfo-10.4-1.5.osg33.el6\nglobus-gridftp-server-devel-10.4-1.5.osg33.el6\nglobus-gridftp-server-progs-10.4-1.5.osg33.el6\ngratia-probe-1.17.0-2.6.osg33.el6\ngratia-probe-bdii-status-1.17.0-2.6.osg33.el6\ngratia-probe-common-1.17.0-2.6.osg33.el6\ngratia-probe-condor-1.17.0-2.6.osg33.el6\ngratia-probe-condor-events-1.17.0-2.6.osg33.el6\ngratia-probe-dcache-storage-1.17.0-2.6.osg33.el6\ngratia-probe-dcache-storagegroup-1.17.0-2.6.osg33.el6\ngratia-probe-dcache-transfer-1.17.0-2.6.osg33.el6\ngratia-probe-debuginfo-1.17.0-2.6.osg33.el6\ngratia-probe-enstore-storage-1.17.0-2.6.osg33.el6\ngratia-probe-enstore-tapedrive-1.17.0-2.6.osg33.el6\ngratia-probe-enstore-transfer-1.17.0-2.6.osg33.el6\ngratia-probe-glexec-1.17.0-2.6.osg33.el6\ngratia-probe-glideinwms-1.17.0-2.6.osg33.el6\ngratia-probe-gram-1.17.0-2.6.osg33.el6\ngratia-probe-gridftp-transfer-1.17.0-2.6.osg33.el6\ngratia-probe-hadoop-storage-1.17.0-2.6.osg33.el6\ngratia-probe-htcondor-ce-1.17.0-2.6.osg33.el6\ngratia-probe-lsf-1.17.0-2.6.osg33.el6\ngratia-probe-metric-1.17.0-2.6.osg33.el6\ngratia-probe-onevm-1.17.0-2.6.osg33.el6\ngratia-probe-pbs-lsf-1.17.0-2.6.osg33.el6\ngratia-probe-services-1.17.0-2.6.osg33.el6\ngratia-probe-sge-1.17.0-2.6.osg33.el6\ngratia-probe-slurm-1.17.0-2.6.osg33.el6\ngratia-probe-xrootd-storage-1.17.0-2.6.osg33.el6\ngratia-probe-xrootd-transfer-1.17.0-2.6.osg33.el6\nhadoop-0.20-conf-pseudo-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-0.20-mapreduce-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-client-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-conf-pseudo-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-debuginfo-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-doc-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-hdfs-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-hdfs-datanode-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-hdfs-fuse-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-hdfs-fuse-selinux-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-hdfs-journalnode-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-hdfs-namenode-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-hdfs-secondarynamenode-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-hdfs-zkfc-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-httpfs-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-libhdfs-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-mapreduce-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-yarn-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhtcondor-ce-2.1.1-1.osg33.el6\nhtcondor-ce-bosco-2.1.1-1.osg33.el6\nhtcondor-ce-client-2.1.1-1.osg33.el6\nhtcondor-ce-collector-2.1.1-1.osg33.el6\nhtcondor-ce-condor-2.1.1-1.osg33.el6\nhtcondor-ce-lsf-2.1.1-1.osg33.el6\nhtcondor-ce-pbs-2.1.1-1.osg33.el6\nhtcondor-ce-sge-2.1.1-1.osg33.el6\nhtcondor-ce-view-2.1.1-1.osg33.el6\nmyproxy-6.1.18-1.4.osg33.el6\nmyproxy-admin-6.1.18-1.4.osg33.el6\nmyproxy-debuginfo-6.1.18-1.4.osg33.el6\nmyproxy-devel-6.1.18-1.4.osg33.el6\nmyproxy-doc-6.1.18-1.4.osg33.el6\nmyproxy-libs-6.1.18-1.4.osg33.el6\nmyproxy-server-6.1.18-1.4.osg33.el6\nmyproxy-voms-6.1.18-1.4.osg33.el6\nosg-configure-1.5.3-1.osg33.el6\nosg-configure-bosco-1.5.3-1.osg33.el6\nosg-configure-ce-1.5.3-1.osg33.el6\nosg-configure-cemon-1.5.3-1.osg33.el6\nosg-configure-condor-1.5.3-1.osg33.el6\nosg-configure-gateway-1.5.3-1.osg33.el6\nosg-configure-gip-1.5.3-1.osg33.el6\nosg-configure-gratia-1.5.3-1.osg33.el6\nosg-configure-infoservices-1.5.3-1.osg33.el6\nosg-configure-lsf-1.5.3-1.osg33.el6\nosg-configure-managedfork-1.5.3-1.osg33.el6\nosg-configure-misc-1.5.3-1.osg33.el6\nosg-configure-monalisa-1.5.3-1.osg33.el6\nosg-configure-network-1.5.3-1.osg33.el6\nosg-configure-pbs-1.5.3-1.osg33.el6\nosg-configure-rsv-1.5.3-1.osg33.el6\nosg-configure-sge-1.5.3-1.osg33.el6\nosg-configure-slurm-1.5.3-1.osg33.el6\nosg-configure-squid-1.5.3-1.osg33.el6\nosg-configure-tests-1.5.3-1.osg33.el6\nosg-version-3.3.19-1.osg33.el6\nosg-vo-map-0.0.2-1.osg33.el6\nosg-wn-client-3.3-6.osg33.el6\nosg-wn-client-glexec-3.3-6.osg33.el6\nrsv-3.14.0-1.osg33.el6\nrsv-consumers-3.14.0-1.osg33.el6\nrsv-core-3.14.0-1.osg33.el6\nrsv-metrics-3.14.0-1.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nglobus-gridftp-server-10.4-1.5.osg33.el7\nglobus-gridftp-server-debuginfo-10.4-1.5.osg33.el7\nglobus-gridftp-server-devel-10.4-1.5.osg33.el7\nglobus-gridftp-server-progs-10.4-1.5.osg33.el7\ngratia-probe-1.17.0-2.6.osg33.el7\ngratia-probe-bdii-status-1.17.0-2.6.osg33.el7\ngratia-probe-common-1.17.0-2.6.osg33.el7\ngratia-probe-condor-1.17.0-2.6.osg33.el7\ngratia-probe-condor-events-1.17.0-2.6.osg33.el7\ngratia-probe-dcache-storage-1.17.0-2.6.osg33.el7\ngratia-probe-dcache-storagegroup-1.17.0-2.6.osg33.el7\ngratia-probe-dcache-transfer-1.17.0-2.6.osg33.el7\ngratia-probe-debuginfo-1.17.0-2.6.osg33.el7\ngratia-probe-enstore-storage-1.17.0-2.6.osg33.el7\ngratia-probe-enstore-tapedrive-1.17.0-2.6.osg33.el7\ngratia-probe-enstore-transfer-1.17.0-2.6.osg33.el7\ngratia-probe-glexec-1.17.0-2.6.osg33.el7\ngratia-probe-glideinwms-1.17.0-2.6.osg33.el7\ngratia-probe-gram-1.17.0-2.6.osg33.el7\ngratia-probe-gridftp-transfer-1.17.0-2.6.osg33.el7\ngratia-probe-hadoop-storage-1.17.0-2.6.osg33.el7\ngratia-probe-htcondor-ce-1.17.0-2.6.osg33.el7\ngratia-probe-lsf-1.17.0-2.6.osg33.el7\ngratia-probe-metric-1.17.0-2.6.osg33.el7\ngratia-probe-onevm-1.17.0-2.6.osg33.el7\ngratia-probe-pbs-lsf-1.17.0-2.6.osg33.el7\ngratia-probe-services-1.17.0-2.6.osg33.el7\ngratia-probe-sge-1.17.0-2.6.osg33.el7\ngratia-probe-slurm-1.17.0-2.6.osg33.el7\ngratia-probe-xrootd-storage-1.17.0-2.6.osg33.el7\ngratia-probe-xrootd-transfer-1.17.0-2.6.osg33.el7\nhadoop-0.20-conf-pseudo-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-0.20-mapreduce-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-client-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-conf-pseudo-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-debuginfo-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-doc-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-hdfs-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-hdfs-datanode-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-hdfs-fuse-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-hdfs-fuse-selinux-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-hdfs-journalnode-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-hdfs-namenode-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-hdfs-secondarynamenode-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-hdfs-zkfc-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-httpfs-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-libhdfs-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-mapreduce-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-yarn-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhtcondor-ce-2.1.1-1.osg33.el7\nhtcondor-ce-bosco-2.1.1-1.osg33.el7\nhtcondor-ce-client-2.1.1-1.osg33.el7\nhtcondor-ce-collector-2.1.1-1.osg33.el7\nhtcondor-ce-condor-2.1.1-1.osg33.el7\nhtcondor-ce-lsf-2.1.1-1.osg33.el7\nhtcondor-ce-pbs-2.1.1-1.osg33.el7\nhtcondor-ce-sge-2.1.1-1.osg33.el7\nhtcondor-ce-view-2.1.1-1.osg33.el7\nmyproxy-6.1.18-1.4.osg33.el7\nmyproxy-admin-6.1.18-1.4.osg33.el7\nmyproxy-debuginfo-6.1.18-1.4.osg33.el7\nmyproxy-devel-6.1.18-1.4.osg33.el7\nmyproxy-doc-6.1.18-1.4.osg33.el7\nmyproxy-libs-6.1.18-1.4.osg33.el7\nmyproxy-server-6.1.18-1.4.osg33.el7\nmyproxy-voms-6.1.18-1.4.osg33.el7\nosg-configure-1.5.3-1.osg33.el7\nosg-configure-bosco-1.5.3-1.osg33.el7\nosg-configure-ce-1.5.3-1.osg33.el7\nosg-configure-cemon-1.5.3-1.osg33.el7\nosg-configure-condor-1.5.3-1.osg33.el7\nosg-configure-gateway-1.5.3-1.osg33.el7\nosg-configure-gip-1.5.3-1.osg33.el7\nosg-configure-gratia-1.5.3-1.osg33.el7\nosg-configure-infoservices-1.5.3-1.osg33.el7\nosg-configure-lsf-1.5.3-1.osg33.el7\nosg-configure-managedfork-1.5.3-1.osg33.el7\nosg-configure-misc-1.5.3-1.osg33.el7\nosg-configure-monalisa-1.5.3-1.osg33.el7\nosg-configure-network-1.5.3-1.osg33.el7\nosg-configure-pbs-1.5.3-1.osg33.el7\nosg-configure-rsv-1.5.3-1.osg33.el7\nosg-configure-sge-1.5.3-1.osg33.el7\nosg-configure-slurm-1.5.3-1.osg33.el7\nosg-configure-squid-1.5.3-1.osg33.el7\nosg-configure-tests-1.5.3-1.osg33.el7\nosg-version-3.3.19-1.osg33.el7\nosg-vo-map-0.0.2-1.osg33.el7\nosg-wn-client-3.3-6.osg33.el7\nosg-wn-client-glexec-3.3-6.osg33.el7\nrsv-3.14.0-1.osg33.el7\nrsv-consumers-3.14.0-1.osg33.el7\nrsv-core-3.14.0-1.osg33.el7\nrsv-metrics-3.14.0-1.osg33.el7\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nfrontier-squid-3.5.22-2.1.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nfrontier-squid-3.5.22-2.1.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nfrontier-squid frontier-squid-debuginfo\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nfrontier-squid-3.5.22-2.1.osgup.el6\nfrontier-squid-debuginfo-3.5.22-2.1.osgup.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nfrontier-squid-3.5.22-2.1.osgup.el7\nfrontier-squid-debuginfo-3.5.22-2.1.osgup.el7", 
            "title": "OSG Release 3.3.19"
        }, 
        {
            "location": "/release/3.3/release-3-3-19/#osg-software-release-3319", 
            "text": "Release Date : 2016-12-13", 
            "title": "OSG Software Release 3.3.19"
        }, 
        {
            "location": "/release/3.3/release-3-3-19/#summary-of-changes", 
            "text": "This release contains:   HTCondor-CE 2.1.1  Update HTCondor-CE to provide data needed by the ATLAS AGIS system  Provide better hold messages when the Job Router does not route a job    Augment osg-configure 1.5.2 to process \"Resource Entry\" sections needed to provide data to AGIS  handle double quotes in batch system configuration input    Provide a way for Gratia to avoid reporting local, non-OSG jobs records  See known issues section below for configuration information    Systemd service files  RSV  Globus gridFTP server    Remove inaccurate comments from wn-client/setup.sh  osg-vo-map: Put the correct month in log timestamps  tmpfiles.d configuration for hadoop packages  Check folder ownership of /var/run/myproxy  Update to frontier-squid 3 in the Upcoming repository  See the  upgrading section of the upstream documentation  for the system administrators view of the changes     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-19/#known-issues", 
            "text": "The previous version ( 1.17.0-2.5 ) of the Gratia probes required changes in the HTCondor configuration to operate properly. Now, these changes need to be reverted to use this version ( 1.17.0-2.6 ) of the probes. The lines below are likely to be found in  condor_config.local  or a new file in  /etc/condor/config.d  directory. Delete these lines and run  condor_reconfig .   # GlideinWMS Gratia commands  PER_JOB_HISTORY_DIR   =  /var/lib/gratia/data JOBGLIDEIN_ResourceName = $$ ([IfThenElse(IsUndefined(TARGET.GLIDEIN_ResourceName), IfThenElse(IsUndefined(TARGET.GLIDEIN_Site), IfThenElse(IsUndefined(TARGET.FileSystemDomain), \\ Local Job\\ , TARGET.FileSystemDomain), TARGET.GLIDEIN_Site), TARGET.GLIDEIN_ResourceName)])  SUBMIT_EXPRS   =   $( SUBMIT_EXPRS )  JOBGLIDEIN_ResourceName      On EL7, your version of voms-proxy-init may come from the voms-clients-java package; this version will continue to make legacy proxies by default. If you want the new behavior, install the voms-clients-cpp package and uninstall the voms-clients-java package.", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.3/release-3-3-19/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-19/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-19/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-19/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-19/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-19/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-19/#enterprise-linux-6", 
            "text": "globus-gridftp-server-10.4-1.5.osg33.el6  gratia-probe-1.17.0-2.6.osg33.el6  hadoop-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6  htcondor-ce-2.1.1-1.osg33.el6  myproxy-6.1.18-1.4.osg33.el6  osg-configure-1.5.3-1.osg33.el6  osg-version-3.3.19-1.osg33.el6  osg-vo-map-0.0.2-1.osg33.el6  osg-wn-client-3.3-6.osg33.el6  rsv-3.14.0-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-19/#enterprise-linux-7", 
            "text": "globus-gridftp-server-10.4-1.5.osg33.el7  gratia-probe-1.17.0-2.6.osg33.el7  hadoop-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7  htcondor-ce-2.1.1-1.osg33.el7  myproxy-6.1.18-1.4.osg33.el7  osg-configure-1.5.3-1.osg33.el7  osg-version-3.3.19-1.osg33.el7  osg-vo-map-0.0.2-1.osg33.el7  osg-wn-client-3.3-6.osg33.el7  rsv-3.14.0-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-19/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  globus-gridftp-server globus-gridftp-server-debuginfo globus-gridftp-server-devel globus-gridftp-server-progs gratia-probe-bdii-status gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-p\nrobe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glexec gratia-probe-glideinwms gratia-probe-gram gratia-pr\nobe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage g\nratia-probe-xrootd-transfer hadoop hadoop-0.20-conf-pseudo hadoop-0.20-mapreduce hadoop-client hadoop-conf-pseudo hadoop-debuginfo hadoop-doc hadoop-hdfs hadoop-hdfs-datanode hadoop-hdfs-fuse hadoop-hdfs-fuse-selinux hadoop-hdfs-journalno\nde hadoop-hdfs-namenode hadoop-hdfs-secondarynamenode hadoop-hdfs-zkfc hadoop-httpfs hadoop-libhdfs hadoop-mapreduce hadoop-yarn htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htc\nondor-ce-pbs htcondor-ce-sge htcondor-ce-view myproxy myproxy-admin myproxy-debuginfo myproxy-devel myproxy-doc myproxy-libs myproxy-server myproxy-voms osg-configure osg-configure-bosco osg-configure-ce osg-configure-cemon osg-configure-\ncondor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv\n osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-version osg-vo-map osg-wn-client osg-wn-client-glexec rsv rsv-consumers rsv-core rsv-metrics  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-19/#enterprise-linux-6_1", 
            "text": "globus-gridftp-server-10.4-1.5.osg33.el6\nglobus-gridftp-server-debuginfo-10.4-1.5.osg33.el6\nglobus-gridftp-server-devel-10.4-1.5.osg33.el6\nglobus-gridftp-server-progs-10.4-1.5.osg33.el6\ngratia-probe-1.17.0-2.6.osg33.el6\ngratia-probe-bdii-status-1.17.0-2.6.osg33.el6\ngratia-probe-common-1.17.0-2.6.osg33.el6\ngratia-probe-condor-1.17.0-2.6.osg33.el6\ngratia-probe-condor-events-1.17.0-2.6.osg33.el6\ngratia-probe-dcache-storage-1.17.0-2.6.osg33.el6\ngratia-probe-dcache-storagegroup-1.17.0-2.6.osg33.el6\ngratia-probe-dcache-transfer-1.17.0-2.6.osg33.el6\ngratia-probe-debuginfo-1.17.0-2.6.osg33.el6\ngratia-probe-enstore-storage-1.17.0-2.6.osg33.el6\ngratia-probe-enstore-tapedrive-1.17.0-2.6.osg33.el6\ngratia-probe-enstore-transfer-1.17.0-2.6.osg33.el6\ngratia-probe-glexec-1.17.0-2.6.osg33.el6\ngratia-probe-glideinwms-1.17.0-2.6.osg33.el6\ngratia-probe-gram-1.17.0-2.6.osg33.el6\ngratia-probe-gridftp-transfer-1.17.0-2.6.osg33.el6\ngratia-probe-hadoop-storage-1.17.0-2.6.osg33.el6\ngratia-probe-htcondor-ce-1.17.0-2.6.osg33.el6\ngratia-probe-lsf-1.17.0-2.6.osg33.el6\ngratia-probe-metric-1.17.0-2.6.osg33.el6\ngratia-probe-onevm-1.17.0-2.6.osg33.el6\ngratia-probe-pbs-lsf-1.17.0-2.6.osg33.el6\ngratia-probe-services-1.17.0-2.6.osg33.el6\ngratia-probe-sge-1.17.0-2.6.osg33.el6\ngratia-probe-slurm-1.17.0-2.6.osg33.el6\ngratia-probe-xrootd-storage-1.17.0-2.6.osg33.el6\ngratia-probe-xrootd-transfer-1.17.0-2.6.osg33.el6\nhadoop-0.20-conf-pseudo-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-0.20-mapreduce-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-client-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-conf-pseudo-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-debuginfo-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-doc-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-hdfs-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-hdfs-datanode-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-hdfs-fuse-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-hdfs-fuse-selinux-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-hdfs-journalnode-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-hdfs-namenode-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-hdfs-secondarynamenode-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-hdfs-zkfc-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-httpfs-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-libhdfs-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-mapreduce-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhadoop-yarn-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el6\nhtcondor-ce-2.1.1-1.osg33.el6\nhtcondor-ce-bosco-2.1.1-1.osg33.el6\nhtcondor-ce-client-2.1.1-1.osg33.el6\nhtcondor-ce-collector-2.1.1-1.osg33.el6\nhtcondor-ce-condor-2.1.1-1.osg33.el6\nhtcondor-ce-lsf-2.1.1-1.osg33.el6\nhtcondor-ce-pbs-2.1.1-1.osg33.el6\nhtcondor-ce-sge-2.1.1-1.osg33.el6\nhtcondor-ce-view-2.1.1-1.osg33.el6\nmyproxy-6.1.18-1.4.osg33.el6\nmyproxy-admin-6.1.18-1.4.osg33.el6\nmyproxy-debuginfo-6.1.18-1.4.osg33.el6\nmyproxy-devel-6.1.18-1.4.osg33.el6\nmyproxy-doc-6.1.18-1.4.osg33.el6\nmyproxy-libs-6.1.18-1.4.osg33.el6\nmyproxy-server-6.1.18-1.4.osg33.el6\nmyproxy-voms-6.1.18-1.4.osg33.el6\nosg-configure-1.5.3-1.osg33.el6\nosg-configure-bosco-1.5.3-1.osg33.el6\nosg-configure-ce-1.5.3-1.osg33.el6\nosg-configure-cemon-1.5.3-1.osg33.el6\nosg-configure-condor-1.5.3-1.osg33.el6\nosg-configure-gateway-1.5.3-1.osg33.el6\nosg-configure-gip-1.5.3-1.osg33.el6\nosg-configure-gratia-1.5.3-1.osg33.el6\nosg-configure-infoservices-1.5.3-1.osg33.el6\nosg-configure-lsf-1.5.3-1.osg33.el6\nosg-configure-managedfork-1.5.3-1.osg33.el6\nosg-configure-misc-1.5.3-1.osg33.el6\nosg-configure-monalisa-1.5.3-1.osg33.el6\nosg-configure-network-1.5.3-1.osg33.el6\nosg-configure-pbs-1.5.3-1.osg33.el6\nosg-configure-rsv-1.5.3-1.osg33.el6\nosg-configure-sge-1.5.3-1.osg33.el6\nosg-configure-slurm-1.5.3-1.osg33.el6\nosg-configure-squid-1.5.3-1.osg33.el6\nosg-configure-tests-1.5.3-1.osg33.el6\nosg-version-3.3.19-1.osg33.el6\nosg-vo-map-0.0.2-1.osg33.el6\nosg-wn-client-3.3-6.osg33.el6\nosg-wn-client-glexec-3.3-6.osg33.el6\nrsv-3.14.0-1.osg33.el6\nrsv-consumers-3.14.0-1.osg33.el6\nrsv-core-3.14.0-1.osg33.el6\nrsv-metrics-3.14.0-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-19/#enterprise-linux-7_1", 
            "text": "globus-gridftp-server-10.4-1.5.osg33.el7\nglobus-gridftp-server-debuginfo-10.4-1.5.osg33.el7\nglobus-gridftp-server-devel-10.4-1.5.osg33.el7\nglobus-gridftp-server-progs-10.4-1.5.osg33.el7\ngratia-probe-1.17.0-2.6.osg33.el7\ngratia-probe-bdii-status-1.17.0-2.6.osg33.el7\ngratia-probe-common-1.17.0-2.6.osg33.el7\ngratia-probe-condor-1.17.0-2.6.osg33.el7\ngratia-probe-condor-events-1.17.0-2.6.osg33.el7\ngratia-probe-dcache-storage-1.17.0-2.6.osg33.el7\ngratia-probe-dcache-storagegroup-1.17.0-2.6.osg33.el7\ngratia-probe-dcache-transfer-1.17.0-2.6.osg33.el7\ngratia-probe-debuginfo-1.17.0-2.6.osg33.el7\ngratia-probe-enstore-storage-1.17.0-2.6.osg33.el7\ngratia-probe-enstore-tapedrive-1.17.0-2.6.osg33.el7\ngratia-probe-enstore-transfer-1.17.0-2.6.osg33.el7\ngratia-probe-glexec-1.17.0-2.6.osg33.el7\ngratia-probe-glideinwms-1.17.0-2.6.osg33.el7\ngratia-probe-gram-1.17.0-2.6.osg33.el7\ngratia-probe-gridftp-transfer-1.17.0-2.6.osg33.el7\ngratia-probe-hadoop-storage-1.17.0-2.6.osg33.el7\ngratia-probe-htcondor-ce-1.17.0-2.6.osg33.el7\ngratia-probe-lsf-1.17.0-2.6.osg33.el7\ngratia-probe-metric-1.17.0-2.6.osg33.el7\ngratia-probe-onevm-1.17.0-2.6.osg33.el7\ngratia-probe-pbs-lsf-1.17.0-2.6.osg33.el7\ngratia-probe-services-1.17.0-2.6.osg33.el7\ngratia-probe-sge-1.17.0-2.6.osg33.el7\ngratia-probe-slurm-1.17.0-2.6.osg33.el7\ngratia-probe-xrootd-storage-1.17.0-2.6.osg33.el7\ngratia-probe-xrootd-transfer-1.17.0-2.6.osg33.el7\nhadoop-0.20-conf-pseudo-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-0.20-mapreduce-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-client-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-conf-pseudo-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-debuginfo-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-doc-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-hdfs-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-hdfs-datanode-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-hdfs-fuse-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-hdfs-fuse-selinux-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-hdfs-journalnode-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-hdfs-namenode-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-hdfs-secondarynamenode-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-hdfs-zkfc-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-httpfs-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-libhdfs-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-mapreduce-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhadoop-yarn-2.0.0+1612-1.cdh4.7.1.p0.12.5.osg33.el7\nhtcondor-ce-2.1.1-1.osg33.el7\nhtcondor-ce-bosco-2.1.1-1.osg33.el7\nhtcondor-ce-client-2.1.1-1.osg33.el7\nhtcondor-ce-collector-2.1.1-1.osg33.el7\nhtcondor-ce-condor-2.1.1-1.osg33.el7\nhtcondor-ce-lsf-2.1.1-1.osg33.el7\nhtcondor-ce-pbs-2.1.1-1.osg33.el7\nhtcondor-ce-sge-2.1.1-1.osg33.el7\nhtcondor-ce-view-2.1.1-1.osg33.el7\nmyproxy-6.1.18-1.4.osg33.el7\nmyproxy-admin-6.1.18-1.4.osg33.el7\nmyproxy-debuginfo-6.1.18-1.4.osg33.el7\nmyproxy-devel-6.1.18-1.4.osg33.el7\nmyproxy-doc-6.1.18-1.4.osg33.el7\nmyproxy-libs-6.1.18-1.4.osg33.el7\nmyproxy-server-6.1.18-1.4.osg33.el7\nmyproxy-voms-6.1.18-1.4.osg33.el7\nosg-configure-1.5.3-1.osg33.el7\nosg-configure-bosco-1.5.3-1.osg33.el7\nosg-configure-ce-1.5.3-1.osg33.el7\nosg-configure-cemon-1.5.3-1.osg33.el7\nosg-configure-condor-1.5.3-1.osg33.el7\nosg-configure-gateway-1.5.3-1.osg33.el7\nosg-configure-gip-1.5.3-1.osg33.el7\nosg-configure-gratia-1.5.3-1.osg33.el7\nosg-configure-infoservices-1.5.3-1.osg33.el7\nosg-configure-lsf-1.5.3-1.osg33.el7\nosg-configure-managedfork-1.5.3-1.osg33.el7\nosg-configure-misc-1.5.3-1.osg33.el7\nosg-configure-monalisa-1.5.3-1.osg33.el7\nosg-configure-network-1.5.3-1.osg33.el7\nosg-configure-pbs-1.5.3-1.osg33.el7\nosg-configure-rsv-1.5.3-1.osg33.el7\nosg-configure-sge-1.5.3-1.osg33.el7\nosg-configure-slurm-1.5.3-1.osg33.el7\nosg-configure-squid-1.5.3-1.osg33.el7\nosg-configure-tests-1.5.3-1.osg33.el7\nosg-version-3.3.19-1.osg33.el7\nosg-vo-map-0.0.2-1.osg33.el7\nosg-wn-client-3.3-6.osg33.el7\nosg-wn-client-glexec-3.3-6.osg33.el7\nrsv-3.14.0-1.osg33.el7\nrsv-consumers-3.14.0-1.osg33.el7\nrsv-core-3.14.0-1.osg33.el7\nrsv-metrics-3.14.0-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-19/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-19/#enterprise-linux-6_2", 
            "text": "frontier-squid-3.5.22-2.1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-19/#enterprise-linux-7_2", 
            "text": "frontier-squid-3.5.22-2.1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-19/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  frontier-squid frontier-squid-debuginfo  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-19/#enterprise-linux-6_3", 
            "text": "frontier-squid-3.5.22-2.1.osgup.el6\nfrontier-squid-debuginfo-3.5.22-2.1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-19/#enterprise-linux-7_3", 
            "text": "frontier-squid-3.5.22-2.1.osgup.el7\nfrontier-squid-debuginfo-3.5.22-2.1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-18/", 
            "text": "OSG Software Release 3.3.18\n\n\nRelease Date\n: 2016-11-08\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nGlideinWMS 3.2.16\n\n\nEnhancements\n\n\nable to specify the BOSCO user in the frontend\n\n\ncan start a glidein manually\n\n\n\n\n\n\nBug fixes\n\n\ncorrection of some job counters\n\n\nmore resiliency in the communication with the HTCondor daemons\n\n\n\n\n\n\n\n\n\n\nSSLv3 is disabled on the following Globus tools\n\n\nGRAM gatekeeper\n\n\nGridFTP server\n\n\nMyProxy\n\n\ngsissh\n\n\n\n\n\n\nGlobus GridFTP server control patch: Avoid server process hang when client immediately closes a new connection\n\n\nedg-mkgridmap 4.0.4: fix simple omission that caused fatal errors on EL7 (worked fine on EL6)\n\n\nPKI tools now generate Certificate Signing Requests using SHA2\n\n\nFix blahp qstat call to support torque-4.2.9\n\n\nBlahp: Fix crash when using glexec and limited proxies\n\n\nAugment Gratia PBS probe to process \"exec_host\" when \"ALLPROCS\" flag is present, to accurately account for resource utilization\n\n\nHTCondor-CE 2.0.11\n: Minor fixes\n\n\nAccept all DaemonCore options in htcondor-ce-view\n\n\nFix incorrect comment in htcondor-ce-pbs template config\n\n\n\n\n\n\nGridFTP server script now returns correct value when the service isn't running\n\n\nAdd htcondor-ce-view test to the automated test suite\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nKnown Issues\n\n\n\n\nOn EL7, your version of voms-proxy-init may come from the voms-clients-java package; this version will continue to make legacy proxies by default. If you want the new behavior, install the voms-clients-cpp package and uninstall the voms-clients-java package.\n\n\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.28.bosco-1.osg33.el6\n\n\nedg-mkgridmap-4.0.4-1.osg33.el6\n\n\nglideinwms-3.2.16-1.osg33.el6\n\n\nglobus-gatekeeper-10.10-1.3.osg33.el6\n\n\nglobus-gridftp-server-10.4-1.4.osg33.el6\n\n\nglobus-gridftp-server-control-4.1-1.3.osg33.el6\n\n\ngratia-probe-1.17.0-2.5.osg33.el6\n\n\ngsi-openssh-7.1p2f-1.2.osg33.el6\n\n\nhtcondor-ce-2.0.11-1.osg33.el6\n\n\nmyproxy-6.1.18-1.3.osg33.el6\n\n\nosg-pki-tools-1.2.20-1.osg33.el6\n\n\nosg-test-1.9.1-1.osg33.el6\n\n\nosg-tested-internal-3.3-16.osg33.el6\n\n\nosg-version-3.3.18-1.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.28.bosco-1.osg33.el7\n\n\nedg-mkgridmap-4.0.4-1.osg33.el7\n\n\nglideinwms-3.2.16-1.osg33.el7\n\n\nglobus-gatekeeper-10.10-1.3.osg33.el7\n\n\nglobus-gridftp-server-10.4-1.4.osg33.el7\n\n\nglobus-gridftp-server-control-4.1-1.3.osg33.el7\n\n\ngratia-probe-1.17.0-2.5.osg33.el7\n\n\ngsi-openssh-7.1p2f-1.2.osg33.el7\n\n\nhtcondor-ce-2.0.11-1.osg33.el7\n\n\nmyproxy-6.1.18-1.3.osg33.el7\n\n\nosg-pki-tools-1.2.20-1.osg33.el7\n\n\nosg-test-1.9.1-1.osg33.el7\n\n\nosg-tested-internal-3.3-16.osg33.el7\n\n\nosg-version-3.3.18-1.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp blahp-debuginfo edg-mkgridmap glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone globus-gatekeeper globus-gatekeeper-debuginfo globus-gridftp-server globus-gridftp-server-control globus-gridftp-server-control-debuginfo globus-gridftp-server-control-devel globus-gridftp-server-debuginfo globus-gridftp-server-devel globus-gridftp-server-progs gratia-probe-bdii-status gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glexec gratia-probe-glideinwms gratia-probe-gram gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer gsi-openssh gsi-openssh-clients gsi-openssh-debuginfo gsi-openssh-server htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-view igtf-ca-certs myproxy myproxy-admin myproxy-debuginfo myproxy-devel myproxy-doc myproxy-libs myproxy-server myproxy-voms osg-ca-certs osg-pki-tools osg-pki-tools-tests osg-test osg-tested-internal osg-tested-internal-gram osg-version\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp-1.18.28.bosco-1.osg33.el6\nblahp-debuginfo-1.18.28.bosco-1.osg33.el6\nedg-mkgridmap-4.0.4-1.osg33.el6\nglideinwms-3.2.16-1.osg33.el6\nglideinwms-common-tools-3.2.16-1.osg33.el6\nglideinwms-condor-common-config-3.2.16-1.osg33.el6\nglideinwms-factory-3.2.16-1.osg33.el6\nglideinwms-factory-condor-3.2.16-1.osg33.el6\nglideinwms-glidecondor-tools-3.2.16-1.osg33.el6\nglideinwms-libs-3.2.16-1.osg33.el6\nglideinwms-minimal-condor-3.2.16-1.osg33.el6\nglideinwms-usercollector-3.2.16-1.osg33.el6\nglideinwms-userschedd-3.2.16-1.osg33.el6\nglideinwms-vofrontend-3.2.16-1.osg33.el6\nglideinwms-vofrontend-standalone-3.2.16-1.osg33.el6\nglobus-gatekeeper-10.10-1.3.osg33.el6\nglobus-gatekeeper-debuginfo-10.10-1.3.osg33.el6\nglobus-gridftp-server-10.4-1.4.osg33.el6\nglobus-gridftp-server-control-4.1-1.3.osg33.el6\nglobus-gridftp-server-control-debuginfo-4.1-1.3.osg33.el6\nglobus-gridftp-server-control-devel-4.1-1.3.osg33.el6\nglobus-gridftp-server-debuginfo-10.4-1.4.osg33.el6\nglobus-gridftp-server-devel-10.4-1.4.osg33.el6\nglobus-gridftp-server-progs-10.4-1.4.osg33.el6\ngratia-probe-1.17.0-2.5.osg33.el6\ngratia-probe-bdii-status-1.17.0-2.5.osg33.el6\ngratia-probe-common-1.17.0-2.5.osg33.el6\ngratia-probe-condor-1.17.0-2.5.osg33.el6\ngratia-probe-condor-events-1.17.0-2.5.osg33.el6\ngratia-probe-dcache-storage-1.17.0-2.5.osg33.el6\ngratia-probe-dcache-storagegroup-1.17.0-2.5.osg33.el6\ngratia-probe-dcache-transfer-1.17.0-2.5.osg33.el6\ngratia-probe-debuginfo-1.17.0-2.5.osg33.el6\ngratia-probe-enstore-storage-1.17.0-2.5.osg33.el6\ngratia-probe-enstore-tapedrive-1.17.0-2.5.osg33.el6\ngratia-probe-enstore-transfer-1.17.0-2.5.osg33.el6\ngratia-probe-glexec-1.17.0-2.5.osg33.el6\ngratia-probe-glideinwms-1.17.0-2.5.osg33.el6\ngratia-probe-gram-1.17.0-2.5.osg33.el6\ngratia-probe-gridftp-transfer-1.17.0-2.5.osg33.el6\ngratia-probe-hadoop-storage-1.17.0-2.5.osg33.el6\ngratia-probe-htcondor-ce-1.17.0-2.5.osg33.el6\ngratia-probe-lsf-1.17.0-2.5.osg33.el6\ngratia-probe-metric-1.17.0-2.5.osg33.el6\ngratia-probe-onevm-1.17.0-2.5.osg33.el6\ngratia-probe-pbs-lsf-1.17.0-2.5.osg33.el6\ngratia-probe-services-1.17.0-2.5.osg33.el6\ngratia-probe-sge-1.17.0-2.5.osg33.el6\ngratia-probe-slurm-1.17.0-2.5.osg33.el6\ngratia-probe-xrootd-storage-1.17.0-2.5.osg33.el6\ngratia-probe-xrootd-transfer-1.17.0-2.5.osg33.el6\ngsi-openssh-7.1p2f-1.2.osg33.el6\ngsi-openssh-clients-7.1p2f-1.2.osg33.el6\ngsi-openssh-debuginfo-7.1p2f-1.2.osg33.el6\ngsi-openssh-server-7.1p2f-1.2.osg33.el6\nhtcondor-ce-2.0.11-1.osg33.el6\nhtcondor-ce-bosco-2.0.11-1.osg33.el6\nhtcondor-ce-client-2.0.11-1.osg33.el6\nhtcondor-ce-collector-2.0.11-1.osg33.el6\nhtcondor-ce-condor-2.0.11-1.osg33.el6\nhtcondor-ce-lsf-2.0.11-1.osg33.el6\nhtcondor-ce-pbs-2.0.11-1.osg33.el6\nhtcondor-ce-sge-2.0.11-1.osg33.el6\nhtcondor-ce-view-2.0.11-1.osg33.el6\nmyproxy-6.1.18-1.3.osg33.el6\nmyproxy-admin-6.1.18-1.3.osg33.el6\nmyproxy-debuginfo-6.1.18-1.3.osg33.el6\nmyproxy-devel-6.1.18-1.3.osg33.el6\nmyproxy-doc-6.1.18-1.3.osg33.el6\nmyproxy-libs-6.1.18-1.3.osg33.el6\nmyproxy-server-6.1.18-1.3.osg33.el6\nmyproxy-voms-6.1.18-1.3.osg33.el6\nosg-pki-tools-1.2.20-1.osg33.el6\nosg-pki-tools-tests-1.2.20-1.osg33.el6\nosg-test-1.9.1-1.osg33.el6\nosg-tested-internal-3.3-16.osg33.el6\nosg-tested-internal-gram-3.3-16.osg33.el6\nosg-version-3.3.18-1.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp-1.18.28.bosco-1.osg33.el7\nblahp-debuginfo-1.18.28.bosco-1.osg33.el7\nedg-mkgridmap-4.0.4-1.osg33.el7\nglideinwms-3.2.16-1.osg33.el7\nglideinwms-common-tools-3.2.16-1.osg33.el7\nglideinwms-condor-common-config-3.2.16-1.osg33.el7\nglideinwms-factory-3.2.16-1.osg33.el7\nglideinwms-factory-condor-3.2.16-1.osg33.el7\nglideinwms-glidecondor-tools-3.2.16-1.osg33.el7\nglideinwms-libs-3.2.16-1.osg33.el7\nglideinwms-minimal-condor-3.2.16-1.osg33.el7\nglideinwms-usercollector-3.2.16-1.osg33.el7\nglideinwms-userschedd-3.2.16-1.osg33.el7\nglideinwms-vofrontend-3.2.16-1.osg33.el7\nglideinwms-vofrontend-standalone-3.2.16-1.osg33.el7\nglobus-gatekeeper-10.10-1.3.osg33.el7\nglobus-gatekeeper-debuginfo-10.10-1.3.osg33.el7\nglobus-gridftp-server-10.4-1.4.osg33.el7\nglobus-gridftp-server-control-4.1-1.3.osg33.el7\nglobus-gridftp-server-control-debuginfo-4.1-1.3.osg33.el7\nglobus-gridftp-server-control-devel-4.1-1.3.osg33.el7\nglobus-gridftp-server-debuginfo-10.4-1.4.osg33.el7\nglobus-gridftp-server-devel-10.4-1.4.osg33.el7\nglobus-gridftp-server-progs-10.4-1.4.osg33.el7\ngratia-probe-1.17.0-2.5.osg33.el7\ngratia-probe-bdii-status-1.17.0-2.5.osg33.el7\ngratia-probe-common-1.17.0-2.5.osg33.el7\ngratia-probe-condor-1.17.0-2.5.osg33.el7\ngratia-probe-condor-events-1.17.0-2.5.osg33.el7\ngratia-probe-dcache-storage-1.17.0-2.5.osg33.el7\ngratia-probe-dcache-storagegroup-1.17.0-2.5.osg33.el7\ngratia-probe-dcache-transfer-1.17.0-2.5.osg33.el7\ngratia-probe-debuginfo-1.17.0-2.5.osg33.el7\ngratia-probe-enstore-storage-1.17.0-2.5.osg33.el7\ngratia-probe-enstore-tapedrive-1.17.0-2.5.osg33.el7\ngratia-probe-enstore-transfer-1.17.0-2.5.osg33.el7\ngratia-probe-glexec-1.17.0-2.5.osg33.el7\ngratia-probe-glideinwms-1.17.0-2.5.osg33.el7\ngratia-probe-gram-1.17.0-2.5.osg33.el7\ngratia-probe-gridftp-transfer-1.17.0-2.5.osg33.el7\ngratia-probe-hadoop-storage-1.17.0-2.5.osg33.el7\ngratia-probe-htcondor-ce-1.17.0-2.5.osg33.el7\ngratia-probe-lsf-1.17.0-2.5.osg33.el7\ngratia-probe-metric-1.17.0-2.5.osg33.el7\ngratia-probe-onevm-1.17.0-2.5.osg33.el7\ngratia-probe-pbs-lsf-1.17.0-2.5.osg33.el7\ngratia-probe-services-1.17.0-2.5.osg33.el7\ngratia-probe-sge-1.17.0-2.5.osg33.el7\ngratia-probe-slurm-1.17.0-2.5.osg33.el7\ngratia-probe-xrootd-storage-1.17.0-2.5.osg33.el7\ngratia-probe-xrootd-transfer-1.17.0-2.5.osg33.el7\ngsi-openssh-7.1p2f-1.2.osg33.el7\ngsi-openssh-clients-7.1p2f-1.2.osg33.el7\ngsi-openssh-debuginfo-7.1p2f-1.2.osg33.el7\ngsi-openssh-server-7.1p2f-1.2.osg33.el7\nhtcondor-ce-2.0.11-1.osg33.el7\nhtcondor-ce-bosco-2.0.11-1.osg33.el7\nhtcondor-ce-client-2.0.11-1.osg33.el7\nhtcondor-ce-collector-2.0.11-1.osg33.el7\nhtcondor-ce-condor-2.0.11-1.osg33.el7\nhtcondor-ce-lsf-2.0.11-1.osg33.el7\nhtcondor-ce-pbs-2.0.11-1.osg33.el7\nhtcondor-ce-sge-2.0.11-1.osg33.el7\nhtcondor-ce-view-2.0.11-1.osg33.el7\nmyproxy-6.1.18-1.3.osg33.el7\nmyproxy-admin-6.1.18-1.3.osg33.el7\nmyproxy-debuginfo-6.1.18-1.3.osg33.el7\nmyproxy-devel-6.1.18-1.3.osg33.el7\nmyproxy-doc-6.1.18-1.3.osg33.el7\nmyproxy-libs-6.1.18-1.3.osg33.el7\nmyproxy-server-6.1.18-1.3.osg33.el7\nmyproxy-voms-6.1.18-1.3.osg33.el7\nosg-pki-tools-1.2.20-1.osg33.el7\nosg-pki-tools-tests-1.2.20-1.osg33.el7\nosg-test-1.9.1-1.osg33.el7\nosg-tested-internal-3.3-16.osg33.el7\nosg-tested-internal-gram-3.3-16.osg33.el7\nosg-version-3.3.18-1.osg33.el7\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.28.bosco-1.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.28.bosco-1.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp blahp-debuginfo\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp-1.18.28.bosco-1.osgup.el6\nblahp-debuginfo-1.18.28.bosco-1.osgup.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp-1.18.28.bosco-1.osgup.el7\nblahp-debuginfo-1.18.28.bosco-1.osgup.el7", 
            "title": "OSG Release 3.3.18"
        }, 
        {
            "location": "/release/3.3/release-3-3-18/#osg-software-release-3318", 
            "text": "Release Date : 2016-11-08", 
            "title": "OSG Software Release 3.3.18"
        }, 
        {
            "location": "/release/3.3/release-3-3-18/#summary-of-changes", 
            "text": "This release contains:   GlideinWMS 3.2.16  Enhancements  able to specify the BOSCO user in the frontend  can start a glidein manually    Bug fixes  correction of some job counters  more resiliency in the communication with the HTCondor daemons      SSLv3 is disabled on the following Globus tools  GRAM gatekeeper  GridFTP server  MyProxy  gsissh    Globus GridFTP server control patch: Avoid server process hang when client immediately closes a new connection  edg-mkgridmap 4.0.4: fix simple omission that caused fatal errors on EL7 (worked fine on EL6)  PKI tools now generate Certificate Signing Requests using SHA2  Fix blahp qstat call to support torque-4.2.9  Blahp: Fix crash when using glexec and limited proxies  Augment Gratia PBS probe to process \"exec_host\" when \"ALLPROCS\" flag is present, to accurately account for resource utilization  HTCondor-CE 2.0.11 : Minor fixes  Accept all DaemonCore options in htcondor-ce-view  Fix incorrect comment in htcondor-ce-pbs template config    GridFTP server script now returns correct value when the service isn't running  Add htcondor-ce-view test to the automated test suite   These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-18/#known-issues", 
            "text": "On EL7, your version of voms-proxy-init may come from the voms-clients-java package; this version will continue to make legacy proxies by default. If you want the new behavior, install the voms-clients-cpp package and uninstall the voms-clients-java package.", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.3/release-3-3-18/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-18/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-18/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-18/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-18/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-18/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-18/#enterprise-linux-6", 
            "text": "blahp-1.18.28.bosco-1.osg33.el6  edg-mkgridmap-4.0.4-1.osg33.el6  glideinwms-3.2.16-1.osg33.el6  globus-gatekeeper-10.10-1.3.osg33.el6  globus-gridftp-server-10.4-1.4.osg33.el6  globus-gridftp-server-control-4.1-1.3.osg33.el6  gratia-probe-1.17.0-2.5.osg33.el6  gsi-openssh-7.1p2f-1.2.osg33.el6  htcondor-ce-2.0.11-1.osg33.el6  myproxy-6.1.18-1.3.osg33.el6  osg-pki-tools-1.2.20-1.osg33.el6  osg-test-1.9.1-1.osg33.el6  osg-tested-internal-3.3-16.osg33.el6  osg-version-3.3.18-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-18/#enterprise-linux-7", 
            "text": "blahp-1.18.28.bosco-1.osg33.el7  edg-mkgridmap-4.0.4-1.osg33.el7  glideinwms-3.2.16-1.osg33.el7  globus-gatekeeper-10.10-1.3.osg33.el7  globus-gridftp-server-10.4-1.4.osg33.el7  globus-gridftp-server-control-4.1-1.3.osg33.el7  gratia-probe-1.17.0-2.5.osg33.el7  gsi-openssh-7.1p2f-1.2.osg33.el7  htcondor-ce-2.0.11-1.osg33.el7  myproxy-6.1.18-1.3.osg33.el7  osg-pki-tools-1.2.20-1.osg33.el7  osg-test-1.9.1-1.osg33.el7  osg-tested-internal-3.3-16.osg33.el7  osg-version-3.3.18-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-18/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp blahp-debuginfo edg-mkgridmap glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone globus-gatekeeper globus-gatekeeper-debuginfo globus-gridftp-server globus-gridftp-server-control globus-gridftp-server-control-debuginfo globus-gridftp-server-control-devel globus-gridftp-server-debuginfo globus-gridftp-server-devel globus-gridftp-server-progs gratia-probe-bdii-status gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glexec gratia-probe-glideinwms gratia-probe-gram gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer gsi-openssh gsi-openssh-clients gsi-openssh-debuginfo gsi-openssh-server htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-view igtf-ca-certs myproxy myproxy-admin myproxy-debuginfo myproxy-devel myproxy-doc myproxy-libs myproxy-server myproxy-voms osg-ca-certs osg-pki-tools osg-pki-tools-tests osg-test osg-tested-internal osg-tested-internal-gram osg-version  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-18/#enterprise-linux-6_1", 
            "text": "blahp-1.18.28.bosco-1.osg33.el6\nblahp-debuginfo-1.18.28.bosco-1.osg33.el6\nedg-mkgridmap-4.0.4-1.osg33.el6\nglideinwms-3.2.16-1.osg33.el6\nglideinwms-common-tools-3.2.16-1.osg33.el6\nglideinwms-condor-common-config-3.2.16-1.osg33.el6\nglideinwms-factory-3.2.16-1.osg33.el6\nglideinwms-factory-condor-3.2.16-1.osg33.el6\nglideinwms-glidecondor-tools-3.2.16-1.osg33.el6\nglideinwms-libs-3.2.16-1.osg33.el6\nglideinwms-minimal-condor-3.2.16-1.osg33.el6\nglideinwms-usercollector-3.2.16-1.osg33.el6\nglideinwms-userschedd-3.2.16-1.osg33.el6\nglideinwms-vofrontend-3.2.16-1.osg33.el6\nglideinwms-vofrontend-standalone-3.2.16-1.osg33.el6\nglobus-gatekeeper-10.10-1.3.osg33.el6\nglobus-gatekeeper-debuginfo-10.10-1.3.osg33.el6\nglobus-gridftp-server-10.4-1.4.osg33.el6\nglobus-gridftp-server-control-4.1-1.3.osg33.el6\nglobus-gridftp-server-control-debuginfo-4.1-1.3.osg33.el6\nglobus-gridftp-server-control-devel-4.1-1.3.osg33.el6\nglobus-gridftp-server-debuginfo-10.4-1.4.osg33.el6\nglobus-gridftp-server-devel-10.4-1.4.osg33.el6\nglobus-gridftp-server-progs-10.4-1.4.osg33.el6\ngratia-probe-1.17.0-2.5.osg33.el6\ngratia-probe-bdii-status-1.17.0-2.5.osg33.el6\ngratia-probe-common-1.17.0-2.5.osg33.el6\ngratia-probe-condor-1.17.0-2.5.osg33.el6\ngratia-probe-condor-events-1.17.0-2.5.osg33.el6\ngratia-probe-dcache-storage-1.17.0-2.5.osg33.el6\ngratia-probe-dcache-storagegroup-1.17.0-2.5.osg33.el6\ngratia-probe-dcache-transfer-1.17.0-2.5.osg33.el6\ngratia-probe-debuginfo-1.17.0-2.5.osg33.el6\ngratia-probe-enstore-storage-1.17.0-2.5.osg33.el6\ngratia-probe-enstore-tapedrive-1.17.0-2.5.osg33.el6\ngratia-probe-enstore-transfer-1.17.0-2.5.osg33.el6\ngratia-probe-glexec-1.17.0-2.5.osg33.el6\ngratia-probe-glideinwms-1.17.0-2.5.osg33.el6\ngratia-probe-gram-1.17.0-2.5.osg33.el6\ngratia-probe-gridftp-transfer-1.17.0-2.5.osg33.el6\ngratia-probe-hadoop-storage-1.17.0-2.5.osg33.el6\ngratia-probe-htcondor-ce-1.17.0-2.5.osg33.el6\ngratia-probe-lsf-1.17.0-2.5.osg33.el6\ngratia-probe-metric-1.17.0-2.5.osg33.el6\ngratia-probe-onevm-1.17.0-2.5.osg33.el6\ngratia-probe-pbs-lsf-1.17.0-2.5.osg33.el6\ngratia-probe-services-1.17.0-2.5.osg33.el6\ngratia-probe-sge-1.17.0-2.5.osg33.el6\ngratia-probe-slurm-1.17.0-2.5.osg33.el6\ngratia-probe-xrootd-storage-1.17.0-2.5.osg33.el6\ngratia-probe-xrootd-transfer-1.17.0-2.5.osg33.el6\ngsi-openssh-7.1p2f-1.2.osg33.el6\ngsi-openssh-clients-7.1p2f-1.2.osg33.el6\ngsi-openssh-debuginfo-7.1p2f-1.2.osg33.el6\ngsi-openssh-server-7.1p2f-1.2.osg33.el6\nhtcondor-ce-2.0.11-1.osg33.el6\nhtcondor-ce-bosco-2.0.11-1.osg33.el6\nhtcondor-ce-client-2.0.11-1.osg33.el6\nhtcondor-ce-collector-2.0.11-1.osg33.el6\nhtcondor-ce-condor-2.0.11-1.osg33.el6\nhtcondor-ce-lsf-2.0.11-1.osg33.el6\nhtcondor-ce-pbs-2.0.11-1.osg33.el6\nhtcondor-ce-sge-2.0.11-1.osg33.el6\nhtcondor-ce-view-2.0.11-1.osg33.el6\nmyproxy-6.1.18-1.3.osg33.el6\nmyproxy-admin-6.1.18-1.3.osg33.el6\nmyproxy-debuginfo-6.1.18-1.3.osg33.el6\nmyproxy-devel-6.1.18-1.3.osg33.el6\nmyproxy-doc-6.1.18-1.3.osg33.el6\nmyproxy-libs-6.1.18-1.3.osg33.el6\nmyproxy-server-6.1.18-1.3.osg33.el6\nmyproxy-voms-6.1.18-1.3.osg33.el6\nosg-pki-tools-1.2.20-1.osg33.el6\nosg-pki-tools-tests-1.2.20-1.osg33.el6\nosg-test-1.9.1-1.osg33.el6\nosg-tested-internal-3.3-16.osg33.el6\nosg-tested-internal-gram-3.3-16.osg33.el6\nosg-version-3.3.18-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-18/#enterprise-linux-7_1", 
            "text": "blahp-1.18.28.bosco-1.osg33.el7\nblahp-debuginfo-1.18.28.bosco-1.osg33.el7\nedg-mkgridmap-4.0.4-1.osg33.el7\nglideinwms-3.2.16-1.osg33.el7\nglideinwms-common-tools-3.2.16-1.osg33.el7\nglideinwms-condor-common-config-3.2.16-1.osg33.el7\nglideinwms-factory-3.2.16-1.osg33.el7\nglideinwms-factory-condor-3.2.16-1.osg33.el7\nglideinwms-glidecondor-tools-3.2.16-1.osg33.el7\nglideinwms-libs-3.2.16-1.osg33.el7\nglideinwms-minimal-condor-3.2.16-1.osg33.el7\nglideinwms-usercollector-3.2.16-1.osg33.el7\nglideinwms-userschedd-3.2.16-1.osg33.el7\nglideinwms-vofrontend-3.2.16-1.osg33.el7\nglideinwms-vofrontend-standalone-3.2.16-1.osg33.el7\nglobus-gatekeeper-10.10-1.3.osg33.el7\nglobus-gatekeeper-debuginfo-10.10-1.3.osg33.el7\nglobus-gridftp-server-10.4-1.4.osg33.el7\nglobus-gridftp-server-control-4.1-1.3.osg33.el7\nglobus-gridftp-server-control-debuginfo-4.1-1.3.osg33.el7\nglobus-gridftp-server-control-devel-4.1-1.3.osg33.el7\nglobus-gridftp-server-debuginfo-10.4-1.4.osg33.el7\nglobus-gridftp-server-devel-10.4-1.4.osg33.el7\nglobus-gridftp-server-progs-10.4-1.4.osg33.el7\ngratia-probe-1.17.0-2.5.osg33.el7\ngratia-probe-bdii-status-1.17.0-2.5.osg33.el7\ngratia-probe-common-1.17.0-2.5.osg33.el7\ngratia-probe-condor-1.17.0-2.5.osg33.el7\ngratia-probe-condor-events-1.17.0-2.5.osg33.el7\ngratia-probe-dcache-storage-1.17.0-2.5.osg33.el7\ngratia-probe-dcache-storagegroup-1.17.0-2.5.osg33.el7\ngratia-probe-dcache-transfer-1.17.0-2.5.osg33.el7\ngratia-probe-debuginfo-1.17.0-2.5.osg33.el7\ngratia-probe-enstore-storage-1.17.0-2.5.osg33.el7\ngratia-probe-enstore-tapedrive-1.17.0-2.5.osg33.el7\ngratia-probe-enstore-transfer-1.17.0-2.5.osg33.el7\ngratia-probe-glexec-1.17.0-2.5.osg33.el7\ngratia-probe-glideinwms-1.17.0-2.5.osg33.el7\ngratia-probe-gram-1.17.0-2.5.osg33.el7\ngratia-probe-gridftp-transfer-1.17.0-2.5.osg33.el7\ngratia-probe-hadoop-storage-1.17.0-2.5.osg33.el7\ngratia-probe-htcondor-ce-1.17.0-2.5.osg33.el7\ngratia-probe-lsf-1.17.0-2.5.osg33.el7\ngratia-probe-metric-1.17.0-2.5.osg33.el7\ngratia-probe-onevm-1.17.0-2.5.osg33.el7\ngratia-probe-pbs-lsf-1.17.0-2.5.osg33.el7\ngratia-probe-services-1.17.0-2.5.osg33.el7\ngratia-probe-sge-1.17.0-2.5.osg33.el7\ngratia-probe-slurm-1.17.0-2.5.osg33.el7\ngratia-probe-xrootd-storage-1.17.0-2.5.osg33.el7\ngratia-probe-xrootd-transfer-1.17.0-2.5.osg33.el7\ngsi-openssh-7.1p2f-1.2.osg33.el7\ngsi-openssh-clients-7.1p2f-1.2.osg33.el7\ngsi-openssh-debuginfo-7.1p2f-1.2.osg33.el7\ngsi-openssh-server-7.1p2f-1.2.osg33.el7\nhtcondor-ce-2.0.11-1.osg33.el7\nhtcondor-ce-bosco-2.0.11-1.osg33.el7\nhtcondor-ce-client-2.0.11-1.osg33.el7\nhtcondor-ce-collector-2.0.11-1.osg33.el7\nhtcondor-ce-condor-2.0.11-1.osg33.el7\nhtcondor-ce-lsf-2.0.11-1.osg33.el7\nhtcondor-ce-pbs-2.0.11-1.osg33.el7\nhtcondor-ce-sge-2.0.11-1.osg33.el7\nhtcondor-ce-view-2.0.11-1.osg33.el7\nmyproxy-6.1.18-1.3.osg33.el7\nmyproxy-admin-6.1.18-1.3.osg33.el7\nmyproxy-debuginfo-6.1.18-1.3.osg33.el7\nmyproxy-devel-6.1.18-1.3.osg33.el7\nmyproxy-doc-6.1.18-1.3.osg33.el7\nmyproxy-libs-6.1.18-1.3.osg33.el7\nmyproxy-server-6.1.18-1.3.osg33.el7\nmyproxy-voms-6.1.18-1.3.osg33.el7\nosg-pki-tools-1.2.20-1.osg33.el7\nosg-pki-tools-tests-1.2.20-1.osg33.el7\nosg-test-1.9.1-1.osg33.el7\nosg-tested-internal-3.3-16.osg33.el7\nosg-tested-internal-gram-3.3-16.osg33.el7\nosg-version-3.3.18-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-18/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-18/#enterprise-linux-6_2", 
            "text": "blahp-1.18.28.bosco-1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-18/#enterprise-linux-7_2", 
            "text": "blahp-1.18.28.bosco-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-18/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp blahp-debuginfo  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-18/#enterprise-linux-6_3", 
            "text": "blahp-1.18.28.bosco-1.osgup.el6\nblahp-debuginfo-1.18.28.bosco-1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-18/#enterprise-linux-7_3", 
            "text": "blahp-1.18.28.bosco-1.osgup.el7\nblahp-debuginfo-1.18.28.bosco-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-17-2/", 
            "text": "OSG Software Stack -- Data Release\n\n\nRelease Date\n: 2016-10-19\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nCA Certificates based on \nIGTF 1.78\n\n\nRemoved superseded INFN-CA-2006 CA (IT)\n\n\nUpdated Debian packaging to support APT security improvements\n\n\nUpdated namespaces and signing_policy files for CILogon Basic CA to permit DNs without \"/C=US\" (US)\n\n\nAdded G2 series (sha-2) QuoVadis Root 2 and Grid ICA G2 (BM)\n\n\nRemoved discontinued UniandesCA (CO)\n\n\n\n\n\n\n\n\nThis JIRA ticket (\nSOFTWARE-2469\n) was addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nigtf-ca-certs-1.78-1.osg33.el6\n\n\nosg-ca-certs-1.58-1.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nigtf-ca-certs-1.78-1.osg33.el7\n\n\nosg-ca-certs-1.58-1.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nigtf-ca-certs osg-ca-certs\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nigtf-ca-certs-1.78-1.osg33.el6\nosg-ca-certs-1.58-1.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nigtf-ca-certs-1.78-1.osg33.el6\nosg-ca-certs-1.58-1.osg33.el6", 
            "title": "OSG Release 3.3.17-2"
        }, 
        {
            "location": "/release/3.3/release-3-3-17-2/#osg-software-stack-data-release", 
            "text": "Release Date : 2016-10-19", 
            "title": "OSG Software Stack -- Data Release"
        }, 
        {
            "location": "/release/3.3/release-3-3-17-2/#summary-of-changes", 
            "text": "This release contains:   CA Certificates based on  IGTF 1.78  Removed superseded INFN-CA-2006 CA (IT)  Updated Debian packaging to support APT security improvements  Updated namespaces and signing_policy files for CILogon Basic CA to permit DNs without \"/C=US\" (US)  Added G2 series (sha-2) QuoVadis Root 2 and Grid ICA G2 (BM)  Removed discontinued UniandesCA (CO)     This JIRA ticket ( SOFTWARE-2469 ) was addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-17-2/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-17-2/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-17-2/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-17-2/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-17-2/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-17-2/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-17-2/#enterprise-linux-6", 
            "text": "igtf-ca-certs-1.78-1.osg33.el6  osg-ca-certs-1.58-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-17-2/#enterprise-linux-7", 
            "text": "igtf-ca-certs-1.78-1.osg33.el7  osg-ca-certs-1.58-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-17-2/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  igtf-ca-certs osg-ca-certs  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-17-2/#enterprise-linux-6_1", 
            "text": "igtf-ca-certs-1.78-1.osg33.el6\nosg-ca-certs-1.58-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-17-2/#enterprise-linux-7_1", 
            "text": "igtf-ca-certs-1.78-1.osg33.el6\nosg-ca-certs-1.58-1.osg33.el6", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-17/", 
            "text": "OSG Software Release 3.3.17\n\n\nRelease Date\n: 2016-10-13\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nHTCondor 8.4.9\n: Job Router prompts schedd reschedule, other bug fixes\n\n\nHTCondor-CE 2.0.10\n\n\nDetect and refuse to start with an invalid configuration\n\n\nHandle unbounded HTCondor-CE accounting directory\n\n\nProperly check against 'undefined' for undefined values\n\n\n\n\n\n\ngratia-probe-1.17.0-2.3\n\n\nUpdate gratia probe to work with more recent versions of Slurm\n\n\nAdd fallback default in gratia probe for HTCondor-CE history folder\n\n\n\n\n\n\nfrontier-squid-2.7.STABLE9-27\n - fix unbounded growth of swap.state\n\n\nCVMFS 2.3.2\n: support for secured (using VOMS X.509 proxies) access to data in osgstorage.org repositories\n\n\nXRootD 4.4.0\n\n\nSeveral configuration updates to better mesh with EL7 and systemd\n\n\nosg-control now uses systemd interfaces where appropriate\n\n\nsystemd tmpfile mechanism employed for HTCondor, HTCondor-CE, gratia\n\n\nglobus-gatekeeper init script now works properly with systemd\n\n\n\n\n\n\nAdd RPM package version list to tarballs\n\n\nHTCondor 8.5.7\n in Upcoming: the schedd can perform job ClassAd transformations\n\n\nUpdated to \nVO Package v69\n: Added: miniclean VO; Removed: LNBE, CDF INFN\n\n\nRSV-perfSONAR 1.1.4\n - Have probes look farther back into the past for information\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nKnown Issues\n\n\n\n\nOn EL7, your version of voms-proxy-init may come from the voms-clients-java package; this version will continue to make legacy proxies by default. If you want the new behavior, install the voms-clients-cpp package and uninstall the voms-clients-java package.\n\n\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.26.bosco-1.osg33.el6\n\n\ncondor-8.4.9-1.osg33.el6\n\n\ncondor-cron-1.1.1-2.osg33.el6\n\n\ncvmfs-2.3.2-1.osg33.el6\n\n\ncvmfs-config-osg-1.2-5.osg33.el6\n\n\ncvmfs-x509-helper-0.9-1.osg33.el6\n\n\nfrontier-squid-2.7.STABLE9-27.1.osg33.el6\n\n\nglobus-gatekeeper-10.10-1.2.osg33.el6\n\n\ngratia-probe-1.17.0-2.3.osg33.el6\n\n\nhtcondor-ce-2.0.10-1.osg33.el6\n\n\nosg-control-1.1.0-1.osg33.el6\n\n\nosg-oasis-7-5.osg33.el6\n\n\nosg-test-1.9.0-1.osg33.el6\n\n\nosg-tested-internal-3.3-15.osg33.el6\n\n\nosg-version-3.3.17-1.osg33.el6\n\n\nrsv-perfsonar-1.1.4-1.osg33.el6\n\n\nvo-client-69-1.osg33.el6\n\n\nxrootd-4.4.0-1.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.26.bosco-1.osg33.el7\n\n\ncondor-8.4.9-1.osg33.el7\n\n\ncondor-cron-1.1.1-2.osg33.el7\n\n\ncvmfs-2.3.2-1.osg33.el7\n\n\ncvmfs-config-osg-1.2-5.osg33.el7\n\n\ncvmfs-x509-helper-0.9-1.osg33.el7\n\n\nfrontier-squid-2.7.STABLE9-27.1.osg33.el7\n\n\nglobus-gatekeeper-10.10-1.2.osg33.el7\n\n\ngratia-probe-1.17.0-2.3.osg33.el7\n\n\nhtcondor-ce-2.0.10-1.osg33.el7\n\n\nosg-control-1.1.0-1.osg33.el7\n\n\nosg-oasis-7-5.osg33.el7\n\n\nosg-test-1.9.0-1.osg33.el7\n\n\nosg-tested-internal-3.3-15.osg33.el7\n\n\nosg-version-3.3.17-1.osg33.el7\n\n\nvo-client-69-1.osg33.el7\n\n\nxrootd-4.4.0-1.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-cron condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp cvmfs cvmfs-config-osg cvmfs-devel cvmfs-server cvmfs-unittests cvmfs-x509-helper cvmfs-x509-helper-debuginfo frontier-squid frontier-squid-debuginfo globus-gatekeeper globus-gatekeeper-debuginfo gratia-probe-bdii-status gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glexec gratia-probe-glideinwms gratia-probe-gram gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-view osg-control osg-gums-config osg-oasis osg-test osg-tested-internal osg-tested-internal-gram osg-version rsv-perfsonar vo-client vo-client-edgmkgridmap xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-libs xrootd-private-devel xrootd-python xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp-1.18.26.bosco-1.osg33.el6\nblahp-debuginfo-1.18.26.bosco-1.osg33.el6\ncondor-8.4.9-1.osg33.el6\ncondor-all-8.4.9-1.osg33.el6\ncondor-bosco-8.4.9-1.osg33.el6\ncondor-classads-8.4.9-1.osg33.el6\ncondor-classads-devel-8.4.9-1.osg33.el6\ncondor-cream-gahp-8.4.9-1.osg33.el6\ncondor-cron-1.1.1-2.osg33.el6\ncondor-debuginfo-8.4.9-1.osg33.el6\ncondor-kbdd-8.4.9-1.osg33.el6\ncondor-procd-8.4.9-1.osg33.el6\ncondor-python-8.4.9-1.osg33.el6\ncondor-std-universe-8.4.9-1.osg33.el6\ncondor-test-8.4.9-1.osg33.el6\ncondor-vm-gahp-8.4.9-1.osg33.el6\ncvmfs-2.3.2-1.osg33.el6\ncvmfs-config-osg-1.2-5.osg33.el6\ncvmfs-devel-2.3.2-1.osg33.el6\ncvmfs-server-2.3.2-1.osg33.el6\ncvmfs-unittests-2.3.2-1.osg33.el6\ncvmfs-x509-helper-0.9-1.osg33.el6\ncvmfs-x509-helper-debuginfo-0.9-1.osg33.el6\nfrontier-squid-2.7.STABLE9-27.1.osg33.el6\nfrontier-squid-debuginfo-2.7.STABLE9-27.1.osg33.el6\nglobus-gatekeeper-10.10-1.2.osg33.el6\nglobus-gatekeeper-debuginfo-10.10-1.2.osg33.el6\ngratia-probe-1.17.0-2.3.osg33.el6\ngratia-probe-bdii-status-1.17.0-2.3.osg33.el6\ngratia-probe-common-1.17.0-2.3.osg33.el6\ngratia-probe-condor-1.17.0-2.3.osg33.el6\ngratia-probe-condor-events-1.17.0-2.3.osg33.el6\ngratia-probe-dcache-storage-1.17.0-2.3.osg33.el6\ngratia-probe-dcache-storagegroup-1.17.0-2.3.osg33.el6\ngratia-probe-dcache-transfer-1.17.0-2.3.osg33.el6\ngratia-probe-debuginfo-1.17.0-2.3.osg33.el6\ngratia-probe-enstore-storage-1.17.0-2.3.osg33.el6\ngratia-probe-enstore-tapedrive-1.17.0-2.3.osg33.el6\ngratia-probe-enstore-transfer-1.17.0-2.3.osg33.el6\ngratia-probe-glexec-1.17.0-2.3.osg33.el6\ngratia-probe-glideinwms-1.17.0-2.3.osg33.el6\ngratia-probe-gram-1.17.0-2.3.osg33.el6\ngratia-probe-gridftp-transfer-1.17.0-2.3.osg33.el6\ngratia-probe-hadoop-storage-1.17.0-2.3.osg33.el6\ngratia-probe-htcondor-ce-1.17.0-2.3.osg33.el6\ngratia-probe-lsf-1.17.0-2.3.osg33.el6\ngratia-probe-metric-1.17.0-2.3.osg33.el6\ngratia-probe-onevm-1.17.0-2.3.osg33.el6\ngratia-probe-pbs-lsf-1.17.0-2.3.osg33.el6\ngratia-probe-services-1.17.0-2.3.osg33.el6\ngratia-probe-sge-1.17.0-2.3.osg33.el6\ngratia-probe-slurm-1.17.0-2.3.osg33.el6\ngratia-probe-xrootd-storage-1.17.0-2.3.osg33.el6\ngratia-probe-xrootd-transfer-1.17.0-2.3.osg33.el6\nhtcondor-ce-2.0.10-1.osg33.el6\nhtcondor-ce-bosco-2.0.10-1.osg33.el6\nhtcondor-ce-client-2.0.10-1.osg33.el6\nhtcondor-ce-collector-2.0.10-1.osg33.el6\nhtcondor-ce-condor-2.0.10-1.osg33.el6\nhtcondor-ce-lsf-2.0.10-1.osg33.el6\nhtcondor-ce-pbs-2.0.10-1.osg33.el6\nhtcondor-ce-sge-2.0.10-1.osg33.el6\nhtcondor-ce-view-2.0.10-1.osg33.el6\nosg-control-1.1.0-1.osg33.el6\nosg-gums-config-69-1.osg33.el6\nosg-oasis-7-5.osg33.el6\nosg-test-1.9.0-1.osg33.el6\nosg-tested-internal-3.3-15.osg33.el6\nosg-tested-internal-gram-3.3-15.osg33.el6\nosg-version-3.3.17-1.osg33.el6\nrsv-perfsonar-1.1.4-1.osg33.el6\nvo-client-69-1.osg33.el6\nvo-client-edgmkgridmap-69-1.osg33.el6\nxrootd-4.4.0-1.osg33.el6\nxrootd-client-4.4.0-1.osg33.el6\nxrootd-client-devel-4.4.0-1.osg33.el6\nxrootd-client-libs-4.4.0-1.osg33.el6\nxrootd-debuginfo-4.4.0-1.osg33.el6\nxrootd-devel-4.4.0-1.osg33.el6\nxrootd-doc-4.4.0-1.osg33.el6\nxrootd-fuse-4.4.0-1.osg33.el6\nxrootd-libs-4.4.0-1.osg33.el6\nxrootd-private-devel-4.4.0-1.osg33.el6\nxrootd-python-4.4.0-1.osg33.el6\nxrootd-selinux-4.4.0-1.osg33.el6\nxrootd-server-4.4.0-1.osg33.el6\nxrootd-server-devel-4.4.0-1.osg33.el6\nxrootd-server-libs-4.4.0-1.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp-1.18.26.bosco-1.osg33.el7\nblahp-debuginfo-1.18.26.bosco-1.osg33.el7\ncondor-8.4.9-1.osg33.el7\ncondor-all-8.4.9-1.osg33.el7\ncondor-bosco-8.4.9-1.osg33.el7\ncondor-classads-8.4.9-1.osg33.el7\ncondor-classads-devel-8.4.9-1.osg33.el7\ncondor-cream-gahp-8.4.9-1.osg33.el7\ncondor-cron-1.1.1-2.osg33.el7\ncondor-debuginfo-8.4.9-1.osg33.el7\ncondor-kbdd-8.4.9-1.osg33.el7\ncondor-procd-8.4.9-1.osg33.el7\ncondor-python-8.4.9-1.osg33.el7\ncondor-test-8.4.9-1.osg33.el7\ncondor-vm-gahp-8.4.9-1.osg33.el7\ncvmfs-2.3.2-1.osg33.el7\ncvmfs-config-osg-1.2-5.osg33.el7\ncvmfs-devel-2.3.2-1.osg33.el7\ncvmfs-server-2.3.2-1.osg33.el7\ncvmfs-unittests-2.3.2-1.osg33.el7\ncvmfs-x509-helper-0.9-1.osg33.el7\ncvmfs-x509-helper-debuginfo-0.9-1.osg33.el7\nfrontier-squid-2.7.STABLE9-27.1.osg33.el7\nfrontier-squid-debuginfo-2.7.STABLE9-27.1.osg33.el7\nglobus-gatekeeper-10.10-1.2.osg33.el7\nglobus-gatekeeper-debuginfo-10.10-1.2.osg33.el7\ngratia-probe-1.17.0-2.3.osg33.el7\ngratia-probe-bdii-status-1.17.0-2.3.osg33.el7\ngratia-probe-common-1.17.0-2.3.osg33.el7\ngratia-probe-condor-1.17.0-2.3.osg33.el7\ngratia-probe-condor-events-1.17.0-2.3.osg33.el7\ngratia-probe-dcache-storage-1.17.0-2.3.osg33.el7\ngratia-probe-dcache-storagegroup-1.17.0-2.3.osg33.el7\ngratia-probe-dcache-transfer-1.17.0-2.3.osg33.el7\ngratia-probe-debuginfo-1.17.0-2.3.osg33.el7\ngratia-probe-enstore-storage-1.17.0-2.3.osg33.el7\ngratia-probe-enstore-tapedrive-1.17.0-2.3.osg33.el7\ngratia-probe-enstore-transfer-1.17.0-2.3.osg33.el7\ngratia-probe-glexec-1.17.0-2.3.osg33.el7\ngratia-probe-glideinwms-1.17.0-2.3.osg33.el7\ngratia-probe-gram-1.17.0-2.3.osg33.el7\ngratia-probe-gridftp-transfer-1.17.0-2.3.osg33.el7\ngratia-probe-hadoop-storage-1.17.0-2.3.osg33.el7\ngratia-probe-htcondor-ce-1.17.0-2.3.osg33.el7\ngratia-probe-lsf-1.17.0-2.3.osg33.el7\ngratia-probe-metric-1.17.0-2.3.osg33.el7\ngratia-probe-onevm-1.17.0-2.3.osg33.el7\ngratia-probe-pbs-lsf-1.17.0-2.3.osg33.el7\ngratia-probe-services-1.17.0-2.3.osg33.el7\ngratia-probe-sge-1.17.0-2.3.osg33.el7\ngratia-probe-slurm-1.17.0-2.3.osg33.el7\ngratia-probe-xrootd-storage-1.17.0-2.3.osg33.el7\ngratia-probe-xrootd-transfer-1.17.0-2.3.osg33.el7\nhtcondor-ce-2.0.10-1.osg33.el7\nhtcondor-ce-bosco-2.0.10-1.osg33.el7\nhtcondor-ce-client-2.0.10-1.osg33.el7\nhtcondor-ce-collector-2.0.10-1.osg33.el7\nhtcondor-ce-condor-2.0.10-1.osg33.el7\nhtcondor-ce-lsf-2.0.10-1.osg33.el7\nhtcondor-ce-pbs-2.0.10-1.osg33.el7\nhtcondor-ce-sge-2.0.10-1.osg33.el7\nhtcondor-ce-view-2.0.10-1.osg33.el7\nosg-control-1.1.0-1.osg33.el7\nosg-gums-config-69-1.osg33.el7\nosg-oasis-7-5.osg33.el7\nosg-test-1.9.0-1.osg33.el7\nosg-tested-internal-3.3-15.osg33.el7\nosg-tested-internal-gram-3.3-15.osg33.el7\nosg-version-3.3.17-1.osg33.el7\nvo-client-69-1.osg33.el7\nvo-client-edgmkgridmap-69-1.osg33.el7\nxrootd-4.4.0-1.osg33.el7\nxrootd-client-4.4.0-1.osg33.el7\nxrootd-client-devel-4.4.0-1.osg33.el7\nxrootd-client-libs-4.4.0-1.osg33.el7\nxrootd-debuginfo-4.4.0-1.osg33.el7\nxrootd-devel-4.4.0-1.osg33.el7\nxrootd-doc-4.4.0-1.osg33.el7\nxrootd-fuse-4.4.0-1.osg33.el7\nxrootd-libs-4.4.0-1.osg33.el7\nxrootd-private-devel-4.4.0-1.osg33.el7\nxrootd-python-4.4.0-1.osg33.el7\nxrootd-selinux-4.4.0-1.osg33.el7\nxrootd-server-4.4.0-1.osg33.el7\nxrootd-server-devel-4.4.0-1.osg33.el7\nxrootd-server-libs-4.4.0-1.osg33.el7\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.26.bosco-1.osgup.el6\n\n\ncondor-8.5.7-1.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.26.bosco-1.osgup.el7\n\n\ncondor-8.5.7-1.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-ec2 condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp-1.18.26.bosco-1.osgup.el6\nblahp-debuginfo-1.18.26.bosco-1.osgup.el6\ncondor-8.5.7-1.osgup.el6\ncondor-all-8.5.7-1.osgup.el6\ncondor-bosco-8.5.7-1.osgup.el6\ncondor-classads-8.5.7-1.osgup.el6\ncondor-classads-devel-8.5.7-1.osgup.el6\ncondor-cream-gahp-8.5.7-1.osgup.el6\ncondor-debuginfo-8.5.7-1.osgup.el6\ncondor-ec2-8.5.7-1.osgup.el6\ncondor-kbdd-8.5.7-1.osgup.el6\ncondor-procd-8.5.7-1.osgup.el6\ncondor-python-8.5.7-1.osgup.el6\ncondor-std-universe-8.5.7-1.osgup.el6\ncondor-test-8.5.7-1.osgup.el6\ncondor-vm-gahp-8.5.7-1.osgup.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp-1.18.26.bosco-1.osgup.el7\nblahp-debuginfo-1.18.26.bosco-1.osgup.el7\ncondor-8.5.7-1.osgup.el7\ncondor-all-8.5.7-1.osgup.el7\ncondor-bosco-8.5.7-1.osgup.el7\ncondor-classads-8.5.7-1.osgup.el7\ncondor-classads-devel-8.5.7-1.osgup.el7\ncondor-cream-gahp-8.5.7-1.osgup.el7\ncondor-debuginfo-8.5.7-1.osgup.el7\ncondor-ec2-8.5.7-1.osgup.el7\ncondor-kbdd-8.5.7-1.osgup.el7\ncondor-procd-8.5.7-1.osgup.el7\ncondor-python-8.5.7-1.osgup.el7\ncondor-test-8.5.7-1.osgup.el7\ncondor-vm-gahp-8.5.7-1.osgup.el7", 
            "title": "OSG Release 3.3.17"
        }, 
        {
            "location": "/release/3.3/release-3-3-17/#osg-software-release-3317", 
            "text": "Release Date : 2016-10-13", 
            "title": "OSG Software Release 3.3.17"
        }, 
        {
            "location": "/release/3.3/release-3-3-17/#summary-of-changes", 
            "text": "This release contains:   HTCondor 8.4.9 : Job Router prompts schedd reschedule, other bug fixes  HTCondor-CE 2.0.10  Detect and refuse to start with an invalid configuration  Handle unbounded HTCondor-CE accounting directory  Properly check against 'undefined' for undefined values    gratia-probe-1.17.0-2.3  Update gratia probe to work with more recent versions of Slurm  Add fallback default in gratia probe for HTCondor-CE history folder    frontier-squid-2.7.STABLE9-27  - fix unbounded growth of swap.state  CVMFS 2.3.2 : support for secured (using VOMS X.509 proxies) access to data in osgstorage.org repositories  XRootD 4.4.0  Several configuration updates to better mesh with EL7 and systemd  osg-control now uses systemd interfaces where appropriate  systemd tmpfile mechanism employed for HTCondor, HTCondor-CE, gratia  globus-gatekeeper init script now works properly with systemd    Add RPM package version list to tarballs  HTCondor 8.5.7  in Upcoming: the schedd can perform job ClassAd transformations  Updated to  VO Package v69 : Added: miniclean VO; Removed: LNBE, CDF INFN  RSV-perfSONAR 1.1.4  - Have probes look farther back into the past for information   These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-17/#known-issues", 
            "text": "On EL7, your version of voms-proxy-init may come from the voms-clients-java package; this version will continue to make legacy proxies by default. If you want the new behavior, install the voms-clients-cpp package and uninstall the voms-clients-java package.", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.3/release-3-3-17/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-17/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-17/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-17/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-17/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-17/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-17/#enterprise-linux-6", 
            "text": "blahp-1.18.26.bosco-1.osg33.el6  condor-8.4.9-1.osg33.el6  condor-cron-1.1.1-2.osg33.el6  cvmfs-2.3.2-1.osg33.el6  cvmfs-config-osg-1.2-5.osg33.el6  cvmfs-x509-helper-0.9-1.osg33.el6  frontier-squid-2.7.STABLE9-27.1.osg33.el6  globus-gatekeeper-10.10-1.2.osg33.el6  gratia-probe-1.17.0-2.3.osg33.el6  htcondor-ce-2.0.10-1.osg33.el6  osg-control-1.1.0-1.osg33.el6  osg-oasis-7-5.osg33.el6  osg-test-1.9.0-1.osg33.el6  osg-tested-internal-3.3-15.osg33.el6  osg-version-3.3.17-1.osg33.el6  rsv-perfsonar-1.1.4-1.osg33.el6  vo-client-69-1.osg33.el6  xrootd-4.4.0-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-17/#enterprise-linux-7", 
            "text": "blahp-1.18.26.bosco-1.osg33.el7  condor-8.4.9-1.osg33.el7  condor-cron-1.1.1-2.osg33.el7  cvmfs-2.3.2-1.osg33.el7  cvmfs-config-osg-1.2-5.osg33.el7  cvmfs-x509-helper-0.9-1.osg33.el7  frontier-squid-2.7.STABLE9-27.1.osg33.el7  globus-gatekeeper-10.10-1.2.osg33.el7  gratia-probe-1.17.0-2.3.osg33.el7  htcondor-ce-2.0.10-1.osg33.el7  osg-control-1.1.0-1.osg33.el7  osg-oasis-7-5.osg33.el7  osg-test-1.9.0-1.osg33.el7  osg-tested-internal-3.3-15.osg33.el7  osg-version-3.3.17-1.osg33.el7  vo-client-69-1.osg33.el7  xrootd-4.4.0-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-17/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-cron condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp cvmfs cvmfs-config-osg cvmfs-devel cvmfs-server cvmfs-unittests cvmfs-x509-helper cvmfs-x509-helper-debuginfo frontier-squid frontier-squid-debuginfo globus-gatekeeper globus-gatekeeper-debuginfo gratia-probe-bdii-status gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glexec gratia-probe-glideinwms gratia-probe-gram gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-view osg-control osg-gums-config osg-oasis osg-test osg-tested-internal osg-tested-internal-gram osg-version rsv-perfsonar vo-client vo-client-edgmkgridmap xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-libs xrootd-private-devel xrootd-python xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-17/#enterprise-linux-6_1", 
            "text": "blahp-1.18.26.bosco-1.osg33.el6\nblahp-debuginfo-1.18.26.bosco-1.osg33.el6\ncondor-8.4.9-1.osg33.el6\ncondor-all-8.4.9-1.osg33.el6\ncondor-bosco-8.4.9-1.osg33.el6\ncondor-classads-8.4.9-1.osg33.el6\ncondor-classads-devel-8.4.9-1.osg33.el6\ncondor-cream-gahp-8.4.9-1.osg33.el6\ncondor-cron-1.1.1-2.osg33.el6\ncondor-debuginfo-8.4.9-1.osg33.el6\ncondor-kbdd-8.4.9-1.osg33.el6\ncondor-procd-8.4.9-1.osg33.el6\ncondor-python-8.4.9-1.osg33.el6\ncondor-std-universe-8.4.9-1.osg33.el6\ncondor-test-8.4.9-1.osg33.el6\ncondor-vm-gahp-8.4.9-1.osg33.el6\ncvmfs-2.3.2-1.osg33.el6\ncvmfs-config-osg-1.2-5.osg33.el6\ncvmfs-devel-2.3.2-1.osg33.el6\ncvmfs-server-2.3.2-1.osg33.el6\ncvmfs-unittests-2.3.2-1.osg33.el6\ncvmfs-x509-helper-0.9-1.osg33.el6\ncvmfs-x509-helper-debuginfo-0.9-1.osg33.el6\nfrontier-squid-2.7.STABLE9-27.1.osg33.el6\nfrontier-squid-debuginfo-2.7.STABLE9-27.1.osg33.el6\nglobus-gatekeeper-10.10-1.2.osg33.el6\nglobus-gatekeeper-debuginfo-10.10-1.2.osg33.el6\ngratia-probe-1.17.0-2.3.osg33.el6\ngratia-probe-bdii-status-1.17.0-2.3.osg33.el6\ngratia-probe-common-1.17.0-2.3.osg33.el6\ngratia-probe-condor-1.17.0-2.3.osg33.el6\ngratia-probe-condor-events-1.17.0-2.3.osg33.el6\ngratia-probe-dcache-storage-1.17.0-2.3.osg33.el6\ngratia-probe-dcache-storagegroup-1.17.0-2.3.osg33.el6\ngratia-probe-dcache-transfer-1.17.0-2.3.osg33.el6\ngratia-probe-debuginfo-1.17.0-2.3.osg33.el6\ngratia-probe-enstore-storage-1.17.0-2.3.osg33.el6\ngratia-probe-enstore-tapedrive-1.17.0-2.3.osg33.el6\ngratia-probe-enstore-transfer-1.17.0-2.3.osg33.el6\ngratia-probe-glexec-1.17.0-2.3.osg33.el6\ngratia-probe-glideinwms-1.17.0-2.3.osg33.el6\ngratia-probe-gram-1.17.0-2.3.osg33.el6\ngratia-probe-gridftp-transfer-1.17.0-2.3.osg33.el6\ngratia-probe-hadoop-storage-1.17.0-2.3.osg33.el6\ngratia-probe-htcondor-ce-1.17.0-2.3.osg33.el6\ngratia-probe-lsf-1.17.0-2.3.osg33.el6\ngratia-probe-metric-1.17.0-2.3.osg33.el6\ngratia-probe-onevm-1.17.0-2.3.osg33.el6\ngratia-probe-pbs-lsf-1.17.0-2.3.osg33.el6\ngratia-probe-services-1.17.0-2.3.osg33.el6\ngratia-probe-sge-1.17.0-2.3.osg33.el6\ngratia-probe-slurm-1.17.0-2.3.osg33.el6\ngratia-probe-xrootd-storage-1.17.0-2.3.osg33.el6\ngratia-probe-xrootd-transfer-1.17.0-2.3.osg33.el6\nhtcondor-ce-2.0.10-1.osg33.el6\nhtcondor-ce-bosco-2.0.10-1.osg33.el6\nhtcondor-ce-client-2.0.10-1.osg33.el6\nhtcondor-ce-collector-2.0.10-1.osg33.el6\nhtcondor-ce-condor-2.0.10-1.osg33.el6\nhtcondor-ce-lsf-2.0.10-1.osg33.el6\nhtcondor-ce-pbs-2.0.10-1.osg33.el6\nhtcondor-ce-sge-2.0.10-1.osg33.el6\nhtcondor-ce-view-2.0.10-1.osg33.el6\nosg-control-1.1.0-1.osg33.el6\nosg-gums-config-69-1.osg33.el6\nosg-oasis-7-5.osg33.el6\nosg-test-1.9.0-1.osg33.el6\nosg-tested-internal-3.3-15.osg33.el6\nosg-tested-internal-gram-3.3-15.osg33.el6\nosg-version-3.3.17-1.osg33.el6\nrsv-perfsonar-1.1.4-1.osg33.el6\nvo-client-69-1.osg33.el6\nvo-client-edgmkgridmap-69-1.osg33.el6\nxrootd-4.4.0-1.osg33.el6\nxrootd-client-4.4.0-1.osg33.el6\nxrootd-client-devel-4.4.0-1.osg33.el6\nxrootd-client-libs-4.4.0-1.osg33.el6\nxrootd-debuginfo-4.4.0-1.osg33.el6\nxrootd-devel-4.4.0-1.osg33.el6\nxrootd-doc-4.4.0-1.osg33.el6\nxrootd-fuse-4.4.0-1.osg33.el6\nxrootd-libs-4.4.0-1.osg33.el6\nxrootd-private-devel-4.4.0-1.osg33.el6\nxrootd-python-4.4.0-1.osg33.el6\nxrootd-selinux-4.4.0-1.osg33.el6\nxrootd-server-4.4.0-1.osg33.el6\nxrootd-server-devel-4.4.0-1.osg33.el6\nxrootd-server-libs-4.4.0-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-17/#enterprise-linux-7_1", 
            "text": "blahp-1.18.26.bosco-1.osg33.el7\nblahp-debuginfo-1.18.26.bosco-1.osg33.el7\ncondor-8.4.9-1.osg33.el7\ncondor-all-8.4.9-1.osg33.el7\ncondor-bosco-8.4.9-1.osg33.el7\ncondor-classads-8.4.9-1.osg33.el7\ncondor-classads-devel-8.4.9-1.osg33.el7\ncondor-cream-gahp-8.4.9-1.osg33.el7\ncondor-cron-1.1.1-2.osg33.el7\ncondor-debuginfo-8.4.9-1.osg33.el7\ncondor-kbdd-8.4.9-1.osg33.el7\ncondor-procd-8.4.9-1.osg33.el7\ncondor-python-8.4.9-1.osg33.el7\ncondor-test-8.4.9-1.osg33.el7\ncondor-vm-gahp-8.4.9-1.osg33.el7\ncvmfs-2.3.2-1.osg33.el7\ncvmfs-config-osg-1.2-5.osg33.el7\ncvmfs-devel-2.3.2-1.osg33.el7\ncvmfs-server-2.3.2-1.osg33.el7\ncvmfs-unittests-2.3.2-1.osg33.el7\ncvmfs-x509-helper-0.9-1.osg33.el7\ncvmfs-x509-helper-debuginfo-0.9-1.osg33.el7\nfrontier-squid-2.7.STABLE9-27.1.osg33.el7\nfrontier-squid-debuginfo-2.7.STABLE9-27.1.osg33.el7\nglobus-gatekeeper-10.10-1.2.osg33.el7\nglobus-gatekeeper-debuginfo-10.10-1.2.osg33.el7\ngratia-probe-1.17.0-2.3.osg33.el7\ngratia-probe-bdii-status-1.17.0-2.3.osg33.el7\ngratia-probe-common-1.17.0-2.3.osg33.el7\ngratia-probe-condor-1.17.0-2.3.osg33.el7\ngratia-probe-condor-events-1.17.0-2.3.osg33.el7\ngratia-probe-dcache-storage-1.17.0-2.3.osg33.el7\ngratia-probe-dcache-storagegroup-1.17.0-2.3.osg33.el7\ngratia-probe-dcache-transfer-1.17.0-2.3.osg33.el7\ngratia-probe-debuginfo-1.17.0-2.3.osg33.el7\ngratia-probe-enstore-storage-1.17.0-2.3.osg33.el7\ngratia-probe-enstore-tapedrive-1.17.0-2.3.osg33.el7\ngratia-probe-enstore-transfer-1.17.0-2.3.osg33.el7\ngratia-probe-glexec-1.17.0-2.3.osg33.el7\ngratia-probe-glideinwms-1.17.0-2.3.osg33.el7\ngratia-probe-gram-1.17.0-2.3.osg33.el7\ngratia-probe-gridftp-transfer-1.17.0-2.3.osg33.el7\ngratia-probe-hadoop-storage-1.17.0-2.3.osg33.el7\ngratia-probe-htcondor-ce-1.17.0-2.3.osg33.el7\ngratia-probe-lsf-1.17.0-2.3.osg33.el7\ngratia-probe-metric-1.17.0-2.3.osg33.el7\ngratia-probe-onevm-1.17.0-2.3.osg33.el7\ngratia-probe-pbs-lsf-1.17.0-2.3.osg33.el7\ngratia-probe-services-1.17.0-2.3.osg33.el7\ngratia-probe-sge-1.17.0-2.3.osg33.el7\ngratia-probe-slurm-1.17.0-2.3.osg33.el7\ngratia-probe-xrootd-storage-1.17.0-2.3.osg33.el7\ngratia-probe-xrootd-transfer-1.17.0-2.3.osg33.el7\nhtcondor-ce-2.0.10-1.osg33.el7\nhtcondor-ce-bosco-2.0.10-1.osg33.el7\nhtcondor-ce-client-2.0.10-1.osg33.el7\nhtcondor-ce-collector-2.0.10-1.osg33.el7\nhtcondor-ce-condor-2.0.10-1.osg33.el7\nhtcondor-ce-lsf-2.0.10-1.osg33.el7\nhtcondor-ce-pbs-2.0.10-1.osg33.el7\nhtcondor-ce-sge-2.0.10-1.osg33.el7\nhtcondor-ce-view-2.0.10-1.osg33.el7\nosg-control-1.1.0-1.osg33.el7\nosg-gums-config-69-1.osg33.el7\nosg-oasis-7-5.osg33.el7\nosg-test-1.9.0-1.osg33.el7\nosg-tested-internal-3.3-15.osg33.el7\nosg-tested-internal-gram-3.3-15.osg33.el7\nosg-version-3.3.17-1.osg33.el7\nvo-client-69-1.osg33.el7\nvo-client-edgmkgridmap-69-1.osg33.el7\nxrootd-4.4.0-1.osg33.el7\nxrootd-client-4.4.0-1.osg33.el7\nxrootd-client-devel-4.4.0-1.osg33.el7\nxrootd-client-libs-4.4.0-1.osg33.el7\nxrootd-debuginfo-4.4.0-1.osg33.el7\nxrootd-devel-4.4.0-1.osg33.el7\nxrootd-doc-4.4.0-1.osg33.el7\nxrootd-fuse-4.4.0-1.osg33.el7\nxrootd-libs-4.4.0-1.osg33.el7\nxrootd-private-devel-4.4.0-1.osg33.el7\nxrootd-python-4.4.0-1.osg33.el7\nxrootd-selinux-4.4.0-1.osg33.el7\nxrootd-server-4.4.0-1.osg33.el7\nxrootd-server-devel-4.4.0-1.osg33.el7\nxrootd-server-libs-4.4.0-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-17/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-17/#enterprise-linux-6_2", 
            "text": "blahp-1.18.26.bosco-1.osgup.el6  condor-8.5.7-1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-17/#enterprise-linux-7_2", 
            "text": "blahp-1.18.26.bosco-1.osgup.el7  condor-8.5.7-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-17/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-ec2 condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-17/#enterprise-linux-6_3", 
            "text": "blahp-1.18.26.bosco-1.osgup.el6\nblahp-debuginfo-1.18.26.bosco-1.osgup.el6\ncondor-8.5.7-1.osgup.el6\ncondor-all-8.5.7-1.osgup.el6\ncondor-bosco-8.5.7-1.osgup.el6\ncondor-classads-8.5.7-1.osgup.el6\ncondor-classads-devel-8.5.7-1.osgup.el6\ncondor-cream-gahp-8.5.7-1.osgup.el6\ncondor-debuginfo-8.5.7-1.osgup.el6\ncondor-ec2-8.5.7-1.osgup.el6\ncondor-kbdd-8.5.7-1.osgup.el6\ncondor-procd-8.5.7-1.osgup.el6\ncondor-python-8.5.7-1.osgup.el6\ncondor-std-universe-8.5.7-1.osgup.el6\ncondor-test-8.5.7-1.osgup.el6\ncondor-vm-gahp-8.5.7-1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-17/#enterprise-linux-7_3", 
            "text": "blahp-1.18.26.bosco-1.osgup.el7\nblahp-debuginfo-1.18.26.bosco-1.osgup.el7\ncondor-8.5.7-1.osgup.el7\ncondor-all-8.5.7-1.osgup.el7\ncondor-bosco-8.5.7-1.osgup.el7\ncondor-classads-8.5.7-1.osgup.el7\ncondor-classads-devel-8.5.7-1.osgup.el7\ncondor-cream-gahp-8.5.7-1.osgup.el7\ncondor-debuginfo-8.5.7-1.osgup.el7\ncondor-ec2-8.5.7-1.osgup.el7\ncondor-kbdd-8.5.7-1.osgup.el7\ncondor-procd-8.5.7-1.osgup.el7\ncondor-python-8.5.7-1.osgup.el7\ncondor-test-8.5.7-1.osgup.el7\ncondor-vm-gahp-8.5.7-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-16/", 
            "text": "OSG Software Release 3.3.16\n\n\nRelease Date\n: 2016-09-13\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nUpdated most Globus Packages to latest available from EPEL\n\n\nNote: Now Globus Toolkit strictly checks host names against certificates\n\n\nThe MyProxy server now produces RFC compliant proxies\n\n\n\n\n\n\nBLAHP 1.18.25\n\n\nAdded the ability to set SGE parallel environment policy\n\n\nAdded multi-core support for PBS Pro\n\n\nAdded mutli-core, partition, and remote_cerequirement support for Slurm\n\n\n\n\n\n\nGlideinWMS 3.2.15\n\n\nFixed major scalability problem in GUMS on EL7\n\n\nHTCondor-CE 2.0.8\n\n\nAllow mapping of Terana eScience hostcerts (SOFTWARE-2433)\n\n\nForce 'condor_ce_q -allusers' until QUEUE_SUPER_USER is fixed to be able to use CERTIFICATE_MAPFILE in 8.5.6 (SOFTWARE-2412)\n\n\nFixed OnExitHold to be set to expressions rather than their evaluated forms\n\n\nEnsure lockdir and rundir exist with correct permissions on startup\n\n\nRemoved the HTCondor-CE init script on EL7 (SOFTWARE-2419)\n\n\n\n\n\n\nFixed load-balancing in Globus GridFTP when using IPv6 addresses\n\n\nAdded the HTCondor CREAM GAHP for EL7 platforms\n\n\nCompleted porting components of OSG Software Stack to EL7\n\n\nAdded \nRSV GlideinWMS Tester\n for VO Front-ends to test site support\n\n\nUpdated to \nVO Package v68\n: Added project8 VO\n\n\nosg-pki-tools 1.2.19\n\n\nReword 'bad VO info' error from osg-*cert-request\n\n\nFix formatting of CSRs\n\n\n\n\n\n\nUpdated xrootd-dsi RPM to direct administrators where to add configuration changes and not destroy those changes upon update.\n\n\nUpdated to lcas-lcmaps-gt4-interface to version 0.3.1: Added support for Globus 'sharing' service\n\n\nSimplified Gratia GridFTP probe by removing dependencies on gums-client\n\n\nAdded JSON interfaces in GUMS for VO mapping\n\n\nUpdated osg-configure package to require condor-python\n\n\nAdded support for user certificates to osg-ca-generator\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nKnown Issues\n\n\n\n\nOn EL7, your version of voms-proxy-init may come from the voms-clients-java package; this version will continue to make legacy proxies by default. If you want the new behavior, install the voms-clients-cpp package and uninstall the voms-clients-java package.\n\n\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.25.bosco-1.osg33.el6\n\n\ncondor-8.4.8-1.2.osg33.el6\n\n\nglideinwms-3.2.15-1.osg33.el6\n\n\nglite-ce-cream-client-api-c-1.15.4-2.2.osg33.el6\n\n\nglite-ce-wsdl-1.15.1-1.1.osg33.el6\n\n\nglite-lbjp-common-gsoap-plugin-3.2.12-1.1.osg33.el6\n\n\nglite-lbjp-common-gss-3.2.16-1.osg33.el6\n\n\nglobus-authz-3.12-1.osg33.el6\n\n\nglobus-authz-callout-error-3.5-2.osg33.el6\n\n\nglobus-callout-3.14-1.osg33.el6\n\n\nglobus-common-16.4-1.osg33.el6\n\n\nglobus-ftp-client-8.29-1.1.osg33.el6\n\n\nglobus-ftp-control-6.10-1.1.osg33.el6\n\n\nglobus-gass-cache-9.8-1.osg33.el6\n\n\nglobus-gass-cache-program-6.5-2.osg33.el6\n\n\nglobus-gass-copy-9.19-1.osg33.el6\n\n\nglobus-gass-server-ez-5.7-2.osg33.el6\n\n\nglobus-gass-transfer-8.9-1.osg33.el6\n\n\nglobus-gatekeeper-10.10-1.1.osg33.el6\n\n\nglobus-gfork-4.8-1.osg33.el6\n\n\nglobus-gram-audit-4.4-2.osg33.el6\n\n\nglobus-gram-client-13.13-1.osg33.el6\n\n\nglobus-gram-client-tools-11.8-1.osg33.el6\n\n\nglobus-gram-job-manager-14.27-3.1.osg33.el6\n\n\nglobus-gram-job-manager-callout-error-3.5-2.osg33.el6\n\n\nglobus-gram-job-manager-condor-2.5-2.1.osg33.el6\n\n\nglobus-gram-job-manager-fork-2.4-2.1.osg33.el6\n\n\nglobus-gram-job-manager-lsf-2.6-2.1.osg33.el6\n\n\nglobus-gram-job-manager-scripts-6.7-2.osg33.el6\n\n\nglobus-gram-protocol-12.12-3.osg33.el6\n\n\nglobus-gridftp-server-10.4-1.2.osg33.el6\n\n\nglobus-gridftp-server-control-4.1-1.2.osg33.el6\n\n\nglobus-gridmap-callout-error-2.4-2.osg33.el6\n\n\nglobus-gsi-callback-5.8-1.osg33.el6\n\n\nglobus-gsi-cert-utils-9.12-1.osg33.el6\n\n\nglobus-gsi-credential-7.9-1.osg33.el6\n\n\nglobus-gsi-openssl-error-3.5-2.osg33.el6\n\n\nglobus-gsi-proxy-core-7.9-1.osg33.el6\n\n\nglobus-gsi-proxy-ssl-5.8-1.osg33.el6\n\n\nglobus-gsi-sysconfig-6.9-1.osg33.el6\n\n\nglobus-gss-assist-10.15-1.osg33.el6\n\n\nglobus-gssapi-error-5.4-2.osg33.el6\n\n\nglobus-gssapi-gsi-12.1-1.osg33.el6\n\n\nglobus-io-11.5-1.osg33.el6\n\n\nglobus-openssl-module-4.6-2.osg33.el6\n\n\nglobus-proxy-utils-6.15-1.osg33.el6\n\n\nglobus-rsl-10.10-1.osg33.el6\n\n\nglobus-scheduler-event-generator-5.11-1.1.osg33.el6\n\n\nglobus-simple-ca-4.22-1.osg33.el6\n\n\nglobus-usage-4.4-2.osg33.el6\n\n\nglobus-xio-5.12-1.1.osg33.el6\n\n\nglobus-xio-pipe-driver-3.8-1.osg33.el6\n\n\nglobus-xio-popen-driver-3.5-2.osg33.el6\n\n\nglobus-xio-udt-driver-1.23-1.osg33.el6\n\n\nglobus-xioperf-4.4-2.osg33.el6\n\n\ngratia-probe-1.17.0-2.osg33.el6\n\n\ngums-1.5.2-9.osg33.el6\n\n\nhtcondor-ce-2.0.8-2.osg33.el6\n\n\nlcas-lcmaps-gt4-interface-0.3.1-1.2.osg33.el6\n\n\nmyproxy-6.1.18-1.1.osg33.el6\n\n\nosg-build-1.7.1-1.osg33.el6\n\n\nosg-ca-generator-1.2.0-1.osg33.el6\n\n\nosg-configure-1.4.2-2.osg33.el6\n\n\nosg-gridftp-3.3-3.osg33.el6\n\n\nosg-gridftp-hdfs-3.3-4.osg33.el6\n\n\nosg-gridftp-xrootd-3.3-3.osg33.el6\n\n\nosg-pki-tools-1.2.19-1.osg33.el6\n\n\nosg-test-1.8.4-1.osg33.el6\n\n\nosg-tested-internal-3.3-13.osg33.el6\n\n\nosg-version-3.3.16-1.osg33.el6\n\n\nrsv-gwms-tester-1.1.2-1.osg33.el6\n\n\nvo-client-68-2.osg33.el6\n\n\nxrootd-dsi-3.0.4-22.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.25.bosco-1.osg33.el7\n\n\ncondor-8.4.8-1.2.osg33.el7\n\n\nglideinwms-3.2.15-1.osg33.el7\n\n\nglite-ce-cream-client-api-c-1.15.4-2.2.osg33.el7\n\n\nglite-ce-wsdl-1.15.1-1.1.osg33.el7\n\n\nglite-lbjp-common-gsoap-plugin-3.2.12-1.1.osg33.el7\n\n\nglite-lbjp-common-gss-3.2.16-1.osg33.el7\n\n\nglobus-authz-3.12-1.osg33.el7\n\n\nglobus-authz-callout-error-3.5-2.osg33.el7\n\n\nglobus-callout-3.14-1.osg33.el7\n\n\nglobus-common-16.4-1.osg33.el7\n\n\nglobus-ftp-client-8.29-1.1.osg33.el7\n\n\nglobus-ftp-control-6.10-1.1.osg33.el7\n\n\nglobus-gass-cache-9.8-1.osg33.el7\n\n\nglobus-gass-cache-program-6.5-2.osg33.el7\n\n\nglobus-gass-copy-9.19-1.osg33.el7\n\n\nglobus-gass-server-ez-5.7-2.osg33.el7\n\n\nglobus-gass-transfer-8.9-1.osg33.el7\n\n\nglobus-gatekeeper-10.10-1.1.osg33.el7\n\n\nglobus-gfork-4.8-1.osg33.el7\n\n\nglobus-gram-audit-4.4-2.osg33.el7\n\n\nglobus-gram-client-13.13-1.osg33.el7\n\n\nglobus-gram-client-tools-11.8-1.osg33.el7\n\n\nglobus-gram-job-manager-14.27-3.1.osg33.el7\n\n\nglobus-gram-job-manager-callout-error-3.5-2.osg33.el7\n\n\nglobus-gram-job-manager-condor-2.5-2.1.osg33.el7\n\n\nglobus-gram-job-manager-fork-2.4-2.1.osg33.el7\n\n\nglobus-gram-job-manager-lsf-2.6-2.1.osg33.el7\n\n\nglobus-gram-job-manager-scripts-6.7-2.osg33.el7\n\n\nglobus-gram-protocol-12.12-3.osg33.el7\n\n\nglobus-gridftp-server-10.4-1.2.osg33.el7\n\n\nglobus-gridftp-server-control-4.1-1.2.osg33.el7\n\n\nglobus-gridmap-callout-error-2.4-2.osg33.el7\n\n\nglobus-gsi-callback-5.8-1.osg33.el7\n\n\nglobus-gsi-cert-utils-9.12-1.osg33.el7\n\n\nglobus-gsi-credential-7.9-1.osg33.el7\n\n\nglobus-gsi-openssl-error-3.5-2.osg33.el7\n\n\nglobus-gsi-proxy-core-7.9-1.osg33.el7\n\n\nglobus-gsi-proxy-ssl-5.8-1.osg33.el7\n\n\nglobus-gsi-sysconfig-6.9-1.osg33.el7\n\n\nglobus-gss-assist-10.15-1.osg33.el7\n\n\nglobus-gssapi-error-5.4-2.osg33.el7\n\n\nglobus-gssapi-gsi-12.1-1.osg33.el7\n\n\nglobus-io-11.5-1.osg33.el7\n\n\nglobus-openssl-module-4.6-2.osg33.el7\n\n\nglobus-proxy-utils-6.15-1.osg33.el7\n\n\nglobus-rsl-10.10-1.osg33.el7\n\n\nglobus-scheduler-event-generator-5.11-1.1.osg33.el7\n\n\nglobus-simple-ca-4.22-1.osg33.el7\n\n\nglobus-usage-4.4-2.osg33.el7\n\n\nglobus-xio-5.12-1.1.osg33.el7\n\n\nglobus-xio-pipe-driver-3.8-1.osg33.el7\n\n\nglobus-xio-popen-driver-3.5-2.osg33.el7\n\n\nglobus-xio-udt-driver-1.23-1.osg33.el7\n\n\nglobus-xioperf-4.4-2.osg33.el7\n\n\ngratia-probe-1.17.0-2.osg33.el7\n\n\ngums-1.5.2-9.osg33.el7\n\n\nhtcondor-ce-2.0.8-2.osg33.el7\n\n\nlcas-lcmaps-gt4-interface-0.3.1-1.2.osg33.el7\n\n\nmyproxy-6.1.18-1.1.osg33.el7\n\n\nosg-build-1.7.1-1.osg33.el7\n\n\nosg-ca-generator-1.2.0-1.osg33.el7\n\n\nosg-configure-1.4.2-2.osg33.el7\n\n\nosg-gridftp-3.3-3.osg33.el7\n\n\nosg-gridftp-hdfs-3.3-4.osg33.el7\n\n\nosg-gridftp-xrootd-3.3-3.osg33.el7\n\n\nosg-pki-tools-1.2.19-1.osg33.el7\n\n\nosg-test-1.8.4-1.osg33.el7\n\n\nosg-tested-internal-3.3-13.osg33.el7\n\n\nosg-version-3.3.16-1.osg33.el7\n\n\nrsv-gwms-tester-1.1.2-1.osg33.el7\n\n\nvo-client-68-2.osg33.el7\n\n\nxrootd-dsi-3.0.4-22.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp glideinwms-common-tools gl\nideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-s\ntandalone glite-ce-cream-client-api-c glite-ce-cream-client-devel glite-ce-wsdl glite-lbjp-common-gsoap-plugin glite-lbjp-common-gsoap-plugin-debuginfo glite-lbjp-common-gsoap-plugin-devel glite-lbjp-common-gss glite-lbjp-common-gss-debug\ninfo glite-lbjp-common-gss-devel globus-authz globus-authz-callout-error globus-authz-callout-error-debuginfo globus-authz-callout-error-devel globus-authz-callout-error-doc globus-authz-debuginfo globus-authz-devel globus-authz-doc globu\ns-callout globus-callout-debuginfo globus-callout-devel globus-callout-doc globus-common globus-common-debuginfo globus-common-devel globus-common-doc globus-common-progs globus-ftp-client globus-ftp-client-debuginfo globus-ftp-client-dev\nel globus-ftp-client-doc globus-ftp-control globus-ftp-control-debuginfo globus-ftp-control-devel globus-ftp-control-doc globus-gass-cache globus-gass-cache-debuginfo globus-gass-cache-devel globus-gass-cache-doc globus-gass-cache-program\n globus-gass-cache-program-debuginfo globus-gass-copy globus-gass-copy-debuginfo globus-gass-copy-devel globus-gass-copy-doc globus-gass-copy-progs globus-gass-server-ez globus-gass-server-ez-debuginfo globus-gass-server-ez-devel globus-g\nass-server-ez-progs globus-gass-transfer globus-gass-transfer-debuginfo globus-gass-transfer-devel globus-gass-transfer-doc globus-gatekeeper globus-gatekeeper-debuginfo globus-gfork globus-gfork-debuginfo globus-gfork-devel globus-gfork-\nprogs globus-gram-audit globus-gram-client globus-gram-client-debuginfo globus-gram-client-devel globus-gram-client-doc globus-gram-client-tools globus-gram-client-tools-debuginfo globus-gram-job-manager globus-gram-job-manager-callout-er\nror globus-gram-job-manager-callout-error-debuginfo globus-gram-job-manager-callout-error-devel globus-gram-job-manager-callout-error-doc globus-gram-job-manager-condor globus-gram-job-manager-debuginfo globus-gram-job-manager-fork globus\n-gram-job-manager-fork-debuginfo globus-gram-job-manager-fork-setup-poll globus-gram-job-manager-fork-setup-seg globus-gram-job-manager-lsf globus-gram-job-manager-lsf-debuginfo globus-gram-job-manager-lsf-setup-poll globus-gram-job-manag\ner-lsf-setup-seg globus-gram-job-manager-scripts globus-gram-job-manager-scripts-doc globus-gram-protocol globus-gram-protocol-debuginfo globus-gram-protocol-devel globus-gram-protocol-doc globus-gridftp-server globus-gridftp-server-contr\nol globus-gridftp-server-control-debuginfo globus-gridftp-server-control-devel globus-gridftp-server-debuginfo globus-gridftp-server-devel globus-gridftp-server-progs globus-gridmap-callout-error globus-gridmap-callout-error-debuginfo glo\nbus-gridmap-callout-error-devel globus-gridmap-callout-error-doc globus-gsi-callback globus-gsi-callback-debuginfo globus-gsi-callback-devel globus-gsi-callback-doc globus-gsi-cert-utils globus-gsi-cert-utils-debuginfo globus-gsi-cert-uti\nls-devel globus-gsi-cert-utils-doc globus-gsi-cert-utils-progs globus-gsi-credential globus-gsi-credential-debuginfo globus-gsi-credential-devel globus-gsi-credential-doc globus-gsi-openssl-error globus-gsi-openssl-error-debuginfo globus-\ngsi-openssl-error-devel globus-gsi-openssl-error-doc globus-gsi-proxy-core globus-gsi-proxy-core-debuginfo globus-gsi-proxy-core-devel globus-gsi-proxy-core-doc globus-gsi-proxy-ssl globus-gsi-proxy-ssl-debuginfo globus-gsi-proxy-ssl-deve\nl globus-gsi-proxy-ssl-doc globus-gsi-sysconfig globus-gsi-sysconfig-debuginfo globus-gsi-sysconfig-devel globus-gsi-sysconfig-doc globus-gssapi-error globus-gssapi-error-debuginfo globus-gssapi-error-devel globus-gssapi-error-doc globus-\ngssapi-gsi globus-gssapi-gsi-debuginfo globus-gssapi-gsi-devel globus-gssapi-gsi-doc globus-gss-assist globus-gss-assist-debuginfo globus-gss-assist-devel globus-gss-assist-doc globus-gss-assist-progs globus-io globus-io-debuginfo globus-\nio-devel globus-openssl-module globus-openssl-module-debuginfo globus-openssl-module-devel globus-openssl-module-doc globus-proxy-utils globus-proxy-utils-debuginfo globus-rsl globus-rsl-debuginfo globus-rsl-devel globus-rsl-doc globus-sc\nheduler-event-generator globus-scheduler-event-generator-debuginfo globus-scheduler-event-generator-devel globus-scheduler-event-generator-doc globus-scheduler-event-generator-progs globus-simple-ca globus-usage globus-usage-debuginfo glo\nbus-usage-devel globus-xio globus-xio-debuginfo globus-xio-devel globus-xio-doc globus-xioperf globus-xioperf-debuginfo globus-xio-pipe-driver globus-xio-pipe-driver-debuginfo globus-xio-pipe-driver-devel globus-xio-popen-driver globus-xi\no-popen-driver-debuginfo globus-xio-popen-driver-devel globus-xio-udt-driver globus-xio-udt-driver-debuginfo globus-xio-udt-driver-devel gratia-probe-bdii-status gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-pr\nobe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glexec gratia-probe-glideinwms \ngratia-probe-gram gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gr\natia-probe-xrootd-storage gratia-probe-xrootd-transfer gums gums-client gums-service htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-vie\nw lcas-lcmaps-gt4-interface lcas-lcmaps-gt4-interface-debuginfo myproxy myproxy-admin myproxy-debuginfo myproxy-devel myproxy-doc myproxy-libs myproxy-server myproxy-voms osg-build osg-ca-generator osg-configure osg-configure-bosco osg-co\nnfigure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configur\ne-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-gridftp osg-gridftp-hdfs osg-gridftp-xrootd osg-gums-config osg-pki-tools osg-pki-tools-tests osg-test osg-tes\nted-internal osg-tested-internal-gram osg-version rsv-gwms-tester vo-client vo-client-edgmkgridmap xrootd-dsi xrootd-dsi-debuginfo\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp-1.18.25.bosco-1.osg33.el6\nblahp-debuginfo-1.18.25.bosco-1.osg33.el6\ncondor-8.4.8-1.2.osg33.el6\ncondor-all-8.4.8-1.2.osg33.el6\ncondor-bosco-8.4.8-1.2.osg33.el6\ncondor-classads-8.4.8-1.2.osg33.el6\ncondor-classads-devel-8.4.8-1.2.osg33.el6\ncondor-cream-gahp-8.4.8-1.2.osg33.el6\ncondor-debuginfo-8.4.8-1.2.osg33.el6\ncondor-kbdd-8.4.8-1.2.osg33.el6\ncondor-procd-8.4.8-1.2.osg33.el6\ncondor-python-8.4.8-1.2.osg33.el6\ncondor-std-universe-8.4.8-1.2.osg33.el6\ncondor-test-8.4.8-1.2.osg33.el6\ncondor-vm-gahp-8.4.8-1.2.osg33.el6\nglideinwms-3.2.15-1.osg33.el6\nglideinwms-common-tools-3.2.15-1.osg33.el6\nglideinwms-condor-common-config-3.2.15-1.osg33.el6\nglideinwms-factory-3.2.15-1.osg33.el6\nglideinwms-factory-condor-3.2.15-1.osg33.el6\nglideinwms-glidecondor-tools-3.2.15-1.osg33.el6\nglideinwms-libs-3.2.15-1.osg33.el6\nglideinwms-minimal-condor-3.2.15-1.osg33.el6\nglideinwms-usercollector-3.2.15-1.osg33.el6\nglideinwms-userschedd-3.2.15-1.osg33.el6\nglideinwms-vofrontend-3.2.15-1.osg33.el6\nglideinwms-vofrontend-standalone-3.2.15-1.osg33.el6\nglite-ce-cream-client-api-c-1.15.4-2.2.osg33.el6\nglite-ce-cream-client-devel-1.15.4-2.2.osg33.el6\nglite-ce-wsdl-1.15.1-1.1.osg33.el6\nglite-lbjp-common-gsoap-plugin-3.2.12-1.1.osg33.el6\nglite-lbjp-common-gsoap-plugin-debuginfo-3.2.12-1.1.osg33.el6\nglite-lbjp-common-gsoap-plugin-devel-3.2.12-1.1.osg33.el6\nglite-lbjp-common-gss-3.2.16-1.osg33.el6\nglite-lbjp-common-gss-debuginfo-3.2.16-1.osg33.el6\nglite-lbjp-common-gss-devel-3.2.16-1.osg33.el6\nglobus-authz-3.12-1.osg33.el6\nglobus-authz-callout-error-3.5-2.osg33.el6\nglobus-authz-callout-error-debuginfo-3.5-2.osg33.el6\nglobus-authz-callout-error-devel-3.5-2.osg33.el6\nglobus-authz-callout-error-doc-3.5-2.osg33.el6\nglobus-authz-debuginfo-3.12-1.osg33.el6\nglobus-authz-devel-3.12-1.osg33.el6\nglobus-authz-doc-3.12-1.osg33.el6\nglobus-callout-3.14-1.osg33.el6\nglobus-callout-debuginfo-3.14-1.osg33.el6\nglobus-callout-devel-3.14-1.osg33.el6\nglobus-callout-doc-3.14-1.osg33.el6\nglobus-common-16.4-1.osg33.el6\nglobus-common-debuginfo-16.4-1.osg33.el6\nglobus-common-devel-16.4-1.osg33.el6\nglobus-common-doc-16.4-1.osg33.el6\nglobus-common-progs-16.4-1.osg33.el6\nglobus-ftp-client-8.29-1.1.osg33.el6\nglobus-ftp-client-debuginfo-8.29-1.1.osg33.el6\nglobus-ftp-client-devel-8.29-1.1.osg33.el6\nglobus-ftp-client-doc-8.29-1.1.osg33.el6\nglobus-ftp-control-6.10-1.1.osg33.el6\nglobus-ftp-control-debuginfo-6.10-1.1.osg33.el6\nglobus-ftp-control-devel-6.10-1.1.osg33.el6\nglobus-ftp-control-doc-6.10-1.1.osg33.el6\nglobus-gass-cache-9.8-1.osg33.el6\nglobus-gass-cache-debuginfo-9.8-1.osg33.el6\nglobus-gass-cache-devel-9.8-1.osg33.el6\nglobus-gass-cache-doc-9.8-1.osg33.el6\nglobus-gass-cache-program-6.5-2.osg33.el6\nglobus-gass-cache-program-debuginfo-6.5-2.osg33.el6\nglobus-gass-copy-9.19-1.osg33.el6\nglobus-gass-copy-debuginfo-9.19-1.osg33.el6\nglobus-gass-copy-devel-9.19-1.osg33.el6\nglobus-gass-copy-doc-9.19-1.osg33.el6\nglobus-gass-copy-progs-9.19-1.osg33.el6\nglobus-gass-server-ez-5.7-2.osg33.el6\nglobus-gass-server-ez-debuginfo-5.7-2.osg33.el6\nglobus-gass-server-ez-devel-5.7-2.osg33.el6\nglobus-gass-server-ez-progs-5.7-2.osg33.el6\nglobus-gass-transfer-8.9-1.osg33.el6\nglobus-gass-transfer-debuginfo-8.9-1.osg33.el6\nglobus-gass-transfer-devel-8.9-1.osg33.el6\nglobus-gass-transfer-doc-8.9-1.osg33.el6\nglobus-gatekeeper-10.10-1.1.osg33.el6\nglobus-gatekeeper-debuginfo-10.10-1.1.osg33.el6\nglobus-gfork-4.8-1.osg33.el6\nglobus-gfork-debuginfo-4.8-1.osg33.el6\nglobus-gfork-devel-4.8-1.osg33.el6\nglobus-gfork-progs-4.8-1.osg33.el6\nglobus-gram-audit-4.4-2.osg33.el6\nglobus-gram-client-13.13-1.osg33.el6\nglobus-gram-client-debuginfo-13.13-1.osg33.el6\nglobus-gram-client-devel-13.13-1.osg33.el6\nglobus-gram-client-doc-13.13-1.osg33.el6\nglobus-gram-client-tools-11.8-1.osg33.el6\nglobus-gram-client-tools-debuginfo-11.8-1.osg33.el6\nglobus-gram-job-manager-14.27-3.1.osg33.el6\nglobus-gram-job-manager-callout-error-3.5-2.osg33.el6\nglobus-gram-job-manager-callout-error-debuginfo-3.5-2.osg33.el6\nglobus-gram-job-manager-callout-error-devel-3.5-2.osg33.el6\nglobus-gram-job-manager-callout-error-doc-3.5-2.osg33.el6\nglobus-gram-job-manager-condor-2.5-2.1.osg33.el6\nglobus-gram-job-manager-debuginfo-14.27-3.1.osg33.el6\nglobus-gram-job-manager-fork-2.4-2.1.osg33.el6\nglobus-gram-job-manager-fork-debuginfo-2.4-2.1.osg33.el6\nglobus-gram-job-manager-fork-setup-poll-2.4-2.1.osg33.el6\nglobus-gram-job-manager-fork-setup-seg-2.4-2.1.osg33.el6\nglobus-gram-job-manager-lsf-2.6-2.1.osg33.el6\nglobus-gram-job-manager-lsf-debuginfo-2.6-2.1.osg33.el6\nglobus-gram-job-manager-lsf-setup-poll-2.6-2.1.osg33.el6\nglobus-gram-job-manager-lsf-setup-seg-2.6-2.1.osg33.el6\nglobus-gram-job-manager-scripts-6.7-2.osg33.el6\nglobus-gram-job-manager-scripts-doc-6.7-2.osg33.el6\nglobus-gram-protocol-12.12-3.osg33.el6\nglobus-gram-protocol-debuginfo-12.12-3.osg33.el6\nglobus-gram-protocol-devel-12.12-3.osg33.el6\nglobus-gram-protocol-doc-12.12-3.osg33.el6\nglobus-gridftp-server-10.4-1.2.osg33.el6\nglobus-gridftp-server-control-4.1-1.2.osg33.el6\nglobus-gridftp-server-control-debuginfo-4.1-1.2.osg33.el6\nglobus-gridftp-server-control-devel-4.1-1.2.osg33.el6\nglobus-gridftp-server-debuginfo-10.4-1.2.osg33.el6\nglobus-gridftp-server-devel-10.4-1.2.osg33.el6\nglobus-gridftp-server-progs-10.4-1.2.osg33.el6\nglobus-gridmap-callout-error-2.4-2.osg33.el6\nglobus-gridmap-callout-error-debuginfo-2.4-2.osg33.el6\nglobus-gridmap-callout-error-devel-2.4-2.osg33.el6\nglobus-gridmap-callout-error-doc-2.4-2.osg33.el6\nglobus-gsi-callback-5.8-1.osg33.el6\nglobus-gsi-callback-debuginfo-5.8-1.osg33.el6\nglobus-gsi-callback-devel-5.8-1.osg33.el6\nglobus-gsi-callback-doc-5.8-1.osg33.el6\nglobus-gsi-cert-utils-9.12-1.osg33.el6\nglobus-gsi-cert-utils-debuginfo-9.12-1.osg33.el6\nglobus-gsi-cert-utils-devel-9.12-1.osg33.el6\nglobus-gsi-cert-utils-doc-9.12-1.osg33.el6\nglobus-gsi-cert-utils-progs-9.12-1.osg33.el6\nglobus-gsi-credential-7.9-1.osg33.el6\nglobus-gsi-credential-debuginfo-7.9-1.osg33.el6\nglobus-gsi-credential-devel-7.9-1.osg33.el6\nglobus-gsi-credential-doc-7.9-1.osg33.el6\nglobus-gsi-openssl-error-3.5-2.osg33.el6\nglobus-gsi-openssl-error-debuginfo-3.5-2.osg33.el6\nglobus-gsi-openssl-error-devel-3.5-2.osg33.el6\nglobus-gsi-openssl-error-doc-3.5-2.osg33.el6\nglobus-gsi-proxy-core-7.9-1.osg33.el6\nglobus-gsi-proxy-core-debuginfo-7.9-1.osg33.el6\nglobus-gsi-proxy-core-devel-7.9-1.osg33.el6\nglobus-gsi-proxy-core-doc-7.9-1.osg33.el6\nglobus-gsi-proxy-ssl-5.8-1.osg33.el6\nglobus-gsi-proxy-ssl-debuginfo-5.8-1.osg33.el6\nglobus-gsi-proxy-ssl-devel-5.8-1.osg33.el6\nglobus-gsi-proxy-ssl-doc-5.8-1.osg33.el6\nglobus-gsi-sysconfig-6.9-1.osg33.el6\nglobus-gsi-sysconfig-debuginfo-6.9-1.osg33.el6\nglobus-gsi-sysconfig-devel-6.9-1.osg33.el6\nglobus-gsi-sysconfig-doc-6.9-1.osg33.el6\nglobus-gssapi-error-5.4-2.osg33.el6\nglobus-gssapi-error-debuginfo-5.4-2.osg33.el6\nglobus-gssapi-error-devel-5.4-2.osg33.el6\nglobus-gssapi-error-doc-5.4-2.osg33.el6\nglobus-gssapi-gsi-12.1-1.osg33.el6\nglobus-gssapi-gsi-debuginfo-12.1-1.osg33.el6\nglobus-gssapi-gsi-devel-12.1-1.osg33.el6\nglobus-gssapi-gsi-doc-12.1-1.osg33.el6\nglobus-gss-assist-10.15-1.osg33.el6\nglobus-gss-assist-debuginfo-10.15-1.osg33.el6\nglobus-gss-assist-devel-10.15-1.osg33.el6\nglobus-gss-assist-doc-10.15-1.osg33.el6\nglobus-gss-assist-progs-10.15-1.osg33.el6\nglobus-io-11.5-1.osg33.el6\nglobus-io-debuginfo-11.5-1.osg33.el6\nglobus-io-devel-11.5-1.osg33.el6\nglobus-openssl-module-4.6-2.osg33.el6\nglobus-openssl-module-debuginfo-4.6-2.osg33.el6\nglobus-openssl-module-devel-4.6-2.osg33.el6\nglobus-openssl-module-doc-4.6-2.osg33.el6\nglobus-proxy-utils-6.15-1.osg33.el6\nglobus-proxy-utils-debuginfo-6.15-1.osg33.el6\nglobus-rsl-10.10-1.osg33.el6\nglobus-rsl-debuginfo-10.10-1.osg33.el6\nglobus-rsl-devel-10.10-1.osg33.el6\nglobus-rsl-doc-10.10-1.osg33.el6\nglobus-scheduler-event-generator-5.11-1.1.osg33.el6\nglobus-scheduler-event-generator-debuginfo-5.11-1.1.osg33.el6\nglobus-scheduler-event-generator-devel-5.11-1.1.osg33.el6\nglobus-scheduler-event-generator-doc-5.11-1.1.osg33.el6\nglobus-scheduler-event-generator-progs-5.11-1.1.osg33.el6\nglobus-simple-ca-4.22-1.osg33.el6\nglobus-usage-4.4-2.osg33.el6\nglobus-usage-debuginfo-4.4-2.osg33.el6\nglobus-usage-devel-4.4-2.osg33.el6\nglobus-xio-5.12-1.1.osg33.el6\nglobus-xio-debuginfo-5.12-1.1.osg33.el6\nglobus-xio-devel-5.12-1.1.osg33.el6\nglobus-xio-doc-5.12-1.1.osg33.el6\nglobus-xioperf-4.4-2.osg33.el6\nglobus-xioperf-debuginfo-4.4-2.osg33.el6\nglobus-xio-pipe-driver-3.8-1.osg33.el6\nglobus-xio-pipe-driver-debuginfo-3.8-1.osg33.el6\nglobus-xio-pipe-driver-devel-3.8-1.osg33.el6\nglobus-xio-popen-driver-3.5-2.osg33.el6\nglobus-xio-popen-driver-debuginfo-3.5-2.osg33.el6\nglobus-xio-popen-driver-devel-3.5-2.osg33.el6\nglobus-xio-udt-driver-1.23-1.osg33.el6\nglobus-xio-udt-driver-debuginfo-1.23-1.osg33.el6\nglobus-xio-udt-driver-devel-1.23-1.osg33.el6\ngratia-probe-1.17.0-2.osg33.el6\ngratia-probe-bdii-status-1.17.0-2.osg33.el6\ngratia-probe-common-1.17.0-2.osg33.el6\ngratia-probe-condor-1.17.0-2.osg33.el6\ngratia-probe-condor-events-1.17.0-2.osg33.el6\ngratia-probe-dcache-storage-1.17.0-2.osg33.el6\ngratia-probe-dcache-storagegroup-1.17.0-2.osg33.el6\ngratia-probe-dcache-transfer-1.17.0-2.osg33.el6\ngratia-probe-debuginfo-1.17.0-2.osg33.el6\ngratia-probe-enstore-storage-1.17.0-2.osg33.el6\ngratia-probe-enstore-tapedrive-1.17.0-2.osg33.el6\ngratia-probe-enstore-transfer-1.17.0-2.osg33.el6\ngratia-probe-glexec-1.17.0-2.osg33.el6\ngratia-probe-glideinwms-1.17.0-2.osg33.el6\ngratia-probe-gram-1.17.0-2.osg33.el6\ngratia-probe-gridftp-transfer-1.17.0-2.osg33.el6\ngratia-probe-hadoop-storage-1.17.0-2.osg33.el6\ngratia-probe-htcondor-ce-1.17.0-2.osg33.el6\ngratia-probe-lsf-1.17.0-2.osg33.el6\ngratia-probe-metric-1.17.0-2.osg33.el6\ngratia-probe-onevm-1.17.0-2.osg33.el6\ngratia-probe-pbs-lsf-1.17.0-2.osg33.el6\ngratia-probe-services-1.17.0-2.osg33.el6\ngratia-probe-sge-1.17.0-2.osg33.el6\ngratia-probe-slurm-1.17.0-2.osg33.el6\ngratia-probe-xrootd-storage-1.17.0-2.osg33.el6\ngratia-probe-xrootd-transfer-1.17.0-2.osg33.el6\ngums-1.5.2-9.osg33.el6\ngums-client-1.5.2-9.osg33.el6\ngums-service-1.5.2-9.osg33.el6\nhtcondor-ce-2.0.8-2.osg33.el6\nhtcondor-ce-bosco-2.0.8-2.osg33.el6\nhtcondor-ce-client-2.0.8-2.osg33.el6\nhtcondor-ce-collector-2.0.8-2.osg33.el6\nhtcondor-ce-condor-2.0.8-2.osg33.el6\nhtcondor-ce-lsf-2.0.8-2.osg33.el6\nhtcondor-ce-pbs-2.0.8-2.osg33.el6\nhtcondor-ce-sge-2.0.8-2.osg33.el6\nhtcondor-ce-view-2.0.8-2.osg33.el6\nlcas-lcmaps-gt4-interface-0.3.1-1.2.osg33.el6\nlcas-lcmaps-gt4-interface-debuginfo-0.3.1-1.2.osg33.el6\nmyproxy-6.1.18-1.1.osg33.el6\nmyproxy-admin-6.1.18-1.1.osg33.el6\nmyproxy-debuginfo-6.1.18-1.1.osg33.el6\nmyproxy-devel-6.1.18-1.1.osg33.el6\nmyproxy-doc-6.1.18-1.1.osg33.el6\nmyproxy-libs-6.1.18-1.1.osg33.el6\nmyproxy-server-6.1.18-1.1.osg33.el6\nmyproxy-voms-6.1.18-1.1.osg33.el6\nosg-build-1.7.1-1.osg33.el6\nosg-ca-generator-1.2.0-1.osg33.el6\nosg-configure-1.4.2-2.osg33.el6\nosg-configure-bosco-1.4.2-2.osg33.el6\nosg-configure-ce-1.4.2-2.osg33.el6\nosg-configure-cemon-1.4.2-2.osg33.el6\nosg-configure-condor-1.4.2-2.osg33.el6\nosg-configure-gateway-1.4.2-2.osg33.el6\nosg-configure-gip-1.4.2-2.osg33.el6\nosg-configure-gratia-1.4.2-2.osg33.el6\nosg-configure-infoservices-1.4.2-2.osg33.el6\nosg-configure-lsf-1.4.2-2.osg33.el6\nosg-configure-managedfork-1.4.2-2.osg33.el6\nosg-configure-misc-1.4.2-2.osg33.el6\nosg-configure-monalisa-1.4.2-2.osg33.el6\nosg-configure-network-1.4.2-2.osg33.el6\nosg-configure-pbs-1.4.2-2.osg33.el6\nosg-configure-rsv-1.4.2-2.osg33.el6\nosg-configure-sge-1.4.2-2.osg33.el6\nosg-configure-slurm-1.4.2-2.osg33.el6\nosg-configure-squid-1.4.2-2.osg33.el6\nosg-configure-tests-1.4.2-2.osg33.el6\nosg-gridftp-3.3-3.osg33.el6\nosg-gridftp-hdfs-3.3-4.osg33.el6\nosg-gridftp-xrootd-3.3-3.osg33.el6\nosg-gums-config-68-2.osg33.el6\nosg-pki-tools-1.2.19-1.osg33.el6\nosg-pki-tools-tests-1.2.19-1.osg33.el6\nosg-test-1.8.4-1.osg33.el6\nosg-tested-internal-3.3-13.osg33.el6\nosg-tested-internal-gram-3.3-13.osg33.el6\nosg-version-3.3.16-1.osg33.el6\nrsv-gwms-tester-1.1.2-1.osg33.el6\nvo-client-68-2.osg33.el6\nvo-client-edgmkgridmap-68-2.osg33.el6\nxrootd-dsi-3.0.4-22.osg33.el6\nxrootd-dsi-debuginfo-3.0.4-22.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp-1.18.25.bosco-1.osg33.el7\nblahp-debuginfo-1.18.25.bosco-1.osg33.el7\ncondor-8.4.8-1.2.osg33.el7\ncondor-all-8.4.8-1.2.osg33.el7\ncondor-bosco-8.4.8-1.2.osg33.el7\ncondor-classads-8.4.8-1.2.osg33.el7\ncondor-classads-devel-8.4.8-1.2.osg33.el7\ncondor-cream-gahp-8.4.8-1.2.osg33.el7\ncondor-debuginfo-8.4.8-1.2.osg33.el7\ncondor-kbdd-8.4.8-1.2.osg33.el7\ncondor-procd-8.4.8-1.2.osg33.el7\ncondor-python-8.4.8-1.2.osg33.el7\ncondor-test-8.4.8-1.2.osg33.el7\ncondor-vm-gahp-8.4.8-1.2.osg33.el7\nglideinwms-3.2.15-1.osg33.el7\nglideinwms-common-tools-3.2.15-1.osg33.el7\nglideinwms-condor-common-config-3.2.15-1.osg33.el7\nglideinwms-factory-3.2.15-1.osg33.el7\nglideinwms-factory-condor-3.2.15-1.osg33.el7\nglideinwms-glidecondor-tools-3.2.15-1.osg33.el7\nglideinwms-libs-3.2.15-1.osg33.el7\nglideinwms-minimal-condor-3.2.15-1.osg33.el7\nglideinwms-usercollector-3.2.15-1.osg33.el7\nglideinwms-userschedd-3.2.15-1.osg33.el7\nglideinwms-vofrontend-3.2.15-1.osg33.el7\nglideinwms-vofrontend-standalone-3.2.15-1.osg33.el7\nglite-ce-cream-client-api-c-1.15.4-2.2.osg33.el7\nglite-ce-cream-client-devel-1.15.4-2.2.osg33.el7\nglite-ce-wsdl-1.15.1-1.1.osg33.el7\nglite-lbjp-common-gsoap-plugin-3.2.12-1.1.osg33.el7\nglite-lbjp-common-gsoap-plugin-debuginfo-3.2.12-1.1.osg33.el7\nglite-lbjp-common-gsoap-plugin-devel-3.2.12-1.1.osg33.el7\nglite-lbjp-common-gss-3.2.16-1.osg33.el7\nglite-lbjp-common-gss-debuginfo-3.2.16-1.osg33.el7\nglite-lbjp-common-gss-devel-3.2.16-1.osg33.el7\nglobus-authz-3.12-1.osg33.el7\nglobus-authz-callout-error-3.5-2.osg33.el7\nglobus-authz-callout-error-debuginfo-3.5-2.osg33.el7\nglobus-authz-callout-error-devel-3.5-2.osg33.el7\nglobus-authz-callout-error-doc-3.5-2.osg33.el7\nglobus-authz-debuginfo-3.12-1.osg33.el7\nglobus-authz-devel-3.12-1.osg33.el7\nglobus-authz-doc-3.12-1.osg33.el7\nglobus-callout-3.14-1.osg33.el7\nglobus-callout-debuginfo-3.14-1.osg33.el7\nglobus-callout-devel-3.14-1.osg33.el7\nglobus-callout-doc-3.14-1.osg33.el7\nglobus-common-16.4-1.osg33.el7\nglobus-common-debuginfo-16.4-1.osg33.el7\nglobus-common-devel-16.4-1.osg33.el7\nglobus-common-doc-16.4-1.osg33.el7\nglobus-common-progs-16.4-1.osg33.el7\nglobus-ftp-client-8.29-1.1.osg33.el7\nglobus-ftp-client-debuginfo-8.29-1.1.osg33.el7\nglobus-ftp-client-devel-8.29-1.1.osg33.el7\nglobus-ftp-client-doc-8.29-1.1.osg33.el7\nglobus-ftp-control-6.10-1.1.osg33.el7\nglobus-ftp-control-debuginfo-6.10-1.1.osg33.el7\nglobus-ftp-control-devel-6.10-1.1.osg33.el7\nglobus-ftp-control-doc-6.10-1.1.osg33.el7\nglobus-gass-cache-9.8-1.osg33.el7\nglobus-gass-cache-debuginfo-9.8-1.osg33.el7\nglobus-gass-cache-devel-9.8-1.osg33.el7\nglobus-gass-cache-doc-9.8-1.osg33.el7\nglobus-gass-cache-program-6.5-2.osg33.el7\nglobus-gass-cache-program-debuginfo-6.5-2.osg33.el7\nglobus-gass-copy-9.19-1.osg33.el7\nglobus-gass-copy-debuginfo-9.19-1.osg33.el7\nglobus-gass-copy-devel-9.19-1.osg33.el7\nglobus-gass-copy-doc-9.19-1.osg33.el7\nglobus-gass-copy-progs-9.19-1.osg33.el7\nglobus-gass-server-ez-5.7-2.osg33.el7\nglobus-gass-server-ez-debuginfo-5.7-2.osg33.el7\nglobus-gass-server-ez-devel-5.7-2.osg33.el7\nglobus-gass-server-ez-progs-5.7-2.osg33.el7\nglobus-gass-transfer-8.9-1.osg33.el7\nglobus-gass-transfer-debuginfo-8.9-1.osg33.el7\nglobus-gass-transfer-devel-8.9-1.osg33.el7\nglobus-gass-transfer-doc-8.9-1.osg33.el7\nglobus-gatekeeper-10.10-1.1.osg33.el7\nglobus-gatekeeper-debuginfo-10.10-1.1.osg33.el7\nglobus-gfork-4.8-1.osg33.el7\nglobus-gfork-debuginfo-4.8-1.osg33.el7\nglobus-gfork-devel-4.8-1.osg33.el7\nglobus-gfork-progs-4.8-1.osg33.el7\nglobus-gram-audit-4.4-2.osg33.el7\nglobus-gram-client-13.13-1.osg33.el7\nglobus-gram-client-debuginfo-13.13-1.osg33.el7\nglobus-gram-client-devel-13.13-1.osg33.el7\nglobus-gram-client-doc-13.13-1.osg33.el7\nglobus-gram-client-tools-11.8-1.osg33.el7\nglobus-gram-client-tools-debuginfo-11.8-1.osg33.el7\nglobus-gram-job-manager-14.27-3.1.osg33.el7\nglobus-gram-job-manager-callout-error-3.5-2.osg33.el7\nglobus-gram-job-manager-callout-error-debuginfo-3.5-2.osg33.el7\nglobus-gram-job-manager-callout-error-devel-3.5-2.osg33.el7\nglobus-gram-job-manager-callout-error-doc-3.5-2.osg33.el7\nglobus-gram-job-manager-condor-2.5-2.1.osg33.el7\nglobus-gram-job-manager-debuginfo-14.27-3.1.osg33.el7\nglobus-gram-job-manager-fork-2.4-2.1.osg33.el7\nglobus-gram-job-manager-fork-debuginfo-2.4-2.1.osg33.el7\nglobus-gram-job-manager-fork-setup-poll-2.4-2.1.osg33.el7\nglobus-gram-job-manager-fork-setup-seg-2.4-2.1.osg33.el7\nglobus-gram-job-manager-lsf-2.6-2.1.osg33.el7\nglobus-gram-job-manager-lsf-debuginfo-2.6-2.1.osg33.el7\nglobus-gram-job-manager-lsf-setup-poll-2.6-2.1.osg33.el7\nglobus-gram-job-manager-lsf-setup-seg-2.6-2.1.osg33.el7\nglobus-gram-job-manager-scripts-6.7-2.osg33.el7\nglobus-gram-job-manager-scripts-doc-6.7-2.osg33.el7\nglobus-gram-protocol-12.12-3.osg33.el7\nglobus-gram-protocol-debuginfo-12.12-3.osg33.el7\nglobus-gram-protocol-devel-12.12-3.osg33.el7\nglobus-gram-protocol-doc-12.12-3.osg33.el7\nglobus-gridftp-server-10.4-1.2.osg33.el7\nglobus-gridftp-server-control-4.1-1.2.osg33.el7\nglobus-gridftp-server-control-debuginfo-4.1-1.2.osg33.el7\nglobus-gridftp-server-control-devel-4.1-1.2.osg33.el7\nglobus-gridftp-server-debuginfo-10.4-1.2.osg33.el7\nglobus-gridftp-server-devel-10.4-1.2.osg33.el7\nglobus-gridftp-server-progs-10.4-1.2.osg33.el7\nglobus-gridmap-callout-error-2.4-2.osg33.el7\nglobus-gridmap-callout-error-debuginfo-2.4-2.osg33.el7\nglobus-gridmap-callout-error-devel-2.4-2.osg33.el7\nglobus-gridmap-callout-error-doc-2.4-2.osg33.el7\nglobus-gsi-callback-5.8-1.osg33.el7\nglobus-gsi-callback-debuginfo-5.8-1.osg33.el7\nglobus-gsi-callback-devel-5.8-1.osg33.el7\nglobus-gsi-callback-doc-5.8-1.osg33.el7\nglobus-gsi-cert-utils-9.12-1.osg33.el7\nglobus-gsi-cert-utils-debuginfo-9.12-1.osg33.el7\nglobus-gsi-cert-utils-devel-9.12-1.osg33.el7\nglobus-gsi-cert-utils-doc-9.12-1.osg33.el7\nglobus-gsi-cert-utils-progs-9.12-1.osg33.el7\nglobus-gsi-credential-7.9-1.osg33.el7\nglobus-gsi-credential-debuginfo-7.9-1.osg33.el7\nglobus-gsi-credential-devel-7.9-1.osg33.el7\nglobus-gsi-credential-doc-7.9-1.osg33.el7\nglobus-gsi-openssl-error-3.5-2.osg33.el7\nglobus-gsi-openssl-error-debuginfo-3.5-2.osg33.el7\nglobus-gsi-openssl-error-devel-3.5-2.osg33.el7\nglobus-gsi-openssl-error-doc-3.5-2.osg33.el7\nglobus-gsi-proxy-core-7.9-1.osg33.el7\nglobus-gsi-proxy-core-debuginfo-7.9-1.osg33.el7\nglobus-gsi-proxy-core-devel-7.9-1.osg33.el7\nglobus-gsi-proxy-core-doc-7.9-1.osg33.el7\nglobus-gsi-proxy-ssl-5.8-1.osg33.el7\nglobus-gsi-proxy-ssl-debuginfo-5.8-1.osg33.el7\nglobus-gsi-proxy-ssl-devel-5.8-1.osg33.el7\nglobus-gsi-proxy-ssl-doc-5.8-1.osg33.el7\nglobus-gsi-sysconfig-6.9-1.osg33.el7\nglobus-gsi-sysconfig-debuginfo-6.9-1.osg33.el7\nglobus-gsi-sysconfig-devel-6.9-1.osg33.el7\nglobus-gsi-sysconfig-doc-6.9-1.osg33.el7\nglobus-gssapi-error-5.4-2.osg33.el7\nglobus-gssapi-error-debuginfo-5.4-2.osg33.el7\nglobus-gssapi-error-devel-5.4-2.osg33.el7\nglobus-gssapi-error-doc-5.4-2.osg33.el7\nglobus-gssapi-gsi-12.1-1.osg33.el7\nglobus-gssapi-gsi-debuginfo-12.1-1.osg33.el7\nglobus-gssapi-gsi-devel-12.1-1.osg33.el7\nglobus-gssapi-gsi-doc-12.1-1.osg33.el7\nglobus-gss-assist-10.15-1.osg33.el7\nglobus-gss-assist-debuginfo-10.15-1.osg33.el7\nglobus-gss-assist-devel-10.15-1.osg33.el7\nglobus-gss-assist-doc-10.15-1.osg33.el7\nglobus-gss-assist-progs-10.15-1.osg33.el7\nglobus-io-11.5-1.osg33.el7\nglobus-io-debuginfo-11.5-1.osg33.el7\nglobus-io-devel-11.5-1.osg33.el7\nglobus-openssl-module-4.6-2.osg33.el7\nglobus-openssl-module-debuginfo-4.6-2.osg33.el7\nglobus-openssl-module-devel-4.6-2.osg33.el7\nglobus-openssl-module-doc-4.6-2.osg33.el7\nglobus-proxy-utils-6.15-1.osg33.el7\nglobus-proxy-utils-debuginfo-6.15-1.osg33.el7\nglobus-rsl-10.10-1.osg33.el7\nglobus-rsl-debuginfo-10.10-1.osg33.el7\nglobus-rsl-devel-10.10-1.osg33.el7\nglobus-rsl-doc-10.10-1.osg33.el7\nglobus-scheduler-event-generator-5.11-1.1.osg33.el7\nglobus-scheduler-event-generator-debuginfo-5.11-1.1.osg33.el7\nglobus-scheduler-event-generator-devel-5.11-1.1.osg33.el7\nglobus-scheduler-event-generator-doc-5.11-1.1.osg33.el7\nglobus-scheduler-event-generator-progs-5.11-1.1.osg33.el7\nglobus-simple-ca-4.22-1.osg33.el7\nglobus-usage-4.4-2.osg33.el7\nglobus-usage-debuginfo-4.4-2.osg33.el7\nglobus-usage-devel-4.4-2.osg33.el7\nglobus-xio-5.12-1.1.osg33.el7\nglobus-xio-debuginfo-5.12-1.1.osg33.el7\nglobus-xio-devel-5.12-1.1.osg33.el7\nglobus-xio-doc-5.12-1.1.osg33.el7\nglobus-xioperf-4.4-2.osg33.el7\nglobus-xioperf-debuginfo-4.4-2.osg33.el7\nglobus-xio-pipe-driver-3.8-1.osg33.el7\nglobus-xio-pipe-driver-debuginfo-3.8-1.osg33.el7\nglobus-xio-pipe-driver-devel-3.8-1.osg33.el7\nglobus-xio-popen-driver-3.5-2.osg33.el7\nglobus-xio-popen-driver-debuginfo-3.5-2.osg33.el7\nglobus-xio-popen-driver-devel-3.5-2.osg33.el7\nglobus-xio-udt-driver-1.23-1.osg33.el7\nglobus-xio-udt-driver-debuginfo-1.23-1.osg33.el7\nglobus-xio-udt-driver-devel-1.23-1.osg33.el7\ngratia-probe-1.17.0-2.osg33.el7\ngratia-probe-bdii-status-1.17.0-2.osg33.el7\ngratia-probe-common-1.17.0-2.osg33.el7\ngratia-probe-condor-1.17.0-2.osg33.el7\ngratia-probe-condor-events-1.17.0-2.osg33.el7\ngratia-probe-dcache-storage-1.17.0-2.osg33.el7\ngratia-probe-dcache-storagegroup-1.17.0-2.osg33.el7\ngratia-probe-dcache-transfer-1.17.0-2.osg33.el7\ngratia-probe-debuginfo-1.17.0-2.osg33.el7\ngratia-probe-enstore-storage-1.17.0-2.osg33.el7\ngratia-probe-enstore-tapedrive-1.17.0-2.osg33.el7\ngratia-probe-enstore-transfer-1.17.0-2.osg33.el7\ngratia-probe-glexec-1.17.0-2.osg33.el7\ngratia-probe-glideinwms-1.17.0-2.osg33.el7\ngratia-probe-gram-1.17.0-2.osg33.el7\ngratia-probe-gridftp-transfer-1.17.0-2.osg33.el7\ngratia-probe-hadoop-storage-1.17.0-2.osg33.el7\ngratia-probe-htcondor-ce-1.17.0-2.osg33.el7\ngratia-probe-lsf-1.17.0-2.osg33.el7\ngratia-probe-metric-1.17.0-2.osg33.el7\ngratia-probe-onevm-1.17.0-2.osg33.el7\ngratia-probe-pbs-lsf-1.17.0-2.osg33.el7\ngratia-probe-services-1.17.0-2.osg33.el7\ngratia-probe-sge-1.17.0-2.osg33.el7\ngratia-probe-slurm-1.17.0-2.osg33.el7\ngratia-probe-xrootd-storage-1.17.0-2.osg33.el7\ngratia-probe-xrootd-transfer-1.17.0-2.osg33.el7\ngums-1.5.2-9.osg33.el7\ngums-client-1.5.2-9.osg33.el7\ngums-service-1.5.2-9.osg33.el7\nhtcondor-ce-2.0.8-2.osg33.el7\nhtcondor-ce-bosco-2.0.8-2.osg33.el7\nhtcondor-ce-client-2.0.8-2.osg33.el7\nhtcondor-ce-collector-2.0.8-2.osg33.el7\nhtcondor-ce-condor-2.0.8-2.osg33.el7\nhtcondor-ce-lsf-2.0.8-2.osg33.el7\nhtcondor-ce-pbs-2.0.8-2.osg33.el7\nhtcondor-ce-sge-2.0.8-2.osg33.el7\nhtcondor-ce-view-2.0.8-2.osg33.el7\nlcas-lcmaps-gt4-interface-0.3.1-1.2.osg33.el7\nlcas-lcmaps-gt4-interface-debuginfo-0.3.1-1.2.osg33.el7\nmyproxy-6.1.18-1.1.osg33.el7\nmyproxy-admin-6.1.18-1.1.osg33.el7\nmyproxy-debuginfo-6.1.18-1.1.osg33.el7\nmyproxy-devel-6.1.18-1.1.osg33.el7\nmyproxy-doc-6.1.18-1.1.osg33.el7\nmyproxy-libs-6.1.18-1.1.osg33.el7\nmyproxy-server-6.1.18-1.1.osg33.el7\nmyproxy-voms-6.1.18-1.1.osg33.el7\nosg-build-1.7.1-1.osg33.el7\nosg-ca-generator-1.2.0-1.osg33.el7\nosg-configure-1.4.2-2.osg33.el7\nosg-configure-bosco-1.4.2-2.osg33.el7\nosg-configure-ce-1.4.2-2.osg33.el7\nosg-configure-cemon-1.4.2-2.osg33.el7\nosg-configure-condor-1.4.2-2.osg33.el7\nosg-configure-gateway-1.4.2-2.osg33.el7\nosg-configure-gip-1.4.2-2.osg33.el7\nosg-configure-gratia-1.4.2-2.osg33.el7\nosg-configure-infoservices-1.4.2-2.osg33.el7\nosg-configure-lsf-1.4.2-2.osg33.el7\nosg-configure-managedfork-1.4.2-2.osg33.el7\nosg-configure-misc-1.4.2-2.osg33.el7\nosg-configure-monalisa-1.4.2-2.osg33.el7\nosg-configure-network-1.4.2-2.osg33.el7\nosg-configure-pbs-1.4.2-2.osg33.el7\nosg-configure-rsv-1.4.2-2.osg33.el7\nosg-configure-sge-1.4.2-2.osg33.el7\nosg-configure-slurm-1.4.2-2.osg33.el7\nosg-configure-squid-1.4.2-2.osg33.el7\nosg-configure-tests-1.4.2-2.osg33.el7\nosg-gridftp-3.3-3.osg33.el7\nosg-gridftp-hdfs-3.3-4.osg33.el7\nosg-gridftp-xrootd-3.3-3.osg33.el7\nosg-gums-config-68-2.osg33.el7\nosg-pki-tools-1.2.19-1.osg33.el7\nosg-pki-tools-tests-1.2.19-1.osg33.el7\nosg-test-1.8.4-1.osg33.el7\nosg-tested-internal-3.3-13.osg33.el7\nosg-tested-internal-gram-3.3-13.osg33.el7\nosg-version-3.3.16-1.osg33.el7\nrsv-gwms-tester-1.1.2-1.osg33.el7\nvo-client-68-2.osg33.el7\nvo-client-edgmkgridmap-68-2.osg33.el7\nxrootd-dsi-3.0.4-22.osg33.el7\nxrootd-dsi-debuginfo-3.0.4-22.osg33.el7\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.25.bosco-1.osgup.el6\n\n\ncondor-8.5.6-1.1.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.25.bosco-1.osgup.el7\n\n\ncondor-8.5.6-1.1.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp-1.18.25.bosco-1.osgup.el6\nblahp-debuginfo-1.18.25.bosco-1.osgup.el6\ncondor-8.5.6-1.1.osgup.el6\ncondor-all-8.5.6-1.1.osgup.el6\ncondor-bosco-8.5.6-1.1.osgup.el6\ncondor-classads-8.5.6-1.1.osgup.el6\ncondor-classads-devel-8.5.6-1.1.osgup.el6\ncondor-cream-gahp-8.5.6-1.1.osgup.el6\ncondor-debuginfo-8.5.6-1.1.osgup.el6\ncondor-kbdd-8.5.6-1.1.osgup.el6\ncondor-procd-8.5.6-1.1.osgup.el6\ncondor-python-8.5.6-1.1.osgup.el6\ncondor-std-universe-8.5.6-1.1.osgup.el6\ncondor-test-8.5.6-1.1.osgup.el6\ncondor-vm-gahp-8.5.6-1.1.osgup.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp-1.18.25.bosco-1.osgup.el7\nblahp-debuginfo-1.18.25.bosco-1.osgup.el7\ncondor-8.5.6-1.1.osgup.el7\ncondor-all-8.5.6-1.1.osgup.el7\ncondor-bosco-8.5.6-1.1.osgup.el7\ncondor-classads-8.5.6-1.1.osgup.el7\ncondor-classads-devel-8.5.6-1.1.osgup.el7\ncondor-cream-gahp-8.5.6-1.1.osgup.el7\ncondor-debuginfo-8.5.6-1.1.osgup.el7\ncondor-kbdd-8.5.6-1.1.osgup.el7\ncondor-procd-8.5.6-1.1.osgup.el7\ncondor-python-8.5.6-1.1.osgup.el7\ncondor-test-8.5.6-1.1.osgup.el7\ncondor-vm-gahp-8.5.6-1.1.osgup.el7", 
            "title": "OSG Release 3.3.16"
        }, 
        {
            "location": "/release/3.3/release-3-3-16/#osg-software-release-3316", 
            "text": "Release Date : 2016-09-13", 
            "title": "OSG Software Release 3.3.16"
        }, 
        {
            "location": "/release/3.3/release-3-3-16/#summary-of-changes", 
            "text": "This release contains:   Updated most Globus Packages to latest available from EPEL  Note: Now Globus Toolkit strictly checks host names against certificates  The MyProxy server now produces RFC compliant proxies    BLAHP 1.18.25  Added the ability to set SGE parallel environment policy  Added multi-core support for PBS Pro  Added mutli-core, partition, and remote_cerequirement support for Slurm    GlideinWMS 3.2.15  Fixed major scalability problem in GUMS on EL7  HTCondor-CE 2.0.8  Allow mapping of Terana eScience hostcerts (SOFTWARE-2433)  Force 'condor_ce_q -allusers' until QUEUE_SUPER_USER is fixed to be able to use CERTIFICATE_MAPFILE in 8.5.6 (SOFTWARE-2412)  Fixed OnExitHold to be set to expressions rather than their evaluated forms  Ensure lockdir and rundir exist with correct permissions on startup  Removed the HTCondor-CE init script on EL7 (SOFTWARE-2419)    Fixed load-balancing in Globus GridFTP when using IPv6 addresses  Added the HTCondor CREAM GAHP for EL7 platforms  Completed porting components of OSG Software Stack to EL7  Added  RSV GlideinWMS Tester  for VO Front-ends to test site support  Updated to  VO Package v68 : Added project8 VO  osg-pki-tools 1.2.19  Reword 'bad VO info' error from osg-*cert-request  Fix formatting of CSRs    Updated xrootd-dsi RPM to direct administrators where to add configuration changes and not destroy those changes upon update.  Updated to lcas-lcmaps-gt4-interface to version 0.3.1: Added support for Globus 'sharing' service  Simplified Gratia GridFTP probe by removing dependencies on gums-client  Added JSON interfaces in GUMS for VO mapping  Updated osg-configure package to require condor-python  Added support for user certificates to osg-ca-generator   These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-16/#known-issues", 
            "text": "On EL7, your version of voms-proxy-init may come from the voms-clients-java package; this version will continue to make legacy proxies by default. If you want the new behavior, install the voms-clients-cpp package and uninstall the voms-clients-java package.", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.3/release-3-3-16/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-16/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-16/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-16/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-16/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-16/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-16/#enterprise-linux-6", 
            "text": "blahp-1.18.25.bosco-1.osg33.el6  condor-8.4.8-1.2.osg33.el6  glideinwms-3.2.15-1.osg33.el6  glite-ce-cream-client-api-c-1.15.4-2.2.osg33.el6  glite-ce-wsdl-1.15.1-1.1.osg33.el6  glite-lbjp-common-gsoap-plugin-3.2.12-1.1.osg33.el6  glite-lbjp-common-gss-3.2.16-1.osg33.el6  globus-authz-3.12-1.osg33.el6  globus-authz-callout-error-3.5-2.osg33.el6  globus-callout-3.14-1.osg33.el6  globus-common-16.4-1.osg33.el6  globus-ftp-client-8.29-1.1.osg33.el6  globus-ftp-control-6.10-1.1.osg33.el6  globus-gass-cache-9.8-1.osg33.el6  globus-gass-cache-program-6.5-2.osg33.el6  globus-gass-copy-9.19-1.osg33.el6  globus-gass-server-ez-5.7-2.osg33.el6  globus-gass-transfer-8.9-1.osg33.el6  globus-gatekeeper-10.10-1.1.osg33.el6  globus-gfork-4.8-1.osg33.el6  globus-gram-audit-4.4-2.osg33.el6  globus-gram-client-13.13-1.osg33.el6  globus-gram-client-tools-11.8-1.osg33.el6  globus-gram-job-manager-14.27-3.1.osg33.el6  globus-gram-job-manager-callout-error-3.5-2.osg33.el6  globus-gram-job-manager-condor-2.5-2.1.osg33.el6  globus-gram-job-manager-fork-2.4-2.1.osg33.el6  globus-gram-job-manager-lsf-2.6-2.1.osg33.el6  globus-gram-job-manager-scripts-6.7-2.osg33.el6  globus-gram-protocol-12.12-3.osg33.el6  globus-gridftp-server-10.4-1.2.osg33.el6  globus-gridftp-server-control-4.1-1.2.osg33.el6  globus-gridmap-callout-error-2.4-2.osg33.el6  globus-gsi-callback-5.8-1.osg33.el6  globus-gsi-cert-utils-9.12-1.osg33.el6  globus-gsi-credential-7.9-1.osg33.el6  globus-gsi-openssl-error-3.5-2.osg33.el6  globus-gsi-proxy-core-7.9-1.osg33.el6  globus-gsi-proxy-ssl-5.8-1.osg33.el6  globus-gsi-sysconfig-6.9-1.osg33.el6  globus-gss-assist-10.15-1.osg33.el6  globus-gssapi-error-5.4-2.osg33.el6  globus-gssapi-gsi-12.1-1.osg33.el6  globus-io-11.5-1.osg33.el6  globus-openssl-module-4.6-2.osg33.el6  globus-proxy-utils-6.15-1.osg33.el6  globus-rsl-10.10-1.osg33.el6  globus-scheduler-event-generator-5.11-1.1.osg33.el6  globus-simple-ca-4.22-1.osg33.el6  globus-usage-4.4-2.osg33.el6  globus-xio-5.12-1.1.osg33.el6  globus-xio-pipe-driver-3.8-1.osg33.el6  globus-xio-popen-driver-3.5-2.osg33.el6  globus-xio-udt-driver-1.23-1.osg33.el6  globus-xioperf-4.4-2.osg33.el6  gratia-probe-1.17.0-2.osg33.el6  gums-1.5.2-9.osg33.el6  htcondor-ce-2.0.8-2.osg33.el6  lcas-lcmaps-gt4-interface-0.3.1-1.2.osg33.el6  myproxy-6.1.18-1.1.osg33.el6  osg-build-1.7.1-1.osg33.el6  osg-ca-generator-1.2.0-1.osg33.el6  osg-configure-1.4.2-2.osg33.el6  osg-gridftp-3.3-3.osg33.el6  osg-gridftp-hdfs-3.3-4.osg33.el6  osg-gridftp-xrootd-3.3-3.osg33.el6  osg-pki-tools-1.2.19-1.osg33.el6  osg-test-1.8.4-1.osg33.el6  osg-tested-internal-3.3-13.osg33.el6  osg-version-3.3.16-1.osg33.el6  rsv-gwms-tester-1.1.2-1.osg33.el6  vo-client-68-2.osg33.el6  xrootd-dsi-3.0.4-22.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-16/#enterprise-linux-7", 
            "text": "blahp-1.18.25.bosco-1.osg33.el7  condor-8.4.8-1.2.osg33.el7  glideinwms-3.2.15-1.osg33.el7  glite-ce-cream-client-api-c-1.15.4-2.2.osg33.el7  glite-ce-wsdl-1.15.1-1.1.osg33.el7  glite-lbjp-common-gsoap-plugin-3.2.12-1.1.osg33.el7  glite-lbjp-common-gss-3.2.16-1.osg33.el7  globus-authz-3.12-1.osg33.el7  globus-authz-callout-error-3.5-2.osg33.el7  globus-callout-3.14-1.osg33.el7  globus-common-16.4-1.osg33.el7  globus-ftp-client-8.29-1.1.osg33.el7  globus-ftp-control-6.10-1.1.osg33.el7  globus-gass-cache-9.8-1.osg33.el7  globus-gass-cache-program-6.5-2.osg33.el7  globus-gass-copy-9.19-1.osg33.el7  globus-gass-server-ez-5.7-2.osg33.el7  globus-gass-transfer-8.9-1.osg33.el7  globus-gatekeeper-10.10-1.1.osg33.el7  globus-gfork-4.8-1.osg33.el7  globus-gram-audit-4.4-2.osg33.el7  globus-gram-client-13.13-1.osg33.el7  globus-gram-client-tools-11.8-1.osg33.el7  globus-gram-job-manager-14.27-3.1.osg33.el7  globus-gram-job-manager-callout-error-3.5-2.osg33.el7  globus-gram-job-manager-condor-2.5-2.1.osg33.el7  globus-gram-job-manager-fork-2.4-2.1.osg33.el7  globus-gram-job-manager-lsf-2.6-2.1.osg33.el7  globus-gram-job-manager-scripts-6.7-2.osg33.el7  globus-gram-protocol-12.12-3.osg33.el7  globus-gridftp-server-10.4-1.2.osg33.el7  globus-gridftp-server-control-4.1-1.2.osg33.el7  globus-gridmap-callout-error-2.4-2.osg33.el7  globus-gsi-callback-5.8-1.osg33.el7  globus-gsi-cert-utils-9.12-1.osg33.el7  globus-gsi-credential-7.9-1.osg33.el7  globus-gsi-openssl-error-3.5-2.osg33.el7  globus-gsi-proxy-core-7.9-1.osg33.el7  globus-gsi-proxy-ssl-5.8-1.osg33.el7  globus-gsi-sysconfig-6.9-1.osg33.el7  globus-gss-assist-10.15-1.osg33.el7  globus-gssapi-error-5.4-2.osg33.el7  globus-gssapi-gsi-12.1-1.osg33.el7  globus-io-11.5-1.osg33.el7  globus-openssl-module-4.6-2.osg33.el7  globus-proxy-utils-6.15-1.osg33.el7  globus-rsl-10.10-1.osg33.el7  globus-scheduler-event-generator-5.11-1.1.osg33.el7  globus-simple-ca-4.22-1.osg33.el7  globus-usage-4.4-2.osg33.el7  globus-xio-5.12-1.1.osg33.el7  globus-xio-pipe-driver-3.8-1.osg33.el7  globus-xio-popen-driver-3.5-2.osg33.el7  globus-xio-udt-driver-1.23-1.osg33.el7  globus-xioperf-4.4-2.osg33.el7  gratia-probe-1.17.0-2.osg33.el7  gums-1.5.2-9.osg33.el7  htcondor-ce-2.0.8-2.osg33.el7  lcas-lcmaps-gt4-interface-0.3.1-1.2.osg33.el7  myproxy-6.1.18-1.1.osg33.el7  osg-build-1.7.1-1.osg33.el7  osg-ca-generator-1.2.0-1.osg33.el7  osg-configure-1.4.2-2.osg33.el7  osg-gridftp-3.3-3.osg33.el7  osg-gridftp-hdfs-3.3-4.osg33.el7  osg-gridftp-xrootd-3.3-3.osg33.el7  osg-pki-tools-1.2.19-1.osg33.el7  osg-test-1.8.4-1.osg33.el7  osg-tested-internal-3.3-13.osg33.el7  osg-version-3.3.16-1.osg33.el7  rsv-gwms-tester-1.1.2-1.osg33.el7  vo-client-68-2.osg33.el7  xrootd-dsi-3.0.4-22.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-16/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp glideinwms-common-tools gl\nideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-s\ntandalone glite-ce-cream-client-api-c glite-ce-cream-client-devel glite-ce-wsdl glite-lbjp-common-gsoap-plugin glite-lbjp-common-gsoap-plugin-debuginfo glite-lbjp-common-gsoap-plugin-devel glite-lbjp-common-gss glite-lbjp-common-gss-debug\ninfo glite-lbjp-common-gss-devel globus-authz globus-authz-callout-error globus-authz-callout-error-debuginfo globus-authz-callout-error-devel globus-authz-callout-error-doc globus-authz-debuginfo globus-authz-devel globus-authz-doc globu\ns-callout globus-callout-debuginfo globus-callout-devel globus-callout-doc globus-common globus-common-debuginfo globus-common-devel globus-common-doc globus-common-progs globus-ftp-client globus-ftp-client-debuginfo globus-ftp-client-dev\nel globus-ftp-client-doc globus-ftp-control globus-ftp-control-debuginfo globus-ftp-control-devel globus-ftp-control-doc globus-gass-cache globus-gass-cache-debuginfo globus-gass-cache-devel globus-gass-cache-doc globus-gass-cache-program\n globus-gass-cache-program-debuginfo globus-gass-copy globus-gass-copy-debuginfo globus-gass-copy-devel globus-gass-copy-doc globus-gass-copy-progs globus-gass-server-ez globus-gass-server-ez-debuginfo globus-gass-server-ez-devel globus-g\nass-server-ez-progs globus-gass-transfer globus-gass-transfer-debuginfo globus-gass-transfer-devel globus-gass-transfer-doc globus-gatekeeper globus-gatekeeper-debuginfo globus-gfork globus-gfork-debuginfo globus-gfork-devel globus-gfork-\nprogs globus-gram-audit globus-gram-client globus-gram-client-debuginfo globus-gram-client-devel globus-gram-client-doc globus-gram-client-tools globus-gram-client-tools-debuginfo globus-gram-job-manager globus-gram-job-manager-callout-er\nror globus-gram-job-manager-callout-error-debuginfo globus-gram-job-manager-callout-error-devel globus-gram-job-manager-callout-error-doc globus-gram-job-manager-condor globus-gram-job-manager-debuginfo globus-gram-job-manager-fork globus\n-gram-job-manager-fork-debuginfo globus-gram-job-manager-fork-setup-poll globus-gram-job-manager-fork-setup-seg globus-gram-job-manager-lsf globus-gram-job-manager-lsf-debuginfo globus-gram-job-manager-lsf-setup-poll globus-gram-job-manag\ner-lsf-setup-seg globus-gram-job-manager-scripts globus-gram-job-manager-scripts-doc globus-gram-protocol globus-gram-protocol-debuginfo globus-gram-protocol-devel globus-gram-protocol-doc globus-gridftp-server globus-gridftp-server-contr\nol globus-gridftp-server-control-debuginfo globus-gridftp-server-control-devel globus-gridftp-server-debuginfo globus-gridftp-server-devel globus-gridftp-server-progs globus-gridmap-callout-error globus-gridmap-callout-error-debuginfo glo\nbus-gridmap-callout-error-devel globus-gridmap-callout-error-doc globus-gsi-callback globus-gsi-callback-debuginfo globus-gsi-callback-devel globus-gsi-callback-doc globus-gsi-cert-utils globus-gsi-cert-utils-debuginfo globus-gsi-cert-uti\nls-devel globus-gsi-cert-utils-doc globus-gsi-cert-utils-progs globus-gsi-credential globus-gsi-credential-debuginfo globus-gsi-credential-devel globus-gsi-credential-doc globus-gsi-openssl-error globus-gsi-openssl-error-debuginfo globus-\ngsi-openssl-error-devel globus-gsi-openssl-error-doc globus-gsi-proxy-core globus-gsi-proxy-core-debuginfo globus-gsi-proxy-core-devel globus-gsi-proxy-core-doc globus-gsi-proxy-ssl globus-gsi-proxy-ssl-debuginfo globus-gsi-proxy-ssl-deve\nl globus-gsi-proxy-ssl-doc globus-gsi-sysconfig globus-gsi-sysconfig-debuginfo globus-gsi-sysconfig-devel globus-gsi-sysconfig-doc globus-gssapi-error globus-gssapi-error-debuginfo globus-gssapi-error-devel globus-gssapi-error-doc globus-\ngssapi-gsi globus-gssapi-gsi-debuginfo globus-gssapi-gsi-devel globus-gssapi-gsi-doc globus-gss-assist globus-gss-assist-debuginfo globus-gss-assist-devel globus-gss-assist-doc globus-gss-assist-progs globus-io globus-io-debuginfo globus-\nio-devel globus-openssl-module globus-openssl-module-debuginfo globus-openssl-module-devel globus-openssl-module-doc globus-proxy-utils globus-proxy-utils-debuginfo globus-rsl globus-rsl-debuginfo globus-rsl-devel globus-rsl-doc globus-sc\nheduler-event-generator globus-scheduler-event-generator-debuginfo globus-scheduler-event-generator-devel globus-scheduler-event-generator-doc globus-scheduler-event-generator-progs globus-simple-ca globus-usage globus-usage-debuginfo glo\nbus-usage-devel globus-xio globus-xio-debuginfo globus-xio-devel globus-xio-doc globus-xioperf globus-xioperf-debuginfo globus-xio-pipe-driver globus-xio-pipe-driver-debuginfo globus-xio-pipe-driver-devel globus-xio-popen-driver globus-xi\no-popen-driver-debuginfo globus-xio-popen-driver-devel globus-xio-udt-driver globus-xio-udt-driver-debuginfo globus-xio-udt-driver-devel gratia-probe-bdii-status gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-pr\nobe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glexec gratia-probe-glideinwms \ngratia-probe-gram gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gr\natia-probe-xrootd-storage gratia-probe-xrootd-transfer gums gums-client gums-service htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-vie\nw lcas-lcmaps-gt4-interface lcas-lcmaps-gt4-interface-debuginfo myproxy myproxy-admin myproxy-debuginfo myproxy-devel myproxy-doc myproxy-libs myproxy-server myproxy-voms osg-build osg-ca-generator osg-configure osg-configure-bosco osg-co\nnfigure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configur\ne-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-gridftp osg-gridftp-hdfs osg-gridftp-xrootd osg-gums-config osg-pki-tools osg-pki-tools-tests osg-test osg-tes\nted-internal osg-tested-internal-gram osg-version rsv-gwms-tester vo-client vo-client-edgmkgridmap xrootd-dsi xrootd-dsi-debuginfo  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-16/#enterprise-linux-6_1", 
            "text": "blahp-1.18.25.bosco-1.osg33.el6\nblahp-debuginfo-1.18.25.bosco-1.osg33.el6\ncondor-8.4.8-1.2.osg33.el6\ncondor-all-8.4.8-1.2.osg33.el6\ncondor-bosco-8.4.8-1.2.osg33.el6\ncondor-classads-8.4.8-1.2.osg33.el6\ncondor-classads-devel-8.4.8-1.2.osg33.el6\ncondor-cream-gahp-8.4.8-1.2.osg33.el6\ncondor-debuginfo-8.4.8-1.2.osg33.el6\ncondor-kbdd-8.4.8-1.2.osg33.el6\ncondor-procd-8.4.8-1.2.osg33.el6\ncondor-python-8.4.8-1.2.osg33.el6\ncondor-std-universe-8.4.8-1.2.osg33.el6\ncondor-test-8.4.8-1.2.osg33.el6\ncondor-vm-gahp-8.4.8-1.2.osg33.el6\nglideinwms-3.2.15-1.osg33.el6\nglideinwms-common-tools-3.2.15-1.osg33.el6\nglideinwms-condor-common-config-3.2.15-1.osg33.el6\nglideinwms-factory-3.2.15-1.osg33.el6\nglideinwms-factory-condor-3.2.15-1.osg33.el6\nglideinwms-glidecondor-tools-3.2.15-1.osg33.el6\nglideinwms-libs-3.2.15-1.osg33.el6\nglideinwms-minimal-condor-3.2.15-1.osg33.el6\nglideinwms-usercollector-3.2.15-1.osg33.el6\nglideinwms-userschedd-3.2.15-1.osg33.el6\nglideinwms-vofrontend-3.2.15-1.osg33.el6\nglideinwms-vofrontend-standalone-3.2.15-1.osg33.el6\nglite-ce-cream-client-api-c-1.15.4-2.2.osg33.el6\nglite-ce-cream-client-devel-1.15.4-2.2.osg33.el6\nglite-ce-wsdl-1.15.1-1.1.osg33.el6\nglite-lbjp-common-gsoap-plugin-3.2.12-1.1.osg33.el6\nglite-lbjp-common-gsoap-plugin-debuginfo-3.2.12-1.1.osg33.el6\nglite-lbjp-common-gsoap-plugin-devel-3.2.12-1.1.osg33.el6\nglite-lbjp-common-gss-3.2.16-1.osg33.el6\nglite-lbjp-common-gss-debuginfo-3.2.16-1.osg33.el6\nglite-lbjp-common-gss-devel-3.2.16-1.osg33.el6\nglobus-authz-3.12-1.osg33.el6\nglobus-authz-callout-error-3.5-2.osg33.el6\nglobus-authz-callout-error-debuginfo-3.5-2.osg33.el6\nglobus-authz-callout-error-devel-3.5-2.osg33.el6\nglobus-authz-callout-error-doc-3.5-2.osg33.el6\nglobus-authz-debuginfo-3.12-1.osg33.el6\nglobus-authz-devel-3.12-1.osg33.el6\nglobus-authz-doc-3.12-1.osg33.el6\nglobus-callout-3.14-1.osg33.el6\nglobus-callout-debuginfo-3.14-1.osg33.el6\nglobus-callout-devel-3.14-1.osg33.el6\nglobus-callout-doc-3.14-1.osg33.el6\nglobus-common-16.4-1.osg33.el6\nglobus-common-debuginfo-16.4-1.osg33.el6\nglobus-common-devel-16.4-1.osg33.el6\nglobus-common-doc-16.4-1.osg33.el6\nglobus-common-progs-16.4-1.osg33.el6\nglobus-ftp-client-8.29-1.1.osg33.el6\nglobus-ftp-client-debuginfo-8.29-1.1.osg33.el6\nglobus-ftp-client-devel-8.29-1.1.osg33.el6\nglobus-ftp-client-doc-8.29-1.1.osg33.el6\nglobus-ftp-control-6.10-1.1.osg33.el6\nglobus-ftp-control-debuginfo-6.10-1.1.osg33.el6\nglobus-ftp-control-devel-6.10-1.1.osg33.el6\nglobus-ftp-control-doc-6.10-1.1.osg33.el6\nglobus-gass-cache-9.8-1.osg33.el6\nglobus-gass-cache-debuginfo-9.8-1.osg33.el6\nglobus-gass-cache-devel-9.8-1.osg33.el6\nglobus-gass-cache-doc-9.8-1.osg33.el6\nglobus-gass-cache-program-6.5-2.osg33.el6\nglobus-gass-cache-program-debuginfo-6.5-2.osg33.el6\nglobus-gass-copy-9.19-1.osg33.el6\nglobus-gass-copy-debuginfo-9.19-1.osg33.el6\nglobus-gass-copy-devel-9.19-1.osg33.el6\nglobus-gass-copy-doc-9.19-1.osg33.el6\nglobus-gass-copy-progs-9.19-1.osg33.el6\nglobus-gass-server-ez-5.7-2.osg33.el6\nglobus-gass-server-ez-debuginfo-5.7-2.osg33.el6\nglobus-gass-server-ez-devel-5.7-2.osg33.el6\nglobus-gass-server-ez-progs-5.7-2.osg33.el6\nglobus-gass-transfer-8.9-1.osg33.el6\nglobus-gass-transfer-debuginfo-8.9-1.osg33.el6\nglobus-gass-transfer-devel-8.9-1.osg33.el6\nglobus-gass-transfer-doc-8.9-1.osg33.el6\nglobus-gatekeeper-10.10-1.1.osg33.el6\nglobus-gatekeeper-debuginfo-10.10-1.1.osg33.el6\nglobus-gfork-4.8-1.osg33.el6\nglobus-gfork-debuginfo-4.8-1.osg33.el6\nglobus-gfork-devel-4.8-1.osg33.el6\nglobus-gfork-progs-4.8-1.osg33.el6\nglobus-gram-audit-4.4-2.osg33.el6\nglobus-gram-client-13.13-1.osg33.el6\nglobus-gram-client-debuginfo-13.13-1.osg33.el6\nglobus-gram-client-devel-13.13-1.osg33.el6\nglobus-gram-client-doc-13.13-1.osg33.el6\nglobus-gram-client-tools-11.8-1.osg33.el6\nglobus-gram-client-tools-debuginfo-11.8-1.osg33.el6\nglobus-gram-job-manager-14.27-3.1.osg33.el6\nglobus-gram-job-manager-callout-error-3.5-2.osg33.el6\nglobus-gram-job-manager-callout-error-debuginfo-3.5-2.osg33.el6\nglobus-gram-job-manager-callout-error-devel-3.5-2.osg33.el6\nglobus-gram-job-manager-callout-error-doc-3.5-2.osg33.el6\nglobus-gram-job-manager-condor-2.5-2.1.osg33.el6\nglobus-gram-job-manager-debuginfo-14.27-3.1.osg33.el6\nglobus-gram-job-manager-fork-2.4-2.1.osg33.el6\nglobus-gram-job-manager-fork-debuginfo-2.4-2.1.osg33.el6\nglobus-gram-job-manager-fork-setup-poll-2.4-2.1.osg33.el6\nglobus-gram-job-manager-fork-setup-seg-2.4-2.1.osg33.el6\nglobus-gram-job-manager-lsf-2.6-2.1.osg33.el6\nglobus-gram-job-manager-lsf-debuginfo-2.6-2.1.osg33.el6\nglobus-gram-job-manager-lsf-setup-poll-2.6-2.1.osg33.el6\nglobus-gram-job-manager-lsf-setup-seg-2.6-2.1.osg33.el6\nglobus-gram-job-manager-scripts-6.7-2.osg33.el6\nglobus-gram-job-manager-scripts-doc-6.7-2.osg33.el6\nglobus-gram-protocol-12.12-3.osg33.el6\nglobus-gram-protocol-debuginfo-12.12-3.osg33.el6\nglobus-gram-protocol-devel-12.12-3.osg33.el6\nglobus-gram-protocol-doc-12.12-3.osg33.el6\nglobus-gridftp-server-10.4-1.2.osg33.el6\nglobus-gridftp-server-control-4.1-1.2.osg33.el6\nglobus-gridftp-server-control-debuginfo-4.1-1.2.osg33.el6\nglobus-gridftp-server-control-devel-4.1-1.2.osg33.el6\nglobus-gridftp-server-debuginfo-10.4-1.2.osg33.el6\nglobus-gridftp-server-devel-10.4-1.2.osg33.el6\nglobus-gridftp-server-progs-10.4-1.2.osg33.el6\nglobus-gridmap-callout-error-2.4-2.osg33.el6\nglobus-gridmap-callout-error-debuginfo-2.4-2.osg33.el6\nglobus-gridmap-callout-error-devel-2.4-2.osg33.el6\nglobus-gridmap-callout-error-doc-2.4-2.osg33.el6\nglobus-gsi-callback-5.8-1.osg33.el6\nglobus-gsi-callback-debuginfo-5.8-1.osg33.el6\nglobus-gsi-callback-devel-5.8-1.osg33.el6\nglobus-gsi-callback-doc-5.8-1.osg33.el6\nglobus-gsi-cert-utils-9.12-1.osg33.el6\nglobus-gsi-cert-utils-debuginfo-9.12-1.osg33.el6\nglobus-gsi-cert-utils-devel-9.12-1.osg33.el6\nglobus-gsi-cert-utils-doc-9.12-1.osg33.el6\nglobus-gsi-cert-utils-progs-9.12-1.osg33.el6\nglobus-gsi-credential-7.9-1.osg33.el6\nglobus-gsi-credential-debuginfo-7.9-1.osg33.el6\nglobus-gsi-credential-devel-7.9-1.osg33.el6\nglobus-gsi-credential-doc-7.9-1.osg33.el6\nglobus-gsi-openssl-error-3.5-2.osg33.el6\nglobus-gsi-openssl-error-debuginfo-3.5-2.osg33.el6\nglobus-gsi-openssl-error-devel-3.5-2.osg33.el6\nglobus-gsi-openssl-error-doc-3.5-2.osg33.el6\nglobus-gsi-proxy-core-7.9-1.osg33.el6\nglobus-gsi-proxy-core-debuginfo-7.9-1.osg33.el6\nglobus-gsi-proxy-core-devel-7.9-1.osg33.el6\nglobus-gsi-proxy-core-doc-7.9-1.osg33.el6\nglobus-gsi-proxy-ssl-5.8-1.osg33.el6\nglobus-gsi-proxy-ssl-debuginfo-5.8-1.osg33.el6\nglobus-gsi-proxy-ssl-devel-5.8-1.osg33.el6\nglobus-gsi-proxy-ssl-doc-5.8-1.osg33.el6\nglobus-gsi-sysconfig-6.9-1.osg33.el6\nglobus-gsi-sysconfig-debuginfo-6.9-1.osg33.el6\nglobus-gsi-sysconfig-devel-6.9-1.osg33.el6\nglobus-gsi-sysconfig-doc-6.9-1.osg33.el6\nglobus-gssapi-error-5.4-2.osg33.el6\nglobus-gssapi-error-debuginfo-5.4-2.osg33.el6\nglobus-gssapi-error-devel-5.4-2.osg33.el6\nglobus-gssapi-error-doc-5.4-2.osg33.el6\nglobus-gssapi-gsi-12.1-1.osg33.el6\nglobus-gssapi-gsi-debuginfo-12.1-1.osg33.el6\nglobus-gssapi-gsi-devel-12.1-1.osg33.el6\nglobus-gssapi-gsi-doc-12.1-1.osg33.el6\nglobus-gss-assist-10.15-1.osg33.el6\nglobus-gss-assist-debuginfo-10.15-1.osg33.el6\nglobus-gss-assist-devel-10.15-1.osg33.el6\nglobus-gss-assist-doc-10.15-1.osg33.el6\nglobus-gss-assist-progs-10.15-1.osg33.el6\nglobus-io-11.5-1.osg33.el6\nglobus-io-debuginfo-11.5-1.osg33.el6\nglobus-io-devel-11.5-1.osg33.el6\nglobus-openssl-module-4.6-2.osg33.el6\nglobus-openssl-module-debuginfo-4.6-2.osg33.el6\nglobus-openssl-module-devel-4.6-2.osg33.el6\nglobus-openssl-module-doc-4.6-2.osg33.el6\nglobus-proxy-utils-6.15-1.osg33.el6\nglobus-proxy-utils-debuginfo-6.15-1.osg33.el6\nglobus-rsl-10.10-1.osg33.el6\nglobus-rsl-debuginfo-10.10-1.osg33.el6\nglobus-rsl-devel-10.10-1.osg33.el6\nglobus-rsl-doc-10.10-1.osg33.el6\nglobus-scheduler-event-generator-5.11-1.1.osg33.el6\nglobus-scheduler-event-generator-debuginfo-5.11-1.1.osg33.el6\nglobus-scheduler-event-generator-devel-5.11-1.1.osg33.el6\nglobus-scheduler-event-generator-doc-5.11-1.1.osg33.el6\nglobus-scheduler-event-generator-progs-5.11-1.1.osg33.el6\nglobus-simple-ca-4.22-1.osg33.el6\nglobus-usage-4.4-2.osg33.el6\nglobus-usage-debuginfo-4.4-2.osg33.el6\nglobus-usage-devel-4.4-2.osg33.el6\nglobus-xio-5.12-1.1.osg33.el6\nglobus-xio-debuginfo-5.12-1.1.osg33.el6\nglobus-xio-devel-5.12-1.1.osg33.el6\nglobus-xio-doc-5.12-1.1.osg33.el6\nglobus-xioperf-4.4-2.osg33.el6\nglobus-xioperf-debuginfo-4.4-2.osg33.el6\nglobus-xio-pipe-driver-3.8-1.osg33.el6\nglobus-xio-pipe-driver-debuginfo-3.8-1.osg33.el6\nglobus-xio-pipe-driver-devel-3.8-1.osg33.el6\nglobus-xio-popen-driver-3.5-2.osg33.el6\nglobus-xio-popen-driver-debuginfo-3.5-2.osg33.el6\nglobus-xio-popen-driver-devel-3.5-2.osg33.el6\nglobus-xio-udt-driver-1.23-1.osg33.el6\nglobus-xio-udt-driver-debuginfo-1.23-1.osg33.el6\nglobus-xio-udt-driver-devel-1.23-1.osg33.el6\ngratia-probe-1.17.0-2.osg33.el6\ngratia-probe-bdii-status-1.17.0-2.osg33.el6\ngratia-probe-common-1.17.0-2.osg33.el6\ngratia-probe-condor-1.17.0-2.osg33.el6\ngratia-probe-condor-events-1.17.0-2.osg33.el6\ngratia-probe-dcache-storage-1.17.0-2.osg33.el6\ngratia-probe-dcache-storagegroup-1.17.0-2.osg33.el6\ngratia-probe-dcache-transfer-1.17.0-2.osg33.el6\ngratia-probe-debuginfo-1.17.0-2.osg33.el6\ngratia-probe-enstore-storage-1.17.0-2.osg33.el6\ngratia-probe-enstore-tapedrive-1.17.0-2.osg33.el6\ngratia-probe-enstore-transfer-1.17.0-2.osg33.el6\ngratia-probe-glexec-1.17.0-2.osg33.el6\ngratia-probe-glideinwms-1.17.0-2.osg33.el6\ngratia-probe-gram-1.17.0-2.osg33.el6\ngratia-probe-gridftp-transfer-1.17.0-2.osg33.el6\ngratia-probe-hadoop-storage-1.17.0-2.osg33.el6\ngratia-probe-htcondor-ce-1.17.0-2.osg33.el6\ngratia-probe-lsf-1.17.0-2.osg33.el6\ngratia-probe-metric-1.17.0-2.osg33.el6\ngratia-probe-onevm-1.17.0-2.osg33.el6\ngratia-probe-pbs-lsf-1.17.0-2.osg33.el6\ngratia-probe-services-1.17.0-2.osg33.el6\ngratia-probe-sge-1.17.0-2.osg33.el6\ngratia-probe-slurm-1.17.0-2.osg33.el6\ngratia-probe-xrootd-storage-1.17.0-2.osg33.el6\ngratia-probe-xrootd-transfer-1.17.0-2.osg33.el6\ngums-1.5.2-9.osg33.el6\ngums-client-1.5.2-9.osg33.el6\ngums-service-1.5.2-9.osg33.el6\nhtcondor-ce-2.0.8-2.osg33.el6\nhtcondor-ce-bosco-2.0.8-2.osg33.el6\nhtcondor-ce-client-2.0.8-2.osg33.el6\nhtcondor-ce-collector-2.0.8-2.osg33.el6\nhtcondor-ce-condor-2.0.8-2.osg33.el6\nhtcondor-ce-lsf-2.0.8-2.osg33.el6\nhtcondor-ce-pbs-2.0.8-2.osg33.el6\nhtcondor-ce-sge-2.0.8-2.osg33.el6\nhtcondor-ce-view-2.0.8-2.osg33.el6\nlcas-lcmaps-gt4-interface-0.3.1-1.2.osg33.el6\nlcas-lcmaps-gt4-interface-debuginfo-0.3.1-1.2.osg33.el6\nmyproxy-6.1.18-1.1.osg33.el6\nmyproxy-admin-6.1.18-1.1.osg33.el6\nmyproxy-debuginfo-6.1.18-1.1.osg33.el6\nmyproxy-devel-6.1.18-1.1.osg33.el6\nmyproxy-doc-6.1.18-1.1.osg33.el6\nmyproxy-libs-6.1.18-1.1.osg33.el6\nmyproxy-server-6.1.18-1.1.osg33.el6\nmyproxy-voms-6.1.18-1.1.osg33.el6\nosg-build-1.7.1-1.osg33.el6\nosg-ca-generator-1.2.0-1.osg33.el6\nosg-configure-1.4.2-2.osg33.el6\nosg-configure-bosco-1.4.2-2.osg33.el6\nosg-configure-ce-1.4.2-2.osg33.el6\nosg-configure-cemon-1.4.2-2.osg33.el6\nosg-configure-condor-1.4.2-2.osg33.el6\nosg-configure-gateway-1.4.2-2.osg33.el6\nosg-configure-gip-1.4.2-2.osg33.el6\nosg-configure-gratia-1.4.2-2.osg33.el6\nosg-configure-infoservices-1.4.2-2.osg33.el6\nosg-configure-lsf-1.4.2-2.osg33.el6\nosg-configure-managedfork-1.4.2-2.osg33.el6\nosg-configure-misc-1.4.2-2.osg33.el6\nosg-configure-monalisa-1.4.2-2.osg33.el6\nosg-configure-network-1.4.2-2.osg33.el6\nosg-configure-pbs-1.4.2-2.osg33.el6\nosg-configure-rsv-1.4.2-2.osg33.el6\nosg-configure-sge-1.4.2-2.osg33.el6\nosg-configure-slurm-1.4.2-2.osg33.el6\nosg-configure-squid-1.4.2-2.osg33.el6\nosg-configure-tests-1.4.2-2.osg33.el6\nosg-gridftp-3.3-3.osg33.el6\nosg-gridftp-hdfs-3.3-4.osg33.el6\nosg-gridftp-xrootd-3.3-3.osg33.el6\nosg-gums-config-68-2.osg33.el6\nosg-pki-tools-1.2.19-1.osg33.el6\nosg-pki-tools-tests-1.2.19-1.osg33.el6\nosg-test-1.8.4-1.osg33.el6\nosg-tested-internal-3.3-13.osg33.el6\nosg-tested-internal-gram-3.3-13.osg33.el6\nosg-version-3.3.16-1.osg33.el6\nrsv-gwms-tester-1.1.2-1.osg33.el6\nvo-client-68-2.osg33.el6\nvo-client-edgmkgridmap-68-2.osg33.el6\nxrootd-dsi-3.0.4-22.osg33.el6\nxrootd-dsi-debuginfo-3.0.4-22.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-16/#enterprise-linux-7_1", 
            "text": "blahp-1.18.25.bosco-1.osg33.el7\nblahp-debuginfo-1.18.25.bosco-1.osg33.el7\ncondor-8.4.8-1.2.osg33.el7\ncondor-all-8.4.8-1.2.osg33.el7\ncondor-bosco-8.4.8-1.2.osg33.el7\ncondor-classads-8.4.8-1.2.osg33.el7\ncondor-classads-devel-8.4.8-1.2.osg33.el7\ncondor-cream-gahp-8.4.8-1.2.osg33.el7\ncondor-debuginfo-8.4.8-1.2.osg33.el7\ncondor-kbdd-8.4.8-1.2.osg33.el7\ncondor-procd-8.4.8-1.2.osg33.el7\ncondor-python-8.4.8-1.2.osg33.el7\ncondor-test-8.4.8-1.2.osg33.el7\ncondor-vm-gahp-8.4.8-1.2.osg33.el7\nglideinwms-3.2.15-1.osg33.el7\nglideinwms-common-tools-3.2.15-1.osg33.el7\nglideinwms-condor-common-config-3.2.15-1.osg33.el7\nglideinwms-factory-3.2.15-1.osg33.el7\nglideinwms-factory-condor-3.2.15-1.osg33.el7\nglideinwms-glidecondor-tools-3.2.15-1.osg33.el7\nglideinwms-libs-3.2.15-1.osg33.el7\nglideinwms-minimal-condor-3.2.15-1.osg33.el7\nglideinwms-usercollector-3.2.15-1.osg33.el7\nglideinwms-userschedd-3.2.15-1.osg33.el7\nglideinwms-vofrontend-3.2.15-1.osg33.el7\nglideinwms-vofrontend-standalone-3.2.15-1.osg33.el7\nglite-ce-cream-client-api-c-1.15.4-2.2.osg33.el7\nglite-ce-cream-client-devel-1.15.4-2.2.osg33.el7\nglite-ce-wsdl-1.15.1-1.1.osg33.el7\nglite-lbjp-common-gsoap-plugin-3.2.12-1.1.osg33.el7\nglite-lbjp-common-gsoap-plugin-debuginfo-3.2.12-1.1.osg33.el7\nglite-lbjp-common-gsoap-plugin-devel-3.2.12-1.1.osg33.el7\nglite-lbjp-common-gss-3.2.16-1.osg33.el7\nglite-lbjp-common-gss-debuginfo-3.2.16-1.osg33.el7\nglite-lbjp-common-gss-devel-3.2.16-1.osg33.el7\nglobus-authz-3.12-1.osg33.el7\nglobus-authz-callout-error-3.5-2.osg33.el7\nglobus-authz-callout-error-debuginfo-3.5-2.osg33.el7\nglobus-authz-callout-error-devel-3.5-2.osg33.el7\nglobus-authz-callout-error-doc-3.5-2.osg33.el7\nglobus-authz-debuginfo-3.12-1.osg33.el7\nglobus-authz-devel-3.12-1.osg33.el7\nglobus-authz-doc-3.12-1.osg33.el7\nglobus-callout-3.14-1.osg33.el7\nglobus-callout-debuginfo-3.14-1.osg33.el7\nglobus-callout-devel-3.14-1.osg33.el7\nglobus-callout-doc-3.14-1.osg33.el7\nglobus-common-16.4-1.osg33.el7\nglobus-common-debuginfo-16.4-1.osg33.el7\nglobus-common-devel-16.4-1.osg33.el7\nglobus-common-doc-16.4-1.osg33.el7\nglobus-common-progs-16.4-1.osg33.el7\nglobus-ftp-client-8.29-1.1.osg33.el7\nglobus-ftp-client-debuginfo-8.29-1.1.osg33.el7\nglobus-ftp-client-devel-8.29-1.1.osg33.el7\nglobus-ftp-client-doc-8.29-1.1.osg33.el7\nglobus-ftp-control-6.10-1.1.osg33.el7\nglobus-ftp-control-debuginfo-6.10-1.1.osg33.el7\nglobus-ftp-control-devel-6.10-1.1.osg33.el7\nglobus-ftp-control-doc-6.10-1.1.osg33.el7\nglobus-gass-cache-9.8-1.osg33.el7\nglobus-gass-cache-debuginfo-9.8-1.osg33.el7\nglobus-gass-cache-devel-9.8-1.osg33.el7\nglobus-gass-cache-doc-9.8-1.osg33.el7\nglobus-gass-cache-program-6.5-2.osg33.el7\nglobus-gass-cache-program-debuginfo-6.5-2.osg33.el7\nglobus-gass-copy-9.19-1.osg33.el7\nglobus-gass-copy-debuginfo-9.19-1.osg33.el7\nglobus-gass-copy-devel-9.19-1.osg33.el7\nglobus-gass-copy-doc-9.19-1.osg33.el7\nglobus-gass-copy-progs-9.19-1.osg33.el7\nglobus-gass-server-ez-5.7-2.osg33.el7\nglobus-gass-server-ez-debuginfo-5.7-2.osg33.el7\nglobus-gass-server-ez-devel-5.7-2.osg33.el7\nglobus-gass-server-ez-progs-5.7-2.osg33.el7\nglobus-gass-transfer-8.9-1.osg33.el7\nglobus-gass-transfer-debuginfo-8.9-1.osg33.el7\nglobus-gass-transfer-devel-8.9-1.osg33.el7\nglobus-gass-transfer-doc-8.9-1.osg33.el7\nglobus-gatekeeper-10.10-1.1.osg33.el7\nglobus-gatekeeper-debuginfo-10.10-1.1.osg33.el7\nglobus-gfork-4.8-1.osg33.el7\nglobus-gfork-debuginfo-4.8-1.osg33.el7\nglobus-gfork-devel-4.8-1.osg33.el7\nglobus-gfork-progs-4.8-1.osg33.el7\nglobus-gram-audit-4.4-2.osg33.el7\nglobus-gram-client-13.13-1.osg33.el7\nglobus-gram-client-debuginfo-13.13-1.osg33.el7\nglobus-gram-client-devel-13.13-1.osg33.el7\nglobus-gram-client-doc-13.13-1.osg33.el7\nglobus-gram-client-tools-11.8-1.osg33.el7\nglobus-gram-client-tools-debuginfo-11.8-1.osg33.el7\nglobus-gram-job-manager-14.27-3.1.osg33.el7\nglobus-gram-job-manager-callout-error-3.5-2.osg33.el7\nglobus-gram-job-manager-callout-error-debuginfo-3.5-2.osg33.el7\nglobus-gram-job-manager-callout-error-devel-3.5-2.osg33.el7\nglobus-gram-job-manager-callout-error-doc-3.5-2.osg33.el7\nglobus-gram-job-manager-condor-2.5-2.1.osg33.el7\nglobus-gram-job-manager-debuginfo-14.27-3.1.osg33.el7\nglobus-gram-job-manager-fork-2.4-2.1.osg33.el7\nglobus-gram-job-manager-fork-debuginfo-2.4-2.1.osg33.el7\nglobus-gram-job-manager-fork-setup-poll-2.4-2.1.osg33.el7\nglobus-gram-job-manager-fork-setup-seg-2.4-2.1.osg33.el7\nglobus-gram-job-manager-lsf-2.6-2.1.osg33.el7\nglobus-gram-job-manager-lsf-debuginfo-2.6-2.1.osg33.el7\nglobus-gram-job-manager-lsf-setup-poll-2.6-2.1.osg33.el7\nglobus-gram-job-manager-lsf-setup-seg-2.6-2.1.osg33.el7\nglobus-gram-job-manager-scripts-6.7-2.osg33.el7\nglobus-gram-job-manager-scripts-doc-6.7-2.osg33.el7\nglobus-gram-protocol-12.12-3.osg33.el7\nglobus-gram-protocol-debuginfo-12.12-3.osg33.el7\nglobus-gram-protocol-devel-12.12-3.osg33.el7\nglobus-gram-protocol-doc-12.12-3.osg33.el7\nglobus-gridftp-server-10.4-1.2.osg33.el7\nglobus-gridftp-server-control-4.1-1.2.osg33.el7\nglobus-gridftp-server-control-debuginfo-4.1-1.2.osg33.el7\nglobus-gridftp-server-control-devel-4.1-1.2.osg33.el7\nglobus-gridftp-server-debuginfo-10.4-1.2.osg33.el7\nglobus-gridftp-server-devel-10.4-1.2.osg33.el7\nglobus-gridftp-server-progs-10.4-1.2.osg33.el7\nglobus-gridmap-callout-error-2.4-2.osg33.el7\nglobus-gridmap-callout-error-debuginfo-2.4-2.osg33.el7\nglobus-gridmap-callout-error-devel-2.4-2.osg33.el7\nglobus-gridmap-callout-error-doc-2.4-2.osg33.el7\nglobus-gsi-callback-5.8-1.osg33.el7\nglobus-gsi-callback-debuginfo-5.8-1.osg33.el7\nglobus-gsi-callback-devel-5.8-1.osg33.el7\nglobus-gsi-callback-doc-5.8-1.osg33.el7\nglobus-gsi-cert-utils-9.12-1.osg33.el7\nglobus-gsi-cert-utils-debuginfo-9.12-1.osg33.el7\nglobus-gsi-cert-utils-devel-9.12-1.osg33.el7\nglobus-gsi-cert-utils-doc-9.12-1.osg33.el7\nglobus-gsi-cert-utils-progs-9.12-1.osg33.el7\nglobus-gsi-credential-7.9-1.osg33.el7\nglobus-gsi-credential-debuginfo-7.9-1.osg33.el7\nglobus-gsi-credential-devel-7.9-1.osg33.el7\nglobus-gsi-credential-doc-7.9-1.osg33.el7\nglobus-gsi-openssl-error-3.5-2.osg33.el7\nglobus-gsi-openssl-error-debuginfo-3.5-2.osg33.el7\nglobus-gsi-openssl-error-devel-3.5-2.osg33.el7\nglobus-gsi-openssl-error-doc-3.5-2.osg33.el7\nglobus-gsi-proxy-core-7.9-1.osg33.el7\nglobus-gsi-proxy-core-debuginfo-7.9-1.osg33.el7\nglobus-gsi-proxy-core-devel-7.9-1.osg33.el7\nglobus-gsi-proxy-core-doc-7.9-1.osg33.el7\nglobus-gsi-proxy-ssl-5.8-1.osg33.el7\nglobus-gsi-proxy-ssl-debuginfo-5.8-1.osg33.el7\nglobus-gsi-proxy-ssl-devel-5.8-1.osg33.el7\nglobus-gsi-proxy-ssl-doc-5.8-1.osg33.el7\nglobus-gsi-sysconfig-6.9-1.osg33.el7\nglobus-gsi-sysconfig-debuginfo-6.9-1.osg33.el7\nglobus-gsi-sysconfig-devel-6.9-1.osg33.el7\nglobus-gsi-sysconfig-doc-6.9-1.osg33.el7\nglobus-gssapi-error-5.4-2.osg33.el7\nglobus-gssapi-error-debuginfo-5.4-2.osg33.el7\nglobus-gssapi-error-devel-5.4-2.osg33.el7\nglobus-gssapi-error-doc-5.4-2.osg33.el7\nglobus-gssapi-gsi-12.1-1.osg33.el7\nglobus-gssapi-gsi-debuginfo-12.1-1.osg33.el7\nglobus-gssapi-gsi-devel-12.1-1.osg33.el7\nglobus-gssapi-gsi-doc-12.1-1.osg33.el7\nglobus-gss-assist-10.15-1.osg33.el7\nglobus-gss-assist-debuginfo-10.15-1.osg33.el7\nglobus-gss-assist-devel-10.15-1.osg33.el7\nglobus-gss-assist-doc-10.15-1.osg33.el7\nglobus-gss-assist-progs-10.15-1.osg33.el7\nglobus-io-11.5-1.osg33.el7\nglobus-io-debuginfo-11.5-1.osg33.el7\nglobus-io-devel-11.5-1.osg33.el7\nglobus-openssl-module-4.6-2.osg33.el7\nglobus-openssl-module-debuginfo-4.6-2.osg33.el7\nglobus-openssl-module-devel-4.6-2.osg33.el7\nglobus-openssl-module-doc-4.6-2.osg33.el7\nglobus-proxy-utils-6.15-1.osg33.el7\nglobus-proxy-utils-debuginfo-6.15-1.osg33.el7\nglobus-rsl-10.10-1.osg33.el7\nglobus-rsl-debuginfo-10.10-1.osg33.el7\nglobus-rsl-devel-10.10-1.osg33.el7\nglobus-rsl-doc-10.10-1.osg33.el7\nglobus-scheduler-event-generator-5.11-1.1.osg33.el7\nglobus-scheduler-event-generator-debuginfo-5.11-1.1.osg33.el7\nglobus-scheduler-event-generator-devel-5.11-1.1.osg33.el7\nglobus-scheduler-event-generator-doc-5.11-1.1.osg33.el7\nglobus-scheduler-event-generator-progs-5.11-1.1.osg33.el7\nglobus-simple-ca-4.22-1.osg33.el7\nglobus-usage-4.4-2.osg33.el7\nglobus-usage-debuginfo-4.4-2.osg33.el7\nglobus-usage-devel-4.4-2.osg33.el7\nglobus-xio-5.12-1.1.osg33.el7\nglobus-xio-debuginfo-5.12-1.1.osg33.el7\nglobus-xio-devel-5.12-1.1.osg33.el7\nglobus-xio-doc-5.12-1.1.osg33.el7\nglobus-xioperf-4.4-2.osg33.el7\nglobus-xioperf-debuginfo-4.4-2.osg33.el7\nglobus-xio-pipe-driver-3.8-1.osg33.el7\nglobus-xio-pipe-driver-debuginfo-3.8-1.osg33.el7\nglobus-xio-pipe-driver-devel-3.8-1.osg33.el7\nglobus-xio-popen-driver-3.5-2.osg33.el7\nglobus-xio-popen-driver-debuginfo-3.5-2.osg33.el7\nglobus-xio-popen-driver-devel-3.5-2.osg33.el7\nglobus-xio-udt-driver-1.23-1.osg33.el7\nglobus-xio-udt-driver-debuginfo-1.23-1.osg33.el7\nglobus-xio-udt-driver-devel-1.23-1.osg33.el7\ngratia-probe-1.17.0-2.osg33.el7\ngratia-probe-bdii-status-1.17.0-2.osg33.el7\ngratia-probe-common-1.17.0-2.osg33.el7\ngratia-probe-condor-1.17.0-2.osg33.el7\ngratia-probe-condor-events-1.17.0-2.osg33.el7\ngratia-probe-dcache-storage-1.17.0-2.osg33.el7\ngratia-probe-dcache-storagegroup-1.17.0-2.osg33.el7\ngratia-probe-dcache-transfer-1.17.0-2.osg33.el7\ngratia-probe-debuginfo-1.17.0-2.osg33.el7\ngratia-probe-enstore-storage-1.17.0-2.osg33.el7\ngratia-probe-enstore-tapedrive-1.17.0-2.osg33.el7\ngratia-probe-enstore-transfer-1.17.0-2.osg33.el7\ngratia-probe-glexec-1.17.0-2.osg33.el7\ngratia-probe-glideinwms-1.17.0-2.osg33.el7\ngratia-probe-gram-1.17.0-2.osg33.el7\ngratia-probe-gridftp-transfer-1.17.0-2.osg33.el7\ngratia-probe-hadoop-storage-1.17.0-2.osg33.el7\ngratia-probe-htcondor-ce-1.17.0-2.osg33.el7\ngratia-probe-lsf-1.17.0-2.osg33.el7\ngratia-probe-metric-1.17.0-2.osg33.el7\ngratia-probe-onevm-1.17.0-2.osg33.el7\ngratia-probe-pbs-lsf-1.17.0-2.osg33.el7\ngratia-probe-services-1.17.0-2.osg33.el7\ngratia-probe-sge-1.17.0-2.osg33.el7\ngratia-probe-slurm-1.17.0-2.osg33.el7\ngratia-probe-xrootd-storage-1.17.0-2.osg33.el7\ngratia-probe-xrootd-transfer-1.17.0-2.osg33.el7\ngums-1.5.2-9.osg33.el7\ngums-client-1.5.2-9.osg33.el7\ngums-service-1.5.2-9.osg33.el7\nhtcondor-ce-2.0.8-2.osg33.el7\nhtcondor-ce-bosco-2.0.8-2.osg33.el7\nhtcondor-ce-client-2.0.8-2.osg33.el7\nhtcondor-ce-collector-2.0.8-2.osg33.el7\nhtcondor-ce-condor-2.0.8-2.osg33.el7\nhtcondor-ce-lsf-2.0.8-2.osg33.el7\nhtcondor-ce-pbs-2.0.8-2.osg33.el7\nhtcondor-ce-sge-2.0.8-2.osg33.el7\nhtcondor-ce-view-2.0.8-2.osg33.el7\nlcas-lcmaps-gt4-interface-0.3.1-1.2.osg33.el7\nlcas-lcmaps-gt4-interface-debuginfo-0.3.1-1.2.osg33.el7\nmyproxy-6.1.18-1.1.osg33.el7\nmyproxy-admin-6.1.18-1.1.osg33.el7\nmyproxy-debuginfo-6.1.18-1.1.osg33.el7\nmyproxy-devel-6.1.18-1.1.osg33.el7\nmyproxy-doc-6.1.18-1.1.osg33.el7\nmyproxy-libs-6.1.18-1.1.osg33.el7\nmyproxy-server-6.1.18-1.1.osg33.el7\nmyproxy-voms-6.1.18-1.1.osg33.el7\nosg-build-1.7.1-1.osg33.el7\nosg-ca-generator-1.2.0-1.osg33.el7\nosg-configure-1.4.2-2.osg33.el7\nosg-configure-bosco-1.4.2-2.osg33.el7\nosg-configure-ce-1.4.2-2.osg33.el7\nosg-configure-cemon-1.4.2-2.osg33.el7\nosg-configure-condor-1.4.2-2.osg33.el7\nosg-configure-gateway-1.4.2-2.osg33.el7\nosg-configure-gip-1.4.2-2.osg33.el7\nosg-configure-gratia-1.4.2-2.osg33.el7\nosg-configure-infoservices-1.4.2-2.osg33.el7\nosg-configure-lsf-1.4.2-2.osg33.el7\nosg-configure-managedfork-1.4.2-2.osg33.el7\nosg-configure-misc-1.4.2-2.osg33.el7\nosg-configure-monalisa-1.4.2-2.osg33.el7\nosg-configure-network-1.4.2-2.osg33.el7\nosg-configure-pbs-1.4.2-2.osg33.el7\nosg-configure-rsv-1.4.2-2.osg33.el7\nosg-configure-sge-1.4.2-2.osg33.el7\nosg-configure-slurm-1.4.2-2.osg33.el7\nosg-configure-squid-1.4.2-2.osg33.el7\nosg-configure-tests-1.4.2-2.osg33.el7\nosg-gridftp-3.3-3.osg33.el7\nosg-gridftp-hdfs-3.3-4.osg33.el7\nosg-gridftp-xrootd-3.3-3.osg33.el7\nosg-gums-config-68-2.osg33.el7\nosg-pki-tools-1.2.19-1.osg33.el7\nosg-pki-tools-tests-1.2.19-1.osg33.el7\nosg-test-1.8.4-1.osg33.el7\nosg-tested-internal-3.3-13.osg33.el7\nosg-tested-internal-gram-3.3-13.osg33.el7\nosg-version-3.3.16-1.osg33.el7\nrsv-gwms-tester-1.1.2-1.osg33.el7\nvo-client-68-2.osg33.el7\nvo-client-edgmkgridmap-68-2.osg33.el7\nxrootd-dsi-3.0.4-22.osg33.el7\nxrootd-dsi-debuginfo-3.0.4-22.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-16/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-16/#enterprise-linux-6_2", 
            "text": "blahp-1.18.25.bosco-1.osgup.el6  condor-8.5.6-1.1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-16/#enterprise-linux-7_2", 
            "text": "blahp-1.18.25.bosco-1.osgup.el7  condor-8.5.6-1.1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-16/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-16/#enterprise-linux-6_3", 
            "text": "blahp-1.18.25.bosco-1.osgup.el6\nblahp-debuginfo-1.18.25.bosco-1.osgup.el6\ncondor-8.5.6-1.1.osgup.el6\ncondor-all-8.5.6-1.1.osgup.el6\ncondor-bosco-8.5.6-1.1.osgup.el6\ncondor-classads-8.5.6-1.1.osgup.el6\ncondor-classads-devel-8.5.6-1.1.osgup.el6\ncondor-cream-gahp-8.5.6-1.1.osgup.el6\ncondor-debuginfo-8.5.6-1.1.osgup.el6\ncondor-kbdd-8.5.6-1.1.osgup.el6\ncondor-procd-8.5.6-1.1.osgup.el6\ncondor-python-8.5.6-1.1.osgup.el6\ncondor-std-universe-8.5.6-1.1.osgup.el6\ncondor-test-8.5.6-1.1.osgup.el6\ncondor-vm-gahp-8.5.6-1.1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-16/#enterprise-linux-7_3", 
            "text": "blahp-1.18.25.bosco-1.osgup.el7\nblahp-debuginfo-1.18.25.bosco-1.osgup.el7\ncondor-8.5.6-1.1.osgup.el7\ncondor-all-8.5.6-1.1.osgup.el7\ncondor-bosco-8.5.6-1.1.osgup.el7\ncondor-classads-8.5.6-1.1.osgup.el7\ncondor-classads-devel-8.5.6-1.1.osgup.el7\ncondor-cream-gahp-8.5.6-1.1.osgup.el7\ncondor-debuginfo-8.5.6-1.1.osgup.el7\ncondor-kbdd-8.5.6-1.1.osgup.el7\ncondor-procd-8.5.6-1.1.osgup.el7\ncondor-python-8.5.6-1.1.osgup.el7\ncondor-test-8.5.6-1.1.osgup.el7\ncondor-vm-gahp-8.5.6-1.1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-15/", 
            "text": "OSG Software Release 3.3.15\n\n\nRelease Date\n: 2016-08-09\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nCA Certificates based on \nIGTF 1.76\n\n\nUpdated to \nVO Package v67\n\n\nImplemented SLURM scalability enhancements in the BLAHP\n\n\nFixed a bug in the BLAHP where HTCondor could not remove a SLURM job\n\n\nEnabled XRootD-HDFS to use native HDFS libraries if available\n\n\nAdded an extension to the GridFTP server to report space usage on the server\n\n\nFixed GUMS to properly display long Pool Account lists\n\n\nUpdated the RSV service will start even though its state file is corrupt\n\n\nUpdated GSI-OpenSSH from \n5.7-4.3\n to \n7.1p2f\n\n\nUpdated voms-proxy-init to generate RFC compliant proxies by default\n\n\nConfigured voms-server for systemd startup in EL7\n\n\nAdded voms-admin-client for EL7\n\n\nUpdated to \nHTCondor 8.5.6\n in the upcoming repository\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nKnown Issues\n\n\n\n\nOn EL7, your version of voms-proxy-init may come from the voms-clients-java package; this version will continue to make legacy proxies by default. If you want the new behavior, install the voms-clients-cpp package and uninstall the voms-clients-java package.\n\n\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.23.bosco-1.osg33.el6\n\n\nglobus-gridftp-osg-extensions-0.3-1.osg33.el6\n\n\nglobus-gridftp-server-7.20-1.3.osg33.el6\n\n\ngridftp-hdfs-0.5.4-25.3.osg33.el6\n\n\ngsi-openssh-7.1p2f-1.1.osg33.el6\n\n\ngums-1.5.2-4.osg33.el6\n\n\nigtf-ca-certs-1.76-1.osg33.el6\n\n\nosg-ca-certs-1.57-1.osg33.el6\n\n\nosg-configure-1.4.2-1.osg33.el6\n\n\nosg-version-3.3.15-1.osg33.el6\n\n\nrsv-3.13.1-1.osg33.el6\n\n\nrsv-perfsonar-1.1.3-1.osg33.el6\n\n\nvo-client-67-1.osg33.el6\n\n\nvoms-2.0.12-3.3.osg33.el6\n\n\nxrootd-dsi-3.0.4-20.osg33.el6\n\n\nxrootd-hdfs-1.8.8-1.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nPyXML-0.8.4-29.1.osg33.el7\n\n\nblahp-1.18.23.bosco-1.osg33.el7\n\n\nglobus-gridftp-osg-extensions-0.3-1.osg33.el7\n\n\nglobus-gridftp-server-7.20-1.3.osg33.el7\n\n\ngridftp-hdfs-0.5.4-25.3.osg33.el7\n\n\ngsi-openssh-7.1p2f-1.1.osg33.el7\n\n\ngums-1.5.2-4.osg33.el7\n\n\nigtf-ca-certs-1.76-1.osg33.el7\n\n\nosg-ca-certs-1.57-1.osg33.el7\n\n\nosg-configure-1.4.2-1.osg33.el7\n\n\nosg-version-3.3.15-1.osg33.el7\n\n\npython-ZSI-2.0-6.2.osg33.el7\n\n\nrsv-3.13.1-1.osg33.el7\n\n\nvo-client-67-1.osg33.el7\n\n\nvoms-2.0.12-3.3.osg33.el7\n\n\nvoms-admin-client-2.0.17-1.1.osg33.el7\n\n\nxrootd-dsi-3.0.4-20.osg33.el7\n\n\nxrootd-hdfs-1.8.8-1.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp blahp-debuginfo globus-gridftp-osg-extensions globus-gridftp-osg-extensions-debuginfo globus-gridftp-server globus-gridftp-server-debuginfo globus-gridftp-server-devel globus-gridftp-server-progs gridftp-hdfs gridftp-hdfs-debuginfo gsi-openssh gsi-openssh-clients gsi-openssh-debuginfo gsi-openssh-server gums gums-client gums-service igtf-ca-certs osg-ca-certs osg-configure osg-configure-bosco osg-configure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-gums-config osg-version rsv rsv-consumers rsv-core rsv-metrics rsv-perfsonar vo-client vo-client-edgmkgridmap voms voms-clients-cpp voms-debuginfo voms-devel voms-doc voms-server xrootd-dsi xrootd-dsi-debuginfo xrootd-hdfs xrootd-hdfs-debuginfo xrootd-hdfs-devel\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp-1.18.23.bosco-1.osg33.el6\nblahp-debuginfo-1.18.23.bosco-1.osg33.el6\nglobus-gridftp-osg-extensions-0.3-1.osg33.el6\nglobus-gridftp-osg-extensions-debuginfo-0.3-1.osg33.el6\nglobus-gridftp-server-7.20-1.3.osg33.el6\nglobus-gridftp-server-debuginfo-7.20-1.3.osg33.el6\nglobus-gridftp-server-devel-7.20-1.3.osg33.el6\nglobus-gridftp-server-progs-7.20-1.3.osg33.el6\ngridftp-hdfs-0.5.4-25.3.osg33.el6\ngridftp-hdfs-debuginfo-0.5.4-25.3.osg33.el6\ngsi-openssh-7.1p2f-1.1.osg33.el6\ngsi-openssh-clients-7.1p2f-1.1.osg33.el6\ngsi-openssh-debuginfo-7.1p2f-1.1.osg33.el6\ngsi-openssh-server-7.1p2f-1.1.osg33.el6\ngums-1.5.2-4.osg33.el6\ngums-client-1.5.2-4.osg33.el6\ngums-service-1.5.2-4.osg33.el6\nigtf-ca-certs-1.76-1.osg33.el6\nosg-ca-certs-1.57-1.osg33.el6\nosg-configure-1.4.2-1.osg33.el6\nosg-configure-bosco-1.4.2-1.osg33.el6\nosg-configure-ce-1.4.2-1.osg33.el6\nosg-configure-cemon-1.4.2-1.osg33.el6\nosg-configure-condor-1.4.2-1.osg33.el6\nosg-configure-gateway-1.4.2-1.osg33.el6\nosg-configure-gip-1.4.2-1.osg33.el6\nosg-configure-gratia-1.4.2-1.osg33.el6\nosg-configure-infoservices-1.4.2-1.osg33.el6\nosg-configure-lsf-1.4.2-1.osg33.el6\nosg-configure-managedfork-1.4.2-1.osg33.el6\nosg-configure-misc-1.4.2-1.osg33.el6\nosg-configure-monalisa-1.4.2-1.osg33.el6\nosg-configure-network-1.4.2-1.osg33.el6\nosg-configure-pbs-1.4.2-1.osg33.el6\nosg-configure-rsv-1.4.2-1.osg33.el6\nosg-configure-sge-1.4.2-1.osg33.el6\nosg-configure-slurm-1.4.2-1.osg33.el6\nosg-configure-squid-1.4.2-1.osg33.el6\nosg-configure-tests-1.4.2-1.osg33.el6\nosg-gums-config-67-1.osg33.el6\nosg-version-3.3.15-1.osg33.el6\nrsv-3.13.1-1.osg33.el6\nrsv-consumers-3.13.1-1.osg33.el6\nrsv-core-3.13.1-1.osg33.el6\nrsv-metrics-3.13.1-1.osg33.el6\nrsv-perfsonar-1.1.3-1.osg33.el6\nvo-client-67-1.osg33.el6\nvo-client-edgmkgridmap-67-1.osg33.el6\nvoms-2.0.12-3.3.osg33.el6\nvoms-clients-cpp-2.0.12-3.3.osg33.el6\nvoms-debuginfo-2.0.12-3.3.osg33.el6\nvoms-devel-2.0.12-3.3.osg33.el6\nvoms-doc-2.0.12-3.3.osg33.el6\nvoms-server-2.0.12-3.3.osg33.el6\nxrootd-dsi-3.0.4-20.osg33.el6\nxrootd-dsi-debuginfo-3.0.4-20.osg33.el6\nxrootd-hdfs-1.8.8-1.osg33.el6\nxrootd-hdfs-debuginfo-1.8.8-1.osg33.el6\nxrootd-hdfs-devel-1.8.8-1.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp-1.18.23.bosco-1.osg33.el7\nblahp-debuginfo-1.18.23.bosco-1.osg33.el7\nglobus-gridftp-osg-extensions-0.3-1.osg33.el7\nglobus-gridftp-osg-extensions-debuginfo-0.3-1.osg33.el7\nglobus-gridftp-server-7.20-1.3.osg33.el7\nglobus-gridftp-server-debuginfo-7.20-1.3.osg33.el7\nglobus-gridftp-server-devel-7.20-1.3.osg33.el7\nglobus-gridftp-server-progs-7.20-1.3.osg33.el7\ngridftp-hdfs-0.5.4-25.3.osg33.el7\ngridftp-hdfs-debuginfo-0.5.4-25.3.osg33.el7\ngsi-openssh-7.1p2f-1.1.osg33.el7\ngsi-openssh-clients-7.1p2f-1.1.osg33.el7\ngsi-openssh-debuginfo-7.1p2f-1.1.osg33.el7\ngsi-openssh-server-7.1p2f-1.1.osg33.el7\ngums-1.5.2-4.osg33.el7\ngums-client-1.5.2-4.osg33.el7\ngums-service-1.5.2-4.osg33.el7\nigtf-ca-certs-1.76-1.osg33.el7\nosg-ca-certs-1.57-1.osg33.el7\nosg-configure-1.4.2-1.osg33.el7\nosg-configure-bosco-1.4.2-1.osg33.el7\nosg-configure-ce-1.4.2-1.osg33.el7\nosg-configure-cemon-1.4.2-1.osg33.el7\nosg-configure-condor-1.4.2-1.osg33.el7\nosg-configure-gateway-1.4.2-1.osg33.el7\nosg-configure-gip-1.4.2-1.osg33.el7\nosg-configure-gratia-1.4.2-1.osg33.el7\nosg-configure-infoservices-1.4.2-1.osg33.el7\nosg-configure-lsf-1.4.2-1.osg33.el7\nosg-configure-managedfork-1.4.2-1.osg33.el7\nosg-configure-misc-1.4.2-1.osg33.el7\nosg-configure-monalisa-1.4.2-1.osg33.el7\nosg-configure-network-1.4.2-1.osg33.el7\nosg-configure-pbs-1.4.2-1.osg33.el7\nosg-configure-rsv-1.4.2-1.osg33.el7\nosg-configure-sge-1.4.2-1.osg33.el7\nosg-configure-slurm-1.4.2-1.osg33.el7\nosg-configure-squid-1.4.2-1.osg33.el7\nosg-configure-tests-1.4.2-1.osg33.el7\nosg-gums-config-67-1.osg33.el7\nosg-version-3.3.15-1.osg33.el7\npython-ZSI-2.0-6.2.osg33.el7\nPyXML-0.8.4-29.1.osg33.el7\nPyXML-debuginfo-0.8.4-29.1.osg33.el7\nrsv-3.13.1-1.osg33.el7\nrsv-consumers-3.13.1-1.osg33.el7\nrsv-core-3.13.1-1.osg33.el7\nrsv-metrics-3.13.1-1.osg33.el7\nvo-client-67-1.osg33.el7\nvo-client-edgmkgridmap-67-1.osg33.el7\nvoms-2.0.12-3.3.osg33.el7\nvoms-admin-client-2.0.17-1.1.osg33.el7\nvoms-clients-cpp-2.0.12-3.3.osg33.el7\nvoms-debuginfo-2.0.12-3.3.osg33.el7\nvoms-devel-2.0.12-3.3.osg33.el7\nvoms-doc-2.0.12-3.3.osg33.el7\nvoms-server-2.0.12-3.3.osg33.el7\nxrootd-dsi-3.0.4-20.osg33.el7\nxrootd-dsi-debuginfo-3.0.4-20.osg33.el7\nxrootd-hdfs-1.8.8-1.osg33.el7\nxrootd-hdfs-debuginfo-1.8.8-1.osg33.el7\nxrootd-hdfs-devel-1.8.8-1.osg33.el7\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.23.bosco-1.osgup.el6\n\n\ncondor-8.5.6-1.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.23.bosco-1.osgup.el7\n\n\ncondor-8.5.6-1.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp-1.18.23.bosco-1.osgup.el6\nblahp-debuginfo-1.18.23.bosco-1.osgup.el6\ncondor-8.5.6-1.osgup.el6\ncondor-all-8.5.6-1.osgup.el6\ncondor-bosco-8.5.6-1.osgup.el6\ncondor-classads-8.5.6-1.osgup.el6\ncondor-classads-devel-8.5.6-1.osgup.el6\ncondor-cream-gahp-8.5.6-1.osgup.el6\ncondor-debuginfo-8.5.6-1.osgup.el6\ncondor-kbdd-8.5.6-1.osgup.el6\ncondor-procd-8.5.6-1.osgup.el6\ncondor-python-8.5.6-1.osgup.el6\ncondor-std-universe-8.5.6-1.osgup.el6\ncondor-test-8.5.6-1.osgup.el6\ncondor-vm-gahp-8.5.6-1.osgup.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp-1.18.23.bosco-1.osgup.el7\nblahp-debuginfo-1.18.23.bosco-1.osgup.el7\ncondor-8.5.6-1.osgup.el7\ncondor-all-8.5.6-1.osgup.el7\ncondor-bosco-8.5.6-1.osgup.el7\ncondor-classads-8.5.6-1.osgup.el7\ncondor-classads-devel-8.5.6-1.osgup.el7\ncondor-debuginfo-8.5.6-1.osgup.el7\ncondor-kbdd-8.5.6-1.osgup.el7\ncondor-procd-8.5.6-1.osgup.el7\ncondor-python-8.5.6-1.osgup.el7\ncondor-test-8.5.6-1.osgup.el7\ncondor-vm-gahp-8.5.6-1.osgup.el7", 
            "title": "OSG Release 3.3.15"
        }, 
        {
            "location": "/release/3.3/release-3-3-15/#osg-software-release-3315", 
            "text": "Release Date : 2016-08-09", 
            "title": "OSG Software Release 3.3.15"
        }, 
        {
            "location": "/release/3.3/release-3-3-15/#summary-of-changes", 
            "text": "This release contains:   CA Certificates based on  IGTF 1.76  Updated to  VO Package v67  Implemented SLURM scalability enhancements in the BLAHP  Fixed a bug in the BLAHP where HTCondor could not remove a SLURM job  Enabled XRootD-HDFS to use native HDFS libraries if available  Added an extension to the GridFTP server to report space usage on the server  Fixed GUMS to properly display long Pool Account lists  Updated the RSV service will start even though its state file is corrupt  Updated GSI-OpenSSH from  5.7-4.3  to  7.1p2f  Updated voms-proxy-init to generate RFC compliant proxies by default  Configured voms-server for systemd startup in EL7  Added voms-admin-client for EL7  Updated to  HTCondor 8.5.6  in the upcoming repository   These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-15/#known-issues", 
            "text": "On EL7, your version of voms-proxy-init may come from the voms-clients-java package; this version will continue to make legacy proxies by default. If you want the new behavior, install the voms-clients-cpp package and uninstall the voms-clients-java package.", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.3/release-3-3-15/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-15/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-15/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-15/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-15/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-15/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-15/#enterprise-linux-6", 
            "text": "blahp-1.18.23.bosco-1.osg33.el6  globus-gridftp-osg-extensions-0.3-1.osg33.el6  globus-gridftp-server-7.20-1.3.osg33.el6  gridftp-hdfs-0.5.4-25.3.osg33.el6  gsi-openssh-7.1p2f-1.1.osg33.el6  gums-1.5.2-4.osg33.el6  igtf-ca-certs-1.76-1.osg33.el6  osg-ca-certs-1.57-1.osg33.el6  osg-configure-1.4.2-1.osg33.el6  osg-version-3.3.15-1.osg33.el6  rsv-3.13.1-1.osg33.el6  rsv-perfsonar-1.1.3-1.osg33.el6  vo-client-67-1.osg33.el6  voms-2.0.12-3.3.osg33.el6  xrootd-dsi-3.0.4-20.osg33.el6  xrootd-hdfs-1.8.8-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-15/#enterprise-linux-7", 
            "text": "PyXML-0.8.4-29.1.osg33.el7  blahp-1.18.23.bosco-1.osg33.el7  globus-gridftp-osg-extensions-0.3-1.osg33.el7  globus-gridftp-server-7.20-1.3.osg33.el7  gridftp-hdfs-0.5.4-25.3.osg33.el7  gsi-openssh-7.1p2f-1.1.osg33.el7  gums-1.5.2-4.osg33.el7  igtf-ca-certs-1.76-1.osg33.el7  osg-ca-certs-1.57-1.osg33.el7  osg-configure-1.4.2-1.osg33.el7  osg-version-3.3.15-1.osg33.el7  python-ZSI-2.0-6.2.osg33.el7  rsv-3.13.1-1.osg33.el7  vo-client-67-1.osg33.el7  voms-2.0.12-3.3.osg33.el7  voms-admin-client-2.0.17-1.1.osg33.el7  xrootd-dsi-3.0.4-20.osg33.el7  xrootd-hdfs-1.8.8-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-15/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp blahp-debuginfo globus-gridftp-osg-extensions globus-gridftp-osg-extensions-debuginfo globus-gridftp-server globus-gridftp-server-debuginfo globus-gridftp-server-devel globus-gridftp-server-progs gridftp-hdfs gridftp-hdfs-debuginfo gsi-openssh gsi-openssh-clients gsi-openssh-debuginfo gsi-openssh-server gums gums-client gums-service igtf-ca-certs osg-ca-certs osg-configure osg-configure-bosco osg-configure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-gums-config osg-version rsv rsv-consumers rsv-core rsv-metrics rsv-perfsonar vo-client vo-client-edgmkgridmap voms voms-clients-cpp voms-debuginfo voms-devel voms-doc voms-server xrootd-dsi xrootd-dsi-debuginfo xrootd-hdfs xrootd-hdfs-debuginfo xrootd-hdfs-devel  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-15/#enterprise-linux-6_1", 
            "text": "blahp-1.18.23.bosco-1.osg33.el6\nblahp-debuginfo-1.18.23.bosco-1.osg33.el6\nglobus-gridftp-osg-extensions-0.3-1.osg33.el6\nglobus-gridftp-osg-extensions-debuginfo-0.3-1.osg33.el6\nglobus-gridftp-server-7.20-1.3.osg33.el6\nglobus-gridftp-server-debuginfo-7.20-1.3.osg33.el6\nglobus-gridftp-server-devel-7.20-1.3.osg33.el6\nglobus-gridftp-server-progs-7.20-1.3.osg33.el6\ngridftp-hdfs-0.5.4-25.3.osg33.el6\ngridftp-hdfs-debuginfo-0.5.4-25.3.osg33.el6\ngsi-openssh-7.1p2f-1.1.osg33.el6\ngsi-openssh-clients-7.1p2f-1.1.osg33.el6\ngsi-openssh-debuginfo-7.1p2f-1.1.osg33.el6\ngsi-openssh-server-7.1p2f-1.1.osg33.el6\ngums-1.5.2-4.osg33.el6\ngums-client-1.5.2-4.osg33.el6\ngums-service-1.5.2-4.osg33.el6\nigtf-ca-certs-1.76-1.osg33.el6\nosg-ca-certs-1.57-1.osg33.el6\nosg-configure-1.4.2-1.osg33.el6\nosg-configure-bosco-1.4.2-1.osg33.el6\nosg-configure-ce-1.4.2-1.osg33.el6\nosg-configure-cemon-1.4.2-1.osg33.el6\nosg-configure-condor-1.4.2-1.osg33.el6\nosg-configure-gateway-1.4.2-1.osg33.el6\nosg-configure-gip-1.4.2-1.osg33.el6\nosg-configure-gratia-1.4.2-1.osg33.el6\nosg-configure-infoservices-1.4.2-1.osg33.el6\nosg-configure-lsf-1.4.2-1.osg33.el6\nosg-configure-managedfork-1.4.2-1.osg33.el6\nosg-configure-misc-1.4.2-1.osg33.el6\nosg-configure-monalisa-1.4.2-1.osg33.el6\nosg-configure-network-1.4.2-1.osg33.el6\nosg-configure-pbs-1.4.2-1.osg33.el6\nosg-configure-rsv-1.4.2-1.osg33.el6\nosg-configure-sge-1.4.2-1.osg33.el6\nosg-configure-slurm-1.4.2-1.osg33.el6\nosg-configure-squid-1.4.2-1.osg33.el6\nosg-configure-tests-1.4.2-1.osg33.el6\nosg-gums-config-67-1.osg33.el6\nosg-version-3.3.15-1.osg33.el6\nrsv-3.13.1-1.osg33.el6\nrsv-consumers-3.13.1-1.osg33.el6\nrsv-core-3.13.1-1.osg33.el6\nrsv-metrics-3.13.1-1.osg33.el6\nrsv-perfsonar-1.1.3-1.osg33.el6\nvo-client-67-1.osg33.el6\nvo-client-edgmkgridmap-67-1.osg33.el6\nvoms-2.0.12-3.3.osg33.el6\nvoms-clients-cpp-2.0.12-3.3.osg33.el6\nvoms-debuginfo-2.0.12-3.3.osg33.el6\nvoms-devel-2.0.12-3.3.osg33.el6\nvoms-doc-2.0.12-3.3.osg33.el6\nvoms-server-2.0.12-3.3.osg33.el6\nxrootd-dsi-3.0.4-20.osg33.el6\nxrootd-dsi-debuginfo-3.0.4-20.osg33.el6\nxrootd-hdfs-1.8.8-1.osg33.el6\nxrootd-hdfs-debuginfo-1.8.8-1.osg33.el6\nxrootd-hdfs-devel-1.8.8-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-15/#enterprise-linux-7_1", 
            "text": "blahp-1.18.23.bosco-1.osg33.el7\nblahp-debuginfo-1.18.23.bosco-1.osg33.el7\nglobus-gridftp-osg-extensions-0.3-1.osg33.el7\nglobus-gridftp-osg-extensions-debuginfo-0.3-1.osg33.el7\nglobus-gridftp-server-7.20-1.3.osg33.el7\nglobus-gridftp-server-debuginfo-7.20-1.3.osg33.el7\nglobus-gridftp-server-devel-7.20-1.3.osg33.el7\nglobus-gridftp-server-progs-7.20-1.3.osg33.el7\ngridftp-hdfs-0.5.4-25.3.osg33.el7\ngridftp-hdfs-debuginfo-0.5.4-25.3.osg33.el7\ngsi-openssh-7.1p2f-1.1.osg33.el7\ngsi-openssh-clients-7.1p2f-1.1.osg33.el7\ngsi-openssh-debuginfo-7.1p2f-1.1.osg33.el7\ngsi-openssh-server-7.1p2f-1.1.osg33.el7\ngums-1.5.2-4.osg33.el7\ngums-client-1.5.2-4.osg33.el7\ngums-service-1.5.2-4.osg33.el7\nigtf-ca-certs-1.76-1.osg33.el7\nosg-ca-certs-1.57-1.osg33.el7\nosg-configure-1.4.2-1.osg33.el7\nosg-configure-bosco-1.4.2-1.osg33.el7\nosg-configure-ce-1.4.2-1.osg33.el7\nosg-configure-cemon-1.4.2-1.osg33.el7\nosg-configure-condor-1.4.2-1.osg33.el7\nosg-configure-gateway-1.4.2-1.osg33.el7\nosg-configure-gip-1.4.2-1.osg33.el7\nosg-configure-gratia-1.4.2-1.osg33.el7\nosg-configure-infoservices-1.4.2-1.osg33.el7\nosg-configure-lsf-1.4.2-1.osg33.el7\nosg-configure-managedfork-1.4.2-1.osg33.el7\nosg-configure-misc-1.4.2-1.osg33.el7\nosg-configure-monalisa-1.4.2-1.osg33.el7\nosg-configure-network-1.4.2-1.osg33.el7\nosg-configure-pbs-1.4.2-1.osg33.el7\nosg-configure-rsv-1.4.2-1.osg33.el7\nosg-configure-sge-1.4.2-1.osg33.el7\nosg-configure-slurm-1.4.2-1.osg33.el7\nosg-configure-squid-1.4.2-1.osg33.el7\nosg-configure-tests-1.4.2-1.osg33.el7\nosg-gums-config-67-1.osg33.el7\nosg-version-3.3.15-1.osg33.el7\npython-ZSI-2.0-6.2.osg33.el7\nPyXML-0.8.4-29.1.osg33.el7\nPyXML-debuginfo-0.8.4-29.1.osg33.el7\nrsv-3.13.1-1.osg33.el7\nrsv-consumers-3.13.1-1.osg33.el7\nrsv-core-3.13.1-1.osg33.el7\nrsv-metrics-3.13.1-1.osg33.el7\nvo-client-67-1.osg33.el7\nvo-client-edgmkgridmap-67-1.osg33.el7\nvoms-2.0.12-3.3.osg33.el7\nvoms-admin-client-2.0.17-1.1.osg33.el7\nvoms-clients-cpp-2.0.12-3.3.osg33.el7\nvoms-debuginfo-2.0.12-3.3.osg33.el7\nvoms-devel-2.0.12-3.3.osg33.el7\nvoms-doc-2.0.12-3.3.osg33.el7\nvoms-server-2.0.12-3.3.osg33.el7\nxrootd-dsi-3.0.4-20.osg33.el7\nxrootd-dsi-debuginfo-3.0.4-20.osg33.el7\nxrootd-hdfs-1.8.8-1.osg33.el7\nxrootd-hdfs-debuginfo-1.8.8-1.osg33.el7\nxrootd-hdfs-devel-1.8.8-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-15/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-15/#enterprise-linux-6_2", 
            "text": "blahp-1.18.23.bosco-1.osgup.el6  condor-8.5.6-1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-15/#enterprise-linux-7_2", 
            "text": "blahp-1.18.23.bosco-1.osgup.el7  condor-8.5.6-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-15/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-15/#enterprise-linux-6_3", 
            "text": "blahp-1.18.23.bosco-1.osgup.el6\nblahp-debuginfo-1.18.23.bosco-1.osgup.el6\ncondor-8.5.6-1.osgup.el6\ncondor-all-8.5.6-1.osgup.el6\ncondor-bosco-8.5.6-1.osgup.el6\ncondor-classads-8.5.6-1.osgup.el6\ncondor-classads-devel-8.5.6-1.osgup.el6\ncondor-cream-gahp-8.5.6-1.osgup.el6\ncondor-debuginfo-8.5.6-1.osgup.el6\ncondor-kbdd-8.5.6-1.osgup.el6\ncondor-procd-8.5.6-1.osgup.el6\ncondor-python-8.5.6-1.osgup.el6\ncondor-std-universe-8.5.6-1.osgup.el6\ncondor-test-8.5.6-1.osgup.el6\ncondor-vm-gahp-8.5.6-1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-15/#enterprise-linux-7_3", 
            "text": "blahp-1.18.23.bosco-1.osgup.el7\nblahp-debuginfo-1.18.23.bosco-1.osgup.el7\ncondor-8.5.6-1.osgup.el7\ncondor-all-8.5.6-1.osgup.el7\ncondor-bosco-8.5.6-1.osgup.el7\ncondor-classads-8.5.6-1.osgup.el7\ncondor-classads-devel-8.5.6-1.osgup.el7\ncondor-debuginfo-8.5.6-1.osgup.el7\ncondor-kbdd-8.5.6-1.osgup.el7\ncondor-procd-8.5.6-1.osgup.el7\ncondor-python-8.5.6-1.osgup.el7\ncondor-test-8.5.6-1.osgup.el7\ncondor-vm-gahp-8.5.6-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-14/", 
            "text": "OSG Software Release 3.3.14\n\n\nRelease Date\n: 2016-07-12\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nCA Certificates based on \nIGTF 1.75\n\n\nHTCondor 8.4.8\n containing the following fixes:\n\n\nFixed memory leak triggered by python bindings\n\n\nFixed a bug that could cause Bosco file transfers to fail\n\n\nFixed a bug that could cause the schedd to crash when using schedd cron jobs\n\n\ncondor_schedd now rejects jobs when owner has no account on the machine\n\n\nOther bug fixes\n\n\n\n\n\n\nGlideinWMS 3.2.14.1\n\n\nHTCondor-CE 2.0.7: Added htcondor-ce-bosco sub-package\n\n\nblahp 1.18.21: Slurm improvements\n\n\nxrootd-voms-plugin 0.4.0: added support for 'all' group selection\n\n\ngridFTP 7.20-1.2: adler32 support, fix deadlock\n\n\nosg-configure 1.4.1\n\n\nSet HTCondor-CE configuration for htcondor-ce-bosco\n\n\nDo not overwrite custom bosco routes\n\n\nEnsure that GUMS host resolves\n\n\nWarn user when switching authorization methods in \n/etc/lcmaps.db\n\n\n\n\n\n\nosg-system-profiler 1.4.0: detect unconfigured trustmanager, no longer create profile in-place\n\n\ngridFTP-HDFS 0.5.4: fixed ability to list/remove empty directories\n\n\ncvmfs-config-osg 1.2.5: use new CVMFS fallback policies\n\n\nbigtop-utils 0.6.0+258-1.cdh4.7.1.p0.13.1: Fix default \nJAVA_HOME\n to prevent crash in hdfs utils\n\n\nosg-voms 3.3-3: Remove voms-admin (EL7 only)\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nKnown Issues\n\n\n\n\ncondor_ce_q\n does not show any jobs when run as root with \ncondor-8.5.5\n from upcoming. Work around this by using \ncondor_ce_q -allusers\n instead.\n\n\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nbigtop-utils-0.6.0+248-1.cdh4.7.1.p0.13.1.osg33.el6\n\n\nblahp-1.18.21.bosco-1.osg33.el6\n\n\ncondor-8.4.8-1.osg33.el6\n\n\nglideinwms-3.2.14.1-1.osg33.el6\n\n\nglobus-gridftp-server-7.20-1.2.osg33.el6\n\n\ngridftp-hdfs-0.5.4-25.1.osg33.el6\n\n\nhtcondor-ce-2.0.7-1.osg33.el6\n\n\nigtf-ca-certs-1.75-1.osg33.el6\n\n\nosg-build-1.6.4-1.osg33.el6\n\n\nosg-ca-certs-1.56-1.osg33.el6\n\n\nosg-ce-3.3-7.osg33.el6\n\n\nosg-configure-1.4.1-1.osg33.el6\n\n\nosg-system-profiler-1.4.0-1.osg33.el6\n\n\nosg-test-1.8.2-1.osg33.el6\n\n\nosg-version-3.3.14-1.osg33.el6\n\n\nosg-voms-3.3-3.osg33.el6\n\n\nxrootd-voms-plugin-0.4.0-1.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nbigtop-utils-0.6.0+248-1.cdh4.7.1.p0.13.1.osg33.el7\n\n\nblahp-1.18.21.bosco-1.osg33.el7\n\n\ncondor-8.4.8-1.osg33.el7\n\n\nglideinwms-3.2.14.1-1.osg33.el7\n\n\nglobus-gridftp-server-7.20-1.2.osg33.el7\n\n\ngridftp-hdfs-0.5.4-25.1.osg33.el7\n\n\nhtcondor-ce-2.0.7-1.osg33.el7\n\n\nigtf-ca-certs-1.75-1.osg33.el7\n\n\nosg-build-1.6.4-1.osg33.el7\n\n\nosg-ca-certs-1.56-1.osg33.el7\n\n\nosg-ce-3.3-7_clipped.osg33.el7\n\n\nosg-configure-1.4.1-1.osg33.el7\n\n\nosg-system-profiler-1.4.0-1.osg33.el7\n\n\nosg-test-1.8.2-1.osg33.el7\n\n\nosg-version-3.3.14-1.osg33.el7\n\n\nosg-voms-3.3-3.osg33.el7\n\n\nxrootd-voms-plugin-0.4.0-1.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nbigtop-utils blahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone globus-gridftp-server globus-gridftp-server-debuginfo globus-gridftp-server-devel globus-gridftp-server-progs gridftp-hdfs gridftp-hdfs-debuginfo htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-view igtf-ca-certs osg-base-ce osg-base-ce-bosco osg-base-ce-condor osg-base-ce-lsf osg-base-ce-pbs osg-base-ce-sge osg-base-ce-slurm osg-build osg-ca-certs osg-ce osg-ce-bosco osg-ce-condor osg-ce-lsf osg-ce-pbs osg-ce-sge osg-ce-slurm osg-configure osg-configure-bosco osg-configure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-htcondor-ce osg-htcondor-ce-bosco osg-htcondor-ce-condor osg-htcondor-ce-lsf osg-htcondor-ce-pbs osg-htcondor-ce-sge osg-htcondor-ce-slurm osg-system-profiler osg-system-profiler-viewer osg-test osg-version osg-voms xrootd-voms-plugin xrootd-voms-plugin-debuginfo xrootd-voms-plugin-devel\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nbigtop-utils-0.6.0+248-1.cdh4.7.1.p0.13.1.osg33.el6\nblahp-1.18.21.bosco-1.osg33.el6\nblahp-debuginfo-1.18.21.bosco-1.osg33.el6\ncondor-8.4.8-1.osg33.el6\ncondor-all-8.4.8-1.osg33.el6\ncondor-bosco-8.4.8-1.osg33.el6\ncondor-classads-8.4.8-1.osg33.el6\ncondor-classads-devel-8.4.8-1.osg33.el6\ncondor-cream-gahp-8.4.8-1.osg33.el6\ncondor-debuginfo-8.4.8-1.osg33.el6\ncondor-kbdd-8.4.8-1.osg33.el6\ncondor-procd-8.4.8-1.osg33.el6\ncondor-python-8.4.8-1.osg33.el6\ncondor-std-universe-8.4.8-1.osg33.el6\ncondor-test-8.4.8-1.osg33.el6\ncondor-vm-gahp-8.4.8-1.osg33.el6\nglideinwms-3.2.14.1-1.osg33.el6\nglideinwms-common-tools-3.2.14.1-1.osg33.el6\nglideinwms-condor-common-config-3.2.14.1-1.osg33.el6\nglideinwms-factory-3.2.14.1-1.osg33.el6\nglideinwms-factory-condor-3.2.14.1-1.osg33.el6\nglideinwms-glidecondor-tools-3.2.14.1-1.osg33.el6\nglideinwms-libs-3.2.14.1-1.osg33.el6\nglideinwms-minimal-condor-3.2.14.1-1.osg33.el6\nglideinwms-usercollector-3.2.14.1-1.osg33.el6\nglideinwms-userschedd-3.2.14.1-1.osg33.el6\nglideinwms-vofrontend-3.2.14.1-1.osg33.el6\nglideinwms-vofrontend-standalone-3.2.14.1-1.osg33.el6\nglobus-gridftp-server-7.20-1.2.osg33.el6\nglobus-gridftp-server-debuginfo-7.20-1.2.osg33.el6\nglobus-gridftp-server-devel-7.20-1.2.osg33.el6\nglobus-gridftp-server-progs-7.20-1.2.osg33.el6\ngridftp-hdfs-0.5.4-25.1.osg33.el6\ngridftp-hdfs-debuginfo-0.5.4-25.1.osg33.el6\nhtcondor-ce-2.0.7-1.osg33.el6\nhtcondor-ce-bosco-2.0.7-1.osg33.el6\nhtcondor-ce-client-2.0.7-1.osg33.el6\nhtcondor-ce-collector-2.0.7-1.osg33.el6\nhtcondor-ce-condor-2.0.7-1.osg33.el6\nhtcondor-ce-lsf-2.0.7-1.osg33.el6\nhtcondor-ce-pbs-2.0.7-1.osg33.el6\nhtcondor-ce-sge-2.0.7-1.osg33.el6\nhtcondor-ce-view-2.0.7-1.osg33.el6\nigtf-ca-certs-1.75-1.osg33.el6\nosg-base-ce-3.3-7.osg33.el6\nosg-base-ce-bosco-3.3-7.osg33.el6\nosg-base-ce-condor-3.3-7.osg33.el6\nosg-base-ce-lsf-3.3-7.osg33.el6\nosg-base-ce-pbs-3.3-7.osg33.el6\nosg-base-ce-sge-3.3-7.osg33.el6\nosg-base-ce-slurm-3.3-7.osg33.el6\nosg-build-1.6.4-1.osg33.el6\nosg-ca-certs-1.56-1.osg33.el6\nosg-ce-3.3-7.osg33.el6\nosg-ce-bosco-3.3-7.osg33.el6\nosg-ce-condor-3.3-7.osg33.el6\nosg-ce-lsf-3.3-7.osg33.el6\nosg-ce-pbs-3.3-7.osg33.el6\nosg-ce-sge-3.3-7.osg33.el6\nosg-ce-slurm-3.3-7.osg33.el6\nosg-configure-1.4.1-1.osg33.el6\nosg-configure-bosco-1.4.1-1.osg33.el6\nosg-configure-ce-1.4.1-1.osg33.el6\nosg-configure-cemon-1.4.1-1.osg33.el6\nosg-configure-condor-1.4.1-1.osg33.el6\nosg-configure-gateway-1.4.1-1.osg33.el6\nosg-configure-gip-1.4.1-1.osg33.el6\nosg-configure-gratia-1.4.1-1.osg33.el6\nosg-configure-infoservices-1.4.1-1.osg33.el6\nosg-configure-lsf-1.4.1-1.osg33.el6\nosg-configure-managedfork-1.4.1-1.osg33.el6\nosg-configure-misc-1.4.1-1.osg33.el6\nosg-configure-monalisa-1.4.1-1.osg33.el6\nosg-configure-network-1.4.1-1.osg33.el6\nosg-configure-pbs-1.4.1-1.osg33.el6\nosg-configure-rsv-1.4.1-1.osg33.el6\nosg-configure-sge-1.4.1-1.osg33.el6\nosg-configure-slurm-1.4.1-1.osg33.el6\nosg-configure-squid-1.4.1-1.osg33.el6\nosg-configure-tests-1.4.1-1.osg33.el6\nosg-htcondor-ce-3.3-7.osg33.el6\nosg-htcondor-ce-bosco-3.3-7.osg33.el6\nosg-htcondor-ce-condor-3.3-7.osg33.el6\nosg-htcondor-ce-lsf-3.3-7.osg33.el6\nosg-htcondor-ce-pbs-3.3-7.osg33.el6\nosg-htcondor-ce-sge-3.3-7.osg33.el6\nosg-htcondor-ce-slurm-3.3-7.osg33.el6\nosg-system-profiler-1.4.0-1.osg33.el6\nosg-system-profiler-viewer-1.4.0-1.osg33.el6\nosg-test-1.8.2-1.osg33.el6\nosg-version-3.3.14-1.osg33.el6\nosg-voms-3.3-3.osg33.el6\nxrootd-voms-plugin-0.4.0-1.osg33.el6\nxrootd-voms-plugin-debuginfo-0.4.0-1.osg33.el6\nxrootd-voms-plugin-devel-0.4.0-1.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nbigtop-utils-0.6.0+248-1.cdh4.7.1.p0.13.1.osg33.el7\nblahp-1.18.21.bosco-1.osg33.el7\nblahp-debuginfo-1.18.21.bosco-1.osg33.el7\ncondor-8.4.8-1.osg33.el7\ncondor-all-8.4.8-1.osg33.el7\ncondor-bosco-8.4.8-1.osg33.el7\ncondor-classads-8.4.8-1.osg33.el7\ncondor-classads-devel-8.4.8-1.osg33.el7\ncondor-debuginfo-8.4.8-1.osg33.el7\ncondor-kbdd-8.4.8-1.osg33.el7\ncondor-procd-8.4.8-1.osg33.el7\ncondor-python-8.4.8-1.osg33.el7\ncondor-test-8.4.8-1.osg33.el7\ncondor-vm-gahp-8.4.8-1.osg33.el7\nglideinwms-3.2.14.1-1.osg33.el7\nglideinwms-common-tools-3.2.14.1-1.osg33.el7\nglideinwms-condor-common-config-3.2.14.1-1.osg33.el7\nglideinwms-factory-3.2.14.1-1.osg33.el7\nglideinwms-factory-condor-3.2.14.1-1.osg33.el7\nglideinwms-glidecondor-tools-3.2.14.1-1.osg33.el7\nglideinwms-libs-3.2.14.1-1.osg33.el7\nglideinwms-minimal-condor-3.2.14.1-1.osg33.el7\nglideinwms-usercollector-3.2.14.1-1.osg33.el7\nglideinwms-userschedd-3.2.14.1-1.osg33.el7\nglideinwms-vofrontend-3.2.14.1-1.osg33.el7\nglideinwms-vofrontend-standalone-3.2.14.1-1.osg33.el7\nglobus-gridftp-server-7.20-1.2.osg33.el7\nglobus-gridftp-server-debuginfo-7.20-1.2.osg33.el7\nglobus-gridftp-server-devel-7.20-1.2.osg33.el7\nglobus-gridftp-server-progs-7.20-1.2.osg33.el7\ngridftp-hdfs-0.5.4-25.1.osg33.el7\ngridftp-hdfs-debuginfo-0.5.4-25.1.osg33.el7\nhtcondor-ce-2.0.7-1.osg33.el7\nhtcondor-ce-bosco-2.0.7-1.osg33.el7\nhtcondor-ce-client-2.0.7-1.osg33.el7\nhtcondor-ce-collector-2.0.7-1.osg33.el7\nhtcondor-ce-condor-2.0.7-1.osg33.el7\nhtcondor-ce-lsf-2.0.7-1.osg33.el7\nhtcondor-ce-pbs-2.0.7-1.osg33.el7\nhtcondor-ce-sge-2.0.7-1.osg33.el7\nhtcondor-ce-view-2.0.7-1.osg33.el7\nigtf-ca-certs-1.75-1.osg33.el7\nosg-base-ce-3.3-7_clipped.osg33.el7\nosg-base-ce-bosco-3.3-7_clipped.osg33.el7\nosg-base-ce-condor-3.3-7_clipped.osg33.el7\nosg-base-ce-lsf-3.3-7_clipped.osg33.el7\nosg-base-ce-pbs-3.3-7_clipped.osg33.el7\nosg-base-ce-sge-3.3-7_clipped.osg33.el7\nosg-base-ce-slurm-3.3-7_clipped.osg33.el7\nosg-build-1.6.4-1.osg33.el7\nosg-ca-certs-1.56-1.osg33.el7\nosg-ce-3.3-7_clipped.osg33.el7\nosg-ce-bosco-3.3-7_clipped.osg33.el7\nosg-ce-condor-3.3-7_clipped.osg33.el7\nosg-ce-lsf-3.3-7_clipped.osg33.el7\nosg-ce-pbs-3.3-7_clipped.osg33.el7\nosg-ce-sge-3.3-7_clipped.osg33.el7\nosg-ce-slurm-3.3-7_clipped.osg33.el7\nosg-configure-1.4.1-1.osg33.el7\nosg-configure-bosco-1.4.1-1.osg33.el7\nosg-configure-ce-1.4.1-1.osg33.el7\nosg-configure-cemon-1.4.1-1.osg33.el7\nosg-configure-condor-1.4.1-1.osg33.el7\nosg-configure-gateway-1.4.1-1.osg33.el7\nosg-configure-gip-1.4.1-1.osg33.el7\nosg-configure-gratia-1.4.1-1.osg33.el7\nosg-configure-infoservices-1.4.1-1.osg33.el7\nosg-configure-lsf-1.4.1-1.osg33.el7\nosg-configure-managedfork-1.4.1-1.osg33.el7\nosg-configure-misc-1.4.1-1.osg33.el7\nosg-configure-monalisa-1.4.1-1.osg33.el7\nosg-configure-network-1.4.1-1.osg33.el7\nosg-configure-pbs-1.4.1-1.osg33.el7\nosg-configure-rsv-1.4.1-1.osg33.el7\nosg-configure-sge-1.4.1-1.osg33.el7\nosg-configure-slurm-1.4.1-1.osg33.el7\nosg-configure-squid-1.4.1-1.osg33.el7\nosg-configure-tests-1.4.1-1.osg33.el7\nosg-htcondor-ce-3.3-7_clipped.osg33.el7\nosg-htcondor-ce-bosco-3.3-7_clipped.osg33.el7\nosg-htcondor-ce-condor-3.3-7_clipped.osg33.el7\nosg-htcondor-ce-lsf-3.3-7_clipped.osg33.el7\nosg-htcondor-ce-pbs-3.3-7_clipped.osg33.el7\nosg-htcondor-ce-sge-3.3-7_clipped.osg33.el7\nosg-htcondor-ce-slurm-3.3-7_clipped.osg33.el7\nosg-system-profiler-1.4.0-1.osg33.el7\nosg-system-profiler-viewer-1.4.0-1.osg33.el7\nosg-test-1.8.2-1.osg33.el7\nosg-version-3.3.14-1.osg33.el7\nosg-voms-3.3-3.osg33.el7\nxrootd-voms-plugin-0.4.0-1.osg33.el7\nxrootd-voms-plugin-debuginfo-0.4.0-1.osg33.el7\nxrootd-voms-plugin-devel-0.4.0-1.osg33.el7\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.21.bosco-1.osgup.el6\n\n\ncvmfs-config-osg-1.2-5.osgup.el6\n\n\nosg-oasis-7-3.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.21.bosco-1.osgup.el7\n\n\ncvmfs-config-osg-1.2-5.osgup.el7\n\n\nosg-oasis-7-3.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp blahp-debuginfo cvmfs-config-osg osg-oasis\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp-1.18.21.bosco-1.osgup.el6\nblahp-debuginfo-1.18.21.bosco-1.osgup.el6\ncvmfs-config-osg-1.2-5.osgup.el6\nosg-oasis-7-3.osgup.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp-1.18.21.bosco-1.osgup.el7\nblahp-debuginfo-1.18.21.bosco-1.osgup.el7\ncvmfs-config-osg-1.2-5.osgup.el7\nosg-oasis-7-3.osgup.el7", 
            "title": "OSG Release 3.3.14"
        }, 
        {
            "location": "/release/3.3/release-3-3-14/#osg-software-release-3314", 
            "text": "Release Date : 2016-07-12", 
            "title": "OSG Software Release 3.3.14"
        }, 
        {
            "location": "/release/3.3/release-3-3-14/#summary-of-changes", 
            "text": "This release contains:   CA Certificates based on  IGTF 1.75  HTCondor 8.4.8  containing the following fixes:  Fixed memory leak triggered by python bindings  Fixed a bug that could cause Bosco file transfers to fail  Fixed a bug that could cause the schedd to crash when using schedd cron jobs  condor_schedd now rejects jobs when owner has no account on the machine  Other bug fixes    GlideinWMS 3.2.14.1  HTCondor-CE 2.0.7: Added htcondor-ce-bosco sub-package  blahp 1.18.21: Slurm improvements  xrootd-voms-plugin 0.4.0: added support for 'all' group selection  gridFTP 7.20-1.2: adler32 support, fix deadlock  osg-configure 1.4.1  Set HTCondor-CE configuration for htcondor-ce-bosco  Do not overwrite custom bosco routes  Ensure that GUMS host resolves  Warn user when switching authorization methods in  /etc/lcmaps.db    osg-system-profiler 1.4.0: detect unconfigured trustmanager, no longer create profile in-place  gridFTP-HDFS 0.5.4: fixed ability to list/remove empty directories  cvmfs-config-osg 1.2.5: use new CVMFS fallback policies  bigtop-utils 0.6.0+258-1.cdh4.7.1.p0.13.1: Fix default  JAVA_HOME  to prevent crash in hdfs utils  osg-voms 3.3-3: Remove voms-admin (EL7 only)   These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-14/#known-issues", 
            "text": "condor_ce_q  does not show any jobs when run as root with  condor-8.5.5  from upcoming. Work around this by using  condor_ce_q -allusers  instead.", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.3/release-3-3-14/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-14/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-14/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-14/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-14/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-14/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-14/#enterprise-linux-6", 
            "text": "bigtop-utils-0.6.0+248-1.cdh4.7.1.p0.13.1.osg33.el6  blahp-1.18.21.bosco-1.osg33.el6  condor-8.4.8-1.osg33.el6  glideinwms-3.2.14.1-1.osg33.el6  globus-gridftp-server-7.20-1.2.osg33.el6  gridftp-hdfs-0.5.4-25.1.osg33.el6  htcondor-ce-2.0.7-1.osg33.el6  igtf-ca-certs-1.75-1.osg33.el6  osg-build-1.6.4-1.osg33.el6  osg-ca-certs-1.56-1.osg33.el6  osg-ce-3.3-7.osg33.el6  osg-configure-1.4.1-1.osg33.el6  osg-system-profiler-1.4.0-1.osg33.el6  osg-test-1.8.2-1.osg33.el6  osg-version-3.3.14-1.osg33.el6  osg-voms-3.3-3.osg33.el6  xrootd-voms-plugin-0.4.0-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-14/#enterprise-linux-7", 
            "text": "bigtop-utils-0.6.0+248-1.cdh4.7.1.p0.13.1.osg33.el7  blahp-1.18.21.bosco-1.osg33.el7  condor-8.4.8-1.osg33.el7  glideinwms-3.2.14.1-1.osg33.el7  globus-gridftp-server-7.20-1.2.osg33.el7  gridftp-hdfs-0.5.4-25.1.osg33.el7  htcondor-ce-2.0.7-1.osg33.el7  igtf-ca-certs-1.75-1.osg33.el7  osg-build-1.6.4-1.osg33.el7  osg-ca-certs-1.56-1.osg33.el7  osg-ce-3.3-7_clipped.osg33.el7  osg-configure-1.4.1-1.osg33.el7  osg-system-profiler-1.4.0-1.osg33.el7  osg-test-1.8.2-1.osg33.el7  osg-version-3.3.14-1.osg33.el7  osg-voms-3.3-3.osg33.el7  xrootd-voms-plugin-0.4.0-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-14/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  bigtop-utils blahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone globus-gridftp-server globus-gridftp-server-debuginfo globus-gridftp-server-devel globus-gridftp-server-progs gridftp-hdfs gridftp-hdfs-debuginfo htcondor-ce htcondor-ce-bosco htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-view igtf-ca-certs osg-base-ce osg-base-ce-bosco osg-base-ce-condor osg-base-ce-lsf osg-base-ce-pbs osg-base-ce-sge osg-base-ce-slurm osg-build osg-ca-certs osg-ce osg-ce-bosco osg-ce-condor osg-ce-lsf osg-ce-pbs osg-ce-sge osg-ce-slurm osg-configure osg-configure-bosco osg-configure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-htcondor-ce osg-htcondor-ce-bosco osg-htcondor-ce-condor osg-htcondor-ce-lsf osg-htcondor-ce-pbs osg-htcondor-ce-sge osg-htcondor-ce-slurm osg-system-profiler osg-system-profiler-viewer osg-test osg-version osg-voms xrootd-voms-plugin xrootd-voms-plugin-debuginfo xrootd-voms-plugin-devel  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-14/#enterprise-linux-6_1", 
            "text": "bigtop-utils-0.6.0+248-1.cdh4.7.1.p0.13.1.osg33.el6\nblahp-1.18.21.bosco-1.osg33.el6\nblahp-debuginfo-1.18.21.bosco-1.osg33.el6\ncondor-8.4.8-1.osg33.el6\ncondor-all-8.4.8-1.osg33.el6\ncondor-bosco-8.4.8-1.osg33.el6\ncondor-classads-8.4.8-1.osg33.el6\ncondor-classads-devel-8.4.8-1.osg33.el6\ncondor-cream-gahp-8.4.8-1.osg33.el6\ncondor-debuginfo-8.4.8-1.osg33.el6\ncondor-kbdd-8.4.8-1.osg33.el6\ncondor-procd-8.4.8-1.osg33.el6\ncondor-python-8.4.8-1.osg33.el6\ncondor-std-universe-8.4.8-1.osg33.el6\ncondor-test-8.4.8-1.osg33.el6\ncondor-vm-gahp-8.4.8-1.osg33.el6\nglideinwms-3.2.14.1-1.osg33.el6\nglideinwms-common-tools-3.2.14.1-1.osg33.el6\nglideinwms-condor-common-config-3.2.14.1-1.osg33.el6\nglideinwms-factory-3.2.14.1-1.osg33.el6\nglideinwms-factory-condor-3.2.14.1-1.osg33.el6\nglideinwms-glidecondor-tools-3.2.14.1-1.osg33.el6\nglideinwms-libs-3.2.14.1-1.osg33.el6\nglideinwms-minimal-condor-3.2.14.1-1.osg33.el6\nglideinwms-usercollector-3.2.14.1-1.osg33.el6\nglideinwms-userschedd-3.2.14.1-1.osg33.el6\nglideinwms-vofrontend-3.2.14.1-1.osg33.el6\nglideinwms-vofrontend-standalone-3.2.14.1-1.osg33.el6\nglobus-gridftp-server-7.20-1.2.osg33.el6\nglobus-gridftp-server-debuginfo-7.20-1.2.osg33.el6\nglobus-gridftp-server-devel-7.20-1.2.osg33.el6\nglobus-gridftp-server-progs-7.20-1.2.osg33.el6\ngridftp-hdfs-0.5.4-25.1.osg33.el6\ngridftp-hdfs-debuginfo-0.5.4-25.1.osg33.el6\nhtcondor-ce-2.0.7-1.osg33.el6\nhtcondor-ce-bosco-2.0.7-1.osg33.el6\nhtcondor-ce-client-2.0.7-1.osg33.el6\nhtcondor-ce-collector-2.0.7-1.osg33.el6\nhtcondor-ce-condor-2.0.7-1.osg33.el6\nhtcondor-ce-lsf-2.0.7-1.osg33.el6\nhtcondor-ce-pbs-2.0.7-1.osg33.el6\nhtcondor-ce-sge-2.0.7-1.osg33.el6\nhtcondor-ce-view-2.0.7-1.osg33.el6\nigtf-ca-certs-1.75-1.osg33.el6\nosg-base-ce-3.3-7.osg33.el6\nosg-base-ce-bosco-3.3-7.osg33.el6\nosg-base-ce-condor-3.3-7.osg33.el6\nosg-base-ce-lsf-3.3-7.osg33.el6\nosg-base-ce-pbs-3.3-7.osg33.el6\nosg-base-ce-sge-3.3-7.osg33.el6\nosg-base-ce-slurm-3.3-7.osg33.el6\nosg-build-1.6.4-1.osg33.el6\nosg-ca-certs-1.56-1.osg33.el6\nosg-ce-3.3-7.osg33.el6\nosg-ce-bosco-3.3-7.osg33.el6\nosg-ce-condor-3.3-7.osg33.el6\nosg-ce-lsf-3.3-7.osg33.el6\nosg-ce-pbs-3.3-7.osg33.el6\nosg-ce-sge-3.3-7.osg33.el6\nosg-ce-slurm-3.3-7.osg33.el6\nosg-configure-1.4.1-1.osg33.el6\nosg-configure-bosco-1.4.1-1.osg33.el6\nosg-configure-ce-1.4.1-1.osg33.el6\nosg-configure-cemon-1.4.1-1.osg33.el6\nosg-configure-condor-1.4.1-1.osg33.el6\nosg-configure-gateway-1.4.1-1.osg33.el6\nosg-configure-gip-1.4.1-1.osg33.el6\nosg-configure-gratia-1.4.1-1.osg33.el6\nosg-configure-infoservices-1.4.1-1.osg33.el6\nosg-configure-lsf-1.4.1-1.osg33.el6\nosg-configure-managedfork-1.4.1-1.osg33.el6\nosg-configure-misc-1.4.1-1.osg33.el6\nosg-configure-monalisa-1.4.1-1.osg33.el6\nosg-configure-network-1.4.1-1.osg33.el6\nosg-configure-pbs-1.4.1-1.osg33.el6\nosg-configure-rsv-1.4.1-1.osg33.el6\nosg-configure-sge-1.4.1-1.osg33.el6\nosg-configure-slurm-1.4.1-1.osg33.el6\nosg-configure-squid-1.4.1-1.osg33.el6\nosg-configure-tests-1.4.1-1.osg33.el6\nosg-htcondor-ce-3.3-7.osg33.el6\nosg-htcondor-ce-bosco-3.3-7.osg33.el6\nosg-htcondor-ce-condor-3.3-7.osg33.el6\nosg-htcondor-ce-lsf-3.3-7.osg33.el6\nosg-htcondor-ce-pbs-3.3-7.osg33.el6\nosg-htcondor-ce-sge-3.3-7.osg33.el6\nosg-htcondor-ce-slurm-3.3-7.osg33.el6\nosg-system-profiler-1.4.0-1.osg33.el6\nosg-system-profiler-viewer-1.4.0-1.osg33.el6\nosg-test-1.8.2-1.osg33.el6\nosg-version-3.3.14-1.osg33.el6\nosg-voms-3.3-3.osg33.el6\nxrootd-voms-plugin-0.4.0-1.osg33.el6\nxrootd-voms-plugin-debuginfo-0.4.0-1.osg33.el6\nxrootd-voms-plugin-devel-0.4.0-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-14/#enterprise-linux-7_1", 
            "text": "bigtop-utils-0.6.0+248-1.cdh4.7.1.p0.13.1.osg33.el7\nblahp-1.18.21.bosco-1.osg33.el7\nblahp-debuginfo-1.18.21.bosco-1.osg33.el7\ncondor-8.4.8-1.osg33.el7\ncondor-all-8.4.8-1.osg33.el7\ncondor-bosco-8.4.8-1.osg33.el7\ncondor-classads-8.4.8-1.osg33.el7\ncondor-classads-devel-8.4.8-1.osg33.el7\ncondor-debuginfo-8.4.8-1.osg33.el7\ncondor-kbdd-8.4.8-1.osg33.el7\ncondor-procd-8.4.8-1.osg33.el7\ncondor-python-8.4.8-1.osg33.el7\ncondor-test-8.4.8-1.osg33.el7\ncondor-vm-gahp-8.4.8-1.osg33.el7\nglideinwms-3.2.14.1-1.osg33.el7\nglideinwms-common-tools-3.2.14.1-1.osg33.el7\nglideinwms-condor-common-config-3.2.14.1-1.osg33.el7\nglideinwms-factory-3.2.14.1-1.osg33.el7\nglideinwms-factory-condor-3.2.14.1-1.osg33.el7\nglideinwms-glidecondor-tools-3.2.14.1-1.osg33.el7\nglideinwms-libs-3.2.14.1-1.osg33.el7\nglideinwms-minimal-condor-3.2.14.1-1.osg33.el7\nglideinwms-usercollector-3.2.14.1-1.osg33.el7\nglideinwms-userschedd-3.2.14.1-1.osg33.el7\nglideinwms-vofrontend-3.2.14.1-1.osg33.el7\nglideinwms-vofrontend-standalone-3.2.14.1-1.osg33.el7\nglobus-gridftp-server-7.20-1.2.osg33.el7\nglobus-gridftp-server-debuginfo-7.20-1.2.osg33.el7\nglobus-gridftp-server-devel-7.20-1.2.osg33.el7\nglobus-gridftp-server-progs-7.20-1.2.osg33.el7\ngridftp-hdfs-0.5.4-25.1.osg33.el7\ngridftp-hdfs-debuginfo-0.5.4-25.1.osg33.el7\nhtcondor-ce-2.0.7-1.osg33.el7\nhtcondor-ce-bosco-2.0.7-1.osg33.el7\nhtcondor-ce-client-2.0.7-1.osg33.el7\nhtcondor-ce-collector-2.0.7-1.osg33.el7\nhtcondor-ce-condor-2.0.7-1.osg33.el7\nhtcondor-ce-lsf-2.0.7-1.osg33.el7\nhtcondor-ce-pbs-2.0.7-1.osg33.el7\nhtcondor-ce-sge-2.0.7-1.osg33.el7\nhtcondor-ce-view-2.0.7-1.osg33.el7\nigtf-ca-certs-1.75-1.osg33.el7\nosg-base-ce-3.3-7_clipped.osg33.el7\nosg-base-ce-bosco-3.3-7_clipped.osg33.el7\nosg-base-ce-condor-3.3-7_clipped.osg33.el7\nosg-base-ce-lsf-3.3-7_clipped.osg33.el7\nosg-base-ce-pbs-3.3-7_clipped.osg33.el7\nosg-base-ce-sge-3.3-7_clipped.osg33.el7\nosg-base-ce-slurm-3.3-7_clipped.osg33.el7\nosg-build-1.6.4-1.osg33.el7\nosg-ca-certs-1.56-1.osg33.el7\nosg-ce-3.3-7_clipped.osg33.el7\nosg-ce-bosco-3.3-7_clipped.osg33.el7\nosg-ce-condor-3.3-7_clipped.osg33.el7\nosg-ce-lsf-3.3-7_clipped.osg33.el7\nosg-ce-pbs-3.3-7_clipped.osg33.el7\nosg-ce-sge-3.3-7_clipped.osg33.el7\nosg-ce-slurm-3.3-7_clipped.osg33.el7\nosg-configure-1.4.1-1.osg33.el7\nosg-configure-bosco-1.4.1-1.osg33.el7\nosg-configure-ce-1.4.1-1.osg33.el7\nosg-configure-cemon-1.4.1-1.osg33.el7\nosg-configure-condor-1.4.1-1.osg33.el7\nosg-configure-gateway-1.4.1-1.osg33.el7\nosg-configure-gip-1.4.1-1.osg33.el7\nosg-configure-gratia-1.4.1-1.osg33.el7\nosg-configure-infoservices-1.4.1-1.osg33.el7\nosg-configure-lsf-1.4.1-1.osg33.el7\nosg-configure-managedfork-1.4.1-1.osg33.el7\nosg-configure-misc-1.4.1-1.osg33.el7\nosg-configure-monalisa-1.4.1-1.osg33.el7\nosg-configure-network-1.4.1-1.osg33.el7\nosg-configure-pbs-1.4.1-1.osg33.el7\nosg-configure-rsv-1.4.1-1.osg33.el7\nosg-configure-sge-1.4.1-1.osg33.el7\nosg-configure-slurm-1.4.1-1.osg33.el7\nosg-configure-squid-1.4.1-1.osg33.el7\nosg-configure-tests-1.4.1-1.osg33.el7\nosg-htcondor-ce-3.3-7_clipped.osg33.el7\nosg-htcondor-ce-bosco-3.3-7_clipped.osg33.el7\nosg-htcondor-ce-condor-3.3-7_clipped.osg33.el7\nosg-htcondor-ce-lsf-3.3-7_clipped.osg33.el7\nosg-htcondor-ce-pbs-3.3-7_clipped.osg33.el7\nosg-htcondor-ce-sge-3.3-7_clipped.osg33.el7\nosg-htcondor-ce-slurm-3.3-7_clipped.osg33.el7\nosg-system-profiler-1.4.0-1.osg33.el7\nosg-system-profiler-viewer-1.4.0-1.osg33.el7\nosg-test-1.8.2-1.osg33.el7\nosg-version-3.3.14-1.osg33.el7\nosg-voms-3.3-3.osg33.el7\nxrootd-voms-plugin-0.4.0-1.osg33.el7\nxrootd-voms-plugin-debuginfo-0.4.0-1.osg33.el7\nxrootd-voms-plugin-devel-0.4.0-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-14/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-14/#enterprise-linux-6_2", 
            "text": "blahp-1.18.21.bosco-1.osgup.el6  cvmfs-config-osg-1.2-5.osgup.el6  osg-oasis-7-3.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-14/#enterprise-linux-7_2", 
            "text": "blahp-1.18.21.bosco-1.osgup.el7  cvmfs-config-osg-1.2-5.osgup.el7  osg-oasis-7-3.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-14/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp blahp-debuginfo cvmfs-config-osg osg-oasis  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-14/#enterprise-linux-6_3", 
            "text": "blahp-1.18.21.bosco-1.osgup.el6\nblahp-debuginfo-1.18.21.bosco-1.osgup.el6\ncvmfs-config-osg-1.2-5.osgup.el6\nosg-oasis-7-3.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-14/#enterprise-linux-7_3", 
            "text": "blahp-1.18.21.bosco-1.osgup.el7\nblahp-debuginfo-1.18.21.bosco-1.osgup.el7\ncvmfs-config-osg-1.2-5.osgup.el7\nosg-oasis-7-3.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-13/", 
            "text": "OSG Software Release 3.3.13\n\n\nRelease Date\n: 2016-06-14\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nCA Certificates based on \nIGTF 1.74\n\n\nImprove buffering in gridFTP-HDFS and avoid deadlocks\n\n\nCMVFS 2.2.3\n: Bug fix for chunked files\n\n\nPatch jGlobus to reduce resource consumption, no need to restart BeStMan2 weekly.\n\n\nHTCondor 8.4.7\n: Avoid unresponsive schedd, Docker improvements, and other bug fixes.\n\n\nblahp\n\n\nSupport dynamic assignment of environment variables for non-HTCondor batch systems\n\n\nSupport for multi-core jobs in HTCondor\n\n\n\n\n\n\nRSV support for CREAM and NorduGrid\n\n\nGUMS on EL7 platforms\n\n\nBeStMan\n\n\nSupport for \"sudoCommand=sudo -i\" fixed\n\n\nLog the correct GUMS host DN\n\n\nCompress rotated log files\n\n\nWarn about unexpected change in ownership when installing\n\n\n\n\n\n\nImprove efficiency of HTCondor configuration queries in GIP\n\n\nUpdate lcmaps-plugins-verify-proxy: accept base certificates generated by any CILogin CA\n\n\ncondor_ce_trace now prints and logs exceptions\n\n\nHTCondor 8.5.5\n in the Upcoming repository\n\n\nCVMFS 2.3.0 in the upcoming repository\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nKnown Issues\n\n\n\n\nThe configuration file for GlideinWMS depends on a bug that was fixed in HTCondor version 8.4.7 (and 8.5.5 in upcoming). Unfortunately, this incompatibility was not discovered before HTCondor was released. This affects all GlideinWMS submit hosts running HTCondor 8.4.7 or 8.5.5. It does not affect the factory, it will affect the fronted only if it is used also to submit jobs. In the configuration file \n/etc/condor/config.d/02_gwms_schedds.config\n, the following lines:\\\n\n\n\n\nJobAdInformationAttrs\n \n=\n \n$(\nJOB_GLIDEIN_ATTRS\n)\n \n$(\nJOB_GLIDEIN_SITEWMS_ATTRS\n)\n \n\nSUBMIT_EXPRS\n \n=\n \n$(\nSUBMIT_EXPRS\n)\n \n$(\nJOB_GLIDEIN_ATTRS\n)\n \n$(\nJOB_GLIDEIN_SITEWMS_ATTRS\n)\n JobAdInformationAttrs\n\n\n\n\n\n\\ Should be replaced by:\\\n\n\nif version \n= 8.4.6\n\n    \nJobAdInformationAttrs\n \n=\n \n$(\nJOB_GLIDEIN_ATTRS\n)\n \n$(\nJOB_GLIDEIN_SITEWMS_ATTRS\n)\n \n    \nSUBMIT_EXPRS\n \n=\n \n$(\nSUBMIT_EXPRS\n)\n \n$(\nJOB_GLIDEIN_ATTRS\n)\n \n$(\nJOB_GLIDEIN_SITEWMS_ATTRS\n)\n JobAdInformationAttrs\n\nelif\n \nversion\n \n=\n \n8.5.0\n\n\n    if version \n= 8.5.4\n\n        \nJobAdInformationAttrs\n \n=\n \n$(\nJOB_GLIDEIN_ATTRS\n)\n \n$(\nJOB_GLIDEIN_SITEWMS_ATTRS\n)\n \n        \nSUBMIT_EXPRS\n \n=\n \n$(\nSUBMIT_EXPRS\n)\n \n$(\nJOB_GLIDEIN_ATTRS\n)\n \n$(\nJOB_GLIDEIN_SITEWMS_ATTRS\n)\n JobAdInformationAttrs\n    \nelif\n \nversion\n \n==\n \n8\n.5.5\n        \nJobAdInformationAttrs\n \n=\n \n$(\nJOB_GLIDEIN_ATTRS\n)\n \n$(\nJOB_GLIDEIN_SITEWMS_ATTRS\n)\n \n        \nSUBMIT_EXPRS\n \n=\n \n$(\nSUBMIT_EXPRS\n)\n \n$(\nJOB_GLIDEIN_ATTRS\n)\n \n$(\nJOB_GLIDEIN_SITEWMS_ATTRS\n)\n +JobAdInformationAttrs\n\n    else\n\n        \nJobAdInformationAttrs\n \n=\n \n$(\nJOB_GLIDEIN_ATTRS\n)\n \n$(\nJOB_GLIDEIN_SITEWMS_ATTRS\n)\n \n        \nSUBMIT_EXPRS\n \n=\n \n$(\nSUBMIT_EXPRS\n)\n \n$(\nJOB_GLIDEIN_ATTRS\n)\n \n$(\nJOB_GLIDEIN_SITEWMS_ATTRS\n)\n JobAdInformationAttrs\n\n    endif\n\n\nelif\n \nversion\n \n==\n \n8\n.4.7\n    \nJobAdInformationAttrs\n \n=\n \n$(\nJOB_GLIDEIN_ATTRS\n)\n \n$(\nJOB_GLIDEIN_SITEWMS_ATTRS\n)\n \n    \nSUBMIT_EXPRS\n \n=\n \n$(\nSUBMIT_EXPRS\n)\n \n$(\nJOB_GLIDEIN_ATTRS\n)\n \n$(\nJOB_GLIDEIN_SITEWMS_ATTRS\n)\n +JobAdInformationAttrs\n\nelse\n\n    \nJobAdInformationAttrs\n \n=\n \n$(\nJOB_GLIDEIN_ATTRS\n)\n \n$(\nJOB_GLIDEIN_SITEWMS_ATTRS\n)\n \n    \nSUBMIT_EXPRS\n \n=\n \n$(\nSUBMIT_EXPRS\n)\n \n$(\nJOB_GLIDEIN_ATTRS\n)\n \n$(\nJOB_GLIDEIN_SITEWMS_ATTRS\n)\n JobAdInformationAttrs\n\nendif\n\n\n\n\n\n\n\n\ncondor_ce_q\n does not show any jobs when run as root with \ncondor-8.5.5\n from upcoming. Work around this by using \ncondor_ce_q -allusers\n instead.\n\n\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nbestman2-2.3.0.1-1.osg33.el6\n\n\nblahp-1.18.20.bosco-1.osg33.el6\n\n\ncondor-8.4.7-1.osg33.el6\n\n\ncondor-cron-1.1.0-1.osg33.el6\n\n\ncvmfs-2.2.3-1.osg33.el6\n\n\ngip-1.3.11-9.osg33.el6\n\n\nglobus-ftp-control-6.6-1.2.osg33.el6\n\n\ngratia-1.16.2-1.3.osg33.el6\n\n\ngsi-openssh-5.7-4.3.osg33.el6\n\n\ngums-1.5.2-3.osg33.el6\n\n\nhtcondor-ce-2.0.6-1.osg33.el6\n\n\nigtf-ca-certs-1.74-1.osg33.el6\n\n\njglobus-2.1.0-8.osg33.el6\n\n\nlcmaps-plugins-verify-proxy-1.5.9-1.osg33.el6\n\n\nosg-ca-certs-1.55-1.osg33.el6\n\n\nosg-ca-generator-1.1.0-1.osg33.el6\n\n\nosg-gums-3.3-3.osg33.el6\n\n\nosg-info-services-1.2.2-1.osg33.el6\n\n\nosg-oasis-6-5.osg33.el6\n\n\nosg-pki-tools-1.2.18-1.osg33.el6\n\n\nosg-test-1.8.1-1.osg33.el6\n\n\nosg-tested-internal-3.3-12.osg33.el6\n\n\nosg-version-3.3.13-1.osg33.el6\n\n\nrsv-3.13.0-3.osg33.el6\n\n\nxrootd-4.3.0-2.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nbestman2-2.3.0.1-1.osg33.el7\n\n\nblahp-1.18.20.bosco-1.osg33.el7\n\n\ncondor-8.4.7-1.osg33.el7\n\n\ncondor-cron-1.1.0-1.osg33.el7\n\n\ncvmfs-2.2.3-1.osg33.el7\n\n\ngip-1.3.11-9.osg33.el7\n\n\nglobus-ftp-control-6.6-1.2.osg33.el7\n\n\ngratia-1.16.2-1.3.osg33.el7\n\n\ngsi-openssh-5.7-4.3.osg33.el7\n\n\ngums-1.5.2-3.osg33.el7\n\n\nhtcondor-ce-2.0.6-1.osg33.el7\n\n\nigtf-ca-certs-1.74-1.osg33.el7\n\n\njglobus-2.1.0-8.osg33.el7\n\n\nlcmaps-plugins-verify-proxy-1.5.9-1.osg33.el7\n\n\nosg-ca-certs-1.55-1.osg33.el7\n\n\nosg-ca-generator-1.1.0-1.osg33.el7\n\n\nosg-gums-3.3-3.osg33.el7\n\n\nosg-info-services-1.2.2-1.osg33.el7\n\n\nosg-oasis-6-5.osg33.el7\n\n\nosg-pki-tools-1.2.18-1.osg33.el7\n\n\nosg-test-1.8.1-1.osg33.el7\n\n\nosg-tested-internal-3.3-12.osg33.el7\n\n\nosg-version-3.3.13-1.osg33.el7\n\n\nrsv-3.13.0-3.osg33.el7\n\n\nxrootd-4.3.0-2.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nbestman2-client bestman2-client-libs bestman2-common-libs bestman2-server bestman2-server-dep-libs bestman2-server-libs bestman2-tester bestman2-tester-libs blahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-cron condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp cvmfs cvmfs-devel cvmfs-server cvmfs-unittests gip globus-ftp-control globus-ftp-control-debuginfo globus-ftp-control-devel globus-ftp-control-doc gratia-debuginfo gratia-service gsi-openssh gsi-openssh-clients gsi-openssh-debuginfo gsi-openssh-server gums gums-client gums-service htcondor-ce htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-view igtf-ca-certs jglobus lcmaps-plugins-verify-proxy lcmaps-plugins-verify-proxy-debuginfo osg-ca-certs osg-ca-generator osg-gums osg-info-services osg-oasis osg-pki-tools osg-pki-tools-tests osg-test osg-tested-internal osg-version rsv rsv-consumers rsv-core rsv-metrics xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-libs xrootd-private-devel xrootd-python xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nbestman2-2.3.0.1-1.osg33.el6\nbestman2-client-2.3.0.1-1.osg33.el6\nbestman2-client-libs-2.3.0.1-1.osg33.el6\nbestman2-common-libs-2.3.0.1-1.osg33.el6\nbestman2-server-2.3.0.1-1.osg33.el6\nbestman2-server-dep-libs-2.3.0.1-1.osg33.el6\nbestman2-server-libs-2.3.0.1-1.osg33.el6\nbestman2-tester-2.3.0.1-1.osg33.el6\nbestman2-tester-libs-2.3.0.1-1.osg33.el6\nblahp-1.18.20.bosco-1.osg33.el6\nblahp-debuginfo-1.18.20.bosco-1.osg33.el6\ncondor-8.4.7-1.osg33.el6\ncondor-all-8.4.7-1.osg33.el6\ncondor-bosco-8.4.7-1.osg33.el6\ncondor-classads-8.4.7-1.osg33.el6\ncondor-classads-devel-8.4.7-1.osg33.el6\ncondor-cream-gahp-8.4.7-1.osg33.el6\ncondor-cron-1.1.0-1.osg33.el6\ncondor-debuginfo-8.4.7-1.osg33.el6\ncondor-kbdd-8.4.7-1.osg33.el6\ncondor-procd-8.4.7-1.osg33.el6\ncondor-python-8.4.7-1.osg33.el6\ncondor-std-universe-8.4.7-1.osg33.el6\ncondor-test-8.4.7-1.osg33.el6\ncondor-vm-gahp-8.4.7-1.osg33.el6\ncvmfs-2.2.3-1.osg33.el6\ncvmfs-devel-2.2.3-1.osg33.el6\ncvmfs-server-2.2.3-1.osg33.el6\ncvmfs-unittests-2.2.3-1.osg33.el6\ngip-1.3.11-9.osg33.el6\nglobus-ftp-control-6.6-1.2.osg33.el6\nglobus-ftp-control-debuginfo-6.6-1.2.osg33.el6\nglobus-ftp-control-devel-6.6-1.2.osg33.el6\nglobus-ftp-control-doc-6.6-1.2.osg33.el6\ngratia-1.16.2-1.3.osg33.el6\ngratia-debuginfo-1.16.2-1.3.osg33.el6\ngratia-service-1.16.2-1.3.osg33.el6\ngsi-openssh-5.7-4.3.osg33.el6\ngsi-openssh-clients-5.7-4.3.osg33.el6\ngsi-openssh-debuginfo-5.7-4.3.osg33.el6\ngsi-openssh-server-5.7-4.3.osg33.el6\ngums-1.5.2-3.osg33.el6\ngums-client-1.5.2-3.osg33.el6\ngums-service-1.5.2-3.osg33.el6\nhtcondor-ce-2.0.6-1.osg33.el6\nhtcondor-ce-client-2.0.6-1.osg33.el6\nhtcondor-ce-collector-2.0.6-1.osg33.el6\nhtcondor-ce-condor-2.0.6-1.osg33.el6\nhtcondor-ce-lsf-2.0.6-1.osg33.el6\nhtcondor-ce-pbs-2.0.6-1.osg33.el6\nhtcondor-ce-sge-2.0.6-1.osg33.el6\nhtcondor-ce-view-2.0.6-1.osg33.el6\nigtf-ca-certs-1.74-1.osg33.el6\njglobus-2.1.0-8.osg33.el6\nlcmaps-plugins-verify-proxy-1.5.9-1.osg33.el6\nlcmaps-plugins-verify-proxy-debuginfo-1.5.9-1.osg33.el6\nosg-ca-certs-1.55-1.osg33.el6\nosg-ca-generator-1.1.0-1.osg33.el6\nosg-gums-3.3-3.osg33.el6\nosg-info-services-1.2.2-1.osg33.el6\nosg-oasis-6-5.osg33.el6\nosg-pki-tools-1.2.18-1.osg33.el6\nosg-pki-tools-tests-1.2.18-1.osg33.el6\nosg-test-1.8.1-1.osg33.el6\nosg-tested-internal-3.3-12.osg33.el6\nosg-version-3.3.13-1.osg33.el6\nrsv-3.13.0-3.osg33.el6\nrsv-consumers-3.13.0-3.osg33.el6\nrsv-core-3.13.0-3.osg33.el6\nrsv-metrics-3.13.0-3.osg33.el6\nxrootd-4.3.0-2.osg33.el6\nxrootd-client-4.3.0-2.osg33.el6\nxrootd-client-devel-4.3.0-2.osg33.el6\nxrootd-client-libs-4.3.0-2.osg33.el6\nxrootd-debuginfo-4.3.0-2.osg33.el6\nxrootd-devel-4.3.0-2.osg33.el6\nxrootd-doc-4.3.0-2.osg33.el6\nxrootd-fuse-4.3.0-2.osg33.el6\nxrootd-libs-4.3.0-2.osg33.el6\nxrootd-private-devel-4.3.0-2.osg33.el6\nxrootd-python-4.3.0-2.osg33.el6\nxrootd-selinux-4.3.0-2.osg33.el6\nxrootd-server-4.3.0-2.osg33.el6\nxrootd-server-devel-4.3.0-2.osg33.el6\nxrootd-server-libs-4.3.0-2.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nbestman2-2.3.0.1-1.osg33.el7\nbestman2-client-2.3.0.1-1.osg33.el7\nbestman2-client-libs-2.3.0.1-1.osg33.el7\nbestman2-common-libs-2.3.0.1-1.osg33.el7\nbestman2-server-2.3.0.1-1.osg33.el7\nbestman2-server-dep-libs-2.3.0.1-1.osg33.el7\nbestman2-server-libs-2.3.0.1-1.osg33.el7\nbestman2-tester-2.3.0.1-1.osg33.el7\nbestman2-tester-libs-2.3.0.1-1.osg33.el7\nblahp-1.18.20.bosco-1.osg33.el7\nblahp-debuginfo-1.18.20.bosco-1.osg33.el7\ncondor-8.4.7-1.osg33.el7\ncondor-all-8.4.7-1.osg33.el7\ncondor-bosco-8.4.7-1.osg33.el7\ncondor-classads-8.4.7-1.osg33.el7\ncondor-classads-devel-8.4.7-1.osg33.el7\ncondor-cron-1.1.0-1.osg33.el7\ncondor-debuginfo-8.4.7-1.osg33.el7\ncondor-kbdd-8.4.7-1.osg33.el7\ncondor-procd-8.4.7-1.osg33.el7\ncondor-python-8.4.7-1.osg33.el7\ncondor-test-8.4.7-1.osg33.el7\ncondor-vm-gahp-8.4.7-1.osg33.el7\ncvmfs-2.2.3-1.osg33.el7\ncvmfs-devel-2.2.3-1.osg33.el7\ncvmfs-server-2.2.3-1.osg33.el7\ncvmfs-unittests-2.2.3-1.osg33.el7\ngip-1.3.11-9.osg33.el7\nglobus-ftp-control-6.6-1.2.osg33.el7\nglobus-ftp-control-debuginfo-6.6-1.2.osg33.el7\nglobus-ftp-control-devel-6.6-1.2.osg33.el7\nglobus-ftp-control-doc-6.6-1.2.osg33.el7\ngratia-1.16.2-1.3.osg33.el7\ngratia-debuginfo-1.16.2-1.3.osg33.el7\ngratia-service-1.16.2-1.3.osg33.el7\ngsi-openssh-5.7-4.3.osg33.el7\ngsi-openssh-clients-5.7-4.3.osg33.el7\ngsi-openssh-debuginfo-5.7-4.3.osg33.el7\ngsi-openssh-server-5.7-4.3.osg33.el7\ngums-1.5.2-3.osg33.el7\ngums-client-1.5.2-3.osg33.el7\ngums-service-1.5.2-3.osg33.el7\nhtcondor-ce-2.0.6-1.osg33.el7\nhtcondor-ce-client-2.0.6-1.osg33.el7\nhtcondor-ce-collector-2.0.6-1.osg33.el7\nhtcondor-ce-condor-2.0.6-1.osg33.el7\nhtcondor-ce-lsf-2.0.6-1.osg33.el7\nhtcondor-ce-pbs-2.0.6-1.osg33.el7\nhtcondor-ce-sge-2.0.6-1.osg33.el7\nhtcondor-ce-view-2.0.6-1.osg33.el7\nigtf-ca-certs-1.74-1.osg33.el7\njglobus-2.1.0-8.osg33.el7\nlcmaps-plugins-verify-proxy-1.5.9-1.osg33.el7\nlcmaps-plugins-verify-proxy-debuginfo-1.5.9-1.osg33.el7\nosg-ca-certs-1.55-1.osg33.el7\nosg-ca-generator-1.1.0-1.osg33.el7\nosg-gums-3.3-3.osg33.el7\nosg-info-services-1.2.2-1.osg33.el7\nosg-oasis-6-5.osg33.el7\nosg-pki-tools-1.2.18-1.osg33.el7\nosg-pki-tools-tests-1.2.18-1.osg33.el7\nosg-test-1.8.1-1.osg33.el7\nosg-tested-internal-3.3-12.osg33.el7\nosg-version-3.3.13-1.osg33.el7\nrsv-3.13.0-3.osg33.el7\nrsv-consumers-3.13.0-3.osg33.el7\nrsv-core-3.13.0-3.osg33.el7\nrsv-metrics-3.13.0-3.osg33.el7\nxrootd-4.3.0-2.osg33.el7\nxrootd-client-4.3.0-2.osg33.el7\nxrootd-client-devel-4.3.0-2.osg33.el7\nxrootd-client-libs-4.3.0-2.osg33.el7\nxrootd-debuginfo-4.3.0-2.osg33.el7\nxrootd-devel-4.3.0-2.osg33.el7\nxrootd-doc-4.3.0-2.osg33.el7\nxrootd-fuse-4.3.0-2.osg33.el7\nxrootd-libs-4.3.0-2.osg33.el7\nxrootd-private-devel-4.3.0-2.osg33.el7\nxrootd-python-4.3.0-2.osg33.el7\nxrootd-selinux-4.3.0-2.osg33.el7\nxrootd-server-4.3.0-2.osg33.el7\nxrootd-server-devel-4.3.0-2.osg33.el7\nxrootd-server-libs-4.3.0-2.osg33.el7\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.20.bosco-1.osgup.el6\n\n\ncondor-8.5.5-1.osgup.el6\n\n\ncvmfs-2.3.0-1.osgup.el6\n\n\ncvmfs-x509-helper-0.9-1.osgup.el6\n\n\nosg-oasis-7-1.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.20.bosco-1.osgup.el7\n\n\ncondor-8.5.5-1.osgup.el7\n\n\ncvmfs-2.3.0-1.osgup.el7\n\n\ncvmfs-x509-helper-0.9-1.osgup.el7\n\n\nosg-oasis-7-1.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp cvmfs cvmfs-devel cvmfs-server cvmfs-unittests cvmfs-x509-helper cvmfs-x509-helper-debuginfo osg-oasis\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp-1.18.20.bosco-1.osgup.el6\nblahp-debuginfo-1.18.20.bosco-1.osgup.el6\ncondor-8.5.5-1.osgup.el6\ncondor-all-8.5.5-1.osgup.el6\ncondor-bosco-8.5.5-1.osgup.el6\ncondor-classads-8.5.5-1.osgup.el6\ncondor-classads-devel-8.5.5-1.osgup.el6\ncondor-cream-gahp-8.5.5-1.osgup.el6\ncondor-debuginfo-8.5.5-1.osgup.el6\ncondor-kbdd-8.5.5-1.osgup.el6\ncondor-procd-8.5.5-1.osgup.el6\ncondor-python-8.5.5-1.osgup.el6\ncondor-std-universe-8.5.5-1.osgup.el6\ncondor-test-8.5.5-1.osgup.el6\ncondor-vm-gahp-8.5.5-1.osgup.el6\ncvmfs-2.3.0-1.osgup.el6\ncvmfs-devel-2.3.0-1.osgup.el6\ncvmfs-server-2.3.0-1.osgup.el6\ncvmfs-unittests-2.3.0-1.osgup.el6\ncvmfs-x509-helper-0.9-1.osgup.el6\ncvmfs-x509-helper-debuginfo-0.9-1.osgup.el6\nosg-oasis-7-1.osgup.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp-1.18.20.bosco-1.osgup.el7\nblahp-debuginfo-1.18.20.bosco-1.osgup.el7\ncondor-8.5.5-1.osgup.el7\ncondor-all-8.5.5-1.osgup.el7\ncondor-bosco-8.5.5-1.osgup.el7\ncondor-classads-8.5.5-1.osgup.el7\ncondor-classads-devel-8.5.5-1.osgup.el7\ncondor-debuginfo-8.5.5-1.osgup.el7\ncondor-kbdd-8.5.5-1.osgup.el7\ncondor-procd-8.5.5-1.osgup.el7\ncondor-python-8.5.5-1.osgup.el7\ncondor-test-8.5.5-1.osgup.el7\ncondor-vm-gahp-8.5.5-1.osgup.el7\ncvmfs-2.3.0-1.osgup.el7\ncvmfs-devel-2.3.0-1.osgup.el7\ncvmfs-server-2.3.0-1.osgup.el7\ncvmfs-unittests-2.3.0-1.osgup.el7\ncvmfs-x509-helper-0.9-1.osgup.el7\ncvmfs-x509-helper-debuginfo-0.9-1.osgup.el7\nosg-oasis-7-1.osgup.el7", 
            "title": "OSG Release 3.3.13"
        }, 
        {
            "location": "/release/3.3/release-3-3-13/#osg-software-release-3313", 
            "text": "Release Date : 2016-06-14", 
            "title": "OSG Software Release 3.3.13"
        }, 
        {
            "location": "/release/3.3/release-3-3-13/#summary-of-changes", 
            "text": "This release contains:   CA Certificates based on  IGTF 1.74  Improve buffering in gridFTP-HDFS and avoid deadlocks  CMVFS 2.2.3 : Bug fix for chunked files  Patch jGlobus to reduce resource consumption, no need to restart BeStMan2 weekly.  HTCondor 8.4.7 : Avoid unresponsive schedd, Docker improvements, and other bug fixes.  blahp  Support dynamic assignment of environment variables for non-HTCondor batch systems  Support for multi-core jobs in HTCondor    RSV support for CREAM and NorduGrid  GUMS on EL7 platforms  BeStMan  Support for \"sudoCommand=sudo -i\" fixed  Log the correct GUMS host DN  Compress rotated log files  Warn about unexpected change in ownership when installing    Improve efficiency of HTCondor configuration queries in GIP  Update lcmaps-plugins-verify-proxy: accept base certificates generated by any CILogin CA  condor_ce_trace now prints and logs exceptions  HTCondor 8.5.5  in the Upcoming repository  CVMFS 2.3.0 in the upcoming repository   These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-13/#known-issues", 
            "text": "The configuration file for GlideinWMS depends on a bug that was fixed in HTCondor version 8.4.7 (and 8.5.5 in upcoming). Unfortunately, this incompatibility was not discovered before HTCondor was released. This affects all GlideinWMS submit hosts running HTCondor 8.4.7 or 8.5.5. It does not affect the factory, it will affect the fronted only if it is used also to submit jobs. In the configuration file  /etc/condor/config.d/02_gwms_schedds.config , the following lines:\\   JobAdInformationAttrs   =   $( JOB_GLIDEIN_ATTRS )   $( JOB_GLIDEIN_SITEWMS_ATTRS )   SUBMIT_EXPRS   =   $( SUBMIT_EXPRS )   $( JOB_GLIDEIN_ATTRS )   $( JOB_GLIDEIN_SITEWMS_ATTRS )  JobAdInformationAttrs  \\ Should be replaced by:\\  if version  = 8.4.6 \n     JobAdInformationAttrs   =   $( JOB_GLIDEIN_ATTRS )   $( JOB_GLIDEIN_SITEWMS_ATTRS )  \n     SUBMIT_EXPRS   =   $( SUBMIT_EXPRS )   $( JOB_GLIDEIN_ATTRS )   $( JOB_GLIDEIN_SITEWMS_ATTRS )  JobAdInformationAttrs elif   version   =   8.5.0      if version  = 8.5.4 \n         JobAdInformationAttrs   =   $( JOB_GLIDEIN_ATTRS )   $( JOB_GLIDEIN_SITEWMS_ATTRS )  \n         SUBMIT_EXPRS   =   $( SUBMIT_EXPRS )   $( JOB_GLIDEIN_ATTRS )   $( JOB_GLIDEIN_SITEWMS_ATTRS )  JobAdInformationAttrs\n     elif   version   ==   8 .5.5\n         JobAdInformationAttrs   =   $( JOB_GLIDEIN_ATTRS )   $( JOB_GLIDEIN_SITEWMS_ATTRS )  \n         SUBMIT_EXPRS   =   $( SUBMIT_EXPRS )   $( JOB_GLIDEIN_ATTRS )   $( JOB_GLIDEIN_SITEWMS_ATTRS )  +JobAdInformationAttrs     else \n         JobAdInformationAttrs   =   $( JOB_GLIDEIN_ATTRS )   $( JOB_GLIDEIN_SITEWMS_ATTRS )  \n         SUBMIT_EXPRS   =   $( SUBMIT_EXPRS )   $( JOB_GLIDEIN_ATTRS )   $( JOB_GLIDEIN_SITEWMS_ATTRS )  JobAdInformationAttrs     endif  elif   version   ==   8 .4.7\n     JobAdInformationAttrs   =   $( JOB_GLIDEIN_ATTRS )   $( JOB_GLIDEIN_SITEWMS_ATTRS )  \n     SUBMIT_EXPRS   =   $( SUBMIT_EXPRS )   $( JOB_GLIDEIN_ATTRS )   $( JOB_GLIDEIN_SITEWMS_ATTRS )  +JobAdInformationAttrs else \n     JobAdInformationAttrs   =   $( JOB_GLIDEIN_ATTRS )   $( JOB_GLIDEIN_SITEWMS_ATTRS )  \n     SUBMIT_EXPRS   =   $( SUBMIT_EXPRS )   $( JOB_GLIDEIN_ATTRS )   $( JOB_GLIDEIN_SITEWMS_ATTRS )  JobAdInformationAttrs endif    condor_ce_q  does not show any jobs when run as root with  condor-8.5.5  from upcoming. Work around this by using  condor_ce_q -allusers  instead.", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.3/release-3-3-13/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-13/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-13/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-13/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-13/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-13/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-13/#enterprise-linux-6", 
            "text": "bestman2-2.3.0.1-1.osg33.el6  blahp-1.18.20.bosco-1.osg33.el6  condor-8.4.7-1.osg33.el6  condor-cron-1.1.0-1.osg33.el6  cvmfs-2.2.3-1.osg33.el6  gip-1.3.11-9.osg33.el6  globus-ftp-control-6.6-1.2.osg33.el6  gratia-1.16.2-1.3.osg33.el6  gsi-openssh-5.7-4.3.osg33.el6  gums-1.5.2-3.osg33.el6  htcondor-ce-2.0.6-1.osg33.el6  igtf-ca-certs-1.74-1.osg33.el6  jglobus-2.1.0-8.osg33.el6  lcmaps-plugins-verify-proxy-1.5.9-1.osg33.el6  osg-ca-certs-1.55-1.osg33.el6  osg-ca-generator-1.1.0-1.osg33.el6  osg-gums-3.3-3.osg33.el6  osg-info-services-1.2.2-1.osg33.el6  osg-oasis-6-5.osg33.el6  osg-pki-tools-1.2.18-1.osg33.el6  osg-test-1.8.1-1.osg33.el6  osg-tested-internal-3.3-12.osg33.el6  osg-version-3.3.13-1.osg33.el6  rsv-3.13.0-3.osg33.el6  xrootd-4.3.0-2.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-13/#enterprise-linux-7", 
            "text": "bestman2-2.3.0.1-1.osg33.el7  blahp-1.18.20.bosco-1.osg33.el7  condor-8.4.7-1.osg33.el7  condor-cron-1.1.0-1.osg33.el7  cvmfs-2.2.3-1.osg33.el7  gip-1.3.11-9.osg33.el7  globus-ftp-control-6.6-1.2.osg33.el7  gratia-1.16.2-1.3.osg33.el7  gsi-openssh-5.7-4.3.osg33.el7  gums-1.5.2-3.osg33.el7  htcondor-ce-2.0.6-1.osg33.el7  igtf-ca-certs-1.74-1.osg33.el7  jglobus-2.1.0-8.osg33.el7  lcmaps-plugins-verify-proxy-1.5.9-1.osg33.el7  osg-ca-certs-1.55-1.osg33.el7  osg-ca-generator-1.1.0-1.osg33.el7  osg-gums-3.3-3.osg33.el7  osg-info-services-1.2.2-1.osg33.el7  osg-oasis-6-5.osg33.el7  osg-pki-tools-1.2.18-1.osg33.el7  osg-test-1.8.1-1.osg33.el7  osg-tested-internal-3.3-12.osg33.el7  osg-version-3.3.13-1.osg33.el7  rsv-3.13.0-3.osg33.el7  xrootd-4.3.0-2.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-13/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  bestman2-client bestman2-client-libs bestman2-common-libs bestman2-server bestman2-server-dep-libs bestman2-server-libs bestman2-tester bestman2-tester-libs blahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-cron condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp cvmfs cvmfs-devel cvmfs-server cvmfs-unittests gip globus-ftp-control globus-ftp-control-debuginfo globus-ftp-control-devel globus-ftp-control-doc gratia-debuginfo gratia-service gsi-openssh gsi-openssh-clients gsi-openssh-debuginfo gsi-openssh-server gums gums-client gums-service htcondor-ce htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-view igtf-ca-certs jglobus lcmaps-plugins-verify-proxy lcmaps-plugins-verify-proxy-debuginfo osg-ca-certs osg-ca-generator osg-gums osg-info-services osg-oasis osg-pki-tools osg-pki-tools-tests osg-test osg-tested-internal osg-version rsv rsv-consumers rsv-core rsv-metrics xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-libs xrootd-private-devel xrootd-python xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-13/#enterprise-linux-6_1", 
            "text": "bestman2-2.3.0.1-1.osg33.el6\nbestman2-client-2.3.0.1-1.osg33.el6\nbestman2-client-libs-2.3.0.1-1.osg33.el6\nbestman2-common-libs-2.3.0.1-1.osg33.el6\nbestman2-server-2.3.0.1-1.osg33.el6\nbestman2-server-dep-libs-2.3.0.1-1.osg33.el6\nbestman2-server-libs-2.3.0.1-1.osg33.el6\nbestman2-tester-2.3.0.1-1.osg33.el6\nbestman2-tester-libs-2.3.0.1-1.osg33.el6\nblahp-1.18.20.bosco-1.osg33.el6\nblahp-debuginfo-1.18.20.bosco-1.osg33.el6\ncondor-8.4.7-1.osg33.el6\ncondor-all-8.4.7-1.osg33.el6\ncondor-bosco-8.4.7-1.osg33.el6\ncondor-classads-8.4.7-1.osg33.el6\ncondor-classads-devel-8.4.7-1.osg33.el6\ncondor-cream-gahp-8.4.7-1.osg33.el6\ncondor-cron-1.1.0-1.osg33.el6\ncondor-debuginfo-8.4.7-1.osg33.el6\ncondor-kbdd-8.4.7-1.osg33.el6\ncondor-procd-8.4.7-1.osg33.el6\ncondor-python-8.4.7-1.osg33.el6\ncondor-std-universe-8.4.7-1.osg33.el6\ncondor-test-8.4.7-1.osg33.el6\ncondor-vm-gahp-8.4.7-1.osg33.el6\ncvmfs-2.2.3-1.osg33.el6\ncvmfs-devel-2.2.3-1.osg33.el6\ncvmfs-server-2.2.3-1.osg33.el6\ncvmfs-unittests-2.2.3-1.osg33.el6\ngip-1.3.11-9.osg33.el6\nglobus-ftp-control-6.6-1.2.osg33.el6\nglobus-ftp-control-debuginfo-6.6-1.2.osg33.el6\nglobus-ftp-control-devel-6.6-1.2.osg33.el6\nglobus-ftp-control-doc-6.6-1.2.osg33.el6\ngratia-1.16.2-1.3.osg33.el6\ngratia-debuginfo-1.16.2-1.3.osg33.el6\ngratia-service-1.16.2-1.3.osg33.el6\ngsi-openssh-5.7-4.3.osg33.el6\ngsi-openssh-clients-5.7-4.3.osg33.el6\ngsi-openssh-debuginfo-5.7-4.3.osg33.el6\ngsi-openssh-server-5.7-4.3.osg33.el6\ngums-1.5.2-3.osg33.el6\ngums-client-1.5.2-3.osg33.el6\ngums-service-1.5.2-3.osg33.el6\nhtcondor-ce-2.0.6-1.osg33.el6\nhtcondor-ce-client-2.0.6-1.osg33.el6\nhtcondor-ce-collector-2.0.6-1.osg33.el6\nhtcondor-ce-condor-2.0.6-1.osg33.el6\nhtcondor-ce-lsf-2.0.6-1.osg33.el6\nhtcondor-ce-pbs-2.0.6-1.osg33.el6\nhtcondor-ce-sge-2.0.6-1.osg33.el6\nhtcondor-ce-view-2.0.6-1.osg33.el6\nigtf-ca-certs-1.74-1.osg33.el6\njglobus-2.1.0-8.osg33.el6\nlcmaps-plugins-verify-proxy-1.5.9-1.osg33.el6\nlcmaps-plugins-verify-proxy-debuginfo-1.5.9-1.osg33.el6\nosg-ca-certs-1.55-1.osg33.el6\nosg-ca-generator-1.1.0-1.osg33.el6\nosg-gums-3.3-3.osg33.el6\nosg-info-services-1.2.2-1.osg33.el6\nosg-oasis-6-5.osg33.el6\nosg-pki-tools-1.2.18-1.osg33.el6\nosg-pki-tools-tests-1.2.18-1.osg33.el6\nosg-test-1.8.1-1.osg33.el6\nosg-tested-internal-3.3-12.osg33.el6\nosg-version-3.3.13-1.osg33.el6\nrsv-3.13.0-3.osg33.el6\nrsv-consumers-3.13.0-3.osg33.el6\nrsv-core-3.13.0-3.osg33.el6\nrsv-metrics-3.13.0-3.osg33.el6\nxrootd-4.3.0-2.osg33.el6\nxrootd-client-4.3.0-2.osg33.el6\nxrootd-client-devel-4.3.0-2.osg33.el6\nxrootd-client-libs-4.3.0-2.osg33.el6\nxrootd-debuginfo-4.3.0-2.osg33.el6\nxrootd-devel-4.3.0-2.osg33.el6\nxrootd-doc-4.3.0-2.osg33.el6\nxrootd-fuse-4.3.0-2.osg33.el6\nxrootd-libs-4.3.0-2.osg33.el6\nxrootd-private-devel-4.3.0-2.osg33.el6\nxrootd-python-4.3.0-2.osg33.el6\nxrootd-selinux-4.3.0-2.osg33.el6\nxrootd-server-4.3.0-2.osg33.el6\nxrootd-server-devel-4.3.0-2.osg33.el6\nxrootd-server-libs-4.3.0-2.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-13/#enterprise-linux-7_1", 
            "text": "bestman2-2.3.0.1-1.osg33.el7\nbestman2-client-2.3.0.1-1.osg33.el7\nbestman2-client-libs-2.3.0.1-1.osg33.el7\nbestman2-common-libs-2.3.0.1-1.osg33.el7\nbestman2-server-2.3.0.1-1.osg33.el7\nbestman2-server-dep-libs-2.3.0.1-1.osg33.el7\nbestman2-server-libs-2.3.0.1-1.osg33.el7\nbestman2-tester-2.3.0.1-1.osg33.el7\nbestman2-tester-libs-2.3.0.1-1.osg33.el7\nblahp-1.18.20.bosco-1.osg33.el7\nblahp-debuginfo-1.18.20.bosco-1.osg33.el7\ncondor-8.4.7-1.osg33.el7\ncondor-all-8.4.7-1.osg33.el7\ncondor-bosco-8.4.7-1.osg33.el7\ncondor-classads-8.4.7-1.osg33.el7\ncondor-classads-devel-8.4.7-1.osg33.el7\ncondor-cron-1.1.0-1.osg33.el7\ncondor-debuginfo-8.4.7-1.osg33.el7\ncondor-kbdd-8.4.7-1.osg33.el7\ncondor-procd-8.4.7-1.osg33.el7\ncondor-python-8.4.7-1.osg33.el7\ncondor-test-8.4.7-1.osg33.el7\ncondor-vm-gahp-8.4.7-1.osg33.el7\ncvmfs-2.2.3-1.osg33.el7\ncvmfs-devel-2.2.3-1.osg33.el7\ncvmfs-server-2.2.3-1.osg33.el7\ncvmfs-unittests-2.2.3-1.osg33.el7\ngip-1.3.11-9.osg33.el7\nglobus-ftp-control-6.6-1.2.osg33.el7\nglobus-ftp-control-debuginfo-6.6-1.2.osg33.el7\nglobus-ftp-control-devel-6.6-1.2.osg33.el7\nglobus-ftp-control-doc-6.6-1.2.osg33.el7\ngratia-1.16.2-1.3.osg33.el7\ngratia-debuginfo-1.16.2-1.3.osg33.el7\ngratia-service-1.16.2-1.3.osg33.el7\ngsi-openssh-5.7-4.3.osg33.el7\ngsi-openssh-clients-5.7-4.3.osg33.el7\ngsi-openssh-debuginfo-5.7-4.3.osg33.el7\ngsi-openssh-server-5.7-4.3.osg33.el7\ngums-1.5.2-3.osg33.el7\ngums-client-1.5.2-3.osg33.el7\ngums-service-1.5.2-3.osg33.el7\nhtcondor-ce-2.0.6-1.osg33.el7\nhtcondor-ce-client-2.0.6-1.osg33.el7\nhtcondor-ce-collector-2.0.6-1.osg33.el7\nhtcondor-ce-condor-2.0.6-1.osg33.el7\nhtcondor-ce-lsf-2.0.6-1.osg33.el7\nhtcondor-ce-pbs-2.0.6-1.osg33.el7\nhtcondor-ce-sge-2.0.6-1.osg33.el7\nhtcondor-ce-view-2.0.6-1.osg33.el7\nigtf-ca-certs-1.74-1.osg33.el7\njglobus-2.1.0-8.osg33.el7\nlcmaps-plugins-verify-proxy-1.5.9-1.osg33.el7\nlcmaps-plugins-verify-proxy-debuginfo-1.5.9-1.osg33.el7\nosg-ca-certs-1.55-1.osg33.el7\nosg-ca-generator-1.1.0-1.osg33.el7\nosg-gums-3.3-3.osg33.el7\nosg-info-services-1.2.2-1.osg33.el7\nosg-oasis-6-5.osg33.el7\nosg-pki-tools-1.2.18-1.osg33.el7\nosg-pki-tools-tests-1.2.18-1.osg33.el7\nosg-test-1.8.1-1.osg33.el7\nosg-tested-internal-3.3-12.osg33.el7\nosg-version-3.3.13-1.osg33.el7\nrsv-3.13.0-3.osg33.el7\nrsv-consumers-3.13.0-3.osg33.el7\nrsv-core-3.13.0-3.osg33.el7\nrsv-metrics-3.13.0-3.osg33.el7\nxrootd-4.3.0-2.osg33.el7\nxrootd-client-4.3.0-2.osg33.el7\nxrootd-client-devel-4.3.0-2.osg33.el7\nxrootd-client-libs-4.3.0-2.osg33.el7\nxrootd-debuginfo-4.3.0-2.osg33.el7\nxrootd-devel-4.3.0-2.osg33.el7\nxrootd-doc-4.3.0-2.osg33.el7\nxrootd-fuse-4.3.0-2.osg33.el7\nxrootd-libs-4.3.0-2.osg33.el7\nxrootd-private-devel-4.3.0-2.osg33.el7\nxrootd-python-4.3.0-2.osg33.el7\nxrootd-selinux-4.3.0-2.osg33.el7\nxrootd-server-4.3.0-2.osg33.el7\nxrootd-server-devel-4.3.0-2.osg33.el7\nxrootd-server-libs-4.3.0-2.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-13/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-13/#enterprise-linux-6_2", 
            "text": "blahp-1.18.20.bosco-1.osgup.el6  condor-8.5.5-1.osgup.el6  cvmfs-2.3.0-1.osgup.el6  cvmfs-x509-helper-0.9-1.osgup.el6  osg-oasis-7-1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-13/#enterprise-linux-7_2", 
            "text": "blahp-1.18.20.bosco-1.osgup.el7  condor-8.5.5-1.osgup.el7  cvmfs-2.3.0-1.osgup.el7  cvmfs-x509-helper-0.9-1.osgup.el7  osg-oasis-7-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-13/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp cvmfs cvmfs-devel cvmfs-server cvmfs-unittests cvmfs-x509-helper cvmfs-x509-helper-debuginfo osg-oasis  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-13/#enterprise-linux-6_3", 
            "text": "blahp-1.18.20.bosco-1.osgup.el6\nblahp-debuginfo-1.18.20.bosco-1.osgup.el6\ncondor-8.5.5-1.osgup.el6\ncondor-all-8.5.5-1.osgup.el6\ncondor-bosco-8.5.5-1.osgup.el6\ncondor-classads-8.5.5-1.osgup.el6\ncondor-classads-devel-8.5.5-1.osgup.el6\ncondor-cream-gahp-8.5.5-1.osgup.el6\ncondor-debuginfo-8.5.5-1.osgup.el6\ncondor-kbdd-8.5.5-1.osgup.el6\ncondor-procd-8.5.5-1.osgup.el6\ncondor-python-8.5.5-1.osgup.el6\ncondor-std-universe-8.5.5-1.osgup.el6\ncondor-test-8.5.5-1.osgup.el6\ncondor-vm-gahp-8.5.5-1.osgup.el6\ncvmfs-2.3.0-1.osgup.el6\ncvmfs-devel-2.3.0-1.osgup.el6\ncvmfs-server-2.3.0-1.osgup.el6\ncvmfs-unittests-2.3.0-1.osgup.el6\ncvmfs-x509-helper-0.9-1.osgup.el6\ncvmfs-x509-helper-debuginfo-0.9-1.osgup.el6\nosg-oasis-7-1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-13/#enterprise-linux-7_3", 
            "text": "blahp-1.18.20.bosco-1.osgup.el7\nblahp-debuginfo-1.18.20.bosco-1.osgup.el7\ncondor-8.5.5-1.osgup.el7\ncondor-all-8.5.5-1.osgup.el7\ncondor-bosco-8.5.5-1.osgup.el7\ncondor-classads-8.5.5-1.osgup.el7\ncondor-classads-devel-8.5.5-1.osgup.el7\ncondor-debuginfo-8.5.5-1.osgup.el7\ncondor-kbdd-8.5.5-1.osgup.el7\ncondor-procd-8.5.5-1.osgup.el7\ncondor-python-8.5.5-1.osgup.el7\ncondor-test-8.5.5-1.osgup.el7\ncondor-vm-gahp-8.5.5-1.osgup.el7\ncvmfs-2.3.0-1.osgup.el7\ncvmfs-devel-2.3.0-1.osgup.el7\ncvmfs-server-2.3.0-1.osgup.el7\ncvmfs-unittests-2.3.0-1.osgup.el7\ncvmfs-x509-helper-0.9-1.osgup.el7\ncvmfs-x509-helper-debuginfo-0.9-1.osgup.el7\nosg-oasis-7-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-12/", 
            "text": "OSG Software Release 3.3.12\n\n\nRelease Date\n: 2016-05-10\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nVO Package v66\n - More OSG CA migrations\n\n\nHTCondor-CE-BOSCO: Setup a CE with only SSH access to the cluster\n\n\nThe HTCondor-CE-Bosco is designed to make it easier to contribute opportunistic resources to the OSG. With the HTCondor-CE-Bosco, an organization can setup a CE with only SSH access to the cluster, rather than the long list of requirements for a regular HTCondor-CE.\n\n\nHTCondor-CE-Bosco talk at OSG All Hands Meeting 2016\n\n\nAuthor\u2019s Blog post\n\n\n\n\n\n\nHTCondor 8.4.6\n: Fixes serious regression in HTCondor 8.4.5\n\n\nCVMFS 2.2.2\n\n\nThe update to CVMFS 2.2.1 includes a major overhaul to the default OSG configuration. Sites who have performed extensive changes to the default configuration are advised to verify their configurations before deploying.\n\n\n\n\n\n\nOSG CE packages will no longer install GRAM components\n\n\nBeStMan on EL7 platforms\n\n\nHTCondor-CE 2.0.5\n: HTCondor-CE-CEView bug fixes, support HTCondor-CE-BOSCO\n\n\nBLAHP 1.18.19: Fix memory request for PBS, updated SLURM support\n\n\nPegasus 4.6.1\n: Updated from version 4.3.1\n\n\nosg-pki-tools 1.2.17:\n\n\nFix timeout handling, improved error messages\n\n\nAnother bug fix\n\n\n\n\n\n\nosg-configure 1.4.0\n\n\nAdded support for configuring HTCondor-CE-Bosco\n\n\nFixed bug in RSV configuration where HTCondor-CE probes are sometimes not enabled\n\n\nAdd the \"slurm_cluster\" configuration option to the 20-slurm.ini configuraton file\n\n\nAdded updating the LCMAPS configuration when changing the security mechanism in 10-misc.ini\n\n\n\n\n\n\nHTCondor 8.5.4\n in the Upcoming repository\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nKnown Issues\n\n\n\n\ncondor_ce_q\n does not show any jobs when run as root with \ncondor-8.5.4\n from upcoming. Work around this by using \ncondor_ce_q -allusers\n instead.\n\n\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nbestman2-2.3.0-29.osg33.el6\n\n\nblahp-1.18.19.bosco-1.osg33.el6\n\n\ncondor-8.4.6-1.osg33.el6\n\n\ncvmfs-2.2.2-1.osg33.el6\n\n\ncvmfs-config-osg-1.2-3.osg33.el6\n\n\nglobus-gram-job-manager-pbs-2.5-1.1.osg33.el6\n\n\ngratia-probe-1.16.0-1.osg33.el6\n\n\nhtcondor-ce-2.0.5-1.osg33.el6\n\n\nosg-build-1.6.3-1.osg33.el6\n\n\nosg-ce-3.3-6.osg33.el6\n\n\nosg-configure-1.4.0-1.osg33.el6\n\n\nosg-info-services-1.2.1-1.osg33.el6\n\n\nosg-oasis-6-4.osg33.el6\n\n\nosg-pki-tools-1.2.17-1.osg33.el6\n\n\nosg-se-bestman-3.3-3.osg33.el6\n\n\nosg-se-bestman-xrootd-3.3-3.osg33.el6\n\n\nosg-system-profiler-1.3.0-1.osg33.el6\n\n\nosg-test-1.7.0-1.osg33.el6\n\n\nosg-tested-internal-3.3-11.osg33.el6\n\n\nosg-version-3.3.12-1.osg33.el6\n\n\npegasus-4.6.1-1.1.osg33.el6\n\n\nprivilege-xacml-2.6.5-2.osg33.el6\n\n\nvo-client-66-1.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nbestman2-2.3.0-29.osg33.el7\n\n\nblahp-1.18.19.bosco-1.osg33.el7\n\n\ncondor-8.4.6-1.osg33.el7\n\n\ncvmfs-2.2.2-1.osg33.el7\n\n\ncvmfs-config-osg-1.2-3.osg33.el7\n\n\nglobus-gram-job-manager-pbs-2.5-1.1.osg33.el7\n\n\ngratia-probe-1.16.0-1.osg33.el7\n\n\nhtcondor-ce-2.0.5-1.osg33.el7\n\n\nosg-build-1.6.3-1.osg33.el7\n\n\nosg-ce-3.3-6_clipped.osg33.el7\n\n\nosg-configure-1.4.0-1.osg33.el7\n\n\nosg-info-services-1.2.1-1.osg33.el7\n\n\nosg-oasis-6-4.osg33.el7\n\n\nosg-pki-tools-1.2.17-1.osg33.el7\n\n\nosg-se-bestman-3.3-3_clipped.osg33.el7\n\n\nosg-se-bestman-xrootd-3.3-3_clipped.osg33.el7\n\n\nosg-system-profiler-1.3.0-1.osg33.el7\n\n\nosg-test-1.7.0-1.osg33.el7\n\n\nosg-tested-internal-3.3-11.osg33.el7\n\n\nosg-version-3.3.12-1.osg33.el7\n\n\npegasus-4.6.1-1.1.osg33.el7\n\n\nprivilege-xacml-2.6.5-2.osg33.el7\n\n\nvo-client-66-1.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nbestman2-client bestman2-client-libs bestman2-common-libs bestman2-server bestman2-server-dep-libs bestman2-server-libs bestman2-tester bestman2-tester-libs blahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp cvmfs cvmfs-config-osg cvmfs-devel cvmfs-server cvmfs-unittests globus-gram-job-manager-pbs globus-gram-job-manager-pbs-debuginfo globus-gram-job-manager-pbs-setup-poll globus-gram-job-manager-pbs-setup-seg gratia-probe-bdii-status gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glexec gratia-probe-glideinwms gratia-probe-gram gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer htcondor-ce htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-view osg-base-ce osg-base-ce-bosco osg-base-ce-condor osg-base-ce-lsf osg-base-ce-pbs osg-base-ce-sge osg-base-ce-slurm osg-build osg-ce osg-ce-bosco osg-ce-condor osg-ce-lsf osg-ce-pbs osg-ce-sge osg-ce-slurm osg-configure osg-configure-bosco osg-configure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-gums-config osg-htcondor-ce osg-htcondor-ce-bosco osg-htcondor-ce-condor osg-htcondor-ce-lsf osg-htcondor-ce-pbs osg-htcondor-ce-sge osg-htcondor-ce-slurm osg-info-services osg-oasis osg-pki-tools osg-pki-tools-tests osg-se-bestman osg-se-bestman-xrootd osg-system-profiler osg-system-profiler-viewer osg-test osg-tested-internal osg-version pegasus pegasus-debuginfo privilege-xacml vo-client vo-client-edgmkgridmap\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nbestman2-2.3.0-29.osg33.el6\nbestman2-client-2.3.0-29.osg33.el6\nbestman2-client-libs-2.3.0-29.osg33.el6\nbestman2-common-libs-2.3.0-29.osg33.el6\nbestman2-server-2.3.0-29.osg33.el6\nbestman2-server-dep-libs-2.3.0-29.osg33.el6\nbestman2-server-libs-2.3.0-29.osg33.el6\nbestman2-tester-2.3.0-29.osg33.el6\nbestman2-tester-libs-2.3.0-29.osg33.el6\nblahp-1.18.19.bosco-1.osg33.el6\nblahp-debuginfo-1.18.19.bosco-1.osg33.el6\ncondor-8.4.6-1.osg33.el6\ncondor-all-8.4.6-1.osg33.el6\ncondor-bosco-8.4.6-1.osg33.el6\ncondor-classads-8.4.6-1.osg33.el6\ncondor-classads-devel-8.4.6-1.osg33.el6\ncondor-cream-gahp-8.4.6-1.osg33.el6\ncondor-debuginfo-8.4.6-1.osg33.el6\ncondor-kbdd-8.4.6-1.osg33.el6\ncondor-procd-8.4.6-1.osg33.el6\ncondor-python-8.4.6-1.osg33.el6\ncondor-std-universe-8.4.6-1.osg33.el6\ncondor-test-8.4.6-1.osg33.el6\ncondor-vm-gahp-8.4.6-1.osg33.el6\ncvmfs-2.2.2-1.osg33.el6\ncvmfs-config-osg-1.2-3.osg33.el6\ncvmfs-devel-2.2.2-1.osg33.el6\ncvmfs-server-2.2.2-1.osg33.el6\ncvmfs-unittests-2.2.2-1.osg33.el6\nglobus-gram-job-manager-pbs-2.5-1.1.osg33.el6\nglobus-gram-job-manager-pbs-debuginfo-2.5-1.1.osg33.el6\nglobus-gram-job-manager-pbs-setup-poll-2.5-1.1.osg33.el6\nglobus-gram-job-manager-pbs-setup-seg-2.5-1.1.osg33.el6\ngratia-probe-1.16.0-1.osg33.el6\ngratia-probe-bdii-status-1.16.0-1.osg33.el6\ngratia-probe-common-1.16.0-1.osg33.el6\ngratia-probe-condor-1.16.0-1.osg33.el6\ngratia-probe-condor-events-1.16.0-1.osg33.el6\ngratia-probe-dcache-storage-1.16.0-1.osg33.el6\ngratia-probe-dcache-storagegroup-1.16.0-1.osg33.el6\ngratia-probe-dcache-transfer-1.16.0-1.osg33.el6\ngratia-probe-debuginfo-1.16.0-1.osg33.el6\ngratia-probe-enstore-storage-1.16.0-1.osg33.el6\ngratia-probe-enstore-tapedrive-1.16.0-1.osg33.el6\ngratia-probe-enstore-transfer-1.16.0-1.osg33.el6\ngratia-probe-glexec-1.16.0-1.osg33.el6\ngratia-probe-glideinwms-1.16.0-1.osg33.el6\ngratia-probe-gram-1.16.0-1.osg33.el6\ngratia-probe-gridftp-transfer-1.16.0-1.osg33.el6\ngratia-probe-hadoop-storage-1.16.0-1.osg33.el6\ngratia-probe-htcondor-ce-1.16.0-1.osg33.el6\ngratia-probe-lsf-1.16.0-1.osg33.el6\ngratia-probe-metric-1.16.0-1.osg33.el6\ngratia-probe-onevm-1.16.0-1.osg33.el6\ngratia-probe-pbs-lsf-1.16.0-1.osg33.el6\ngratia-probe-services-1.16.0-1.osg33.el6\ngratia-probe-sge-1.16.0-1.osg33.el6\ngratia-probe-slurm-1.16.0-1.osg33.el6\ngratia-probe-xrootd-storage-1.16.0-1.osg33.el6\ngratia-probe-xrootd-transfer-1.16.0-1.osg33.el6\nhtcondor-ce-2.0.5-1.osg33.el6\nhtcondor-ce-client-2.0.5-1.osg33.el6\nhtcondor-ce-collector-2.0.5-1.osg33.el6\nhtcondor-ce-condor-2.0.5-1.osg33.el6\nhtcondor-ce-lsf-2.0.5-1.osg33.el6\nhtcondor-ce-pbs-2.0.5-1.osg33.el6\nhtcondor-ce-sge-2.0.5-1.osg33.el6\nhtcondor-ce-view-2.0.5-1.osg33.el6\nosg-base-ce-3.3-6.osg33.el6\nosg-base-ce-bosco-3.3-6.osg33.el6\nosg-base-ce-condor-3.3-6.osg33.el6\nosg-base-ce-lsf-3.3-6.osg33.el6\nosg-base-ce-pbs-3.3-6.osg33.el6\nosg-base-ce-sge-3.3-6.osg33.el6\nosg-base-ce-slurm-3.3-6.osg33.el6\nosg-build-1.6.3-1.osg33.el6\nosg-ce-3.3-6.osg33.el6\nosg-ce-bosco-3.3-6.osg33.el6\nosg-ce-condor-3.3-6.osg33.el6\nosg-ce-lsf-3.3-6.osg33.el6\nosg-ce-pbs-3.3-6.osg33.el6\nosg-ce-sge-3.3-6.osg33.el6\nosg-ce-slurm-3.3-6.osg33.el6\nosg-configure-1.4.0-1.osg33.el6\nosg-configure-bosco-1.4.0-1.osg33.el6\nosg-configure-ce-1.4.0-1.osg33.el6\nosg-configure-cemon-1.4.0-1.osg33.el6\nosg-configure-condor-1.4.0-1.osg33.el6\nosg-configure-gateway-1.4.0-1.osg33.el6\nosg-configure-gip-1.4.0-1.osg33.el6\nosg-configure-gratia-1.4.0-1.osg33.el6\nosg-configure-infoservices-1.4.0-1.osg33.el6\nosg-configure-lsf-1.4.0-1.osg33.el6\nosg-configure-managedfork-1.4.0-1.osg33.el6\nosg-configure-misc-1.4.0-1.osg33.el6\nosg-configure-monalisa-1.4.0-1.osg33.el6\nosg-configure-network-1.4.0-1.osg33.el6\nosg-configure-pbs-1.4.0-1.osg33.el6\nosg-configure-rsv-1.4.0-1.osg33.el6\nosg-configure-sge-1.4.0-1.osg33.el6\nosg-configure-slurm-1.4.0-1.osg33.el6\nosg-configure-squid-1.4.0-1.osg33.el6\nosg-configure-tests-1.4.0-1.osg33.el6\nosg-gums-config-66-1.osg33.el6\nosg-htcondor-ce-3.3-6.osg33.el6\nosg-htcondor-ce-bosco-3.3-6.osg33.el6\nosg-htcondor-ce-condor-3.3-6.osg33.el6\nosg-htcondor-ce-lsf-3.3-6.osg33.el6\nosg-htcondor-ce-pbs-3.3-6.osg33.el6\nosg-htcondor-ce-sge-3.3-6.osg33.el6\nosg-htcondor-ce-slurm-3.3-6.osg33.el6\nosg-info-services-1.2.1-1.osg33.el6\nosg-oasis-6-4.osg33.el6\nosg-pki-tools-1.2.17-1.osg33.el6\nosg-pki-tools-tests-1.2.17-1.osg33.el6\nosg-se-bestman-3.3-3.osg33.el6\nosg-se-bestman-xrootd-3.3-3.osg33.el6\nosg-system-profiler-1.3.0-1.osg33.el6\nosg-system-profiler-viewer-1.3.0-1.osg33.el6\nosg-test-1.7.0-1.osg33.el6\nosg-tested-internal-3.3-11.osg33.el6\nosg-version-3.3.12-1.osg33.el6\npegasus-4.6.1-1.1.osg33.el6\npegasus-debuginfo-4.6.1-1.1.osg33.el6\nprivilege-xacml-2.6.5-2.osg33.el6\nvo-client-66-1.osg33.el6\nvo-client-edgmkgridmap-66-1.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nbestman2-2.3.0-29.osg33.el7\nbestman2-client-2.3.0-29.osg33.el7\nbestman2-client-libs-2.3.0-29.osg33.el7\nbestman2-common-libs-2.3.0-29.osg33.el7\nbestman2-server-2.3.0-29.osg33.el7\nbestman2-server-dep-libs-2.3.0-29.osg33.el7\nbestman2-server-libs-2.3.0-29.osg33.el7\nbestman2-tester-2.3.0-29.osg33.el7\nbestman2-tester-libs-2.3.0-29.osg33.el7\nblahp-1.18.19.bosco-1.osg33.el7\nblahp-debuginfo-1.18.19.bosco-1.osg33.el7\ncondor-8.4.6-1.osg33.el7\ncondor-all-8.4.6-1.osg33.el7\ncondor-bosco-8.4.6-1.osg33.el7\ncondor-classads-8.4.6-1.osg33.el7\ncondor-classads-devel-8.4.6-1.osg33.el7\ncondor-debuginfo-8.4.6-1.osg33.el7\ncondor-kbdd-8.4.6-1.osg33.el7\ncondor-procd-8.4.6-1.osg33.el7\ncondor-python-8.4.6-1.osg33.el7\ncondor-test-8.4.6-1.osg33.el7\ncondor-vm-gahp-8.4.6-1.osg33.el7\ncvmfs-2.2.2-1.osg33.el7\ncvmfs-config-osg-1.2-3.osg33.el7\ncvmfs-devel-2.2.2-1.osg33.el7\ncvmfs-server-2.2.2-1.osg33.el7\ncvmfs-unittests-2.2.2-1.osg33.el7\nglobus-gram-job-manager-pbs-2.5-1.1.osg33.el7\nglobus-gram-job-manager-pbs-debuginfo-2.5-1.1.osg33.el7\nglobus-gram-job-manager-pbs-setup-poll-2.5-1.1.osg33.el7\nglobus-gram-job-manager-pbs-setup-seg-2.5-1.1.osg33.el7\ngratia-probe-1.16.0-1.osg33.el7\ngratia-probe-bdii-status-1.16.0-1.osg33.el7\ngratia-probe-common-1.16.0-1.osg33.el7\ngratia-probe-condor-1.16.0-1.osg33.el7\ngratia-probe-condor-events-1.16.0-1.osg33.el7\ngratia-probe-dcache-storage-1.16.0-1.osg33.el7\ngratia-probe-dcache-storagegroup-1.16.0-1.osg33.el7\ngratia-probe-dcache-transfer-1.16.0-1.osg33.el7\ngratia-probe-debuginfo-1.16.0-1.osg33.el7\ngratia-probe-enstore-storage-1.16.0-1.osg33.el7\ngratia-probe-enstore-tapedrive-1.16.0-1.osg33.el7\ngratia-probe-enstore-transfer-1.16.0-1.osg33.el7\ngratia-probe-glexec-1.16.0-1.osg33.el7\ngratia-probe-glideinwms-1.16.0-1.osg33.el7\ngratia-probe-gram-1.16.0-1.osg33.el7\ngratia-probe-gridftp-transfer-1.16.0-1.osg33.el7\ngratia-probe-hadoop-storage-1.16.0-1.osg33.el7\ngratia-probe-htcondor-ce-1.16.0-1.osg33.el7\ngratia-probe-lsf-1.16.0-1.osg33.el7\ngratia-probe-metric-1.16.0-1.osg33.el7\ngratia-probe-onevm-1.16.0-1.osg33.el7\ngratia-probe-pbs-lsf-1.16.0-1.osg33.el7\ngratia-probe-services-1.16.0-1.osg33.el7\ngratia-probe-sge-1.16.0-1.osg33.el7\ngratia-probe-slurm-1.16.0-1.osg33.el7\ngratia-probe-xrootd-storage-1.16.0-1.osg33.el7\ngratia-probe-xrootd-transfer-1.16.0-1.osg33.el7\nhtcondor-ce-2.0.5-1.osg33.el7\nhtcondor-ce-client-2.0.5-1.osg33.el7\nhtcondor-ce-collector-2.0.5-1.osg33.el7\nhtcondor-ce-condor-2.0.5-1.osg33.el7\nhtcondor-ce-lsf-2.0.5-1.osg33.el7\nhtcondor-ce-pbs-2.0.5-1.osg33.el7\nhtcondor-ce-sge-2.0.5-1.osg33.el7\nhtcondor-ce-view-2.0.5-1.osg33.el7\nosg-base-ce-3.3-6_clipped.osg33.el7\nosg-base-ce-bosco-3.3-6_clipped.osg33.el7\nosg-base-ce-condor-3.3-6_clipped.osg33.el7\nosg-base-ce-lsf-3.3-6_clipped.osg33.el7\nosg-base-ce-pbs-3.3-6_clipped.osg33.el7\nosg-base-ce-sge-3.3-6_clipped.osg33.el7\nosg-base-ce-slurm-3.3-6_clipped.osg33.el7\nosg-build-1.6.3-1.osg33.el7\nosg-ce-3.3-6_clipped.osg33.el7\nosg-ce-bosco-3.3-6_clipped.osg33.el7\nosg-ce-condor-3.3-6_clipped.osg33.el7\nosg-ce-lsf-3.3-6_clipped.osg33.el7\nosg-ce-pbs-3.3-6_clipped.osg33.el7\nosg-ce-sge-3.3-6_clipped.osg33.el7\nosg-ce-slurm-3.3-6_clipped.osg33.el7\nosg-configure-1.4.0-1.osg33.el7\nosg-configure-bosco-1.4.0-1.osg33.el7\nosg-configure-ce-1.4.0-1.osg33.el7\nosg-configure-cemon-1.4.0-1.osg33.el7\nosg-configure-condor-1.4.0-1.osg33.el7\nosg-configure-gateway-1.4.0-1.osg33.el7\nosg-configure-gip-1.4.0-1.osg33.el7\nosg-configure-gratia-1.4.0-1.osg33.el7\nosg-configure-infoservices-1.4.0-1.osg33.el7\nosg-configure-lsf-1.4.0-1.osg33.el7\nosg-configure-managedfork-1.4.0-1.osg33.el7\nosg-configure-misc-1.4.0-1.osg33.el7\nosg-configure-monalisa-1.4.0-1.osg33.el7\nosg-configure-network-1.4.0-1.osg33.el7\nosg-configure-pbs-1.4.0-1.osg33.el7\nosg-configure-rsv-1.4.0-1.osg33.el7\nosg-configure-sge-1.4.0-1.osg33.el7\nosg-configure-slurm-1.4.0-1.osg33.el7\nosg-configure-squid-1.4.0-1.osg33.el7\nosg-configure-tests-1.4.0-1.osg33.el7\nosg-gums-config-66-1.osg33.el7\nosg-htcondor-ce-3.3-6_clipped.osg33.el7\nosg-htcondor-ce-bosco-3.3-6_clipped.osg33.el7\nosg-htcondor-ce-condor-3.3-6_clipped.osg33.el7\nosg-htcondor-ce-lsf-3.3-6_clipped.osg33.el7\nosg-htcondor-ce-pbs-3.3-6_clipped.osg33.el7\nosg-htcondor-ce-sge-3.3-6_clipped.osg33.el7\nosg-htcondor-ce-slurm-3.3-6_clipped.osg33.el7\nosg-info-services-1.2.1-1.osg33.el7\nosg-oasis-6-4.osg33.el7\nosg-pki-tools-1.2.17-1.osg33.el7\nosg-pki-tools-tests-1.2.17-1.osg33.el7\nosg-se-bestman-3.3-3_clipped.osg33.el7\nosg-se-bestman-xrootd-3.3-3_clipped.osg33.el7\nosg-system-profiler-1.3.0-1.osg33.el7\nosg-system-profiler-viewer-1.3.0-1.osg33.el7\nosg-test-1.7.0-1.osg33.el7\nosg-tested-internal-3.3-11.osg33.el7\nosg-version-3.3.12-1.osg33.el7\npegasus-4.6.1-1.1.osg33.el7\npegasus-debuginfo-4.6.1-1.1.osg33.el7\nprivilege-xacml-2.6.5-2.osg33.el7\nvo-client-66-1.osg33.el7\nvo-client-edgmkgridmap-66-1.osg33.el7\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.19.bosco-2.osgup.el6\n\n\ncondor-8.5.4-1.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.19.bosco-2.osgup.el7\n\n\ncondor-8.5.4-1.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp-1.18.19.bosco-2.osgup.el6\nblahp-debuginfo-1.18.19.bosco-2.osgup.el6\ncondor-8.5.4-1.osgup.el6\ncondor-all-8.5.4-1.osgup.el6\ncondor-bosco-8.5.4-1.osgup.el6\ncondor-classads-8.5.4-1.osgup.el6\ncondor-classads-devel-8.5.4-1.osgup.el6\ncondor-cream-gahp-8.5.4-1.osgup.el6\ncondor-debuginfo-8.5.4-1.osgup.el6\ncondor-kbdd-8.5.4-1.osgup.el6\ncondor-procd-8.5.4-1.osgup.el6\ncondor-python-8.5.4-1.osgup.el6\ncondor-std-universe-8.5.4-1.osgup.el6\ncondor-test-8.5.4-1.osgup.el6\ncondor-vm-gahp-8.5.4-1.osgup.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp-1.18.19.bosco-2.osgup.el7\nblahp-debuginfo-1.18.19.bosco-2.osgup.el7\ncondor-8.5.4-1.osgup.el7\ncondor-all-8.5.4-1.osgup.el7\ncondor-bosco-8.5.4-1.osgup.el7\ncondor-classads-8.5.4-1.osgup.el7\ncondor-classads-devel-8.5.4-1.osgup.el7\ncondor-debuginfo-8.5.4-1.osgup.el7\ncondor-kbdd-8.5.4-1.osgup.el7\ncondor-procd-8.5.4-1.osgup.el7\ncondor-python-8.5.4-1.osgup.el7\ncondor-test-8.5.4-1.osgup.el7\ncondor-vm-gahp-8.5.4-1.osgup.el7", 
            "title": "OSG Release 3.3.12"
        }, 
        {
            "location": "/release/3.3/release-3-3-12/#osg-software-release-3312", 
            "text": "Release Date : 2016-05-10", 
            "title": "OSG Software Release 3.3.12"
        }, 
        {
            "location": "/release/3.3/release-3-3-12/#summary-of-changes", 
            "text": "This release contains:   VO Package v66  - More OSG CA migrations  HTCondor-CE-BOSCO: Setup a CE with only SSH access to the cluster  The HTCondor-CE-Bosco is designed to make it easier to contribute opportunistic resources to the OSG. With the HTCondor-CE-Bosco, an organization can setup a CE with only SSH access to the cluster, rather than the long list of requirements for a regular HTCondor-CE.  HTCondor-CE-Bosco talk at OSG All Hands Meeting 2016  Author\u2019s Blog post    HTCondor 8.4.6 : Fixes serious regression in HTCondor 8.4.5  CVMFS 2.2.2  The update to CVMFS 2.2.1 includes a major overhaul to the default OSG configuration. Sites who have performed extensive changes to the default configuration are advised to verify their configurations before deploying.    OSG CE packages will no longer install GRAM components  BeStMan on EL7 platforms  HTCondor-CE 2.0.5 : HTCondor-CE-CEView bug fixes, support HTCondor-CE-BOSCO  BLAHP 1.18.19: Fix memory request for PBS, updated SLURM support  Pegasus 4.6.1 : Updated from version 4.3.1  osg-pki-tools 1.2.17:  Fix timeout handling, improved error messages  Another bug fix    osg-configure 1.4.0  Added support for configuring HTCondor-CE-Bosco  Fixed bug in RSV configuration where HTCondor-CE probes are sometimes not enabled  Add the \"slurm_cluster\" configuration option to the 20-slurm.ini configuraton file  Added updating the LCMAPS configuration when changing the security mechanism in 10-misc.ini    HTCondor 8.5.4  in the Upcoming repository   These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-12/#known-issues", 
            "text": "condor_ce_q  does not show any jobs when run as root with  condor-8.5.4  from upcoming. Work around this by using  condor_ce_q -allusers  instead.", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.3/release-3-3-12/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-12/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-12/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-12/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-12/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-12/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-12/#enterprise-linux-6", 
            "text": "bestman2-2.3.0-29.osg33.el6  blahp-1.18.19.bosco-1.osg33.el6  condor-8.4.6-1.osg33.el6  cvmfs-2.2.2-1.osg33.el6  cvmfs-config-osg-1.2-3.osg33.el6  globus-gram-job-manager-pbs-2.5-1.1.osg33.el6  gratia-probe-1.16.0-1.osg33.el6  htcondor-ce-2.0.5-1.osg33.el6  osg-build-1.6.3-1.osg33.el6  osg-ce-3.3-6.osg33.el6  osg-configure-1.4.0-1.osg33.el6  osg-info-services-1.2.1-1.osg33.el6  osg-oasis-6-4.osg33.el6  osg-pki-tools-1.2.17-1.osg33.el6  osg-se-bestman-3.3-3.osg33.el6  osg-se-bestman-xrootd-3.3-3.osg33.el6  osg-system-profiler-1.3.0-1.osg33.el6  osg-test-1.7.0-1.osg33.el6  osg-tested-internal-3.3-11.osg33.el6  osg-version-3.3.12-1.osg33.el6  pegasus-4.6.1-1.1.osg33.el6  privilege-xacml-2.6.5-2.osg33.el6  vo-client-66-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-12/#enterprise-linux-7", 
            "text": "bestman2-2.3.0-29.osg33.el7  blahp-1.18.19.bosco-1.osg33.el7  condor-8.4.6-1.osg33.el7  cvmfs-2.2.2-1.osg33.el7  cvmfs-config-osg-1.2-3.osg33.el7  globus-gram-job-manager-pbs-2.5-1.1.osg33.el7  gratia-probe-1.16.0-1.osg33.el7  htcondor-ce-2.0.5-1.osg33.el7  osg-build-1.6.3-1.osg33.el7  osg-ce-3.3-6_clipped.osg33.el7  osg-configure-1.4.0-1.osg33.el7  osg-info-services-1.2.1-1.osg33.el7  osg-oasis-6-4.osg33.el7  osg-pki-tools-1.2.17-1.osg33.el7  osg-se-bestman-3.3-3_clipped.osg33.el7  osg-se-bestman-xrootd-3.3-3_clipped.osg33.el7  osg-system-profiler-1.3.0-1.osg33.el7  osg-test-1.7.0-1.osg33.el7  osg-tested-internal-3.3-11.osg33.el7  osg-version-3.3.12-1.osg33.el7  pegasus-4.6.1-1.1.osg33.el7  privilege-xacml-2.6.5-2.osg33.el7  vo-client-66-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-12/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  bestman2-client bestman2-client-libs bestman2-common-libs bestman2-server bestman2-server-dep-libs bestman2-server-libs bestman2-tester bestman2-tester-libs blahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp cvmfs cvmfs-config-osg cvmfs-devel cvmfs-server cvmfs-unittests globus-gram-job-manager-pbs globus-gram-job-manager-pbs-debuginfo globus-gram-job-manager-pbs-setup-poll globus-gram-job-manager-pbs-setup-seg gratia-probe-bdii-status gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glexec gratia-probe-glideinwms gratia-probe-gram gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-htcondor-ce gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer htcondor-ce htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-view osg-base-ce osg-base-ce-bosco osg-base-ce-condor osg-base-ce-lsf osg-base-ce-pbs osg-base-ce-sge osg-base-ce-slurm osg-build osg-ce osg-ce-bosco osg-ce-condor osg-ce-lsf osg-ce-pbs osg-ce-sge osg-ce-slurm osg-configure osg-configure-bosco osg-configure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-gums-config osg-htcondor-ce osg-htcondor-ce-bosco osg-htcondor-ce-condor osg-htcondor-ce-lsf osg-htcondor-ce-pbs osg-htcondor-ce-sge osg-htcondor-ce-slurm osg-info-services osg-oasis osg-pki-tools osg-pki-tools-tests osg-se-bestman osg-se-bestman-xrootd osg-system-profiler osg-system-profiler-viewer osg-test osg-tested-internal osg-version pegasus pegasus-debuginfo privilege-xacml vo-client vo-client-edgmkgridmap  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-12/#enterprise-linux-6_1", 
            "text": "bestman2-2.3.0-29.osg33.el6\nbestman2-client-2.3.0-29.osg33.el6\nbestman2-client-libs-2.3.0-29.osg33.el6\nbestman2-common-libs-2.3.0-29.osg33.el6\nbestman2-server-2.3.0-29.osg33.el6\nbestman2-server-dep-libs-2.3.0-29.osg33.el6\nbestman2-server-libs-2.3.0-29.osg33.el6\nbestman2-tester-2.3.0-29.osg33.el6\nbestman2-tester-libs-2.3.0-29.osg33.el6\nblahp-1.18.19.bosco-1.osg33.el6\nblahp-debuginfo-1.18.19.bosco-1.osg33.el6\ncondor-8.4.6-1.osg33.el6\ncondor-all-8.4.6-1.osg33.el6\ncondor-bosco-8.4.6-1.osg33.el6\ncondor-classads-8.4.6-1.osg33.el6\ncondor-classads-devel-8.4.6-1.osg33.el6\ncondor-cream-gahp-8.4.6-1.osg33.el6\ncondor-debuginfo-8.4.6-1.osg33.el6\ncondor-kbdd-8.4.6-1.osg33.el6\ncondor-procd-8.4.6-1.osg33.el6\ncondor-python-8.4.6-1.osg33.el6\ncondor-std-universe-8.4.6-1.osg33.el6\ncondor-test-8.4.6-1.osg33.el6\ncondor-vm-gahp-8.4.6-1.osg33.el6\ncvmfs-2.2.2-1.osg33.el6\ncvmfs-config-osg-1.2-3.osg33.el6\ncvmfs-devel-2.2.2-1.osg33.el6\ncvmfs-server-2.2.2-1.osg33.el6\ncvmfs-unittests-2.2.2-1.osg33.el6\nglobus-gram-job-manager-pbs-2.5-1.1.osg33.el6\nglobus-gram-job-manager-pbs-debuginfo-2.5-1.1.osg33.el6\nglobus-gram-job-manager-pbs-setup-poll-2.5-1.1.osg33.el6\nglobus-gram-job-manager-pbs-setup-seg-2.5-1.1.osg33.el6\ngratia-probe-1.16.0-1.osg33.el6\ngratia-probe-bdii-status-1.16.0-1.osg33.el6\ngratia-probe-common-1.16.0-1.osg33.el6\ngratia-probe-condor-1.16.0-1.osg33.el6\ngratia-probe-condor-events-1.16.0-1.osg33.el6\ngratia-probe-dcache-storage-1.16.0-1.osg33.el6\ngratia-probe-dcache-storagegroup-1.16.0-1.osg33.el6\ngratia-probe-dcache-transfer-1.16.0-1.osg33.el6\ngratia-probe-debuginfo-1.16.0-1.osg33.el6\ngratia-probe-enstore-storage-1.16.0-1.osg33.el6\ngratia-probe-enstore-tapedrive-1.16.0-1.osg33.el6\ngratia-probe-enstore-transfer-1.16.0-1.osg33.el6\ngratia-probe-glexec-1.16.0-1.osg33.el6\ngratia-probe-glideinwms-1.16.0-1.osg33.el6\ngratia-probe-gram-1.16.0-1.osg33.el6\ngratia-probe-gridftp-transfer-1.16.0-1.osg33.el6\ngratia-probe-hadoop-storage-1.16.0-1.osg33.el6\ngratia-probe-htcondor-ce-1.16.0-1.osg33.el6\ngratia-probe-lsf-1.16.0-1.osg33.el6\ngratia-probe-metric-1.16.0-1.osg33.el6\ngratia-probe-onevm-1.16.0-1.osg33.el6\ngratia-probe-pbs-lsf-1.16.0-1.osg33.el6\ngratia-probe-services-1.16.0-1.osg33.el6\ngratia-probe-sge-1.16.0-1.osg33.el6\ngratia-probe-slurm-1.16.0-1.osg33.el6\ngratia-probe-xrootd-storage-1.16.0-1.osg33.el6\ngratia-probe-xrootd-transfer-1.16.0-1.osg33.el6\nhtcondor-ce-2.0.5-1.osg33.el6\nhtcondor-ce-client-2.0.5-1.osg33.el6\nhtcondor-ce-collector-2.0.5-1.osg33.el6\nhtcondor-ce-condor-2.0.5-1.osg33.el6\nhtcondor-ce-lsf-2.0.5-1.osg33.el6\nhtcondor-ce-pbs-2.0.5-1.osg33.el6\nhtcondor-ce-sge-2.0.5-1.osg33.el6\nhtcondor-ce-view-2.0.5-1.osg33.el6\nosg-base-ce-3.3-6.osg33.el6\nosg-base-ce-bosco-3.3-6.osg33.el6\nosg-base-ce-condor-3.3-6.osg33.el6\nosg-base-ce-lsf-3.3-6.osg33.el6\nosg-base-ce-pbs-3.3-6.osg33.el6\nosg-base-ce-sge-3.3-6.osg33.el6\nosg-base-ce-slurm-3.3-6.osg33.el6\nosg-build-1.6.3-1.osg33.el6\nosg-ce-3.3-6.osg33.el6\nosg-ce-bosco-3.3-6.osg33.el6\nosg-ce-condor-3.3-6.osg33.el6\nosg-ce-lsf-3.3-6.osg33.el6\nosg-ce-pbs-3.3-6.osg33.el6\nosg-ce-sge-3.3-6.osg33.el6\nosg-ce-slurm-3.3-6.osg33.el6\nosg-configure-1.4.0-1.osg33.el6\nosg-configure-bosco-1.4.0-1.osg33.el6\nosg-configure-ce-1.4.0-1.osg33.el6\nosg-configure-cemon-1.4.0-1.osg33.el6\nosg-configure-condor-1.4.0-1.osg33.el6\nosg-configure-gateway-1.4.0-1.osg33.el6\nosg-configure-gip-1.4.0-1.osg33.el6\nosg-configure-gratia-1.4.0-1.osg33.el6\nosg-configure-infoservices-1.4.0-1.osg33.el6\nosg-configure-lsf-1.4.0-1.osg33.el6\nosg-configure-managedfork-1.4.0-1.osg33.el6\nosg-configure-misc-1.4.0-1.osg33.el6\nosg-configure-monalisa-1.4.0-1.osg33.el6\nosg-configure-network-1.4.0-1.osg33.el6\nosg-configure-pbs-1.4.0-1.osg33.el6\nosg-configure-rsv-1.4.0-1.osg33.el6\nosg-configure-sge-1.4.0-1.osg33.el6\nosg-configure-slurm-1.4.0-1.osg33.el6\nosg-configure-squid-1.4.0-1.osg33.el6\nosg-configure-tests-1.4.0-1.osg33.el6\nosg-gums-config-66-1.osg33.el6\nosg-htcondor-ce-3.3-6.osg33.el6\nosg-htcondor-ce-bosco-3.3-6.osg33.el6\nosg-htcondor-ce-condor-3.3-6.osg33.el6\nosg-htcondor-ce-lsf-3.3-6.osg33.el6\nosg-htcondor-ce-pbs-3.3-6.osg33.el6\nosg-htcondor-ce-sge-3.3-6.osg33.el6\nosg-htcondor-ce-slurm-3.3-6.osg33.el6\nosg-info-services-1.2.1-1.osg33.el6\nosg-oasis-6-4.osg33.el6\nosg-pki-tools-1.2.17-1.osg33.el6\nosg-pki-tools-tests-1.2.17-1.osg33.el6\nosg-se-bestman-3.3-3.osg33.el6\nosg-se-bestman-xrootd-3.3-3.osg33.el6\nosg-system-profiler-1.3.0-1.osg33.el6\nosg-system-profiler-viewer-1.3.0-1.osg33.el6\nosg-test-1.7.0-1.osg33.el6\nosg-tested-internal-3.3-11.osg33.el6\nosg-version-3.3.12-1.osg33.el6\npegasus-4.6.1-1.1.osg33.el6\npegasus-debuginfo-4.6.1-1.1.osg33.el6\nprivilege-xacml-2.6.5-2.osg33.el6\nvo-client-66-1.osg33.el6\nvo-client-edgmkgridmap-66-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-12/#enterprise-linux-7_1", 
            "text": "bestman2-2.3.0-29.osg33.el7\nbestman2-client-2.3.0-29.osg33.el7\nbestman2-client-libs-2.3.0-29.osg33.el7\nbestman2-common-libs-2.3.0-29.osg33.el7\nbestman2-server-2.3.0-29.osg33.el7\nbestman2-server-dep-libs-2.3.0-29.osg33.el7\nbestman2-server-libs-2.3.0-29.osg33.el7\nbestman2-tester-2.3.0-29.osg33.el7\nbestman2-tester-libs-2.3.0-29.osg33.el7\nblahp-1.18.19.bosco-1.osg33.el7\nblahp-debuginfo-1.18.19.bosco-1.osg33.el7\ncondor-8.4.6-1.osg33.el7\ncondor-all-8.4.6-1.osg33.el7\ncondor-bosco-8.4.6-1.osg33.el7\ncondor-classads-8.4.6-1.osg33.el7\ncondor-classads-devel-8.4.6-1.osg33.el7\ncondor-debuginfo-8.4.6-1.osg33.el7\ncondor-kbdd-8.4.6-1.osg33.el7\ncondor-procd-8.4.6-1.osg33.el7\ncondor-python-8.4.6-1.osg33.el7\ncondor-test-8.4.6-1.osg33.el7\ncondor-vm-gahp-8.4.6-1.osg33.el7\ncvmfs-2.2.2-1.osg33.el7\ncvmfs-config-osg-1.2-3.osg33.el7\ncvmfs-devel-2.2.2-1.osg33.el7\ncvmfs-server-2.2.2-1.osg33.el7\ncvmfs-unittests-2.2.2-1.osg33.el7\nglobus-gram-job-manager-pbs-2.5-1.1.osg33.el7\nglobus-gram-job-manager-pbs-debuginfo-2.5-1.1.osg33.el7\nglobus-gram-job-manager-pbs-setup-poll-2.5-1.1.osg33.el7\nglobus-gram-job-manager-pbs-setup-seg-2.5-1.1.osg33.el7\ngratia-probe-1.16.0-1.osg33.el7\ngratia-probe-bdii-status-1.16.0-1.osg33.el7\ngratia-probe-common-1.16.0-1.osg33.el7\ngratia-probe-condor-1.16.0-1.osg33.el7\ngratia-probe-condor-events-1.16.0-1.osg33.el7\ngratia-probe-dcache-storage-1.16.0-1.osg33.el7\ngratia-probe-dcache-storagegroup-1.16.0-1.osg33.el7\ngratia-probe-dcache-transfer-1.16.0-1.osg33.el7\ngratia-probe-debuginfo-1.16.0-1.osg33.el7\ngratia-probe-enstore-storage-1.16.0-1.osg33.el7\ngratia-probe-enstore-tapedrive-1.16.0-1.osg33.el7\ngratia-probe-enstore-transfer-1.16.0-1.osg33.el7\ngratia-probe-glexec-1.16.0-1.osg33.el7\ngratia-probe-glideinwms-1.16.0-1.osg33.el7\ngratia-probe-gram-1.16.0-1.osg33.el7\ngratia-probe-gridftp-transfer-1.16.0-1.osg33.el7\ngratia-probe-hadoop-storage-1.16.0-1.osg33.el7\ngratia-probe-htcondor-ce-1.16.0-1.osg33.el7\ngratia-probe-lsf-1.16.0-1.osg33.el7\ngratia-probe-metric-1.16.0-1.osg33.el7\ngratia-probe-onevm-1.16.0-1.osg33.el7\ngratia-probe-pbs-lsf-1.16.0-1.osg33.el7\ngratia-probe-services-1.16.0-1.osg33.el7\ngratia-probe-sge-1.16.0-1.osg33.el7\ngratia-probe-slurm-1.16.0-1.osg33.el7\ngratia-probe-xrootd-storage-1.16.0-1.osg33.el7\ngratia-probe-xrootd-transfer-1.16.0-1.osg33.el7\nhtcondor-ce-2.0.5-1.osg33.el7\nhtcondor-ce-client-2.0.5-1.osg33.el7\nhtcondor-ce-collector-2.0.5-1.osg33.el7\nhtcondor-ce-condor-2.0.5-1.osg33.el7\nhtcondor-ce-lsf-2.0.5-1.osg33.el7\nhtcondor-ce-pbs-2.0.5-1.osg33.el7\nhtcondor-ce-sge-2.0.5-1.osg33.el7\nhtcondor-ce-view-2.0.5-1.osg33.el7\nosg-base-ce-3.3-6_clipped.osg33.el7\nosg-base-ce-bosco-3.3-6_clipped.osg33.el7\nosg-base-ce-condor-3.3-6_clipped.osg33.el7\nosg-base-ce-lsf-3.3-6_clipped.osg33.el7\nosg-base-ce-pbs-3.3-6_clipped.osg33.el7\nosg-base-ce-sge-3.3-6_clipped.osg33.el7\nosg-base-ce-slurm-3.3-6_clipped.osg33.el7\nosg-build-1.6.3-1.osg33.el7\nosg-ce-3.3-6_clipped.osg33.el7\nosg-ce-bosco-3.3-6_clipped.osg33.el7\nosg-ce-condor-3.3-6_clipped.osg33.el7\nosg-ce-lsf-3.3-6_clipped.osg33.el7\nosg-ce-pbs-3.3-6_clipped.osg33.el7\nosg-ce-sge-3.3-6_clipped.osg33.el7\nosg-ce-slurm-3.3-6_clipped.osg33.el7\nosg-configure-1.4.0-1.osg33.el7\nosg-configure-bosco-1.4.0-1.osg33.el7\nosg-configure-ce-1.4.0-1.osg33.el7\nosg-configure-cemon-1.4.0-1.osg33.el7\nosg-configure-condor-1.4.0-1.osg33.el7\nosg-configure-gateway-1.4.0-1.osg33.el7\nosg-configure-gip-1.4.0-1.osg33.el7\nosg-configure-gratia-1.4.0-1.osg33.el7\nosg-configure-infoservices-1.4.0-1.osg33.el7\nosg-configure-lsf-1.4.0-1.osg33.el7\nosg-configure-managedfork-1.4.0-1.osg33.el7\nosg-configure-misc-1.4.0-1.osg33.el7\nosg-configure-monalisa-1.4.0-1.osg33.el7\nosg-configure-network-1.4.0-1.osg33.el7\nosg-configure-pbs-1.4.0-1.osg33.el7\nosg-configure-rsv-1.4.0-1.osg33.el7\nosg-configure-sge-1.4.0-1.osg33.el7\nosg-configure-slurm-1.4.0-1.osg33.el7\nosg-configure-squid-1.4.0-1.osg33.el7\nosg-configure-tests-1.4.0-1.osg33.el7\nosg-gums-config-66-1.osg33.el7\nosg-htcondor-ce-3.3-6_clipped.osg33.el7\nosg-htcondor-ce-bosco-3.3-6_clipped.osg33.el7\nosg-htcondor-ce-condor-3.3-6_clipped.osg33.el7\nosg-htcondor-ce-lsf-3.3-6_clipped.osg33.el7\nosg-htcondor-ce-pbs-3.3-6_clipped.osg33.el7\nosg-htcondor-ce-sge-3.3-6_clipped.osg33.el7\nosg-htcondor-ce-slurm-3.3-6_clipped.osg33.el7\nosg-info-services-1.2.1-1.osg33.el7\nosg-oasis-6-4.osg33.el7\nosg-pki-tools-1.2.17-1.osg33.el7\nosg-pki-tools-tests-1.2.17-1.osg33.el7\nosg-se-bestman-3.3-3_clipped.osg33.el7\nosg-se-bestman-xrootd-3.3-3_clipped.osg33.el7\nosg-system-profiler-1.3.0-1.osg33.el7\nosg-system-profiler-viewer-1.3.0-1.osg33.el7\nosg-test-1.7.0-1.osg33.el7\nosg-tested-internal-3.3-11.osg33.el7\nosg-version-3.3.12-1.osg33.el7\npegasus-4.6.1-1.1.osg33.el7\npegasus-debuginfo-4.6.1-1.1.osg33.el7\nprivilege-xacml-2.6.5-2.osg33.el7\nvo-client-66-1.osg33.el7\nvo-client-edgmkgridmap-66-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-12/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-12/#enterprise-linux-6_2", 
            "text": "blahp-1.18.19.bosco-2.osgup.el6  condor-8.5.4-1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-12/#enterprise-linux-7_2", 
            "text": "blahp-1.18.19.bosco-2.osgup.el7  condor-8.5.4-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-12/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-12/#enterprise-linux-6_3", 
            "text": "blahp-1.18.19.bosco-2.osgup.el6\nblahp-debuginfo-1.18.19.bosco-2.osgup.el6\ncondor-8.5.4-1.osgup.el6\ncondor-all-8.5.4-1.osgup.el6\ncondor-bosco-8.5.4-1.osgup.el6\ncondor-classads-8.5.4-1.osgup.el6\ncondor-classads-devel-8.5.4-1.osgup.el6\ncondor-cream-gahp-8.5.4-1.osgup.el6\ncondor-debuginfo-8.5.4-1.osgup.el6\ncondor-kbdd-8.5.4-1.osgup.el6\ncondor-procd-8.5.4-1.osgup.el6\ncondor-python-8.5.4-1.osgup.el6\ncondor-std-universe-8.5.4-1.osgup.el6\ncondor-test-8.5.4-1.osgup.el6\ncondor-vm-gahp-8.5.4-1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-12/#enterprise-linux-7_3", 
            "text": "blahp-1.18.19.bosco-2.osgup.el7\nblahp-debuginfo-1.18.19.bosco-2.osgup.el7\ncondor-8.5.4-1.osgup.el7\ncondor-all-8.5.4-1.osgup.el7\ncondor-bosco-8.5.4-1.osgup.el7\ncondor-classads-8.5.4-1.osgup.el7\ncondor-classads-devel-8.5.4-1.osgup.el7\ncondor-debuginfo-8.5.4-1.osgup.el7\ncondor-kbdd-8.5.4-1.osgup.el7\ncondor-procd-8.5.4-1.osgup.el7\ncondor-python-8.5.4-1.osgup.el7\ncondor-test-8.5.4-1.osgup.el7\ncondor-vm-gahp-8.5.4-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-11/", 
            "text": "OSG Software Release 3.3.11\n\n\nRelease Date\n: 2016-04-12\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nVO Package v65\n - More OSG CA migrations\n\n\nCA Certificates based on \nIGTF 1.73\n\n\nXRootD 4.3.0\n: Several important fixes for bugs affecting CMS\n\n\nHDFS 2.0.0+1612: Support ACLs, Support the EL7 platform\n\n\nUpdate to \nGlideinWMS 3.2.13\n\n\nAdd gfal functionality to xrootd-dsi\n\n\nHTCondor CE 2.0.4: Accept full subject DNs in extattr_table.txt\n\n\nBLAHP 1.18.18: Changes in the BLAHP to support PBS Pro\n\n\nosg-pki-tools 1.2.15: Better error messages and checking of arguments\n\n\nHTCondor 8.4.5\n: Various bug fixes\n\n\nPull in the required log4j package when installing the emi-trustmanager\n\n\nHTCondor 8.5.3\n in the Upcoming repository\n\n\nSupport for an OSG CVMFS configuration repository in the Upcoming repository\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nKnown Issues\n\n\n\n\ncondor_ce_q\n does not show any jobs when run as root with \ncondor-8.5.3\n from upcoming. Work around this by using \ncondor_ce_q -allusers\n instead.\n\n\n\n\n\n\n\n\n\nThe new HTCondor-CE View has a bug where some graphs show up blank. This may also manifest in errors like the following in \n/var/log/condor-ce/GangliadLog\n: \\ \n\n\n\n\n1/11/16 15:05:54 Failed to execute /usr/share/condor-ce/condor_ce_metric --conf /etc/ganglia/gmond.conf --group HTCondor.Schedd --name SchedulerRecentDaemonCoreDutyCycle --value 1.04449 --type float --units % --slope both --spoof 192.170.227.226:itbv-ce-htcondor.mwt2.org --tmax 120 --dmax 86400: Usage: condor_ce_metric [options]\n\n\ncondor_ce_metric: error: no such option: --conf\n\n\n01/11/16 15:05:54 Failed to publish metric SchedulerRecentDaemonCoreDutyCycle for itbv-ce-htcondor.mwt2.org \n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nbigtop-utils-0.6.0+248-1.cdh4.7.1.p0.13.osg33.el6\n\n\nblahp-1.18.18.bosco-1.osg33.el6\n\n\ncondor-8.4.5-1.osg33.el6\n\n\nemi-trustmanager-3.0.3-11.osg33.el6\n\n\nglideinwms-3.2.13-1.osg33.el6\n\n\ngridftp-hdfs-0.5.4-25.osg33.el6\n\n\nhadoop-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\n\n\nhtcondor-ce-2.0.4-1.osg33.el6\n\n\nigtf-ca-certs-1.73-1.osg33.el6\n\n\nosg-ca-certs-1.54-1.osg33.el6\n\n\nosg-pki-tools-1.2.15-1.osg33.el6\n\n\nosg-release-3.3-5.osg33.el6\n\n\nosg-release-itb-3.3-5.osg33.el6\n\n\nosg-test-1.6.0-1.osg33.el6\n\n\nosg-version-3.3.11-1.osg33.el6\n\n\nvo-client-65-2.osg33.el6\n\n\nxrootd-4.3.0-1.osg33.el6\n\n\nxrootd-dsi-3.0.4-17.osg33.el6\n\n\nxrootd-hdfs-1.8.7-2.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nbigtop-utils-0.6.0+248-1.cdh4.7.1.p0.13.osg33.el7\n\n\nblahp-1.18.18.bosco-1.osg33.el7\n\n\ncondor-8.4.5-1.osg33.el7\n\n\nemi-trustmanager-3.0.3-11.osg33.el7\n\n\nglideinwms-3.2.13-1.osg33.el7\n\n\ngridftp-hdfs-0.5.4-25.osg33.el7\n\n\nhadoop-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\n\n\nhtcondor-ce-2.0.4-1.osg33.el7\n\n\nigtf-ca-certs-1.73-1.osg33.el7\n\n\nosg-ca-certs-1.54-1.osg33.el7\n\n\nosg-pki-tools-1.2.15-1.osg33.el7\n\n\nosg-release-3.3-5.osg33.el7\n\n\nosg-release-itb-3.3-5.osg33.el7\n\n\nosg-test-1.6.0-1.osg33.el7\n\n\nosg-version-3.3.11-1.osg33.el7\n\n\nvo-client-65-2.osg33.el7\n\n\nxrootd-4.3.0-1.osg33.el7\n\n\nxrootd-dsi-3.0.4-17.osg33.el7\n\n\nxrootd-hdfs-1.8.7-2.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nbigtop-utils blahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp emi-trustmanager glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone gridftp-hdfs gridftp-hdfs-debuginfo hadoop hadoop-0.20-conf-pseudo hadoop-0.20-mapreduce hadoop-client hadoop-conf-pseudo hadoop-debuginfo hadoop-doc hadoop-hdfs hadoop-hdfs-datanode hadoop-hdfs-fuse hadoop-hdfs-fuse-selinux hadoop-hdfs-journalnode hadoop-hdfs-namenode hadoop-hdfs-secondarynamenode hadoop-hdfs-zkfc hadoop-httpfs hadoop-libhdfs hadoop-mapreduce hadoop-yarn htcondor-ce htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-view igtf-ca-certs osg-ca-certs osg-gums-config osg-pki-tools osg-pki-tools-tests osg-release osg-release-itb osg-test osg-version vo-client vo-client-edgmkgridmap xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-dsi xrootd-dsi-debuginfo xrootd-fuse xrootd-hdfs xrootd-hdfs-debuginfo xrootd-hdfs-devel xrootd-libs xrootd-private-devel xrootd-python xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nbigtop-utils-0.6.0+248-1.cdh4.7.1.p0.13.osg33.el6\nblahp-1.18.18.bosco-1.osg33.el6\nblahp-debuginfo-1.18.18.bosco-1.osg33.el6\ncondor-8.4.5-1.osg33.el6\ncondor-all-8.4.5-1.osg33.el6\ncondor-bosco-8.4.5-1.osg33.el6\ncondor-classads-8.4.5-1.osg33.el6\ncondor-classads-devel-8.4.5-1.osg33.el6\ncondor-cream-gahp-8.4.5-1.osg33.el6\ncondor-debuginfo-8.4.5-1.osg33.el6\ncondor-kbdd-8.4.5-1.osg33.el6\ncondor-procd-8.4.5-1.osg33.el6\ncondor-python-8.4.5-1.osg33.el6\ncondor-std-universe-8.4.5-1.osg33.el6\ncondor-test-8.4.5-1.osg33.el6\ncondor-vm-gahp-8.4.5-1.osg33.el6\nemi-trustmanager-3.0.3-11.osg33.el6\nglideinwms-3.2.13-1.osg33.el6\nglideinwms-common-tools-3.2.13-1.osg33.el6\nglideinwms-condor-common-config-3.2.13-1.osg33.el6\nglideinwms-factory-3.2.13-1.osg33.el6\nglideinwms-factory-condor-3.2.13-1.osg33.el6\nglideinwms-glidecondor-tools-3.2.13-1.osg33.el6\nglideinwms-libs-3.2.13-1.osg33.el6\nglideinwms-minimal-condor-3.2.13-1.osg33.el6\nglideinwms-usercollector-3.2.13-1.osg33.el6\nglideinwms-userschedd-3.2.13-1.osg33.el6\nglideinwms-vofrontend-3.2.13-1.osg33.el6\nglideinwms-vofrontend-standalone-3.2.13-1.osg33.el6\ngridftp-hdfs-0.5.4-25.osg33.el6\ngridftp-hdfs-debuginfo-0.5.4-25.osg33.el6\nhadoop-0.20-conf-pseudo-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-0.20-mapreduce-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-client-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-conf-pseudo-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-debuginfo-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-doc-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-hdfs-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-hdfs-datanode-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-hdfs-fuse-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-hdfs-fuse-selinux-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-hdfs-journalnode-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-hdfs-namenode-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-hdfs-secondarynamenode-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-hdfs-zkfc-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-httpfs-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-libhdfs-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-mapreduce-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-yarn-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhtcondor-ce-2.0.4-1.osg33.el6\nhtcondor-ce-client-2.0.4-1.osg33.el6\nhtcondor-ce-collector-2.0.4-1.osg33.el6\nhtcondor-ce-condor-2.0.4-1.osg33.el6\nhtcondor-ce-lsf-2.0.4-1.osg33.el6\nhtcondor-ce-pbs-2.0.4-1.osg33.el6\nhtcondor-ce-sge-2.0.4-1.osg33.el6\nhtcondor-ce-view-2.0.4-1.osg33.el6\nigtf-ca-certs-1.73-1.osg33.el6\nosg-ca-certs-1.54-1.osg33.el6\nosg-gums-config-65-2.osg33.el6\nosg-pki-tools-1.2.15-1.osg33.el6\nosg-pki-tools-tests-1.2.15-1.osg33.el6\nosg-release-3.3-5.osg33.el6\nosg-release-itb-3.3-5.osg33.el6\nosg-test-1.6.0-1.osg33.el6\nosg-version-3.3.11-1.osg33.el6\nvo-client-65-2.osg33.el6\nvo-client-edgmkgridmap-65-2.osg33.el6\nxrootd-4.3.0-1.osg33.el6\nxrootd-client-4.3.0-1.osg33.el6\nxrootd-client-devel-4.3.0-1.osg33.el6\nxrootd-client-libs-4.3.0-1.osg33.el6\nxrootd-debuginfo-4.3.0-1.osg33.el6\nxrootd-devel-4.3.0-1.osg33.el6\nxrootd-doc-4.3.0-1.osg33.el6\nxrootd-dsi-3.0.4-17.osg33.el6\nxrootd-dsi-debuginfo-3.0.4-17.osg33.el6\nxrootd-fuse-4.3.0-1.osg33.el6\nxrootd-hdfs-1.8.7-2.osg33.el6\nxrootd-hdfs-debuginfo-1.8.7-2.osg33.el6\nxrootd-hdfs-devel-1.8.7-2.osg33.el6\nxrootd-libs-4.3.0-1.osg33.el6\nxrootd-private-devel-4.3.0-1.osg33.el6\nxrootd-python-4.3.0-1.osg33.el6\nxrootd-selinux-4.3.0-1.osg33.el6\nxrootd-server-4.3.0-1.osg33.el6\nxrootd-server-devel-4.3.0-1.osg33.el6\nxrootd-server-libs-4.3.0-1.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nbigtop-utils-0.6.0+248-1.cdh4.7.1.p0.13.osg33.el7\nblahp-1.18.18.bosco-1.osg33.el7\nblahp-debuginfo-1.18.18.bosco-1.osg33.el7\ncondor-8.4.5-1.osg33.el7\ncondor-all-8.4.5-1.osg33.el7\ncondor-bosco-8.4.5-1.osg33.el7\ncondor-classads-8.4.5-1.osg33.el7\ncondor-classads-devel-8.4.5-1.osg33.el7\ncondor-debuginfo-8.4.5-1.osg33.el7\ncondor-kbdd-8.4.5-1.osg33.el7\ncondor-procd-8.4.5-1.osg33.el7\ncondor-python-8.4.5-1.osg33.el7\ncondor-test-8.4.5-1.osg33.el7\ncondor-vm-gahp-8.4.5-1.osg33.el7\nemi-trustmanager-3.0.3-11.osg33.el7\nglideinwms-3.2.13-1.osg33.el7\nglideinwms-common-tools-3.2.13-1.osg33.el7\nglideinwms-condor-common-config-3.2.13-1.osg33.el7\nglideinwms-factory-3.2.13-1.osg33.el7\nglideinwms-factory-condor-3.2.13-1.osg33.el7\nglideinwms-glidecondor-tools-3.2.13-1.osg33.el7\nglideinwms-libs-3.2.13-1.osg33.el7\nglideinwms-minimal-condor-3.2.13-1.osg33.el7\nglideinwms-usercollector-3.2.13-1.osg33.el7\nglideinwms-userschedd-3.2.13-1.osg33.el7\nglideinwms-vofrontend-3.2.13-1.osg33.el7\nglideinwms-vofrontend-standalone-3.2.13-1.osg33.el7\ngridftp-hdfs-0.5.4-25.osg33.el7\ngridftp-hdfs-debuginfo-0.5.4-25.osg33.el7\nhadoop-0.20-conf-pseudo-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-0.20-mapreduce-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-client-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-conf-pseudo-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-debuginfo-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-doc-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-hdfs-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-hdfs-datanode-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-hdfs-fuse-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-hdfs-fuse-selinux-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-hdfs-journalnode-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-hdfs-namenode-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-hdfs-secondarynamenode-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-hdfs-zkfc-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-httpfs-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-libhdfs-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-mapreduce-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-yarn-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhtcondor-ce-2.0.4-1.osg33.el7\nhtcondor-ce-client-2.0.4-1.osg33.el7\nhtcondor-ce-collector-2.0.4-1.osg33.el7\nhtcondor-ce-condor-2.0.4-1.osg33.el7\nhtcondor-ce-lsf-2.0.4-1.osg33.el7\nhtcondor-ce-pbs-2.0.4-1.osg33.el7\nhtcondor-ce-sge-2.0.4-1.osg33.el7\nhtcondor-ce-view-2.0.4-1.osg33.el7\nigtf-ca-certs-1.73-1.osg33.el7\nosg-ca-certs-1.54-1.osg33.el7\nosg-gums-config-65-2.osg33.el7\nosg-pki-tools-1.2.15-1.osg33.el7\nosg-pki-tools-tests-1.2.15-1.osg33.el7\nosg-release-3.3-5.osg33.el7\nosg-release-itb-3.3-5.osg33.el7\nosg-test-1.6.0-1.osg33.el7\nosg-version-3.3.11-1.osg33.el7\nvo-client-65-2.osg33.el7\nvo-client-edgmkgridmap-65-2.osg33.el7\nxrootd-4.3.0-1.osg33.el7\nxrootd-client-4.3.0-1.osg33.el7\nxrootd-client-devel-4.3.0-1.osg33.el7\nxrootd-client-libs-4.3.0-1.osg33.el7\nxrootd-debuginfo-4.3.0-1.osg33.el7\nxrootd-devel-4.3.0-1.osg33.el7\nxrootd-doc-4.3.0-1.osg33.el7\nxrootd-dsi-3.0.4-17.osg33.el7\nxrootd-dsi-debuginfo-3.0.4-17.osg33.el7\nxrootd-fuse-4.3.0-1.osg33.el7\nxrootd-hdfs-1.8.7-2.osg33.el7\nxrootd-hdfs-debuginfo-1.8.7-2.osg33.el7\nxrootd-hdfs-devel-1.8.7-2.osg33.el7\nxrootd-libs-4.3.0-1.osg33.el7\nxrootd-private-devel-4.3.0-1.osg33.el7\nxrootd-python-4.3.0-1.osg33.el7\nxrootd-selinux-4.3.0-1.osg33.el7\nxrootd-server-4.3.0-1.osg33.el7\nxrootd-server-devel-4.3.0-1.osg33.el7\nxrootd-server-libs-4.3.0-1.osg33.el7\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.18.bosco-1.osgup.el6\n\n\ncondor-8.5.3-1.osgup.el6\n\n\ncvmfs-2.2.1-1.osgup.el6\n\n\ncvmfs-config-osg-1.2-3.osgup.el6\n\n\nosg-oasis-6-4.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.18.bosco-1.osgup.el7\n\n\ncondor-8.5.3-1.osgup.el7\n\n\ncvmfs-2.2.1-1.osgup.el7\n\n\ncvmfs-config-osg-1.2-3.osgup.el7\n\n\nosg-oasis-6-4.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp cvmfs cvmfs-config-osg cvmfs-devel cvmfs-server cvmfs-unittests osg-oasis\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp-1.18.18.bosco-1.osgup.el6\nblahp-debuginfo-1.18.18.bosco-1.osgup.el6\ncondor-8.5.3-1.osgup.el6\ncondor-all-8.5.3-1.osgup.el6\ncondor-bosco-8.5.3-1.osgup.el6\ncondor-classads-8.5.3-1.osgup.el6\ncondor-classads-devel-8.5.3-1.osgup.el6\ncondor-cream-gahp-8.5.3-1.osgup.el6\ncondor-debuginfo-8.5.3-1.osgup.el6\ncondor-kbdd-8.5.3-1.osgup.el6\ncondor-procd-8.5.3-1.osgup.el6\ncondor-python-8.5.3-1.osgup.el6\ncondor-std-universe-8.5.3-1.osgup.el6\ncondor-test-8.5.3-1.osgup.el6\ncondor-vm-gahp-8.5.3-1.osgup.el6\ncvmfs-2.2.1-1.osgup.el6\ncvmfs-config-osg-1.2-3.osgup.el6\ncvmfs-devel-2.2.1-1.osgup.el6\ncvmfs-server-2.2.1-1.osgup.el6\ncvmfs-unittests-2.2.1-1.osgup.el6\nosg-oasis-6-4.osgup.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp-1.18.18.bosco-1.osgup.el7\nblahp-debuginfo-1.18.18.bosco-1.osgup.el7\ncondor-8.5.3-1.osgup.el7\ncondor-all-8.5.3-1.osgup.el7\ncondor-bosco-8.5.3-1.osgup.el7\ncondor-classads-8.5.3-1.osgup.el7\ncondor-classads-devel-8.5.3-1.osgup.el7\ncondor-debuginfo-8.5.3-1.osgup.el7\ncondor-kbdd-8.5.3-1.osgup.el7\ncondor-procd-8.5.3-1.osgup.el7\ncondor-python-8.5.3-1.osgup.el7\ncondor-test-8.5.3-1.osgup.el7\ncondor-vm-gahp-8.5.3-1.osgup.el7\ncvmfs-2.2.1-1.osgup.el7\ncvmfs-config-osg-1.2-3.osgup.el7\ncvmfs-devel-2.2.1-1.osgup.el7\ncvmfs-server-2.2.1-1.osgup.el7\ncvmfs-unittests-2.2.1-1.osgup.el7\nosg-oasis-6-4.osgup.el7", 
            "title": "OSG Release 3.3.11"
        }, 
        {
            "location": "/release/3.3/release-3-3-11/#osg-software-release-3311", 
            "text": "Release Date : 2016-04-12", 
            "title": "OSG Software Release 3.3.11"
        }, 
        {
            "location": "/release/3.3/release-3-3-11/#summary-of-changes", 
            "text": "This release contains:   VO Package v65  - More OSG CA migrations  CA Certificates based on  IGTF 1.73  XRootD 4.3.0 : Several important fixes for bugs affecting CMS  HDFS 2.0.0+1612: Support ACLs, Support the EL7 platform  Update to  GlideinWMS 3.2.13  Add gfal functionality to xrootd-dsi  HTCondor CE 2.0.4: Accept full subject DNs in extattr_table.txt  BLAHP 1.18.18: Changes in the BLAHP to support PBS Pro  osg-pki-tools 1.2.15: Better error messages and checking of arguments  HTCondor 8.4.5 : Various bug fixes  Pull in the required log4j package when installing the emi-trustmanager  HTCondor 8.5.3  in the Upcoming repository  Support for an OSG CVMFS configuration repository in the Upcoming repository   These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-11/#known-issues", 
            "text": "condor_ce_q  does not show any jobs when run as root with  condor-8.5.3  from upcoming. Work around this by using  condor_ce_q -allusers  instead.     The new HTCondor-CE View has a bug where some graphs show up blank. This may also manifest in errors like the following in  /var/log/condor-ce/GangliadLog : \\    1/11/16 15:05:54 Failed to execute /usr/share/condor-ce/condor_ce_metric --conf /etc/ganglia/gmond.conf --group HTCondor.Schedd --name SchedulerRecentDaemonCoreDutyCycle --value 1.04449 --type float --units % --slope both --spoof 192.170.227.226:itbv-ce-htcondor.mwt2.org --tmax 120 --dmax 86400: Usage: condor_ce_metric [options]  condor_ce_metric: error: no such option: --conf  01/11/16 15:05:54 Failed to publish metric SchedulerRecentDaemonCoreDutyCycle for itbv-ce-htcondor.mwt2.org", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.3/release-3-3-11/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-11/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-11/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-11/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-11/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-11/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-11/#enterprise-linux-6", 
            "text": "bigtop-utils-0.6.0+248-1.cdh4.7.1.p0.13.osg33.el6  blahp-1.18.18.bosco-1.osg33.el6  condor-8.4.5-1.osg33.el6  emi-trustmanager-3.0.3-11.osg33.el6  glideinwms-3.2.13-1.osg33.el6  gridftp-hdfs-0.5.4-25.osg33.el6  hadoop-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6  htcondor-ce-2.0.4-1.osg33.el6  igtf-ca-certs-1.73-1.osg33.el6  osg-ca-certs-1.54-1.osg33.el6  osg-pki-tools-1.2.15-1.osg33.el6  osg-release-3.3-5.osg33.el6  osg-release-itb-3.3-5.osg33.el6  osg-test-1.6.0-1.osg33.el6  osg-version-3.3.11-1.osg33.el6  vo-client-65-2.osg33.el6  xrootd-4.3.0-1.osg33.el6  xrootd-dsi-3.0.4-17.osg33.el6  xrootd-hdfs-1.8.7-2.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-11/#enterprise-linux-7", 
            "text": "bigtop-utils-0.6.0+248-1.cdh4.7.1.p0.13.osg33.el7  blahp-1.18.18.bosco-1.osg33.el7  condor-8.4.5-1.osg33.el7  emi-trustmanager-3.0.3-11.osg33.el7  glideinwms-3.2.13-1.osg33.el7  gridftp-hdfs-0.5.4-25.osg33.el7  hadoop-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7  htcondor-ce-2.0.4-1.osg33.el7  igtf-ca-certs-1.73-1.osg33.el7  osg-ca-certs-1.54-1.osg33.el7  osg-pki-tools-1.2.15-1.osg33.el7  osg-release-3.3-5.osg33.el7  osg-release-itb-3.3-5.osg33.el7  osg-test-1.6.0-1.osg33.el7  osg-version-3.3.11-1.osg33.el7  vo-client-65-2.osg33.el7  xrootd-4.3.0-1.osg33.el7  xrootd-dsi-3.0.4-17.osg33.el7  xrootd-hdfs-1.8.7-2.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-11/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  bigtop-utils blahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp emi-trustmanager glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone gridftp-hdfs gridftp-hdfs-debuginfo hadoop hadoop-0.20-conf-pseudo hadoop-0.20-mapreduce hadoop-client hadoop-conf-pseudo hadoop-debuginfo hadoop-doc hadoop-hdfs hadoop-hdfs-datanode hadoop-hdfs-fuse hadoop-hdfs-fuse-selinux hadoop-hdfs-journalnode hadoop-hdfs-namenode hadoop-hdfs-secondarynamenode hadoop-hdfs-zkfc hadoop-httpfs hadoop-libhdfs hadoop-mapreduce hadoop-yarn htcondor-ce htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-view igtf-ca-certs osg-ca-certs osg-gums-config osg-pki-tools osg-pki-tools-tests osg-release osg-release-itb osg-test osg-version vo-client vo-client-edgmkgridmap xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-dsi xrootd-dsi-debuginfo xrootd-fuse xrootd-hdfs xrootd-hdfs-debuginfo xrootd-hdfs-devel xrootd-libs xrootd-private-devel xrootd-python xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-11/#enterprise-linux-6_1", 
            "text": "bigtop-utils-0.6.0+248-1.cdh4.7.1.p0.13.osg33.el6\nblahp-1.18.18.bosco-1.osg33.el6\nblahp-debuginfo-1.18.18.bosco-1.osg33.el6\ncondor-8.4.5-1.osg33.el6\ncondor-all-8.4.5-1.osg33.el6\ncondor-bosco-8.4.5-1.osg33.el6\ncondor-classads-8.4.5-1.osg33.el6\ncondor-classads-devel-8.4.5-1.osg33.el6\ncondor-cream-gahp-8.4.5-1.osg33.el6\ncondor-debuginfo-8.4.5-1.osg33.el6\ncondor-kbdd-8.4.5-1.osg33.el6\ncondor-procd-8.4.5-1.osg33.el6\ncondor-python-8.4.5-1.osg33.el6\ncondor-std-universe-8.4.5-1.osg33.el6\ncondor-test-8.4.5-1.osg33.el6\ncondor-vm-gahp-8.4.5-1.osg33.el6\nemi-trustmanager-3.0.3-11.osg33.el6\nglideinwms-3.2.13-1.osg33.el6\nglideinwms-common-tools-3.2.13-1.osg33.el6\nglideinwms-condor-common-config-3.2.13-1.osg33.el6\nglideinwms-factory-3.2.13-1.osg33.el6\nglideinwms-factory-condor-3.2.13-1.osg33.el6\nglideinwms-glidecondor-tools-3.2.13-1.osg33.el6\nglideinwms-libs-3.2.13-1.osg33.el6\nglideinwms-minimal-condor-3.2.13-1.osg33.el6\nglideinwms-usercollector-3.2.13-1.osg33.el6\nglideinwms-userschedd-3.2.13-1.osg33.el6\nglideinwms-vofrontend-3.2.13-1.osg33.el6\nglideinwms-vofrontend-standalone-3.2.13-1.osg33.el6\ngridftp-hdfs-0.5.4-25.osg33.el6\ngridftp-hdfs-debuginfo-0.5.4-25.osg33.el6\nhadoop-0.20-conf-pseudo-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-0.20-mapreduce-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-client-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-conf-pseudo-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-debuginfo-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-doc-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-hdfs-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-hdfs-datanode-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-hdfs-fuse-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-hdfs-fuse-selinux-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-hdfs-journalnode-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-hdfs-namenode-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-hdfs-secondarynamenode-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-hdfs-zkfc-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-httpfs-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-libhdfs-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-mapreduce-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhadoop-yarn-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el6\nhtcondor-ce-2.0.4-1.osg33.el6\nhtcondor-ce-client-2.0.4-1.osg33.el6\nhtcondor-ce-collector-2.0.4-1.osg33.el6\nhtcondor-ce-condor-2.0.4-1.osg33.el6\nhtcondor-ce-lsf-2.0.4-1.osg33.el6\nhtcondor-ce-pbs-2.0.4-1.osg33.el6\nhtcondor-ce-sge-2.0.4-1.osg33.el6\nhtcondor-ce-view-2.0.4-1.osg33.el6\nigtf-ca-certs-1.73-1.osg33.el6\nosg-ca-certs-1.54-1.osg33.el6\nosg-gums-config-65-2.osg33.el6\nosg-pki-tools-1.2.15-1.osg33.el6\nosg-pki-tools-tests-1.2.15-1.osg33.el6\nosg-release-3.3-5.osg33.el6\nosg-release-itb-3.3-5.osg33.el6\nosg-test-1.6.0-1.osg33.el6\nosg-version-3.3.11-1.osg33.el6\nvo-client-65-2.osg33.el6\nvo-client-edgmkgridmap-65-2.osg33.el6\nxrootd-4.3.0-1.osg33.el6\nxrootd-client-4.3.0-1.osg33.el6\nxrootd-client-devel-4.3.0-1.osg33.el6\nxrootd-client-libs-4.3.0-1.osg33.el6\nxrootd-debuginfo-4.3.0-1.osg33.el6\nxrootd-devel-4.3.0-1.osg33.el6\nxrootd-doc-4.3.0-1.osg33.el6\nxrootd-dsi-3.0.4-17.osg33.el6\nxrootd-dsi-debuginfo-3.0.4-17.osg33.el6\nxrootd-fuse-4.3.0-1.osg33.el6\nxrootd-hdfs-1.8.7-2.osg33.el6\nxrootd-hdfs-debuginfo-1.8.7-2.osg33.el6\nxrootd-hdfs-devel-1.8.7-2.osg33.el6\nxrootd-libs-4.3.0-1.osg33.el6\nxrootd-private-devel-4.3.0-1.osg33.el6\nxrootd-python-4.3.0-1.osg33.el6\nxrootd-selinux-4.3.0-1.osg33.el6\nxrootd-server-4.3.0-1.osg33.el6\nxrootd-server-devel-4.3.0-1.osg33.el6\nxrootd-server-libs-4.3.0-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-11/#enterprise-linux-7_1", 
            "text": "bigtop-utils-0.6.0+248-1.cdh4.7.1.p0.13.osg33.el7\nblahp-1.18.18.bosco-1.osg33.el7\nblahp-debuginfo-1.18.18.bosco-1.osg33.el7\ncondor-8.4.5-1.osg33.el7\ncondor-all-8.4.5-1.osg33.el7\ncondor-bosco-8.4.5-1.osg33.el7\ncondor-classads-8.4.5-1.osg33.el7\ncondor-classads-devel-8.4.5-1.osg33.el7\ncondor-debuginfo-8.4.5-1.osg33.el7\ncondor-kbdd-8.4.5-1.osg33.el7\ncondor-procd-8.4.5-1.osg33.el7\ncondor-python-8.4.5-1.osg33.el7\ncondor-test-8.4.5-1.osg33.el7\ncondor-vm-gahp-8.4.5-1.osg33.el7\nemi-trustmanager-3.0.3-11.osg33.el7\nglideinwms-3.2.13-1.osg33.el7\nglideinwms-common-tools-3.2.13-1.osg33.el7\nglideinwms-condor-common-config-3.2.13-1.osg33.el7\nglideinwms-factory-3.2.13-1.osg33.el7\nglideinwms-factory-condor-3.2.13-1.osg33.el7\nglideinwms-glidecondor-tools-3.2.13-1.osg33.el7\nglideinwms-libs-3.2.13-1.osg33.el7\nglideinwms-minimal-condor-3.2.13-1.osg33.el7\nglideinwms-usercollector-3.2.13-1.osg33.el7\nglideinwms-userschedd-3.2.13-1.osg33.el7\nglideinwms-vofrontend-3.2.13-1.osg33.el7\nglideinwms-vofrontend-standalone-3.2.13-1.osg33.el7\ngridftp-hdfs-0.5.4-25.osg33.el7\ngridftp-hdfs-debuginfo-0.5.4-25.osg33.el7\nhadoop-0.20-conf-pseudo-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-0.20-mapreduce-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-client-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-conf-pseudo-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-debuginfo-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-doc-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-hdfs-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-hdfs-datanode-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-hdfs-fuse-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-hdfs-fuse-selinux-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-hdfs-journalnode-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-hdfs-namenode-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-hdfs-secondarynamenode-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-hdfs-zkfc-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-httpfs-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-libhdfs-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-mapreduce-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhadoop-yarn-2.0.0+1612-1.cdh4.7.1.p0.12.3.osg33.el7\nhtcondor-ce-2.0.4-1.osg33.el7\nhtcondor-ce-client-2.0.4-1.osg33.el7\nhtcondor-ce-collector-2.0.4-1.osg33.el7\nhtcondor-ce-condor-2.0.4-1.osg33.el7\nhtcondor-ce-lsf-2.0.4-1.osg33.el7\nhtcondor-ce-pbs-2.0.4-1.osg33.el7\nhtcondor-ce-sge-2.0.4-1.osg33.el7\nhtcondor-ce-view-2.0.4-1.osg33.el7\nigtf-ca-certs-1.73-1.osg33.el7\nosg-ca-certs-1.54-1.osg33.el7\nosg-gums-config-65-2.osg33.el7\nosg-pki-tools-1.2.15-1.osg33.el7\nosg-pki-tools-tests-1.2.15-1.osg33.el7\nosg-release-3.3-5.osg33.el7\nosg-release-itb-3.3-5.osg33.el7\nosg-test-1.6.0-1.osg33.el7\nosg-version-3.3.11-1.osg33.el7\nvo-client-65-2.osg33.el7\nvo-client-edgmkgridmap-65-2.osg33.el7\nxrootd-4.3.0-1.osg33.el7\nxrootd-client-4.3.0-1.osg33.el7\nxrootd-client-devel-4.3.0-1.osg33.el7\nxrootd-client-libs-4.3.0-1.osg33.el7\nxrootd-debuginfo-4.3.0-1.osg33.el7\nxrootd-devel-4.3.0-1.osg33.el7\nxrootd-doc-4.3.0-1.osg33.el7\nxrootd-dsi-3.0.4-17.osg33.el7\nxrootd-dsi-debuginfo-3.0.4-17.osg33.el7\nxrootd-fuse-4.3.0-1.osg33.el7\nxrootd-hdfs-1.8.7-2.osg33.el7\nxrootd-hdfs-debuginfo-1.8.7-2.osg33.el7\nxrootd-hdfs-devel-1.8.7-2.osg33.el7\nxrootd-libs-4.3.0-1.osg33.el7\nxrootd-private-devel-4.3.0-1.osg33.el7\nxrootd-python-4.3.0-1.osg33.el7\nxrootd-selinux-4.3.0-1.osg33.el7\nxrootd-server-4.3.0-1.osg33.el7\nxrootd-server-devel-4.3.0-1.osg33.el7\nxrootd-server-libs-4.3.0-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-11/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-11/#enterprise-linux-6_2", 
            "text": "blahp-1.18.18.bosco-1.osgup.el6  condor-8.5.3-1.osgup.el6  cvmfs-2.2.1-1.osgup.el6  cvmfs-config-osg-1.2-3.osgup.el6  osg-oasis-6-4.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-11/#enterprise-linux-7_2", 
            "text": "blahp-1.18.18.bosco-1.osgup.el7  condor-8.5.3-1.osgup.el7  cvmfs-2.2.1-1.osgup.el7  cvmfs-config-osg-1.2-3.osgup.el7  osg-oasis-6-4.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-11/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp cvmfs cvmfs-config-osg cvmfs-devel cvmfs-server cvmfs-unittests osg-oasis  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-11/#enterprise-linux-6_3", 
            "text": "blahp-1.18.18.bosco-1.osgup.el6\nblahp-debuginfo-1.18.18.bosco-1.osgup.el6\ncondor-8.5.3-1.osgup.el6\ncondor-all-8.5.3-1.osgup.el6\ncondor-bosco-8.5.3-1.osgup.el6\ncondor-classads-8.5.3-1.osgup.el6\ncondor-classads-devel-8.5.3-1.osgup.el6\ncondor-cream-gahp-8.5.3-1.osgup.el6\ncondor-debuginfo-8.5.3-1.osgup.el6\ncondor-kbdd-8.5.3-1.osgup.el6\ncondor-procd-8.5.3-1.osgup.el6\ncondor-python-8.5.3-1.osgup.el6\ncondor-std-universe-8.5.3-1.osgup.el6\ncondor-test-8.5.3-1.osgup.el6\ncondor-vm-gahp-8.5.3-1.osgup.el6\ncvmfs-2.2.1-1.osgup.el6\ncvmfs-config-osg-1.2-3.osgup.el6\ncvmfs-devel-2.2.1-1.osgup.el6\ncvmfs-server-2.2.1-1.osgup.el6\ncvmfs-unittests-2.2.1-1.osgup.el6\nosg-oasis-6-4.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-11/#enterprise-linux-7_3", 
            "text": "blahp-1.18.18.bosco-1.osgup.el7\nblahp-debuginfo-1.18.18.bosco-1.osgup.el7\ncondor-8.5.3-1.osgup.el7\ncondor-all-8.5.3-1.osgup.el7\ncondor-bosco-8.5.3-1.osgup.el7\ncondor-classads-8.5.3-1.osgup.el7\ncondor-classads-devel-8.5.3-1.osgup.el7\ncondor-debuginfo-8.5.3-1.osgup.el7\ncondor-kbdd-8.5.3-1.osgup.el7\ncondor-procd-8.5.3-1.osgup.el7\ncondor-python-8.5.3-1.osgup.el7\ncondor-test-8.5.3-1.osgup.el7\ncondor-vm-gahp-8.5.3-1.osgup.el7\ncvmfs-2.2.1-1.osgup.el7\ncvmfs-config-osg-1.2-3.osgup.el7\ncvmfs-devel-2.2.1-1.osgup.el7\ncvmfs-server-2.2.1-1.osgup.el7\ncvmfs-unittests-2.2.1-1.osgup.el7\nosg-oasis-6-4.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-10/", 
            "text": "OSG Software Release 3.3.10\n\n\nRelease Date\n: 2016-03-08\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nVO Package v64\n - More OSG CA migrations\n\n\nCA Certificates based on \nIGTF 1.72\n\n\nosg-ca-certs-updater no longer requires certificate compatibility packages\n\n\nGFAL environment variables are now set in the client tarballs\n\n\nBLAHP 1.18.17: Additional support for LSF, SGE, and BOSCO\n\n\nLSF: Properly handle LSF suspended jobs states\n\n\nSGE: Fixed scripts to work with SGE\n\n\nBOSCO: Report RemoteSysCpu so that a gratia record can be created\n\n\n\n\n\n\nosg-configure: Propagate SGE parameters into the BLAHP\n\n\nHTCondor 8.4.4\n: Various bug fixes\n\n\nGUMS 1.5.2: Bug fix for LDAP-based user groups\n\n\nGratia probes: Improve robustness of handling batch system records\n\n\nFixed parsing of GridJobIds that start with \"batch\"\n\n\nImprove performance of certinfo file lookup\n\n\n\n\n\n\nStashCache: Updated configuration templates\n\n\nAutoPyFactory 2.4.6: a provisioning factory layered on top of HTCondor\n\n\nSupport for XRootD and GridFTP HDFS plugins on EL7 platforms\n\n\nHTCondor CE 2.0.2: HTCondor CE view installable with HTCondor CE collector\n\n\nHTCondor 8.5.2\n in the Upcoming repository\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nKnown Issues\n\n\n\n\ncondor_ce_q\n does not show any jobs when run as root with \ncondor-8.5.2\n from upcoming. Work around this by using \ncondor_ce_q -allusers\n instead.\n\n\n\n\n\n\n\n\n\nThe new HTCondor-CE View has a bug where some graphs show up blank. This may also manifest in errors like the following in \n/var/log/condor-ce/GangliadLog\n: \\ \n\n\n\n\n1/11/16 15:05:54 Failed to execute /usr/share/condor-ce/condor_ce_metric --conf /etc/ganglia/gmond.conf --group HTCondor.Schedd --name SchedulerRecentDaemonCoreDutyCycle --value 1.04449 --type float --units % --slope both --spoof 192.170.227.226:itbv-ce-htcondor.mwt2.org --tmax 120 --dmax 86400: Usage: condor_ce_metric [options]\n\n\ncondor_ce_metric: error: no such option: --conf\n\n\n01/11/16 15:05:54 Failed to publish metric SchedulerRecentDaemonCoreDutyCycle for itbv-ce-htcondor.mwt2.org \n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nautopyfactory-2.4.6-4.osg33.el6\n\n\nblahp-1.18.17.bosco-1.osg33.el6\n\n\ncondor-8.4.4-1.osg33.el6\n\n\ngratia-probe-1.15.0-1.osg33.el6\n\n\ngums-1.5.2-1.osg33.el6\n\n\nhtcondor-ce-2.0.2-1.osg33.el6\n\n\nigtf-ca-certs-1.72-1.osg33.el6\n\n\nosg-build-1.6.2-1.osg33.el6\n\n\nosg-ca-certs-1.53-1.osg33.el6\n\n\nosg-ca-certs-updater-1.4-1.osg33.el6\n\n\nosg-ca-generator-1.0.4-1.osg33.el6\n\n\nosg-configure-1.2.6-1.osg33.el6\n\n\nosg-gridftp-hdfs-3.3-3.osg33.el6\n\n\nosg-se-hadoop-3.3-3.osg33.el6\n\n\nosg-test-1.5.3-1.osg33.el6\n\n\nosg-tested-internal-3.3-10.osg33.el6\n\n\nosg-version-3.3.10-1.osg33.el6\n\n\nstashcache-0.6-2.osg33.el6\n\n\nvo-client-64-1.osg33.el6\n\n\nxrootd-hdfs-1.8.7-1.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nautopyfactory-2.4.6-4.osg33.el7\n\n\nblahp-1.18.17.bosco-1.osg33.el7\n\n\ncondor-8.4.4-1.osg33.el7\n\n\ngratia-probe-1.15.0-1.osg33.el7\n\n\ngridftp-hdfs-0.5.4-24.osg33.el7\n\n\nhtcondor-ce-2.0.2-1.osg33.el7\n\n\nigtf-ca-certs-1.72-1.osg33.el7\n\n\nosg-build-1.6.2-1.osg33.el7\n\n\nosg-ca-certs-1.53-1.osg33.el7\n\n\nosg-ca-certs-updater-1.4-1.osg33.el7\n\n\nosg-ca-generator-1.0.4-1.osg33.el7\n\n\nosg-configure-1.2.6-1.osg33.el7\n\n\nosg-gridftp-hdfs-3.3-3.osg33.el7\n\n\nosg-se-hadoop-3.3-3.osg33.el7\n\n\nosg-test-1.5.3-1.osg33.el7\n\n\nosg-tested-internal-3.3-10.osg33.el7\n\n\nosg-version-3.3.10-1.osg33.el7\n\n\nstashcache-0.6-2.osg33.el7\n\n\nvo-client-64-1.osg33.el7\n\n\nxrootd-hdfs-1.8.7-1.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nautopyfactory-cloud autopyfactory-common autopyfactory-panda autopyfactory-plugins-cloud autopyfactory-plugins-local autopyfactory-plugins-monitor autopyfactory-plugins-panda autopyfactory-plugins-remote autopyfactory-plugins-scheds autopyfactory-proxymanager autopyfactory-remote autopyfactory-wms blahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp gratia-probe-bdii-status gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glexec gratia-probe-glideinwms gratia-probe-gram gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer gums gums-client gums-service htcondor-ce htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-debuginfo htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-view igtf-ca-certs osg-build osg-ca-certs osg-ca-certs-updater osg-ca-generator osg-configure osg-configure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-gridftp-hdfs osg-gums-config osg-se-hadoop osg-se-hadoop-client osg-se-hadoop-datanode osg-se-hadoop-gridftp osg-se-hadoop-namenode osg-se-hadoop-secondarynamenode osg-se-hadoop-srm osg-test osg-tested-internal osg-version stashcache-cache-server stashcache-daemon stashcache-origin-server vo-client vo-client-edgmkgridmap xrootd-hdfs xrootd-hdfs-debuginfo xrootd-hdfs-devel\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nautopyfactory-2.4.6-4.osg33.el6\nautopyfactory-cloud-2.4.6-4.osg33.el6\nautopyfactory-common-2.4.6-4.osg33.el6\nautopyfactory-panda-2.4.6-4.osg33.el6\nautopyfactory-plugins-cloud-2.4.6-4.osg33.el6\nautopyfactory-plugins-local-2.4.6-4.osg33.el6\nautopyfactory-plugins-monitor-2.4.6-4.osg33.el6\nautopyfactory-plugins-panda-2.4.6-4.osg33.el6\nautopyfactory-plugins-remote-2.4.6-4.osg33.el6\nautopyfactory-plugins-scheds-2.4.6-4.osg33.el6\nautopyfactory-proxymanager-2.4.6-4.osg33.el6\nautopyfactory-remote-2.4.6-4.osg33.el6\nautopyfactory-wms-2.4.6-4.osg33.el6\nblahp-1.18.17.bosco-1.osg33.el6\nblahp-debuginfo-1.18.17.bosco-1.osg33.el6\ncondor-8.4.4-1.osg33.el6\ncondor-all-8.4.4-1.osg33.el6\ncondor-bosco-8.4.4-1.osg33.el6\ncondor-classads-8.4.4-1.osg33.el6\ncondor-classads-devel-8.4.4-1.osg33.el6\ncondor-cream-gahp-8.4.4-1.osg33.el6\ncondor-debuginfo-8.4.4-1.osg33.el6\ncondor-kbdd-8.4.4-1.osg33.el6\ncondor-procd-8.4.4-1.osg33.el6\ncondor-python-8.4.4-1.osg33.el6\ncondor-std-universe-8.4.4-1.osg33.el6\ncondor-test-8.4.4-1.osg33.el6\ncondor-vm-gahp-8.4.4-1.osg33.el6\ngratia-probe-1.15.0-1.osg33.el6\ngratia-probe-bdii-status-1.15.0-1.osg33.el6\ngratia-probe-common-1.15.0-1.osg33.el6\ngratia-probe-condor-1.15.0-1.osg33.el6\ngratia-probe-condor-events-1.15.0-1.osg33.el6\ngratia-probe-dcache-storage-1.15.0-1.osg33.el6\ngratia-probe-dcache-storagegroup-1.15.0-1.osg33.el6\ngratia-probe-dcache-transfer-1.15.0-1.osg33.el6\ngratia-probe-debuginfo-1.15.0-1.osg33.el6\ngratia-probe-enstore-storage-1.15.0-1.osg33.el6\ngratia-probe-enstore-tapedrive-1.15.0-1.osg33.el6\ngratia-probe-enstore-transfer-1.15.0-1.osg33.el6\ngratia-probe-glexec-1.15.0-1.osg33.el6\ngratia-probe-glideinwms-1.15.0-1.osg33.el6\ngratia-probe-gram-1.15.0-1.osg33.el6\ngratia-probe-gridftp-transfer-1.15.0-1.osg33.el6\ngratia-probe-hadoop-storage-1.15.0-1.osg33.el6\ngratia-probe-lsf-1.15.0-1.osg33.el6\ngratia-probe-metric-1.15.0-1.osg33.el6\ngratia-probe-onevm-1.15.0-1.osg33.el6\ngratia-probe-pbs-lsf-1.15.0-1.osg33.el6\ngratia-probe-services-1.15.0-1.osg33.el6\ngratia-probe-sge-1.15.0-1.osg33.el6\ngratia-probe-slurm-1.15.0-1.osg33.el6\ngratia-probe-xrootd-storage-1.15.0-1.osg33.el6\ngratia-probe-xrootd-transfer-1.15.0-1.osg33.el6\ngums-1.5.2-1.osg33.el6\ngums-client-1.5.2-1.osg33.el6\ngums-service-1.5.2-1.osg33.el6\nhtcondor-ce-2.0.2-1.osg33.el6\nhtcondor-ce-client-2.0.2-1.osg33.el6\nhtcondor-ce-collector-2.0.2-1.osg33.el6\nhtcondor-ce-condor-2.0.2-1.osg33.el6\nhtcondor-ce-debuginfo-2.0.2-1.osg33.el6\nhtcondor-ce-lsf-2.0.2-1.osg33.el6\nhtcondor-ce-pbs-2.0.2-1.osg33.el6\nhtcondor-ce-sge-2.0.2-1.osg33.el6\nhtcondor-ce-view-2.0.2-1.osg33.el6\nigtf-ca-certs-1.72-1.osg33.el6\nosg-build-1.6.2-1.osg33.el6\nosg-ca-certs-1.53-1.osg33.el6\nosg-ca-certs-updater-1.4-1.osg33.el6\nosg-ca-generator-1.0.4-1.osg33.el6\nosg-configure-1.2.6-1.osg33.el6\nosg-configure-ce-1.2.6-1.osg33.el6\nosg-configure-cemon-1.2.6-1.osg33.el6\nosg-configure-condor-1.2.6-1.osg33.el6\nosg-configure-gateway-1.2.6-1.osg33.el6\nosg-configure-gip-1.2.6-1.osg33.el6\nosg-configure-gratia-1.2.6-1.osg33.el6\nosg-configure-infoservices-1.2.6-1.osg33.el6\nosg-configure-lsf-1.2.6-1.osg33.el6\nosg-configure-managedfork-1.2.6-1.osg33.el6\nosg-configure-misc-1.2.6-1.osg33.el6\nosg-configure-monalisa-1.2.6-1.osg33.el6\nosg-configure-network-1.2.6-1.osg33.el6\nosg-configure-pbs-1.2.6-1.osg33.el6\nosg-configure-rsv-1.2.6-1.osg33.el6\nosg-configure-sge-1.2.6-1.osg33.el6\nosg-configure-slurm-1.2.6-1.osg33.el6\nosg-configure-squid-1.2.6-1.osg33.el6\nosg-configure-tests-1.2.6-1.osg33.el6\nosg-gridftp-hdfs-3.3-3.osg33.el6\nosg-gums-config-64-1.osg33.el6\nosg-se-hadoop-3.3-3.osg33.el6\nosg-se-hadoop-client-3.3-3.osg33.el6\nosg-se-hadoop-datanode-3.3-3.osg33.el6\nosg-se-hadoop-gridftp-3.3-3.osg33.el6\nosg-se-hadoop-namenode-3.3-3.osg33.el6\nosg-se-hadoop-secondarynamenode-3.3-3.osg33.el6\nosg-se-hadoop-srm-3.3-3.osg33.el6\nosg-test-1.5.3-1.osg33.el6\nosg-tested-internal-3.3-10.osg33.el6\nosg-version-3.3.10-1.osg33.el6\nstashcache-0.6-2.osg33.el6\nstashcache-cache-server-0.6-2.osg33.el6\nstashcache-daemon-0.6-2.osg33.el6\nstashcache-origin-server-0.6-2.osg33.el6\nvo-client-64-1.osg33.el6\nvo-client-edgmkgridmap-64-1.osg33.el6\nxrootd-hdfs-1.8.7-1.osg33.el6\nxrootd-hdfs-debuginfo-1.8.7-1.osg33.el6\nxrootd-hdfs-devel-1.8.7-1.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nautopyfactory-2.4.6-4.osg33.el7\nautopyfactory-cloud-2.4.6-4.osg33.el7\nautopyfactory-common-2.4.6-4.osg33.el7\nautopyfactory-panda-2.4.6-4.osg33.el7\nautopyfactory-plugins-cloud-2.4.6-4.osg33.el7\nautopyfactory-plugins-local-2.4.6-4.osg33.el7\nautopyfactory-plugins-monitor-2.4.6-4.osg33.el7\nautopyfactory-plugins-panda-2.4.6-4.osg33.el7\nautopyfactory-plugins-remote-2.4.6-4.osg33.el7\nautopyfactory-plugins-scheds-2.4.6-4.osg33.el7\nautopyfactory-proxymanager-2.4.6-4.osg33.el7\nautopyfactory-remote-2.4.6-4.osg33.el7\nautopyfactory-wms-2.4.6-4.osg33.el7\nblahp-1.18.17.bosco-1.osg33.el7\nblahp-debuginfo-1.18.17.bosco-1.osg33.el7\ncondor-8.4.4-1.osg33.el7\ncondor-all-8.4.4-1.osg33.el7\ncondor-bosco-8.4.4-1.osg33.el7\ncondor-classads-8.4.4-1.osg33.el7\ncondor-classads-devel-8.4.4-1.osg33.el7\ncondor-debuginfo-8.4.4-1.osg33.el7\ncondor-kbdd-8.4.4-1.osg33.el7\ncondor-procd-8.4.4-1.osg33.el7\ncondor-python-8.4.4-1.osg33.el7\ncondor-test-8.4.4-1.osg33.el7\ncondor-vm-gahp-8.4.4-1.osg33.el7\ngratia-probe-1.15.0-1.osg33.el7\ngratia-probe-bdii-status-1.15.0-1.osg33.el7\ngratia-probe-common-1.15.0-1.osg33.el7\ngratia-probe-condor-1.15.0-1.osg33.el7\ngratia-probe-condor-events-1.15.0-1.osg33.el7\ngratia-probe-dcache-storage-1.15.0-1.osg33.el7\ngratia-probe-dcache-storagegroup-1.15.0-1.osg33.el7\ngratia-probe-dcache-transfer-1.15.0-1.osg33.el7\ngratia-probe-debuginfo-1.15.0-1.osg33.el7\ngratia-probe-enstore-storage-1.15.0-1.osg33.el7\ngratia-probe-enstore-tapedrive-1.15.0-1.osg33.el7\ngratia-probe-enstore-transfer-1.15.0-1.osg33.el7\ngratia-probe-glexec-1.15.0-1.osg33.el7\ngratia-probe-glideinwms-1.15.0-1.osg33.el7\ngratia-probe-gram-1.15.0-1.osg33.el7\ngratia-probe-gridftp-transfer-1.15.0-1.osg33.el7\ngratia-probe-hadoop-storage-1.15.0-1.osg33.el7\ngratia-probe-lsf-1.15.0-1.osg33.el7\ngratia-probe-metric-1.15.0-1.osg33.el7\ngratia-probe-onevm-1.15.0-1.osg33.el7\ngratia-probe-pbs-lsf-1.15.0-1.osg33.el7\ngratia-probe-services-1.15.0-1.osg33.el7\ngratia-probe-sge-1.15.0-1.osg33.el7\ngratia-probe-slurm-1.15.0-1.osg33.el7\ngratia-probe-xrootd-storage-1.15.0-1.osg33.el7\ngratia-probe-xrootd-transfer-1.15.0-1.osg33.el7\ngridftp-hdfs-0.5.4-24.osg33.el7\ngridftp-hdfs-debuginfo-0.5.4-24.osg33.el7\nhtcondor-ce-2.0.2-1.osg33.el7\nhtcondor-ce-client-2.0.2-1.osg33.el7\nhtcondor-ce-collector-2.0.2-1.osg33.el7\nhtcondor-ce-condor-2.0.2-1.osg33.el7\nhtcondor-ce-debuginfo-2.0.2-1.osg33.el7\nhtcondor-ce-lsf-2.0.2-1.osg33.el7\nhtcondor-ce-pbs-2.0.2-1.osg33.el7\nhtcondor-ce-sge-2.0.2-1.osg33.el7\nhtcondor-ce-view-2.0.2-1.osg33.el7\nigtf-ca-certs-1.72-1.osg33.el7\nosg-build-1.6.2-1.osg33.el7\nosg-ca-certs-1.53-1.osg33.el7\nosg-ca-certs-updater-1.4-1.osg33.el7\nosg-ca-generator-1.0.4-1.osg33.el7\nosg-configure-1.2.6-1.osg33.el7\nosg-configure-ce-1.2.6-1.osg33.el7\nosg-configure-cemon-1.2.6-1.osg33.el7\nosg-configure-condor-1.2.6-1.osg33.el7\nosg-configure-gateway-1.2.6-1.osg33.el7\nosg-configure-gip-1.2.6-1.osg33.el7\nosg-configure-gratia-1.2.6-1.osg33.el7\nosg-configure-infoservices-1.2.6-1.osg33.el7\nosg-configure-lsf-1.2.6-1.osg33.el7\nosg-configure-managedfork-1.2.6-1.osg33.el7\nosg-configure-misc-1.2.6-1.osg33.el7\nosg-configure-monalisa-1.2.6-1.osg33.el7\nosg-configure-network-1.2.6-1.osg33.el7\nosg-configure-pbs-1.2.6-1.osg33.el7\nosg-configure-rsv-1.2.6-1.osg33.el7\nosg-configure-sge-1.2.6-1.osg33.el7\nosg-configure-slurm-1.2.6-1.osg33.el7\nosg-configure-squid-1.2.6-1.osg33.el7\nosg-configure-tests-1.2.6-1.osg33.el7\nosg-gridftp-hdfs-3.3-3.osg33.el7\nosg-gums-config-64-1.osg33.el7\nosg-se-hadoop-3.3-3.osg33.el7\nosg-se-hadoop-client-3.3-3.osg33.el7\nosg-se-hadoop-datanode-3.3-3.osg33.el7\nosg-se-hadoop-gridftp-3.3-3.osg33.el7\nosg-se-hadoop-namenode-3.3-3.osg33.el7\nosg-se-hadoop-secondarynamenode-3.3-3.osg33.el7\nosg-se-hadoop-srm-3.3-3.osg33.el7\nosg-test-1.5.3-1.osg33.el7\nosg-tested-internal-3.3-10.osg33.el7\nosg-version-3.3.10-1.osg33.el7\nstashcache-0.6-2.osg33.el7\nstashcache-cache-server-0.6-2.osg33.el7\nstashcache-daemon-0.6-2.osg33.el7\nstashcache-origin-server-0.6-2.osg33.el7\nvo-client-64-1.osg33.el7\nvo-client-edgmkgridmap-64-1.osg33.el7\nxrootd-hdfs-1.8.7-1.osg33.el7\nxrootd-hdfs-debuginfo-1.8.7-1.osg33.el7\nxrootd-hdfs-devel-1.8.7-1.osg33.el7\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.17.bosco-1.osgup.el6\n\n\ncondor-8.5.2-1.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.17.bosco-1.osgup.el7\n\n\ncondor-8.5.2-1.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp-1.18.17.bosco-1.osgup.el6\nblahp-debuginfo-1.18.17.bosco-1.osgup.el6\ncondor-8.5.2-1.osgup.el6\ncondor-all-8.5.2-1.osgup.el6\ncondor-bosco-8.5.2-1.osgup.el6\ncondor-classads-8.5.2-1.osgup.el6\ncondor-classads-devel-8.5.2-1.osgup.el6\ncondor-cream-gahp-8.5.2-1.osgup.el6\ncondor-debuginfo-8.5.2-1.osgup.el6\ncondor-kbdd-8.5.2-1.osgup.el6\ncondor-procd-8.5.2-1.osgup.el6\ncondor-python-8.5.2-1.osgup.el6\ncondor-std-universe-8.5.2-1.osgup.el6\ncondor-test-8.5.2-1.osgup.el6\ncondor-vm-gahp-8.5.2-1.osgup.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp-1.18.17.bosco-1.osgup.el7\nblahp-debuginfo-1.18.17.bosco-1.osgup.el7\ncondor-8.5.2-1.osgup.el7\ncondor-all-8.5.2-1.osgup.el7\ncondor-bosco-8.5.2-1.osgup.el7\ncondor-classads-8.5.2-1.osgup.el7\ncondor-classads-devel-8.5.2-1.osgup.el7\ncondor-debuginfo-8.5.2-1.osgup.el7\ncondor-kbdd-8.5.2-1.osgup.el7\ncondor-procd-8.5.2-1.osgup.el7\ncondor-python-8.5.2-1.osgup.el7\ncondor-test-8.5.2-1.osgup.el7\ncondor-vm-gahp-8.5.2-1.osgup.el7", 
            "title": "OSG Release 3.3.10"
        }, 
        {
            "location": "/release/3.3/release-3-3-10/#osg-software-release-3310", 
            "text": "Release Date : 2016-03-08", 
            "title": "OSG Software Release 3.3.10"
        }, 
        {
            "location": "/release/3.3/release-3-3-10/#summary-of-changes", 
            "text": "This release contains:   VO Package v64  - More OSG CA migrations  CA Certificates based on  IGTF 1.72  osg-ca-certs-updater no longer requires certificate compatibility packages  GFAL environment variables are now set in the client tarballs  BLAHP 1.18.17: Additional support for LSF, SGE, and BOSCO  LSF: Properly handle LSF suspended jobs states  SGE: Fixed scripts to work with SGE  BOSCO: Report RemoteSysCpu so that a gratia record can be created    osg-configure: Propagate SGE parameters into the BLAHP  HTCondor 8.4.4 : Various bug fixes  GUMS 1.5.2: Bug fix for LDAP-based user groups  Gratia probes: Improve robustness of handling batch system records  Fixed parsing of GridJobIds that start with \"batch\"  Improve performance of certinfo file lookup    StashCache: Updated configuration templates  AutoPyFactory 2.4.6: a provisioning factory layered on top of HTCondor  Support for XRootD and GridFTP HDFS plugins on EL7 platforms  HTCondor CE 2.0.2: HTCondor CE view installable with HTCondor CE collector  HTCondor 8.5.2  in the Upcoming repository   These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-10/#known-issues", 
            "text": "condor_ce_q  does not show any jobs when run as root with  condor-8.5.2  from upcoming. Work around this by using  condor_ce_q -allusers  instead.     The new HTCondor-CE View has a bug where some graphs show up blank. This may also manifest in errors like the following in  /var/log/condor-ce/GangliadLog : \\    1/11/16 15:05:54 Failed to execute /usr/share/condor-ce/condor_ce_metric --conf /etc/ganglia/gmond.conf --group HTCondor.Schedd --name SchedulerRecentDaemonCoreDutyCycle --value 1.04449 --type float --units % --slope both --spoof 192.170.227.226:itbv-ce-htcondor.mwt2.org --tmax 120 --dmax 86400: Usage: condor_ce_metric [options]  condor_ce_metric: error: no such option: --conf  01/11/16 15:05:54 Failed to publish metric SchedulerRecentDaemonCoreDutyCycle for itbv-ce-htcondor.mwt2.org", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.3/release-3-3-10/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-10/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-10/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-10/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-10/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-10/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-10/#enterprise-linux-6", 
            "text": "autopyfactory-2.4.6-4.osg33.el6  blahp-1.18.17.bosco-1.osg33.el6  condor-8.4.4-1.osg33.el6  gratia-probe-1.15.0-1.osg33.el6  gums-1.5.2-1.osg33.el6  htcondor-ce-2.0.2-1.osg33.el6  igtf-ca-certs-1.72-1.osg33.el6  osg-build-1.6.2-1.osg33.el6  osg-ca-certs-1.53-1.osg33.el6  osg-ca-certs-updater-1.4-1.osg33.el6  osg-ca-generator-1.0.4-1.osg33.el6  osg-configure-1.2.6-1.osg33.el6  osg-gridftp-hdfs-3.3-3.osg33.el6  osg-se-hadoop-3.3-3.osg33.el6  osg-test-1.5.3-1.osg33.el6  osg-tested-internal-3.3-10.osg33.el6  osg-version-3.3.10-1.osg33.el6  stashcache-0.6-2.osg33.el6  vo-client-64-1.osg33.el6  xrootd-hdfs-1.8.7-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-10/#enterprise-linux-7", 
            "text": "autopyfactory-2.4.6-4.osg33.el7  blahp-1.18.17.bosco-1.osg33.el7  condor-8.4.4-1.osg33.el7  gratia-probe-1.15.0-1.osg33.el7  gridftp-hdfs-0.5.4-24.osg33.el7  htcondor-ce-2.0.2-1.osg33.el7  igtf-ca-certs-1.72-1.osg33.el7  osg-build-1.6.2-1.osg33.el7  osg-ca-certs-1.53-1.osg33.el7  osg-ca-certs-updater-1.4-1.osg33.el7  osg-ca-generator-1.0.4-1.osg33.el7  osg-configure-1.2.6-1.osg33.el7  osg-gridftp-hdfs-3.3-3.osg33.el7  osg-se-hadoop-3.3-3.osg33.el7  osg-test-1.5.3-1.osg33.el7  osg-tested-internal-3.3-10.osg33.el7  osg-version-3.3.10-1.osg33.el7  stashcache-0.6-2.osg33.el7  vo-client-64-1.osg33.el7  xrootd-hdfs-1.8.7-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-10/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  autopyfactory-cloud autopyfactory-common autopyfactory-panda autopyfactory-plugins-cloud autopyfactory-plugins-local autopyfactory-plugins-monitor autopyfactory-plugins-panda autopyfactory-plugins-remote autopyfactory-plugins-scheds autopyfactory-proxymanager autopyfactory-remote autopyfactory-wms blahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp gratia-probe-bdii-status gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glexec gratia-probe-glideinwms gratia-probe-gram gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer gums gums-client gums-service htcondor-ce htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-debuginfo htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-view igtf-ca-certs osg-build osg-ca-certs osg-ca-certs-updater osg-ca-generator osg-configure osg-configure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-gridftp-hdfs osg-gums-config osg-se-hadoop osg-se-hadoop-client osg-se-hadoop-datanode osg-se-hadoop-gridftp osg-se-hadoop-namenode osg-se-hadoop-secondarynamenode osg-se-hadoop-srm osg-test osg-tested-internal osg-version stashcache-cache-server stashcache-daemon stashcache-origin-server vo-client vo-client-edgmkgridmap xrootd-hdfs xrootd-hdfs-debuginfo xrootd-hdfs-devel  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-10/#enterprise-linux-6_1", 
            "text": "autopyfactory-2.4.6-4.osg33.el6\nautopyfactory-cloud-2.4.6-4.osg33.el6\nautopyfactory-common-2.4.6-4.osg33.el6\nautopyfactory-panda-2.4.6-4.osg33.el6\nautopyfactory-plugins-cloud-2.4.6-4.osg33.el6\nautopyfactory-plugins-local-2.4.6-4.osg33.el6\nautopyfactory-plugins-monitor-2.4.6-4.osg33.el6\nautopyfactory-plugins-panda-2.4.6-4.osg33.el6\nautopyfactory-plugins-remote-2.4.6-4.osg33.el6\nautopyfactory-plugins-scheds-2.4.6-4.osg33.el6\nautopyfactory-proxymanager-2.4.6-4.osg33.el6\nautopyfactory-remote-2.4.6-4.osg33.el6\nautopyfactory-wms-2.4.6-4.osg33.el6\nblahp-1.18.17.bosco-1.osg33.el6\nblahp-debuginfo-1.18.17.bosco-1.osg33.el6\ncondor-8.4.4-1.osg33.el6\ncondor-all-8.4.4-1.osg33.el6\ncondor-bosco-8.4.4-1.osg33.el6\ncondor-classads-8.4.4-1.osg33.el6\ncondor-classads-devel-8.4.4-1.osg33.el6\ncondor-cream-gahp-8.4.4-1.osg33.el6\ncondor-debuginfo-8.4.4-1.osg33.el6\ncondor-kbdd-8.4.4-1.osg33.el6\ncondor-procd-8.4.4-1.osg33.el6\ncondor-python-8.4.4-1.osg33.el6\ncondor-std-universe-8.4.4-1.osg33.el6\ncondor-test-8.4.4-1.osg33.el6\ncondor-vm-gahp-8.4.4-1.osg33.el6\ngratia-probe-1.15.0-1.osg33.el6\ngratia-probe-bdii-status-1.15.0-1.osg33.el6\ngratia-probe-common-1.15.0-1.osg33.el6\ngratia-probe-condor-1.15.0-1.osg33.el6\ngratia-probe-condor-events-1.15.0-1.osg33.el6\ngratia-probe-dcache-storage-1.15.0-1.osg33.el6\ngratia-probe-dcache-storagegroup-1.15.0-1.osg33.el6\ngratia-probe-dcache-transfer-1.15.0-1.osg33.el6\ngratia-probe-debuginfo-1.15.0-1.osg33.el6\ngratia-probe-enstore-storage-1.15.0-1.osg33.el6\ngratia-probe-enstore-tapedrive-1.15.0-1.osg33.el6\ngratia-probe-enstore-transfer-1.15.0-1.osg33.el6\ngratia-probe-glexec-1.15.0-1.osg33.el6\ngratia-probe-glideinwms-1.15.0-1.osg33.el6\ngratia-probe-gram-1.15.0-1.osg33.el6\ngratia-probe-gridftp-transfer-1.15.0-1.osg33.el6\ngratia-probe-hadoop-storage-1.15.0-1.osg33.el6\ngratia-probe-lsf-1.15.0-1.osg33.el6\ngratia-probe-metric-1.15.0-1.osg33.el6\ngratia-probe-onevm-1.15.0-1.osg33.el6\ngratia-probe-pbs-lsf-1.15.0-1.osg33.el6\ngratia-probe-services-1.15.0-1.osg33.el6\ngratia-probe-sge-1.15.0-1.osg33.el6\ngratia-probe-slurm-1.15.0-1.osg33.el6\ngratia-probe-xrootd-storage-1.15.0-1.osg33.el6\ngratia-probe-xrootd-transfer-1.15.0-1.osg33.el6\ngums-1.5.2-1.osg33.el6\ngums-client-1.5.2-1.osg33.el6\ngums-service-1.5.2-1.osg33.el6\nhtcondor-ce-2.0.2-1.osg33.el6\nhtcondor-ce-client-2.0.2-1.osg33.el6\nhtcondor-ce-collector-2.0.2-1.osg33.el6\nhtcondor-ce-condor-2.0.2-1.osg33.el6\nhtcondor-ce-debuginfo-2.0.2-1.osg33.el6\nhtcondor-ce-lsf-2.0.2-1.osg33.el6\nhtcondor-ce-pbs-2.0.2-1.osg33.el6\nhtcondor-ce-sge-2.0.2-1.osg33.el6\nhtcondor-ce-view-2.0.2-1.osg33.el6\nigtf-ca-certs-1.72-1.osg33.el6\nosg-build-1.6.2-1.osg33.el6\nosg-ca-certs-1.53-1.osg33.el6\nosg-ca-certs-updater-1.4-1.osg33.el6\nosg-ca-generator-1.0.4-1.osg33.el6\nosg-configure-1.2.6-1.osg33.el6\nosg-configure-ce-1.2.6-1.osg33.el6\nosg-configure-cemon-1.2.6-1.osg33.el6\nosg-configure-condor-1.2.6-1.osg33.el6\nosg-configure-gateway-1.2.6-1.osg33.el6\nosg-configure-gip-1.2.6-1.osg33.el6\nosg-configure-gratia-1.2.6-1.osg33.el6\nosg-configure-infoservices-1.2.6-1.osg33.el6\nosg-configure-lsf-1.2.6-1.osg33.el6\nosg-configure-managedfork-1.2.6-1.osg33.el6\nosg-configure-misc-1.2.6-1.osg33.el6\nosg-configure-monalisa-1.2.6-1.osg33.el6\nosg-configure-network-1.2.6-1.osg33.el6\nosg-configure-pbs-1.2.6-1.osg33.el6\nosg-configure-rsv-1.2.6-1.osg33.el6\nosg-configure-sge-1.2.6-1.osg33.el6\nosg-configure-slurm-1.2.6-1.osg33.el6\nosg-configure-squid-1.2.6-1.osg33.el6\nosg-configure-tests-1.2.6-1.osg33.el6\nosg-gridftp-hdfs-3.3-3.osg33.el6\nosg-gums-config-64-1.osg33.el6\nosg-se-hadoop-3.3-3.osg33.el6\nosg-se-hadoop-client-3.3-3.osg33.el6\nosg-se-hadoop-datanode-3.3-3.osg33.el6\nosg-se-hadoop-gridftp-3.3-3.osg33.el6\nosg-se-hadoop-namenode-3.3-3.osg33.el6\nosg-se-hadoop-secondarynamenode-3.3-3.osg33.el6\nosg-se-hadoop-srm-3.3-3.osg33.el6\nosg-test-1.5.3-1.osg33.el6\nosg-tested-internal-3.3-10.osg33.el6\nosg-version-3.3.10-1.osg33.el6\nstashcache-0.6-2.osg33.el6\nstashcache-cache-server-0.6-2.osg33.el6\nstashcache-daemon-0.6-2.osg33.el6\nstashcache-origin-server-0.6-2.osg33.el6\nvo-client-64-1.osg33.el6\nvo-client-edgmkgridmap-64-1.osg33.el6\nxrootd-hdfs-1.8.7-1.osg33.el6\nxrootd-hdfs-debuginfo-1.8.7-1.osg33.el6\nxrootd-hdfs-devel-1.8.7-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-10/#enterprise-linux-7_1", 
            "text": "autopyfactory-2.4.6-4.osg33.el7\nautopyfactory-cloud-2.4.6-4.osg33.el7\nautopyfactory-common-2.4.6-4.osg33.el7\nautopyfactory-panda-2.4.6-4.osg33.el7\nautopyfactory-plugins-cloud-2.4.6-4.osg33.el7\nautopyfactory-plugins-local-2.4.6-4.osg33.el7\nautopyfactory-plugins-monitor-2.4.6-4.osg33.el7\nautopyfactory-plugins-panda-2.4.6-4.osg33.el7\nautopyfactory-plugins-remote-2.4.6-4.osg33.el7\nautopyfactory-plugins-scheds-2.4.6-4.osg33.el7\nautopyfactory-proxymanager-2.4.6-4.osg33.el7\nautopyfactory-remote-2.4.6-4.osg33.el7\nautopyfactory-wms-2.4.6-4.osg33.el7\nblahp-1.18.17.bosco-1.osg33.el7\nblahp-debuginfo-1.18.17.bosco-1.osg33.el7\ncondor-8.4.4-1.osg33.el7\ncondor-all-8.4.4-1.osg33.el7\ncondor-bosco-8.4.4-1.osg33.el7\ncondor-classads-8.4.4-1.osg33.el7\ncondor-classads-devel-8.4.4-1.osg33.el7\ncondor-debuginfo-8.4.4-1.osg33.el7\ncondor-kbdd-8.4.4-1.osg33.el7\ncondor-procd-8.4.4-1.osg33.el7\ncondor-python-8.4.4-1.osg33.el7\ncondor-test-8.4.4-1.osg33.el7\ncondor-vm-gahp-8.4.4-1.osg33.el7\ngratia-probe-1.15.0-1.osg33.el7\ngratia-probe-bdii-status-1.15.0-1.osg33.el7\ngratia-probe-common-1.15.0-1.osg33.el7\ngratia-probe-condor-1.15.0-1.osg33.el7\ngratia-probe-condor-events-1.15.0-1.osg33.el7\ngratia-probe-dcache-storage-1.15.0-1.osg33.el7\ngratia-probe-dcache-storagegroup-1.15.0-1.osg33.el7\ngratia-probe-dcache-transfer-1.15.0-1.osg33.el7\ngratia-probe-debuginfo-1.15.0-1.osg33.el7\ngratia-probe-enstore-storage-1.15.0-1.osg33.el7\ngratia-probe-enstore-tapedrive-1.15.0-1.osg33.el7\ngratia-probe-enstore-transfer-1.15.0-1.osg33.el7\ngratia-probe-glexec-1.15.0-1.osg33.el7\ngratia-probe-glideinwms-1.15.0-1.osg33.el7\ngratia-probe-gram-1.15.0-1.osg33.el7\ngratia-probe-gridftp-transfer-1.15.0-1.osg33.el7\ngratia-probe-hadoop-storage-1.15.0-1.osg33.el7\ngratia-probe-lsf-1.15.0-1.osg33.el7\ngratia-probe-metric-1.15.0-1.osg33.el7\ngratia-probe-onevm-1.15.0-1.osg33.el7\ngratia-probe-pbs-lsf-1.15.0-1.osg33.el7\ngratia-probe-services-1.15.0-1.osg33.el7\ngratia-probe-sge-1.15.0-1.osg33.el7\ngratia-probe-slurm-1.15.0-1.osg33.el7\ngratia-probe-xrootd-storage-1.15.0-1.osg33.el7\ngratia-probe-xrootd-transfer-1.15.0-1.osg33.el7\ngridftp-hdfs-0.5.4-24.osg33.el7\ngridftp-hdfs-debuginfo-0.5.4-24.osg33.el7\nhtcondor-ce-2.0.2-1.osg33.el7\nhtcondor-ce-client-2.0.2-1.osg33.el7\nhtcondor-ce-collector-2.0.2-1.osg33.el7\nhtcondor-ce-condor-2.0.2-1.osg33.el7\nhtcondor-ce-debuginfo-2.0.2-1.osg33.el7\nhtcondor-ce-lsf-2.0.2-1.osg33.el7\nhtcondor-ce-pbs-2.0.2-1.osg33.el7\nhtcondor-ce-sge-2.0.2-1.osg33.el7\nhtcondor-ce-view-2.0.2-1.osg33.el7\nigtf-ca-certs-1.72-1.osg33.el7\nosg-build-1.6.2-1.osg33.el7\nosg-ca-certs-1.53-1.osg33.el7\nosg-ca-certs-updater-1.4-1.osg33.el7\nosg-ca-generator-1.0.4-1.osg33.el7\nosg-configure-1.2.6-1.osg33.el7\nosg-configure-ce-1.2.6-1.osg33.el7\nosg-configure-cemon-1.2.6-1.osg33.el7\nosg-configure-condor-1.2.6-1.osg33.el7\nosg-configure-gateway-1.2.6-1.osg33.el7\nosg-configure-gip-1.2.6-1.osg33.el7\nosg-configure-gratia-1.2.6-1.osg33.el7\nosg-configure-infoservices-1.2.6-1.osg33.el7\nosg-configure-lsf-1.2.6-1.osg33.el7\nosg-configure-managedfork-1.2.6-1.osg33.el7\nosg-configure-misc-1.2.6-1.osg33.el7\nosg-configure-monalisa-1.2.6-1.osg33.el7\nosg-configure-network-1.2.6-1.osg33.el7\nosg-configure-pbs-1.2.6-1.osg33.el7\nosg-configure-rsv-1.2.6-1.osg33.el7\nosg-configure-sge-1.2.6-1.osg33.el7\nosg-configure-slurm-1.2.6-1.osg33.el7\nosg-configure-squid-1.2.6-1.osg33.el7\nosg-configure-tests-1.2.6-1.osg33.el7\nosg-gridftp-hdfs-3.3-3.osg33.el7\nosg-gums-config-64-1.osg33.el7\nosg-se-hadoop-3.3-3.osg33.el7\nosg-se-hadoop-client-3.3-3.osg33.el7\nosg-se-hadoop-datanode-3.3-3.osg33.el7\nosg-se-hadoop-gridftp-3.3-3.osg33.el7\nosg-se-hadoop-namenode-3.3-3.osg33.el7\nosg-se-hadoop-secondarynamenode-3.3-3.osg33.el7\nosg-se-hadoop-srm-3.3-3.osg33.el7\nosg-test-1.5.3-1.osg33.el7\nosg-tested-internal-3.3-10.osg33.el7\nosg-version-3.3.10-1.osg33.el7\nstashcache-0.6-2.osg33.el7\nstashcache-cache-server-0.6-2.osg33.el7\nstashcache-daemon-0.6-2.osg33.el7\nstashcache-origin-server-0.6-2.osg33.el7\nvo-client-64-1.osg33.el7\nvo-client-edgmkgridmap-64-1.osg33.el7\nxrootd-hdfs-1.8.7-1.osg33.el7\nxrootd-hdfs-debuginfo-1.8.7-1.osg33.el7\nxrootd-hdfs-devel-1.8.7-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-10/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-10/#enterprise-linux-6_2", 
            "text": "blahp-1.18.17.bosco-1.osgup.el6  condor-8.5.2-1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-10/#enterprise-linux-7_2", 
            "text": "blahp-1.18.17.bosco-1.osgup.el7  condor-8.5.2-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-10/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-10/#enterprise-linux-6_3", 
            "text": "blahp-1.18.17.bosco-1.osgup.el6\nblahp-debuginfo-1.18.17.bosco-1.osgup.el6\ncondor-8.5.2-1.osgup.el6\ncondor-all-8.5.2-1.osgup.el6\ncondor-bosco-8.5.2-1.osgup.el6\ncondor-classads-8.5.2-1.osgup.el6\ncondor-classads-devel-8.5.2-1.osgup.el6\ncondor-cream-gahp-8.5.2-1.osgup.el6\ncondor-debuginfo-8.5.2-1.osgup.el6\ncondor-kbdd-8.5.2-1.osgup.el6\ncondor-procd-8.5.2-1.osgup.el6\ncondor-python-8.5.2-1.osgup.el6\ncondor-std-universe-8.5.2-1.osgup.el6\ncondor-test-8.5.2-1.osgup.el6\ncondor-vm-gahp-8.5.2-1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-10/#enterprise-linux-7_3", 
            "text": "blahp-1.18.17.bosco-1.osgup.el7\nblahp-debuginfo-1.18.17.bosco-1.osgup.el7\ncondor-8.5.2-1.osgup.el7\ncondor-all-8.5.2-1.osgup.el7\ncondor-bosco-8.5.2-1.osgup.el7\ncondor-classads-8.5.2-1.osgup.el7\ncondor-classads-devel-8.5.2-1.osgup.el7\ncondor-debuginfo-8.5.2-1.osgup.el7\ncondor-kbdd-8.5.2-1.osgup.el7\ncondor-procd-8.5.2-1.osgup.el7\ncondor-python-8.5.2-1.osgup.el7\ncondor-test-8.5.2-1.osgup.el7\ncondor-vm-gahp-8.5.2-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-9/", 
            "text": "OSG Software Release 3.3.9\n\n\nRelease Date\n: 2016-02-09\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nUpdate GSI-OpenSSH 5.7 to address \nCVE-2016-0777\n\n\nglideinWMS 3.2.12\n\n\nCVMFS over StashCache in the Upcoming Repository\n\n\nCVMFS 2.2.0\n\n\nConfiguration RPM\n\n\nXRootD LCMAPS 1.2.1\n\n\n\n\n\n\nXRootD HDFS plugin - support non-world-readable files\n\n\nlcmaps-plugins-scas-client - fix memory leak, better man page\n\n\nCA Certificates based on \nIGTF 1.71\n\n\nVO Package v63\n\n\nVOMS admin update to disable unnecessary CA check\n\n\nBetter warnings and message handling in RSV-perfsonar 1.1.2\n\n\nSupport for HDFS on EL7 platforms\n\n\njGlobus - remove tomcat dependency\n\n\nHTCondor CE - requires HTCondor \n= 8.3.7\n\n\nemi-trustmanager update to support gratia service in EL7\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nKnown Issues\n\n\n\n\nThe new HTCondor-CE View has a bug where some graphs show up blank. This may also manifest in errors like the following in \n/var/log/condor-ce/GangliadLog\n: \\ \n\n\n\n\n1/11/16 15:05:54 Failed to execute /usr/share/condor-ce/condor_ce_metric --conf /etc/ganglia/gmond.conf --group HTCondor.Schedd --name SchedulerRecentDaemonCoreDutyCycle --value 1.04449 --type float --units % --slope both --spoof 192.170.227.226:itbv-ce-htcondor.mwt2.org --tmax 120 --dmax 86400: Usage: condor_ce_metric [options]\n\n\ncondor_ce_metric: error: no such option: --conf\n\n\n01/11/16 15:05:54 Failed to publish metric SchedulerRecentDaemonCoreDutyCycle for itbv-ce-htcondor.mwt2.org \n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nemi-trustmanager-3.0.3-10.osg33.el6\n\n\nemi-trustmanager-tomcat-3.0.0-14.osg33.el6\n\n\nglideinwms-3.2.12.1-1.osg33.el6\n\n\ngsi-openssh-5.7-4.1.osg33.el6\n\n\nhadoop-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\n\n\nhtcondor-ce-2.0.0-2.osg33.el6\n\n\nigtf-ca-certs-1.71-1.osg33.el6\n\n\njglobus-2.1.0-7.osg33.el6\n\n\nlcmaps-plugins-scas-client-0.5.6-1.osg33.el6\n\n\nosg-ca-certs-1.52-1.osg33.el6\n\n\nosg-ca-generator-1.0.2-1.osg33.el6\n\n\nosg-test-1.5.1-1.osg33.el6\n\n\nosg-version-3.3.9-1.osg33.el6\n\n\nrsv-perfsonar-1.1.2-3.osg33.el6\n\n\nvo-client-63-1.osg33.el6\n\n\nvoms-admin-server-2.7.0-1.21.osg33.el6\n\n\nvoms-api-java-2.0.8-1.9.osg33.el6\n\n\nxrootd-hdfs-1.8.6-1.osg33.el6\n\n\nxrootd-lcmaps-1.2.1-1.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nemi-trustmanager-3.0.3-10.osg33.el7\n\n\nemi-trustmanager-tomcat-3.0.0-14.osg33.el7\n\n\nglideinwms-3.2.12.1-1.osg33.el7\n\n\ngsi-openssh-5.7-4.1.osg33.el7\n\n\nhadoop-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\n\n\nhtcondor-ce-2.0.0-2.osg33.el7\n\n\nigtf-ca-certs-1.71-1.osg33.el7\n\n\njglobus-2.1.0-7.osg33.el7\n\n\nlcmaps-plugins-scas-client-0.5.6-1.osg33.el7\n\n\nosg-ca-certs-1.52-1.osg33.el7\n\n\nosg-ca-generator-1.0.2-1.osg33.el7\n\n\nosg-test-1.5.1-1.osg33.el7\n\n\nosg-version-3.3.9-1.osg33.el7\n\n\nrsv-perfsonar-1.1.2-3.osg33.el7\n\n\nvo-client-63-1.osg33.el7\n\n\nvoms-api-java-2.0.8-1.9.osg33.el7\n\n\nxrootd-lcmaps-1.2.1-1.osg33.el7\n\n\nzookeeper-3.4.3+15-1.cdh4.0.1.p0.3.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nemi-trustmanager emi-trustmanager-tomcat glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone gsi-openssh gsi-openssh-clients gsi-openssh-debuginfo gsi-openssh-server hadoop hadoop-client hadoop-conf-pseudo hadoop-debuginfo hadoop-doc hadoop-hdfs hadoop-hdfs-datanode hadoop-hdfs-fuse hadoop-hdfs-fuse-selinux hadoop-hdfs-journalnode hadoop-hdfs-namenode hadoop-hdfs-secondarynamenode hadoop-hdfs-zkfc hadoop-httpfs hadoop-libhdfs hadoop-mapreduce hadoop-yarn htcondor-ce htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-debuginfo htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-view igtf-ca-certs jglobus lcmaps-plugins-scas-client lcmaps-plugins-scas-client-debuginfo osg-ca-certs osg-ca-generator osg-gums-config osg-test osg-version rsv-perfsonar vo-client vo-client-edgmkgridmap voms-admin-server voms-api-java voms-api-java-javadoc xrootd-hdfs xrootd-hdfs-debuginfo xrootd-hdfs-devel xrootd-lcmaps xrootd-lcmaps-debuginfo\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nemi-trustmanager-3.0.3-10.osg33.el6\nemi-trustmanager-tomcat-3.0.0-14.osg33.el6\nglideinwms-3.2.12.1-1.osg33.el6\nglideinwms-common-tools-3.2.12.1-1.osg33.el6\nglideinwms-condor-common-config-3.2.12.1-1.osg33.el6\nglideinwms-factory-3.2.12.1-1.osg33.el6\nglideinwms-factory-condor-3.2.12.1-1.osg33.el6\nglideinwms-glidecondor-tools-3.2.12.1-1.osg33.el6\nglideinwms-libs-3.2.12.1-1.osg33.el6\nglideinwms-minimal-condor-3.2.12.1-1.osg33.el6\nglideinwms-usercollector-3.2.12.1-1.osg33.el6\nglideinwms-userschedd-3.2.12.1-1.osg33.el6\nglideinwms-vofrontend-3.2.12.1-1.osg33.el6\nglideinwms-vofrontend-standalone-3.2.12.1-1.osg33.el6\ngsi-openssh-5.7-4.1.osg33.el6\ngsi-openssh-clients-5.7-4.1.osg33.el6\ngsi-openssh-debuginfo-5.7-4.1.osg33.el6\ngsi-openssh-server-5.7-4.1.osg33.el6\nhadoop-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\nhadoop-client-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\nhadoop-conf-pseudo-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\nhadoop-debuginfo-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\nhadoop-doc-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\nhadoop-hdfs-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\nhadoop-hdfs-datanode-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\nhadoop-hdfs-fuse-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\nhadoop-hdfs-fuse-selinux-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\nhadoop-hdfs-journalnode-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\nhadoop-hdfs-namenode-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\nhadoop-hdfs-secondarynamenode-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\nhadoop-hdfs-zkfc-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\nhadoop-httpfs-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\nhadoop-libhdfs-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\nhadoop-mapreduce-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\nhadoop-yarn-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\nhtcondor-ce-2.0.0-2.osg33.el6\nhtcondor-ce-client-2.0.0-2.osg33.el6\nhtcondor-ce-collector-2.0.0-2.osg33.el6\nhtcondor-ce-condor-2.0.0-2.osg33.el6\nhtcondor-ce-debuginfo-2.0.0-2.osg33.el6\nhtcondor-ce-lsf-2.0.0-2.osg33.el6\nhtcondor-ce-pbs-2.0.0-2.osg33.el6\nhtcondor-ce-sge-2.0.0-2.osg33.el6\nhtcondor-ce-view-2.0.0-2.osg33.el6\nigtf-ca-certs-1.71-1.osg33.el6\njglobus-2.1.0-7.osg33.el6\nlcmaps-plugins-scas-client-0.5.6-1.osg33.el6\nlcmaps-plugins-scas-client-debuginfo-0.5.6-1.osg33.el6\nosg-ca-certs-1.52-1.osg33.el6\nosg-ca-generator-1.0.2-1.osg33.el6\nosg-gums-config-63-1.osg33.el6\nosg-test-1.5.1-1.osg33.el6\nosg-version-3.3.9-1.osg33.el6\nrsv-perfsonar-1.1.2-3.osg33.el6\nvo-client-63-1.osg33.el6\nvo-client-edgmkgridmap-63-1.osg33.el6\nvoms-admin-server-2.7.0-1.21.osg33.el6\nvoms-api-java-2.0.8-1.9.osg33.el6\nvoms-api-java-javadoc-2.0.8-1.9.osg33.el6\nxrootd-hdfs-1.8.6-1.osg33.el6\nxrootd-hdfs-debuginfo-1.8.6-1.osg33.el6\nxrootd-hdfs-devel-1.8.6-1.osg33.el6\nxrootd-lcmaps-1.2.1-1.osg33.el6\nxrootd-lcmaps-debuginfo-1.2.1-1.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nemi-trustmanager-3.0.3-10.osg33.el7\nemi-trustmanager-tomcat-3.0.0-14.osg33.el7\nglideinwms-3.2.12.1-1.osg33.el7\nglideinwms-common-tools-3.2.12.1-1.osg33.el7\nglideinwms-condor-common-config-3.2.12.1-1.osg33.el7\nglideinwms-factory-3.2.12.1-1.osg33.el7\nglideinwms-factory-condor-3.2.12.1-1.osg33.el7\nglideinwms-glidecondor-tools-3.2.12.1-1.osg33.el7\nglideinwms-libs-3.2.12.1-1.osg33.el7\nglideinwms-minimal-condor-3.2.12.1-1.osg33.el7\nglideinwms-usercollector-3.2.12.1-1.osg33.el7\nglideinwms-userschedd-3.2.12.1-1.osg33.el7\nglideinwms-vofrontend-3.2.12.1-1.osg33.el7\nglideinwms-vofrontend-standalone-3.2.12.1-1.osg33.el7\ngsi-openssh-5.7-4.1.osg33.el7\ngsi-openssh-clients-5.7-4.1.osg33.el7\ngsi-openssh-debuginfo-5.7-4.1.osg33.el7\ngsi-openssh-server-5.7-4.1.osg33.el7\nhadoop-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\nhadoop-client-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\nhadoop-conf-pseudo-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\nhadoop-debuginfo-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\nhadoop-doc-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\nhadoop-hdfs-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\nhadoop-hdfs-datanode-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\nhadoop-hdfs-fuse-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\nhadoop-hdfs-fuse-selinux-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\nhadoop-hdfs-journalnode-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\nhadoop-hdfs-namenode-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\nhadoop-hdfs-secondarynamenode-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\nhadoop-hdfs-zkfc-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\nhadoop-httpfs-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\nhadoop-libhdfs-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\nhadoop-mapreduce-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\nhadoop-yarn-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\nhtcondor-ce-2.0.0-2.osg33.el7\nhtcondor-ce-client-2.0.0-2.osg33.el7\nhtcondor-ce-collector-2.0.0-2.osg33.el7\nhtcondor-ce-condor-2.0.0-2.osg33.el7\nhtcondor-ce-debuginfo-2.0.0-2.osg33.el7\nhtcondor-ce-lsf-2.0.0-2.osg33.el7\nhtcondor-ce-pbs-2.0.0-2.osg33.el7\nhtcondor-ce-sge-2.0.0-2.osg33.el7\nhtcondor-ce-view-2.0.0-2.osg33.el7\nigtf-ca-certs-1.71-1.osg33.el7\njglobus-2.1.0-7.osg33.el7\nlcmaps-plugins-scas-client-0.5.6-1.osg33.el7\nlcmaps-plugins-scas-client-debuginfo-0.5.6-1.osg33.el7\nosg-ca-certs-1.52-1.osg33.el7\nosg-ca-generator-1.0.2-1.osg33.el7\nosg-gums-config-63-1.osg33.el7\nosg-test-1.5.1-1.osg33.el7\nosg-version-3.3.9-1.osg33.el7\nrsv-perfsonar-1.1.2-3.osg33.el7\nvo-client-63-1.osg33.el7\nvo-client-edgmkgridmap-63-1.osg33.el7\nvoms-api-java-2.0.8-1.9.osg33.el7\nvoms-api-java-javadoc-2.0.8-1.9.osg33.el7\nxrootd-lcmaps-1.2.1-1.osg33.el7\nxrootd-lcmaps-debuginfo-1.2.1-1.osg33.el7\nzookeeper-3.4.3+15-1.cdh4.0.1.p0.3.osg33.el7\nzookeeper-server-3.4.3+15-1.cdh4.0.1.p0.3.osg33.el7\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\ncvmfs-2.2.0-1.osgup.el6\n\n\ncvmfs-config-osg-1.2-2.osgup.el6\n\n\nhtcondor-ce-2.0.0-2.osgup.el6\n\n\nosg-oasis-6-2.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\ncvmfs-2.2.0-1.osgup.el7\n\n\ncvmfs-config-osg-1.2-2.osgup.el7\n\n\nhtcondor-ce-2.0.0-2.osgup.el7\n\n\nosg-oasis-6-2.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\ncvmfs cvmfs-config-osg cvmfs-devel cvmfs-server cvmfs-unittests htcondor-ce htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-debuginfo htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-view osg-oasis\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\ncvmfs-2.2.0-1.osgup.el6\ncvmfs-config-osg-1.2-2.osgup.el6\ncvmfs-devel-2.2.0-1.osgup.el6\ncvmfs-server-2.2.0-1.osgup.el6\ncvmfs-unittests-2.2.0-1.osgup.el6\nhtcondor-ce-2.0.0-2.osgup.el6\nhtcondor-ce-client-2.0.0-2.osgup.el6\nhtcondor-ce-collector-2.0.0-2.osgup.el6\nhtcondor-ce-condor-2.0.0-2.osgup.el6\nhtcondor-ce-debuginfo-2.0.0-2.osgup.el6\nhtcondor-ce-lsf-2.0.0-2.osgup.el6\nhtcondor-ce-pbs-2.0.0-2.osgup.el6\nhtcondor-ce-sge-2.0.0-2.osgup.el6\nhtcondor-ce-view-2.0.0-2.osgup.el6\nosg-oasis-6-2.osgup.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\ncvmfs-2.2.0-1.osgup.el7\ncvmfs-config-osg-1.2-2.osgup.el7\ncvmfs-devel-2.2.0-1.osgup.el7\ncvmfs-server-2.2.0-1.osgup.el7\ncvmfs-unittests-2.2.0-1.osgup.el7\nhtcondor-ce-2.0.0-2.osgup.el7\nhtcondor-ce-client-2.0.0-2.osgup.el7\nhtcondor-ce-collector-2.0.0-2.osgup.el7\nhtcondor-ce-condor-2.0.0-2.osgup.el7\nhtcondor-ce-debuginfo-2.0.0-2.osgup.el7\nhtcondor-ce-lsf-2.0.0-2.osgup.el7\nhtcondor-ce-pbs-2.0.0-2.osgup.el7\nhtcondor-ce-sge-2.0.0-2.osgup.el7\nhtcondor-ce-view-2.0.0-2.osgup.el7\nosg-oasis-6-2.osgup.el7", 
            "title": "OSG Release 3.3.9"
        }, 
        {
            "location": "/release/3.3/release-3-3-9/#osg-software-release-339", 
            "text": "Release Date : 2016-02-09", 
            "title": "OSG Software Release 3.3.9"
        }, 
        {
            "location": "/release/3.3/release-3-3-9/#summary-of-changes", 
            "text": "This release contains:   Update GSI-OpenSSH 5.7 to address  CVE-2016-0777  glideinWMS 3.2.12  CVMFS over StashCache in the Upcoming Repository  CVMFS 2.2.0  Configuration RPM  XRootD LCMAPS 1.2.1    XRootD HDFS plugin - support non-world-readable files  lcmaps-plugins-scas-client - fix memory leak, better man page  CA Certificates based on  IGTF 1.71  VO Package v63  VOMS admin update to disable unnecessary CA check  Better warnings and message handling in RSV-perfsonar 1.1.2  Support for HDFS on EL7 platforms  jGlobus - remove tomcat dependency  HTCondor CE - requires HTCondor  = 8.3.7  emi-trustmanager update to support gratia service in EL7   These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-9/#known-issues", 
            "text": "The new HTCondor-CE View has a bug where some graphs show up blank. This may also manifest in errors like the following in  /var/log/condor-ce/GangliadLog : \\    1/11/16 15:05:54 Failed to execute /usr/share/condor-ce/condor_ce_metric --conf /etc/ganglia/gmond.conf --group HTCondor.Schedd --name SchedulerRecentDaemonCoreDutyCycle --value 1.04449 --type float --units % --slope both --spoof 192.170.227.226:itbv-ce-htcondor.mwt2.org --tmax 120 --dmax 86400: Usage: condor_ce_metric [options]  condor_ce_metric: error: no such option: --conf  01/11/16 15:05:54 Failed to publish metric SchedulerRecentDaemonCoreDutyCycle for itbv-ce-htcondor.mwt2.org", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.3/release-3-3-9/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-9/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-9/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-9/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-9/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-9/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-9/#enterprise-linux-6", 
            "text": "emi-trustmanager-3.0.3-10.osg33.el6  emi-trustmanager-tomcat-3.0.0-14.osg33.el6  glideinwms-3.2.12.1-1.osg33.el6  gsi-openssh-5.7-4.1.osg33.el6  hadoop-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6  htcondor-ce-2.0.0-2.osg33.el6  igtf-ca-certs-1.71-1.osg33.el6  jglobus-2.1.0-7.osg33.el6  lcmaps-plugins-scas-client-0.5.6-1.osg33.el6  osg-ca-certs-1.52-1.osg33.el6  osg-ca-generator-1.0.2-1.osg33.el6  osg-test-1.5.1-1.osg33.el6  osg-version-3.3.9-1.osg33.el6  rsv-perfsonar-1.1.2-3.osg33.el6  vo-client-63-1.osg33.el6  voms-admin-server-2.7.0-1.21.osg33.el6  voms-api-java-2.0.8-1.9.osg33.el6  xrootd-hdfs-1.8.6-1.osg33.el6  xrootd-lcmaps-1.2.1-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-9/#enterprise-linux-7", 
            "text": "emi-trustmanager-3.0.3-10.osg33.el7  emi-trustmanager-tomcat-3.0.0-14.osg33.el7  glideinwms-3.2.12.1-1.osg33.el7  gsi-openssh-5.7-4.1.osg33.el7  hadoop-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7  htcondor-ce-2.0.0-2.osg33.el7  igtf-ca-certs-1.71-1.osg33.el7  jglobus-2.1.0-7.osg33.el7  lcmaps-plugins-scas-client-0.5.6-1.osg33.el7  osg-ca-certs-1.52-1.osg33.el7  osg-ca-generator-1.0.2-1.osg33.el7  osg-test-1.5.1-1.osg33.el7  osg-version-3.3.9-1.osg33.el7  rsv-perfsonar-1.1.2-3.osg33.el7  vo-client-63-1.osg33.el7  voms-api-java-2.0.8-1.9.osg33.el7  xrootd-lcmaps-1.2.1-1.osg33.el7  zookeeper-3.4.3+15-1.cdh4.0.1.p0.3.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-9/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  emi-trustmanager emi-trustmanager-tomcat glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone gsi-openssh gsi-openssh-clients gsi-openssh-debuginfo gsi-openssh-server hadoop hadoop-client hadoop-conf-pseudo hadoop-debuginfo hadoop-doc hadoop-hdfs hadoop-hdfs-datanode hadoop-hdfs-fuse hadoop-hdfs-fuse-selinux hadoop-hdfs-journalnode hadoop-hdfs-namenode hadoop-hdfs-secondarynamenode hadoop-hdfs-zkfc hadoop-httpfs hadoop-libhdfs hadoop-mapreduce hadoop-yarn htcondor-ce htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-debuginfo htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-view igtf-ca-certs jglobus lcmaps-plugins-scas-client lcmaps-plugins-scas-client-debuginfo osg-ca-certs osg-ca-generator osg-gums-config osg-test osg-version rsv-perfsonar vo-client vo-client-edgmkgridmap voms-admin-server voms-api-java voms-api-java-javadoc xrootd-hdfs xrootd-hdfs-debuginfo xrootd-hdfs-devel xrootd-lcmaps xrootd-lcmaps-debuginfo  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-9/#enterprise-linux-6_1", 
            "text": "emi-trustmanager-3.0.3-10.osg33.el6\nemi-trustmanager-tomcat-3.0.0-14.osg33.el6\nglideinwms-3.2.12.1-1.osg33.el6\nglideinwms-common-tools-3.2.12.1-1.osg33.el6\nglideinwms-condor-common-config-3.2.12.1-1.osg33.el6\nglideinwms-factory-3.2.12.1-1.osg33.el6\nglideinwms-factory-condor-3.2.12.1-1.osg33.el6\nglideinwms-glidecondor-tools-3.2.12.1-1.osg33.el6\nglideinwms-libs-3.2.12.1-1.osg33.el6\nglideinwms-minimal-condor-3.2.12.1-1.osg33.el6\nglideinwms-usercollector-3.2.12.1-1.osg33.el6\nglideinwms-userschedd-3.2.12.1-1.osg33.el6\nglideinwms-vofrontend-3.2.12.1-1.osg33.el6\nglideinwms-vofrontend-standalone-3.2.12.1-1.osg33.el6\ngsi-openssh-5.7-4.1.osg33.el6\ngsi-openssh-clients-5.7-4.1.osg33.el6\ngsi-openssh-debuginfo-5.7-4.1.osg33.el6\ngsi-openssh-server-5.7-4.1.osg33.el6\nhadoop-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\nhadoop-client-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\nhadoop-conf-pseudo-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\nhadoop-debuginfo-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\nhadoop-doc-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\nhadoop-hdfs-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\nhadoop-hdfs-datanode-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\nhadoop-hdfs-fuse-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\nhadoop-hdfs-fuse-selinux-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\nhadoop-hdfs-journalnode-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\nhadoop-hdfs-namenode-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\nhadoop-hdfs-secondarynamenode-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\nhadoop-hdfs-zkfc-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\nhadoop-httpfs-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\nhadoop-libhdfs-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\nhadoop-mapreduce-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\nhadoop-yarn-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el6\nhtcondor-ce-2.0.0-2.osg33.el6\nhtcondor-ce-client-2.0.0-2.osg33.el6\nhtcondor-ce-collector-2.0.0-2.osg33.el6\nhtcondor-ce-condor-2.0.0-2.osg33.el6\nhtcondor-ce-debuginfo-2.0.0-2.osg33.el6\nhtcondor-ce-lsf-2.0.0-2.osg33.el6\nhtcondor-ce-pbs-2.0.0-2.osg33.el6\nhtcondor-ce-sge-2.0.0-2.osg33.el6\nhtcondor-ce-view-2.0.0-2.osg33.el6\nigtf-ca-certs-1.71-1.osg33.el6\njglobus-2.1.0-7.osg33.el6\nlcmaps-plugins-scas-client-0.5.6-1.osg33.el6\nlcmaps-plugins-scas-client-debuginfo-0.5.6-1.osg33.el6\nosg-ca-certs-1.52-1.osg33.el6\nosg-ca-generator-1.0.2-1.osg33.el6\nosg-gums-config-63-1.osg33.el6\nosg-test-1.5.1-1.osg33.el6\nosg-version-3.3.9-1.osg33.el6\nrsv-perfsonar-1.1.2-3.osg33.el6\nvo-client-63-1.osg33.el6\nvo-client-edgmkgridmap-63-1.osg33.el6\nvoms-admin-server-2.7.0-1.21.osg33.el6\nvoms-api-java-2.0.8-1.9.osg33.el6\nvoms-api-java-javadoc-2.0.8-1.9.osg33.el6\nxrootd-hdfs-1.8.6-1.osg33.el6\nxrootd-hdfs-debuginfo-1.8.6-1.osg33.el6\nxrootd-hdfs-devel-1.8.6-1.osg33.el6\nxrootd-lcmaps-1.2.1-1.osg33.el6\nxrootd-lcmaps-debuginfo-1.2.1-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-9/#enterprise-linux-7_1", 
            "text": "emi-trustmanager-3.0.3-10.osg33.el7\nemi-trustmanager-tomcat-3.0.0-14.osg33.el7\nglideinwms-3.2.12.1-1.osg33.el7\nglideinwms-common-tools-3.2.12.1-1.osg33.el7\nglideinwms-condor-common-config-3.2.12.1-1.osg33.el7\nglideinwms-factory-3.2.12.1-1.osg33.el7\nglideinwms-factory-condor-3.2.12.1-1.osg33.el7\nglideinwms-glidecondor-tools-3.2.12.1-1.osg33.el7\nglideinwms-libs-3.2.12.1-1.osg33.el7\nglideinwms-minimal-condor-3.2.12.1-1.osg33.el7\nglideinwms-usercollector-3.2.12.1-1.osg33.el7\nglideinwms-userschedd-3.2.12.1-1.osg33.el7\nglideinwms-vofrontend-3.2.12.1-1.osg33.el7\nglideinwms-vofrontend-standalone-3.2.12.1-1.osg33.el7\ngsi-openssh-5.7-4.1.osg33.el7\ngsi-openssh-clients-5.7-4.1.osg33.el7\ngsi-openssh-debuginfo-5.7-4.1.osg33.el7\ngsi-openssh-server-5.7-4.1.osg33.el7\nhadoop-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\nhadoop-client-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\nhadoop-conf-pseudo-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\nhadoop-debuginfo-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\nhadoop-doc-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\nhadoop-hdfs-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\nhadoop-hdfs-datanode-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\nhadoop-hdfs-fuse-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\nhadoop-hdfs-fuse-selinux-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\nhadoop-hdfs-journalnode-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\nhadoop-hdfs-namenode-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\nhadoop-hdfs-secondarynamenode-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\nhadoop-hdfs-zkfc-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\nhadoop-httpfs-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\nhadoop-libhdfs-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\nhadoop-mapreduce-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\nhadoop-yarn-2.0.0+545-1.cdh4.1.1.p0.22.osg33.el7\nhtcondor-ce-2.0.0-2.osg33.el7\nhtcondor-ce-client-2.0.0-2.osg33.el7\nhtcondor-ce-collector-2.0.0-2.osg33.el7\nhtcondor-ce-condor-2.0.0-2.osg33.el7\nhtcondor-ce-debuginfo-2.0.0-2.osg33.el7\nhtcondor-ce-lsf-2.0.0-2.osg33.el7\nhtcondor-ce-pbs-2.0.0-2.osg33.el7\nhtcondor-ce-sge-2.0.0-2.osg33.el7\nhtcondor-ce-view-2.0.0-2.osg33.el7\nigtf-ca-certs-1.71-1.osg33.el7\njglobus-2.1.0-7.osg33.el7\nlcmaps-plugins-scas-client-0.5.6-1.osg33.el7\nlcmaps-plugins-scas-client-debuginfo-0.5.6-1.osg33.el7\nosg-ca-certs-1.52-1.osg33.el7\nosg-ca-generator-1.0.2-1.osg33.el7\nosg-gums-config-63-1.osg33.el7\nosg-test-1.5.1-1.osg33.el7\nosg-version-3.3.9-1.osg33.el7\nrsv-perfsonar-1.1.2-3.osg33.el7\nvo-client-63-1.osg33.el7\nvo-client-edgmkgridmap-63-1.osg33.el7\nvoms-api-java-2.0.8-1.9.osg33.el7\nvoms-api-java-javadoc-2.0.8-1.9.osg33.el7\nxrootd-lcmaps-1.2.1-1.osg33.el7\nxrootd-lcmaps-debuginfo-1.2.1-1.osg33.el7\nzookeeper-3.4.3+15-1.cdh4.0.1.p0.3.osg33.el7\nzookeeper-server-3.4.3+15-1.cdh4.0.1.p0.3.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-9/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-9/#enterprise-linux-6_2", 
            "text": "cvmfs-2.2.0-1.osgup.el6  cvmfs-config-osg-1.2-2.osgup.el6  htcondor-ce-2.0.0-2.osgup.el6  osg-oasis-6-2.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-9/#enterprise-linux-7_2", 
            "text": "cvmfs-2.2.0-1.osgup.el7  cvmfs-config-osg-1.2-2.osgup.el7  htcondor-ce-2.0.0-2.osgup.el7  osg-oasis-6-2.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-9/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  cvmfs cvmfs-config-osg cvmfs-devel cvmfs-server cvmfs-unittests htcondor-ce htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-debuginfo htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-view osg-oasis  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-9/#enterprise-linux-6_3", 
            "text": "cvmfs-2.2.0-1.osgup.el6\ncvmfs-config-osg-1.2-2.osgup.el6\ncvmfs-devel-2.2.0-1.osgup.el6\ncvmfs-server-2.2.0-1.osgup.el6\ncvmfs-unittests-2.2.0-1.osgup.el6\nhtcondor-ce-2.0.0-2.osgup.el6\nhtcondor-ce-client-2.0.0-2.osgup.el6\nhtcondor-ce-collector-2.0.0-2.osgup.el6\nhtcondor-ce-condor-2.0.0-2.osgup.el6\nhtcondor-ce-debuginfo-2.0.0-2.osgup.el6\nhtcondor-ce-lsf-2.0.0-2.osgup.el6\nhtcondor-ce-pbs-2.0.0-2.osgup.el6\nhtcondor-ce-sge-2.0.0-2.osgup.el6\nhtcondor-ce-view-2.0.0-2.osgup.el6\nosg-oasis-6-2.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-9/#enterprise-linux-7_3", 
            "text": "cvmfs-2.2.0-1.osgup.el7\ncvmfs-config-osg-1.2-2.osgup.el7\ncvmfs-devel-2.2.0-1.osgup.el7\ncvmfs-server-2.2.0-1.osgup.el7\ncvmfs-unittests-2.2.0-1.osgup.el7\nhtcondor-ce-2.0.0-2.osgup.el7\nhtcondor-ce-client-2.0.0-2.osgup.el7\nhtcondor-ce-collector-2.0.0-2.osgup.el7\nhtcondor-ce-condor-2.0.0-2.osgup.el7\nhtcondor-ce-debuginfo-2.0.0-2.osgup.el7\nhtcondor-ce-lsf-2.0.0-2.osgup.el7\nhtcondor-ce-pbs-2.0.0-2.osgup.el7\nhtcondor-ce-sge-2.0.0-2.osgup.el7\nhtcondor-ce-view-2.0.0-2.osgup.el7\nosg-oasis-6-2.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-8/", 
            "text": "OSG Software Release 3.3.8\n\n\nRelease Date\n: 2016-01-12\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nHTCondor-CE 2.0.0\n\n\nHTCondor-CE View web app\n\n\nCustom formatting for condor_ce_status\n\n\nblahp updates\n\n\nfixed crash in pbs_status.py when /tmp and /var/tmp on different file systems\n\n\nadded disable limited proxies option\n\n\n\n\n\n\n\n\n\n\nHTCondor 8.4.3\n\n\nHTCondor 8.5.1\n in Upcoming\n\n\nVO Package v62\n\n\nPKI tools: Accept hostname aliases in certificate requests\n\n\ngridftp-hdfs\n\n\nsupport rename and rmdir\n\n\nAdd load (connection) limits\n\n\n\n\n\n\ncctools 4.4.3\n\n\nGUMS: fix locale specific crash\n\n\ncondor-cron: configuration fixes\n\n\nMyProxy updated to a version with no OSG patches (strict pass-through)\n\n\ngratia\n\n\nclosed a couple of vulnerabilities\n\n\nfixed bug where configure_tomcat breaks the init script\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nKnown Issues\n\n\n\n\n\n\nHTCondor 8.4.0 has changed it's behavior in ways that cause the GlideinWMS frontend configuration to break. In order to correct this, the following setting needs to be added to the configuration file:\n\n\nCOLLECTOR_USES_SHARED_PORT = False\n\n\n\n\n\n\n\n\n\nThe new HTCondor-CE View has a bug where some graphs show up blank. This may also manifest in errors like the following in \n/var/log/condor-ce/GangliadLog\n:\n\n\n\n\n\n\n\n\n1/11/16 15:05:54 Failed to execute /usr/share/condor-ce/condor_ce_metric --conf /etc/ganglia/gmond.conf --group HTCondor.Schedd --name SchedulerRecentDaemonCoreDutyCycle --value 1.04449 --type float --units % --slope both --spoof 192.170.227.226:itbv-ce-htcondor.mwt2.org --tmax 120 --dmax 86400: Usage: condor_ce_metric [options]\n\n\ncondor_ce_metric: error: no such option: --conf\n\n\n01/11/16 15:05:54 Failed to publish metric SchedulerRecentDaemonCoreDutyCycle for itbv-ce-htcondor.mwt2.org \n\n\n\n\nSince version 1.14, HTCondor-CE has required condor \n= 8.3.7 but this was not reflected in the packaging. If your routed jobs do not have the proper environment set, your version of HTCondor-CE is newer than 1.14, and your version of condor is older than 8.3.7, consider upgrading your version of condor. This will be fixed in the next release.\n\n\n\n\nStashCache packages need to be manually configured\n\n\n\n\nManual configuration for origin server\n\n\nAssuming that the origin server connects only to a redirector (not directly to cache server), minimal xrootd configuration is required. The configuration file, /etc/xrootd/xrootd-stashcache-origin-server.cfg, in this release is overkill. Here are recommended settings to use:\nxrd.port 1094\nall.role server\nall.manager stash-redirector.example.com 1213\nall.export / nostage\nxrootd.trace emsg login stall redirect\nofs.trace none\nxrd.trace conn\ncms.trace all\nsec.protocol  host\nsec.protbind  * none\nall.adminpath /var/run/xrootd\nall.pidpath /var/run/xrootd\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nManual configuration for cache server\n\n\n\n\nIn contrast to the origin server configuration, one needs to declare \npss.origin \nstash-redirector.example.com\n instead of configuring the cmsd or manager (only the xrootd daemon is required on the cache server). More detailed configuration of cache server for StashCache is \nhere\n.\n\n\n\n\n\n\n\n\nIn both cases, administrator needs to set the path of custom configuration file for its xrootd/cmds instance in /etc/sysconfig/xrootd, For example, change the cmds default from:\n\n\nCMSD_DEFAULT_OPTIONS=\n-l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg -k fifo\n\n\n\n\n\n\n\n\nto\n\n\nCMSD_DEFAULT_OPTIONS=\n-l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-stashcache-origin-server.marian -k fifo\n\n\n\n\n\n\n\n\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.16.bosco-1.osg33.el6\n\n\ncctools-4.4.3-1.osg33.el6\n\n\ncondor-8.4.3-1.osg33.el6\n\n\ncondor-cron-1.0.11-1.osg33.el6\n\n\ngratia-1.16.2-1.2.osg33.el6\n\n\ngridftp-hdfs-0.5.4-24.osg33.el6\n\n\ngums-1.5.1-6.osg33.el6\n\n\nhtcondor-ce-2.0.0-1.osg33.el6\n\n\nmyproxy-6.1.15-2.osg33.el6\n\n\nosg-pki-tools-1.2.14-1.osg33.el6\n\n\nosg-release-3.3-4.osg33.el6\n\n\nosg-test-1.4.33-1.osg33.el6\n\n\nosg-tested-internal-3.3-6.osg33.el6\n\n\nosg-version-3.3.8-1.osg33.el6\n\n\nvo-client-62-2.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.16.bosco-1.osg33.el7\n\n\ncctools-4.4.3-1.osg33.el7\n\n\ncondor-8.4.3-1.osg33.el7\n\n\ncondor-cron-1.0.11-1.osg33.el7\n\n\ngratia-1.16.2-1.2.osg33.el7\n\n\ngums-1.5.1-6.osg33.el7\n\n\nhtcondor-ce-2.0.0-1.osg33.el7\n\n\nmyproxy-6.1.15-2.osg33.el7\n\n\nosg-pki-tools-1.2.14-1.osg33.el7\n\n\nosg-release-3.3-4.osg33.el7\n\n\nosg-test-1.4.33-1.osg33.el7\n\n\nosg-tested-internal-3.3-6.osg33.el7\n\n\nosg-version-3.3.8-1.osg33.el7\n\n\nvo-client-62-2.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp blahp-debuginfo cctools-chirp cctools-debuginfo cctools-doc cctools-dttools cctools-makeflow cctools-parrot cctools-resource_monitor cctools-sand cctools-wavefront cctools-weaver cctools-work_queue condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-cron condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp gratia-debuginfo gratia-service gridftp-hdfs gridftp-hdfs-debuginfo gums gums-client gums-service htcondor-ce htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-debuginfo htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-view myproxy myproxy-admin myproxy-debuginfo myproxy-devel myproxy-doc myproxy-libs myproxy-server myproxy-voms osg-gums-config osg-pki-tools osg-pki-tools-tests osg-release osg-test osg-tested-internal osg-version vo-client vo-client-edgmkgridmap\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp-1.18.16.bosco-1.osg33.el6\nblahp-debuginfo-1.18.16.bosco-1.osg33.el6\ncctools-4.4.3-1.osg33.el6\ncctools-chirp-4.4.3-1.osg33.el6\ncctools-debuginfo-4.4.3-1.osg33.el6\ncctools-doc-4.4.3-1.osg33.el6\ncctools-dttools-4.4.3-1.osg33.el6\ncctools-makeflow-4.4.3-1.osg33.el6\ncctools-parrot-4.4.3-1.osg33.el6\ncctools-resource_monitor-4.4.3-1.osg33.el6\ncctools-sand-4.4.3-1.osg33.el6\ncctools-wavefront-4.4.3-1.osg33.el6\ncctools-weaver-4.4.3-1.osg33.el6\ncctools-work_queue-4.4.3-1.osg33.el6\ncondor-8.4.3-1.osg33.el6\ncondor-all-8.4.3-1.osg33.el6\ncondor-bosco-8.4.3-1.osg33.el6\ncondor-classads-8.4.3-1.osg33.el6\ncondor-classads-devel-8.4.3-1.osg33.el6\ncondor-cream-gahp-8.4.3-1.osg33.el6\ncondor-cron-1.0.11-1.osg33.el6\ncondor-debuginfo-8.4.3-1.osg33.el6\ncondor-kbdd-8.4.3-1.osg33.el6\ncondor-procd-8.4.3-1.osg33.el6\ncondor-python-8.4.3-1.osg33.el6\ncondor-std-universe-8.4.3-1.osg33.el6\ncondor-test-8.4.3-1.osg33.el6\ncondor-vm-gahp-8.4.3-1.osg33.el6\ngratia-1.16.2-1.2.osg33.el6\ngratia-debuginfo-1.16.2-1.2.osg33.el6\ngratia-service-1.16.2-1.2.osg33.el6\ngridftp-hdfs-0.5.4-24.osg33.el6\ngridftp-hdfs-debuginfo-0.5.4-24.osg33.el6\ngums-1.5.1-6.osg33.el6\ngums-client-1.5.1-6.osg33.el6\ngums-service-1.5.1-6.osg33.el6\nhtcondor-ce-2.0.0-1.osg33.el6\nhtcondor-ce-client-2.0.0-1.osg33.el6\nhtcondor-ce-collector-2.0.0-1.osg33.el6\nhtcondor-ce-condor-2.0.0-1.osg33.el6\nhtcondor-ce-debuginfo-2.0.0-1.osg33.el6\nhtcondor-ce-lsf-2.0.0-1.osg33.el6\nhtcondor-ce-pbs-2.0.0-1.osg33.el6\nhtcondor-ce-sge-2.0.0-1.osg33.el6\nhtcondor-ce-view-2.0.0-1.osg33.el6\nmyproxy-6.1.15-2.osg33.el6\nmyproxy-admin-6.1.15-2.osg33.el6\nmyproxy-debuginfo-6.1.15-2.osg33.el6\nmyproxy-devel-6.1.15-2.osg33.el6\nmyproxy-doc-6.1.15-2.osg33.el6\nmyproxy-libs-6.1.15-2.osg33.el6\nmyproxy-server-6.1.15-2.osg33.el6\nmyproxy-voms-6.1.15-2.osg33.el6\nosg-gums-config-62-2.osg33.el6\nosg-pki-tools-1.2.14-1.osg33.el6\nosg-pki-tools-tests-1.2.14-1.osg33.el6\nosg-release-3.3-4.osg33.el6\nosg-test-1.4.33-1.osg33.el6\nosg-tested-internal-3.3-6.osg33.el6\nosg-version-3.3.8-1.osg33.el6\nvo-client-62-2.osg33.el6\nvo-client-edgmkgridmap-62-2.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp-1.18.16.bosco-1.osg33.el7\nblahp-debuginfo-1.18.16.bosco-1.osg33.el7\ncctools-4.4.3-1.osg33.el7\ncctools-chirp-4.4.3-1.osg33.el7\ncctools-debuginfo-4.4.3-1.osg33.el7\ncctools-doc-4.4.3-1.osg33.el7\ncctools-dttools-4.4.3-1.osg33.el7\ncctools-makeflow-4.4.3-1.osg33.el7\ncctools-parrot-4.4.3-1.osg33.el7\ncctools-resource_monitor-4.4.3-1.osg33.el7\ncctools-sand-4.4.3-1.osg33.el7\ncctools-wavefront-4.4.3-1.osg33.el7\ncctools-weaver-4.4.3-1.osg33.el7\ncctools-work_queue-4.4.3-1.osg33.el7\ncondor-8.4.3-1.osg33.el7\ncondor-all-8.4.3-1.osg33.el7\ncondor-bosco-8.4.3-1.osg33.el7\ncondor-classads-8.4.3-1.osg33.el7\ncondor-classads-devel-8.4.3-1.osg33.el7\ncondor-cron-1.0.11-1.osg33.el7\ncondor-debuginfo-8.4.3-1.osg33.el7\ncondor-kbdd-8.4.3-1.osg33.el7\ncondor-procd-8.4.3-1.osg33.el7\ncondor-python-8.4.3-1.osg33.el7\ncondor-test-8.4.3-1.osg33.el7\ncondor-vm-gahp-8.4.3-1.osg33.el7\ngratia-1.16.2-1.2.osg33.el7\ngratia-debuginfo-1.16.2-1.2.osg33.el7\ngratia-service-1.16.2-1.2.osg33.el7\ngums-1.5.1-6.osg33.el7\ngums-client-1.5.1-6.osg33.el7\ngums-service-1.5.1-6.osg33.el7\nhtcondor-ce-2.0.0-1.osg33.el7\nhtcondor-ce-client-2.0.0-1.osg33.el7\nhtcondor-ce-collector-2.0.0-1.osg33.el7\nhtcondor-ce-condor-2.0.0-1.osg33.el7\nhtcondor-ce-debuginfo-2.0.0-1.osg33.el7\nhtcondor-ce-lsf-2.0.0-1.osg33.el7\nhtcondor-ce-pbs-2.0.0-1.osg33.el7\nhtcondor-ce-sge-2.0.0-1.osg33.el7\nhtcondor-ce-view-2.0.0-1.osg33.el7\nmyproxy-6.1.15-2.osg33.el7\nmyproxy-admin-6.1.15-2.osg33.el7\nmyproxy-debuginfo-6.1.15-2.osg33.el7\nmyproxy-devel-6.1.15-2.osg33.el7\nmyproxy-doc-6.1.15-2.osg33.el7\nmyproxy-libs-6.1.15-2.osg33.el7\nmyproxy-server-6.1.15-2.osg33.el7\nmyproxy-voms-6.1.15-2.osg33.el7\nosg-gums-config-62-2.osg33.el7\nosg-pki-tools-1.2.14-1.osg33.el7\nosg-pki-tools-tests-1.2.14-1.osg33.el7\nosg-release-3.3-4.osg33.el7\nosg-test-1.4.33-1.osg33.el7\nosg-tested-internal-3.3-6.osg33.el7\nosg-version-3.3.8-1.osg33.el7\nvo-client-62-2.osg33.el7\nvo-client-edgmkgridmap-62-2.osg33.el7\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.16.bosco-1.osgup.el6\n\n\ncondor-8.5.1-1.osgup.el6\n\n\nhtcondor-ce-2.0.0-1.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.16.bosco-1.osgup.el7\n\n\ncondor-8.5.1-1.osgup.el7\n\n\nhtcondor-ce-2.0.0-1.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp htcondor-ce htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-debuginfo htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-view\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp-1.18.16.bosco-1.osgup.el6\nblahp-debuginfo-1.18.16.bosco-1.osgup.el6\ncondor-8.5.1-1.osgup.el6\ncondor-all-8.5.1-1.osgup.el6\ncondor-bosco-8.5.1-1.osgup.el6\ncondor-classads-8.5.1-1.osgup.el6\ncondor-classads-devel-8.5.1-1.osgup.el6\ncondor-cream-gahp-8.5.1-1.osgup.el6\ncondor-debuginfo-8.5.1-1.osgup.el6\ncondor-kbdd-8.5.1-1.osgup.el6\ncondor-procd-8.5.1-1.osgup.el6\ncondor-python-8.5.1-1.osgup.el6\ncondor-std-universe-8.5.1-1.osgup.el6\ncondor-test-8.5.1-1.osgup.el6\ncondor-vm-gahp-8.5.1-1.osgup.el6\nhtcondor-ce-2.0.0-1.osgup.el6\nhtcondor-ce-client-2.0.0-1.osgup.el6\nhtcondor-ce-collector-2.0.0-1.osgup.el6\nhtcondor-ce-condor-2.0.0-1.osgup.el6\nhtcondor-ce-debuginfo-2.0.0-1.osgup.el6\nhtcondor-ce-lsf-2.0.0-1.osgup.el6\nhtcondor-ce-pbs-2.0.0-1.osgup.el6\nhtcondor-ce-sge-2.0.0-1.osgup.el6\nhtcondor-ce-view-2.0.0-1.osgup.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp-1.18.16.bosco-1.osgup.el7\nblahp-debuginfo-1.18.16.bosco-1.osgup.el7\ncondor-8.5.1-1.osgup.el7\ncondor-all-8.5.1-1.osgup.el7\ncondor-bosco-8.5.1-1.osgup.el7\ncondor-classads-8.5.1-1.osgup.el7\ncondor-classads-devel-8.5.1-1.osgup.el7\ncondor-debuginfo-8.5.1-1.osgup.el7\ncondor-kbdd-8.5.1-1.osgup.el7\ncondor-procd-8.5.1-1.osgup.el7\ncondor-python-8.5.1-1.osgup.el7\ncondor-test-8.5.1-1.osgup.el7\ncondor-vm-gahp-8.5.1-1.osgup.el7\nhtcondor-ce-2.0.0-1.osgup.el7\nhtcondor-ce-client-2.0.0-1.osgup.el7\nhtcondor-ce-collector-2.0.0-1.osgup.el7\nhtcondor-ce-condor-2.0.0-1.osgup.el7\nhtcondor-ce-debuginfo-2.0.0-1.osgup.el7\nhtcondor-ce-lsf-2.0.0-1.osgup.el7\nhtcondor-ce-pbs-2.0.0-1.osgup.el7\nhtcondor-ce-sge-2.0.0-1.osgup.el7\nhtcondor-ce-view-2.0.0-1.osgup.el7", 
            "title": "OSG Release 3.3.8"
        }, 
        {
            "location": "/release/3.3/release-3-3-8/#osg-software-release-338", 
            "text": "Release Date : 2016-01-12", 
            "title": "OSG Software Release 3.3.8"
        }, 
        {
            "location": "/release/3.3/release-3-3-8/#summary-of-changes", 
            "text": "This release contains:   HTCondor-CE 2.0.0  HTCondor-CE View web app  Custom formatting for condor_ce_status  blahp updates  fixed crash in pbs_status.py when /tmp and /var/tmp on different file systems  added disable limited proxies option      HTCondor 8.4.3  HTCondor 8.5.1  in Upcoming  VO Package v62  PKI tools: Accept hostname aliases in certificate requests  gridftp-hdfs  support rename and rmdir  Add load (connection) limits    cctools 4.4.3  GUMS: fix locale specific crash  condor-cron: configuration fixes  MyProxy updated to a version with no OSG patches (strict pass-through)  gratia  closed a couple of vulnerabilities  fixed bug where configure_tomcat breaks the init script     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-8/#known-issues", 
            "text": "HTCondor 8.4.0 has changed it's behavior in ways that cause the GlideinWMS frontend configuration to break. In order to correct this, the following setting needs to be added to the configuration file:  COLLECTOR_USES_SHARED_PORT = False    The new HTCondor-CE View has a bug where some graphs show up blank. This may also manifest in errors like the following in  /var/log/condor-ce/GangliadLog :     1/11/16 15:05:54 Failed to execute /usr/share/condor-ce/condor_ce_metric --conf /etc/ganglia/gmond.conf --group HTCondor.Schedd --name SchedulerRecentDaemonCoreDutyCycle --value 1.04449 --type float --units % --slope both --spoof 192.170.227.226:itbv-ce-htcondor.mwt2.org --tmax 120 --dmax 86400: Usage: condor_ce_metric [options]  condor_ce_metric: error: no such option: --conf  01/11/16 15:05:54 Failed to publish metric SchedulerRecentDaemonCoreDutyCycle for itbv-ce-htcondor.mwt2.org    Since version 1.14, HTCondor-CE has required condor  = 8.3.7 but this was not reflected in the packaging. If your routed jobs do not have the proper environment set, your version of HTCondor-CE is newer than 1.14, and your version of condor is older than 8.3.7, consider upgrading your version of condor. This will be fixed in the next release.   StashCache packages need to be manually configured   Manual configuration for origin server  Assuming that the origin server connects only to a redirector (not directly to cache server), minimal xrootd configuration is required. The configuration file, /etc/xrootd/xrootd-stashcache-origin-server.cfg, in this release is overkill. Here are recommended settings to use: xrd.port 1094\nall.role server\nall.manager stash-redirector.example.com 1213\nall.export / nostage\nxrootd.trace emsg login stall redirect\nofs.trace none\nxrd.trace conn\ncms.trace all\nsec.protocol  host\nsec.protbind  * none\nall.adminpath /var/run/xrootd\nall.pidpath /var/run/xrootd        Manual configuration for cache server   In contrast to the origin server configuration, one needs to declare  pss.origin  stash-redirector.example.com  instead of configuring the cmsd or manager (only the xrootd daemon is required on the cache server). More detailed configuration of cache server for StashCache is  here .     In both cases, administrator needs to set the path of custom configuration file for its xrootd/cmds instance in /etc/sysconfig/xrootd, For example, change the cmds default from:  CMSD_DEFAULT_OPTIONS= -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg -k fifo    to  CMSD_DEFAULT_OPTIONS= -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-stashcache-origin-server.marian -k fifo", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.3/release-3-3-8/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-8/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-8/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-8/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-8/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-8/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-8/#enterprise-linux-6", 
            "text": "blahp-1.18.16.bosco-1.osg33.el6  cctools-4.4.3-1.osg33.el6  condor-8.4.3-1.osg33.el6  condor-cron-1.0.11-1.osg33.el6  gratia-1.16.2-1.2.osg33.el6  gridftp-hdfs-0.5.4-24.osg33.el6  gums-1.5.1-6.osg33.el6  htcondor-ce-2.0.0-1.osg33.el6  myproxy-6.1.15-2.osg33.el6  osg-pki-tools-1.2.14-1.osg33.el6  osg-release-3.3-4.osg33.el6  osg-test-1.4.33-1.osg33.el6  osg-tested-internal-3.3-6.osg33.el6  osg-version-3.3.8-1.osg33.el6  vo-client-62-2.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-8/#enterprise-linux-7", 
            "text": "blahp-1.18.16.bosco-1.osg33.el7  cctools-4.4.3-1.osg33.el7  condor-8.4.3-1.osg33.el7  condor-cron-1.0.11-1.osg33.el7  gratia-1.16.2-1.2.osg33.el7  gums-1.5.1-6.osg33.el7  htcondor-ce-2.0.0-1.osg33.el7  myproxy-6.1.15-2.osg33.el7  osg-pki-tools-1.2.14-1.osg33.el7  osg-release-3.3-4.osg33.el7  osg-test-1.4.33-1.osg33.el7  osg-tested-internal-3.3-6.osg33.el7  osg-version-3.3.8-1.osg33.el7  vo-client-62-2.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-8/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp blahp-debuginfo cctools-chirp cctools-debuginfo cctools-doc cctools-dttools cctools-makeflow cctools-parrot cctools-resource_monitor cctools-sand cctools-wavefront cctools-weaver cctools-work_queue condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-cron condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp gratia-debuginfo gratia-service gridftp-hdfs gridftp-hdfs-debuginfo gums gums-client gums-service htcondor-ce htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-debuginfo htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-view myproxy myproxy-admin myproxy-debuginfo myproxy-devel myproxy-doc myproxy-libs myproxy-server myproxy-voms osg-gums-config osg-pki-tools osg-pki-tools-tests osg-release osg-test osg-tested-internal osg-version vo-client vo-client-edgmkgridmap  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-8/#enterprise-linux-6_1", 
            "text": "blahp-1.18.16.bosco-1.osg33.el6\nblahp-debuginfo-1.18.16.bosco-1.osg33.el6\ncctools-4.4.3-1.osg33.el6\ncctools-chirp-4.4.3-1.osg33.el6\ncctools-debuginfo-4.4.3-1.osg33.el6\ncctools-doc-4.4.3-1.osg33.el6\ncctools-dttools-4.4.3-1.osg33.el6\ncctools-makeflow-4.4.3-1.osg33.el6\ncctools-parrot-4.4.3-1.osg33.el6\ncctools-resource_monitor-4.4.3-1.osg33.el6\ncctools-sand-4.4.3-1.osg33.el6\ncctools-wavefront-4.4.3-1.osg33.el6\ncctools-weaver-4.4.3-1.osg33.el6\ncctools-work_queue-4.4.3-1.osg33.el6\ncondor-8.4.3-1.osg33.el6\ncondor-all-8.4.3-1.osg33.el6\ncondor-bosco-8.4.3-1.osg33.el6\ncondor-classads-8.4.3-1.osg33.el6\ncondor-classads-devel-8.4.3-1.osg33.el6\ncondor-cream-gahp-8.4.3-1.osg33.el6\ncondor-cron-1.0.11-1.osg33.el6\ncondor-debuginfo-8.4.3-1.osg33.el6\ncondor-kbdd-8.4.3-1.osg33.el6\ncondor-procd-8.4.3-1.osg33.el6\ncondor-python-8.4.3-1.osg33.el6\ncondor-std-universe-8.4.3-1.osg33.el6\ncondor-test-8.4.3-1.osg33.el6\ncondor-vm-gahp-8.4.3-1.osg33.el6\ngratia-1.16.2-1.2.osg33.el6\ngratia-debuginfo-1.16.2-1.2.osg33.el6\ngratia-service-1.16.2-1.2.osg33.el6\ngridftp-hdfs-0.5.4-24.osg33.el6\ngridftp-hdfs-debuginfo-0.5.4-24.osg33.el6\ngums-1.5.1-6.osg33.el6\ngums-client-1.5.1-6.osg33.el6\ngums-service-1.5.1-6.osg33.el6\nhtcondor-ce-2.0.0-1.osg33.el6\nhtcondor-ce-client-2.0.0-1.osg33.el6\nhtcondor-ce-collector-2.0.0-1.osg33.el6\nhtcondor-ce-condor-2.0.0-1.osg33.el6\nhtcondor-ce-debuginfo-2.0.0-1.osg33.el6\nhtcondor-ce-lsf-2.0.0-1.osg33.el6\nhtcondor-ce-pbs-2.0.0-1.osg33.el6\nhtcondor-ce-sge-2.0.0-1.osg33.el6\nhtcondor-ce-view-2.0.0-1.osg33.el6\nmyproxy-6.1.15-2.osg33.el6\nmyproxy-admin-6.1.15-2.osg33.el6\nmyproxy-debuginfo-6.1.15-2.osg33.el6\nmyproxy-devel-6.1.15-2.osg33.el6\nmyproxy-doc-6.1.15-2.osg33.el6\nmyproxy-libs-6.1.15-2.osg33.el6\nmyproxy-server-6.1.15-2.osg33.el6\nmyproxy-voms-6.1.15-2.osg33.el6\nosg-gums-config-62-2.osg33.el6\nosg-pki-tools-1.2.14-1.osg33.el6\nosg-pki-tools-tests-1.2.14-1.osg33.el6\nosg-release-3.3-4.osg33.el6\nosg-test-1.4.33-1.osg33.el6\nosg-tested-internal-3.3-6.osg33.el6\nosg-version-3.3.8-1.osg33.el6\nvo-client-62-2.osg33.el6\nvo-client-edgmkgridmap-62-2.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-8/#enterprise-linux-7_1", 
            "text": "blahp-1.18.16.bosco-1.osg33.el7\nblahp-debuginfo-1.18.16.bosco-1.osg33.el7\ncctools-4.4.3-1.osg33.el7\ncctools-chirp-4.4.3-1.osg33.el7\ncctools-debuginfo-4.4.3-1.osg33.el7\ncctools-doc-4.4.3-1.osg33.el7\ncctools-dttools-4.4.3-1.osg33.el7\ncctools-makeflow-4.4.3-1.osg33.el7\ncctools-parrot-4.4.3-1.osg33.el7\ncctools-resource_monitor-4.4.3-1.osg33.el7\ncctools-sand-4.4.3-1.osg33.el7\ncctools-wavefront-4.4.3-1.osg33.el7\ncctools-weaver-4.4.3-1.osg33.el7\ncctools-work_queue-4.4.3-1.osg33.el7\ncondor-8.4.3-1.osg33.el7\ncondor-all-8.4.3-1.osg33.el7\ncondor-bosco-8.4.3-1.osg33.el7\ncondor-classads-8.4.3-1.osg33.el7\ncondor-classads-devel-8.4.3-1.osg33.el7\ncondor-cron-1.0.11-1.osg33.el7\ncondor-debuginfo-8.4.3-1.osg33.el7\ncondor-kbdd-8.4.3-1.osg33.el7\ncondor-procd-8.4.3-1.osg33.el7\ncondor-python-8.4.3-1.osg33.el7\ncondor-test-8.4.3-1.osg33.el7\ncondor-vm-gahp-8.4.3-1.osg33.el7\ngratia-1.16.2-1.2.osg33.el7\ngratia-debuginfo-1.16.2-1.2.osg33.el7\ngratia-service-1.16.2-1.2.osg33.el7\ngums-1.5.1-6.osg33.el7\ngums-client-1.5.1-6.osg33.el7\ngums-service-1.5.1-6.osg33.el7\nhtcondor-ce-2.0.0-1.osg33.el7\nhtcondor-ce-client-2.0.0-1.osg33.el7\nhtcondor-ce-collector-2.0.0-1.osg33.el7\nhtcondor-ce-condor-2.0.0-1.osg33.el7\nhtcondor-ce-debuginfo-2.0.0-1.osg33.el7\nhtcondor-ce-lsf-2.0.0-1.osg33.el7\nhtcondor-ce-pbs-2.0.0-1.osg33.el7\nhtcondor-ce-sge-2.0.0-1.osg33.el7\nhtcondor-ce-view-2.0.0-1.osg33.el7\nmyproxy-6.1.15-2.osg33.el7\nmyproxy-admin-6.1.15-2.osg33.el7\nmyproxy-debuginfo-6.1.15-2.osg33.el7\nmyproxy-devel-6.1.15-2.osg33.el7\nmyproxy-doc-6.1.15-2.osg33.el7\nmyproxy-libs-6.1.15-2.osg33.el7\nmyproxy-server-6.1.15-2.osg33.el7\nmyproxy-voms-6.1.15-2.osg33.el7\nosg-gums-config-62-2.osg33.el7\nosg-pki-tools-1.2.14-1.osg33.el7\nosg-pki-tools-tests-1.2.14-1.osg33.el7\nosg-release-3.3-4.osg33.el7\nosg-test-1.4.33-1.osg33.el7\nosg-tested-internal-3.3-6.osg33.el7\nosg-version-3.3.8-1.osg33.el7\nvo-client-62-2.osg33.el7\nvo-client-edgmkgridmap-62-2.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-8/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-8/#enterprise-linux-6_2", 
            "text": "blahp-1.18.16.bosco-1.osgup.el6  condor-8.5.1-1.osgup.el6  htcondor-ce-2.0.0-1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-8/#enterprise-linux-7_2", 
            "text": "blahp-1.18.16.bosco-1.osgup.el7  condor-8.5.1-1.osgup.el7  htcondor-ce-2.0.0-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-8/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp htcondor-ce htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-debuginfo htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge htcondor-ce-view  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-8/#enterprise-linux-6_3", 
            "text": "blahp-1.18.16.bosco-1.osgup.el6\nblahp-debuginfo-1.18.16.bosco-1.osgup.el6\ncondor-8.5.1-1.osgup.el6\ncondor-all-8.5.1-1.osgup.el6\ncondor-bosco-8.5.1-1.osgup.el6\ncondor-classads-8.5.1-1.osgup.el6\ncondor-classads-devel-8.5.1-1.osgup.el6\ncondor-cream-gahp-8.5.1-1.osgup.el6\ncondor-debuginfo-8.5.1-1.osgup.el6\ncondor-kbdd-8.5.1-1.osgup.el6\ncondor-procd-8.5.1-1.osgup.el6\ncondor-python-8.5.1-1.osgup.el6\ncondor-std-universe-8.5.1-1.osgup.el6\ncondor-test-8.5.1-1.osgup.el6\ncondor-vm-gahp-8.5.1-1.osgup.el6\nhtcondor-ce-2.0.0-1.osgup.el6\nhtcondor-ce-client-2.0.0-1.osgup.el6\nhtcondor-ce-collector-2.0.0-1.osgup.el6\nhtcondor-ce-condor-2.0.0-1.osgup.el6\nhtcondor-ce-debuginfo-2.0.0-1.osgup.el6\nhtcondor-ce-lsf-2.0.0-1.osgup.el6\nhtcondor-ce-pbs-2.0.0-1.osgup.el6\nhtcondor-ce-sge-2.0.0-1.osgup.el6\nhtcondor-ce-view-2.0.0-1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-8/#enterprise-linux-7_3", 
            "text": "blahp-1.18.16.bosco-1.osgup.el7\nblahp-debuginfo-1.18.16.bosco-1.osgup.el7\ncondor-8.5.1-1.osgup.el7\ncondor-all-8.5.1-1.osgup.el7\ncondor-bosco-8.5.1-1.osgup.el7\ncondor-classads-8.5.1-1.osgup.el7\ncondor-classads-devel-8.5.1-1.osgup.el7\ncondor-debuginfo-8.5.1-1.osgup.el7\ncondor-kbdd-8.5.1-1.osgup.el7\ncondor-procd-8.5.1-1.osgup.el7\ncondor-python-8.5.1-1.osgup.el7\ncondor-test-8.5.1-1.osgup.el7\ncondor-vm-gahp-8.5.1-1.osgup.el7\nhtcondor-ce-2.0.0-1.osgup.el7\nhtcondor-ce-client-2.0.0-1.osgup.el7\nhtcondor-ce-collector-2.0.0-1.osgup.el7\nhtcondor-ce-condor-2.0.0-1.osgup.el7\nhtcondor-ce-debuginfo-2.0.0-1.osgup.el7\nhtcondor-ce-lsf-2.0.0-1.osgup.el7\nhtcondor-ce-pbs-2.0.0-1.osgup.el7\nhtcondor-ce-sge-2.0.0-1.osgup.el7\nhtcondor-ce-view-2.0.0-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-7/", 
            "text": "OSG Software Release 3.3.7\n\n\nRelease Date\n: 2015-12-15\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nFixed a problem in the PKI command line tools where \"Certificate Requests\" were being rejected by the CILogon Certificate Authority.\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nKnown Issues\n\n\n\n\n\n\nHTCondor 8.4.0 has changed it's behavior in ways that cause the GlideinWMS frontend configuration to break. In order to correct this, the following setting needs to be added to the configuration file:\n\n\nCOLLECTOR_USES_SHARED_PORT = False\n\n\n\n\n\n\n\n\n\nStashCache packages need to be manually configured\n\n\n\n\nManual configuration for origin server\n\n\nAssuming that the origin server connects only to a redirector (not directly to cache server), minimal xrootd configuration is required. The configuration file, /etc/xrootd/xrootd-stashcache-origin-server.cfg, in this release is overkill. Here are recommended settings to use:\nxrd.port 1094\nall.role server\nall.manager stash-redirector.example.com 1213\nall.export / nostage\nxrootd.trace emsg login stall redirect\nofs.trace none\nxrd.trace conn\ncms.trace all\nsec.protocol  host\nsec.protbind  * none\nall.adminpath /var/run/xrootd\nall.pidpath /var/run/xrootd\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nManual configuration for cache server\n\n\n\n\nIn contrast to the origin server configuration, one needs to declare \npss.origin \nstash-redirector.example.com\n instead of configuring the cmsd or manager (only the xrootd daemon is required on the cache server). More detailed configuration of cache server for StashCache is \nhere\n.\n\n\n\n\n\n\n\n\nIn both cases, administrator needs to set the path of custom configuration file for its xrootd/cmds instance in /etc/sysconfig/xrootd, For example, change the cmds default from:\n\n\nCMSD_DEFAULT_OPTIONS=\n-l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg -k fifo\n\n\n\n\n\n\n\n\nto\n\n\nCMSD_DEFAULT_OPTIONS=\n-l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-stashcache-origin-server.marian -k fifo\n\n\n\n\n\n\n\n\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nosg-pki-tools-1.2.13-1.osg33.el6\n\n\nosg-version-3.3.7-1.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nosg-pki-tools-1.2.13-1.osg33.el7\n\n\nosg-version-3.3.7-1.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nosg-pki-tools osg-pki-tools-tests osg-version\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nosg-pki-tools-1.2.13-1.osg33.el6\nosg-pki-tools-tests-1.2.13-1.osg33.el6\nosg-version-3.3.7-1.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nosg-pki-tools-1.2.13-1.osg33.el7\nosg-pki-tools-tests-1.2.13-1.osg33.el7\nosg-version-3.3.7-1.osg33.el7\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\nEnterprise Linux 7\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\n\n\n\n\n\n\nEnterprise Linux 7", 
            "title": "OSG Release 3.3.7"
        }, 
        {
            "location": "/release/3.3/release-3-3-7/#osg-software-release-337", 
            "text": "Release Date : 2015-12-15", 
            "title": "OSG Software Release 3.3.7"
        }, 
        {
            "location": "/release/3.3/release-3-3-7/#summary-of-changes", 
            "text": "This release contains:   Fixed a problem in the PKI command line tools where \"Certificate Requests\" were being rejected by the CILogon Certificate Authority.   These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-7/#known-issues", 
            "text": "HTCondor 8.4.0 has changed it's behavior in ways that cause the GlideinWMS frontend configuration to break. In order to correct this, the following setting needs to be added to the configuration file:  COLLECTOR_USES_SHARED_PORT = False    StashCache packages need to be manually configured   Manual configuration for origin server  Assuming that the origin server connects only to a redirector (not directly to cache server), minimal xrootd configuration is required. The configuration file, /etc/xrootd/xrootd-stashcache-origin-server.cfg, in this release is overkill. Here are recommended settings to use: xrd.port 1094\nall.role server\nall.manager stash-redirector.example.com 1213\nall.export / nostage\nxrootd.trace emsg login stall redirect\nofs.trace none\nxrd.trace conn\ncms.trace all\nsec.protocol  host\nsec.protbind  * none\nall.adminpath /var/run/xrootd\nall.pidpath /var/run/xrootd        Manual configuration for cache server   In contrast to the origin server configuration, one needs to declare  pss.origin  stash-redirector.example.com  instead of configuring the cmsd or manager (only the xrootd daemon is required on the cache server). More detailed configuration of cache server for StashCache is  here .     In both cases, administrator needs to set the path of custom configuration file for its xrootd/cmds instance in /etc/sysconfig/xrootd, For example, change the cmds default from:  CMSD_DEFAULT_OPTIONS= -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg -k fifo    to  CMSD_DEFAULT_OPTIONS= -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-stashcache-origin-server.marian -k fifo", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.3/release-3-3-7/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-7/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-7/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-7/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-7/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-7/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-7/#enterprise-linux-6", 
            "text": "osg-pki-tools-1.2.13-1.osg33.el6  osg-version-3.3.7-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-7/#enterprise-linux-7", 
            "text": "osg-pki-tools-1.2.13-1.osg33.el7  osg-version-3.3.7-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-7/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  osg-pki-tools osg-pki-tools-tests osg-version  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-7/#enterprise-linux-6_1", 
            "text": "osg-pki-tools-1.2.13-1.osg33.el6\nosg-pki-tools-tests-1.2.13-1.osg33.el6\nosg-version-3.3.7-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-7/#enterprise-linux-7_1", 
            "text": "osg-pki-tools-1.2.13-1.osg33.el7\nosg-pki-tools-tests-1.2.13-1.osg33.el7\nosg-version-3.3.7-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-7/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-7/#enterprise-linux-6_2", 
            "text": "", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-7/#enterprise-linux-7_2", 
            "text": "", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-7/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-7/#enterprise-linux-6_3", 
            "text": "", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-7/#enterprise-linux-7_3", 
            "text": "", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-6/", 
            "text": "OSG Software Release 3.3.6\n\n\nRelease Date\n: 2015-12-08\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nFixed a bug that prevented HTCondor 8.4's CREAM gahp from starting\n\n\nCA certificates based on \nIGTF 1.70\n\n\nUpdated CRL URL hosted by KIT for ArmeSFO (AM)\n\n\nAdded NorduGrid 2015 trust anchor (DK,NO,SE,FI,IS)\n\n\nDiscontinued superseded DigiCertGridCA-1G2-Classic (US)\n\n\n\n\n\n\nRestored the 'Sign AUP on behalf of user' feature in VOMS Admin Server\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nKnown Issues\n\n\n\n\nThe \nosg-cert-request\n and \nosg-gridadmin-cert-request\n may return \nCILogon 500\n errors when retrieving certificates. To fix this, follow the below instructions:\n\n\nSave the following text (including the empty line at the end) to \n~/osgpkitools.patch\n:\n\n\nIndex: OSGPKIUtils.py\n\n\n\n\n\n\n\n\n===============================================================\n --- OSGPKIUtils.py (revision 22192) +++ OSGPKIUtils.py (revision 22193) @@ -362,6 +362,7 @@ #\n\n\nself.X509Request.set_pubkey(pkey=self.PKey) + self.X509Request.set_version(0) self.X509Request.sign(pkey=self.PKey, md='sha1') return self.X509Request\n\n\n\n\n\n\n\ncd\n into the appropriate folder:\n\n\n# For EL5 hosts:\n\n\n\n\nroot@host # cd /usr/lib/python2.4/site-packages/osgpkitools/ # For EL6 hosts: root@host # cd /usr/lib/python2.6/site-packages/osgpkitools/\n\n\n\n\n\n\nApply the patch:\n\n\nroot@host # patch \n ~/osgpkitools.patch\n * HTCondor 8.4.0 has changed it's behavior in ways that cause the GlideinWMS frontend configuration to break. In order to correct this, the following setting needs to be added to the configuration file:\n\n\nCOLLECTOR_USES_SHARED_PORT = False\n\n\n\n\n\n\n\n\n\nStashCache packages need to be manually configured\n\n\n\n\nManual configuration for origin server\n\n\nAssuming that the origin server connects only to a redirector (not directly to cache server), minimal xrootd configuration is required. The configuration file, /etc/xrootd/xrootd-stashcache-origin-server.cfg, in this release is overkill. Here are recommended settings to use:\nxrd.port 1094\nall.role server\nall.manager stash-redirector.example.com 1213\nall.export / nostage\nxrootd.trace emsg login stall redirect\nofs.trace none\nxrd.trace conn\ncms.trace all\nsec.protocol  host\nsec.protbind  * none\nall.adminpath /var/run/xrootd\nall.pidpath /var/run/xrootd\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nManual configuration for cache server\n\n\n\n\nIn contrast to the origin server configuration, one needs to declare \npss.origin \nstash-redirector.example.com\n instead of configuring the cmsd or manager (only the xrootd daemon is required on the cache server). More detailed configuration of cache server for StashCache is \nhere\n.\n\n\n\n\n\n\nIn both cases, administrator needs to set the path of custom configuration file for its xrootd/cmds instance in /etc/sysconfig/xrootd, For example, change the cmds default from:\nCMSD_DEFAULT_OPTIONS=\n-l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg -k fifo\n\n\n\n\n\n\n\n\n\n\n\n\nto\n\n\n    :::file\n    CMSD_DEFAULT_OPTIONS=\n-l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-stashcache-origin-server.marian -k fifo\n\n\n\n\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\ncondor-8.4.2-1.2.osg33.el6\n\n\ngip-1.3.11-8.osg33.el6\n\n\nigtf-ca-certs-1.70-1.osg33.el6\n\n\nosg-ca-certs-1.51-1.osg33.el6\n\n\nosg-configure-1.2.5-1.osg33.el6\n\n\nosg-info-services-1.1.0-1.osg33.el6\n\n\nosg-test-1.4.32-1.osg33.el6\n\n\nosg-version-3.3.6-1.osg33.el6\n\n\nvoms-admin-server-2.7.0-1.17.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\ncondor-8.4.2-1.2.osg33.el7\n\n\ngip-1.3.11-8.osg33.el7\n\n\nigtf-ca-certs-1.70-1.osg33.el7\n\n\nosg-ca-certs-1.51-1.osg33.el7\n\n\nosg-configure-1.2.5-1.osg33.el7\n\n\nosg-info-services-1.1.0-1.osg33.el7\n\n\nosg-test-1.4.32-1.osg33.el7\n\n\nosg-version-3.3.6-1.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\ncondor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp gip igtf-ca-certs osg-ca-certs osg-configure osg-configure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-info-services osg-test osg-version voms-admin-server\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\ncondor-8.4.2-1.2.osg33.el6\ncondor-all-8.4.2-1.2.osg33.el6\ncondor-bosco-8.4.2-1.2.osg33.el6\ncondor-classads-8.4.2-1.2.osg33.el6\ncondor-classads-devel-8.4.2-1.2.osg33.el6\ncondor-cream-gahp-8.4.2-1.2.osg33.el6\ncondor-debuginfo-8.4.2-1.2.osg33.el6\ncondor-kbdd-8.4.2-1.2.osg33.el6\ncondor-procd-8.4.2-1.2.osg33.el6\ncondor-python-8.4.2-1.2.osg33.el6\ncondor-std-universe-8.4.2-1.2.osg33.el6\ncondor-test-8.4.2-1.2.osg33.el6\ncondor-vm-gahp-8.4.2-1.2.osg33.el6\ngip-1.3.11-8.osg33.el6\nigtf-ca-certs-1.70-1.osg33.el6\nosg-ca-certs-1.51-1.osg33.el6\nosg-configure-1.2.5-1.osg33.el6\nosg-configure-ce-1.2.5-1.osg33.el6\nosg-configure-cemon-1.2.5-1.osg33.el6\nosg-configure-condor-1.2.5-1.osg33.el6\nosg-configure-gateway-1.2.5-1.osg33.el6\nosg-configure-gip-1.2.5-1.osg33.el6\nosg-configure-gratia-1.2.5-1.osg33.el6\nosg-configure-infoservices-1.2.5-1.osg33.el6\nosg-configure-lsf-1.2.5-1.osg33.el6\nosg-configure-managedfork-1.2.5-1.osg33.el6\nosg-configure-misc-1.2.5-1.osg33.el6\nosg-configure-monalisa-1.2.5-1.osg33.el6\nosg-configure-network-1.2.5-1.osg33.el6\nosg-configure-pbs-1.2.5-1.osg33.el6\nosg-configure-rsv-1.2.5-1.osg33.el6\nosg-configure-sge-1.2.5-1.osg33.el6\nosg-configure-slurm-1.2.5-1.osg33.el6\nosg-configure-squid-1.2.5-1.osg33.el6\nosg-configure-tests-1.2.5-1.osg33.el6\nosg-info-services-1.1.0-1.osg33.el6\nosg-test-1.4.32-1.osg33.el6\nosg-version-3.3.6-1.osg33.el6\nvoms-admin-server-2.7.0-1.17.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\ncondor-8.4.2-1.2.osg33.el7\ncondor-all-8.4.2-1.2.osg33.el7\ncondor-bosco-8.4.2-1.2.osg33.el7\ncondor-classads-8.4.2-1.2.osg33.el7\ncondor-classads-devel-8.4.2-1.2.osg33.el7\ncondor-debuginfo-8.4.2-1.2.osg33.el7\ncondor-kbdd-8.4.2-1.2.osg33.el7\ncondor-procd-8.4.2-1.2.osg33.el7\ncondor-python-8.4.2-1.2.osg33.el7\ncondor-test-8.4.2-1.2.osg33.el7\ncondor-vm-gahp-8.4.2-1.2.osg33.el7\ngip-1.3.11-8.osg33.el7\nigtf-ca-certs-1.70-1.osg33.el7\nosg-ca-certs-1.51-1.osg33.el7\nosg-configure-1.2.5-1.osg33.el7\nosg-configure-ce-1.2.5-1.osg33.el7\nosg-configure-cemon-1.2.5-1.osg33.el7\nosg-configure-condor-1.2.5-1.osg33.el7\nosg-configure-gateway-1.2.5-1.osg33.el7\nosg-configure-gip-1.2.5-1.osg33.el7\nosg-configure-gratia-1.2.5-1.osg33.el7\nosg-configure-infoservices-1.2.5-1.osg33.el7\nosg-configure-lsf-1.2.5-1.osg33.el7\nosg-configure-managedfork-1.2.5-1.osg33.el7\nosg-configure-misc-1.2.5-1.osg33.el7\nosg-configure-monalisa-1.2.5-1.osg33.el7\nosg-configure-network-1.2.5-1.osg33.el7\nosg-configure-pbs-1.2.5-1.osg33.el7\nosg-configure-rsv-1.2.5-1.osg33.el7\nosg-configure-sge-1.2.5-1.osg33.el7\nosg-configure-slurm-1.2.5-1.osg33.el7\nosg-configure-squid-1.2.5-1.osg33.el7\nosg-configure-tests-1.2.5-1.osg33.el7\nosg-info-services-1.1.0-1.osg33.el7\nosg-test-1.4.32-1.osg33.el7\nosg-version-3.3.6-1.osg33.el7\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\nEnterprise Linux 7\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\n\n\n\n\n\n\nEnterprise Linux 7", 
            "title": "OSG Release 3.3.6"
        }, 
        {
            "location": "/release/3.3/release-3-3-6/#osg-software-release-336", 
            "text": "Release Date : 2015-12-08", 
            "title": "OSG Software Release 3.3.6"
        }, 
        {
            "location": "/release/3.3/release-3-3-6/#summary-of-changes", 
            "text": "This release contains:   Fixed a bug that prevented HTCondor 8.4's CREAM gahp from starting  CA certificates based on  IGTF 1.70  Updated CRL URL hosted by KIT for ArmeSFO (AM)  Added NorduGrid 2015 trust anchor (DK,NO,SE,FI,IS)  Discontinued superseded DigiCertGridCA-1G2-Classic (US)    Restored the 'Sign AUP on behalf of user' feature in VOMS Admin Server   These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-6/#known-issues", 
            "text": "The  osg-cert-request  and  osg-gridadmin-cert-request  may return  CILogon 500  errors when retrieving certificates. To fix this, follow the below instructions:  Save the following text (including the empty line at the end) to  ~/osgpkitools.patch :  Index: OSGPKIUtils.py     ===============================================================  --- OSGPKIUtils.py (revision 22192) +++ OSGPKIUtils.py (revision 22193) @@ -362,6 +362,7 @@ #  self.X509Request.set_pubkey(pkey=self.PKey) + self.X509Request.set_version(0) self.X509Request.sign(pkey=self.PKey, md='sha1') return self.X509Request    cd  into the appropriate folder:  # For EL5 hosts:   root@host # cd /usr/lib/python2.4/site-packages/osgpkitools/ # For EL6 hosts: root@host # cd /usr/lib/python2.6/site-packages/osgpkitools/    Apply the patch:  root@host # patch   ~/osgpkitools.patch  * HTCondor 8.4.0 has changed it's behavior in ways that cause the GlideinWMS frontend configuration to break. In order to correct this, the following setting needs to be added to the configuration file:  COLLECTOR_USES_SHARED_PORT = False    StashCache packages need to be manually configured   Manual configuration for origin server  Assuming that the origin server connects only to a redirector (not directly to cache server), minimal xrootd configuration is required. The configuration file, /etc/xrootd/xrootd-stashcache-origin-server.cfg, in this release is overkill. Here are recommended settings to use: xrd.port 1094\nall.role server\nall.manager stash-redirector.example.com 1213\nall.export / nostage\nxrootd.trace emsg login stall redirect\nofs.trace none\nxrd.trace conn\ncms.trace all\nsec.protocol  host\nsec.protbind  * none\nall.adminpath /var/run/xrootd\nall.pidpath /var/run/xrootd        Manual configuration for cache server   In contrast to the origin server configuration, one needs to declare  pss.origin  stash-redirector.example.com  instead of configuring the cmsd or manager (only the xrootd daemon is required on the cache server). More detailed configuration of cache server for StashCache is  here .    In both cases, administrator needs to set the path of custom configuration file for its xrootd/cmds instance in /etc/sysconfig/xrootd, For example, change the cmds default from: CMSD_DEFAULT_OPTIONS= -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg -k fifo      to      :::file\n    CMSD_DEFAULT_OPTIONS= -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-stashcache-origin-server.marian -k fifo", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.3/release-3-3-6/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-6/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-6/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-6/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-6/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-6/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-6/#enterprise-linux-6", 
            "text": "condor-8.4.2-1.2.osg33.el6  gip-1.3.11-8.osg33.el6  igtf-ca-certs-1.70-1.osg33.el6  osg-ca-certs-1.51-1.osg33.el6  osg-configure-1.2.5-1.osg33.el6  osg-info-services-1.1.0-1.osg33.el6  osg-test-1.4.32-1.osg33.el6  osg-version-3.3.6-1.osg33.el6  voms-admin-server-2.7.0-1.17.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-6/#enterprise-linux-7", 
            "text": "condor-8.4.2-1.2.osg33.el7  gip-1.3.11-8.osg33.el7  igtf-ca-certs-1.70-1.osg33.el7  osg-ca-certs-1.51-1.osg33.el7  osg-configure-1.2.5-1.osg33.el7  osg-info-services-1.1.0-1.osg33.el7  osg-test-1.4.32-1.osg33.el7  osg-version-3.3.6-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-6/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp gip igtf-ca-certs osg-ca-certs osg-configure osg-configure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-info-services osg-test osg-version voms-admin-server  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-6/#enterprise-linux-6_1", 
            "text": "condor-8.4.2-1.2.osg33.el6\ncondor-all-8.4.2-1.2.osg33.el6\ncondor-bosco-8.4.2-1.2.osg33.el6\ncondor-classads-8.4.2-1.2.osg33.el6\ncondor-classads-devel-8.4.2-1.2.osg33.el6\ncondor-cream-gahp-8.4.2-1.2.osg33.el6\ncondor-debuginfo-8.4.2-1.2.osg33.el6\ncondor-kbdd-8.4.2-1.2.osg33.el6\ncondor-procd-8.4.2-1.2.osg33.el6\ncondor-python-8.4.2-1.2.osg33.el6\ncondor-std-universe-8.4.2-1.2.osg33.el6\ncondor-test-8.4.2-1.2.osg33.el6\ncondor-vm-gahp-8.4.2-1.2.osg33.el6\ngip-1.3.11-8.osg33.el6\nigtf-ca-certs-1.70-1.osg33.el6\nosg-ca-certs-1.51-1.osg33.el6\nosg-configure-1.2.5-1.osg33.el6\nosg-configure-ce-1.2.5-1.osg33.el6\nosg-configure-cemon-1.2.5-1.osg33.el6\nosg-configure-condor-1.2.5-1.osg33.el6\nosg-configure-gateway-1.2.5-1.osg33.el6\nosg-configure-gip-1.2.5-1.osg33.el6\nosg-configure-gratia-1.2.5-1.osg33.el6\nosg-configure-infoservices-1.2.5-1.osg33.el6\nosg-configure-lsf-1.2.5-1.osg33.el6\nosg-configure-managedfork-1.2.5-1.osg33.el6\nosg-configure-misc-1.2.5-1.osg33.el6\nosg-configure-monalisa-1.2.5-1.osg33.el6\nosg-configure-network-1.2.5-1.osg33.el6\nosg-configure-pbs-1.2.5-1.osg33.el6\nosg-configure-rsv-1.2.5-1.osg33.el6\nosg-configure-sge-1.2.5-1.osg33.el6\nosg-configure-slurm-1.2.5-1.osg33.el6\nosg-configure-squid-1.2.5-1.osg33.el6\nosg-configure-tests-1.2.5-1.osg33.el6\nosg-info-services-1.1.0-1.osg33.el6\nosg-test-1.4.32-1.osg33.el6\nosg-version-3.3.6-1.osg33.el6\nvoms-admin-server-2.7.0-1.17.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-6/#enterprise-linux-7_1", 
            "text": "condor-8.4.2-1.2.osg33.el7\ncondor-all-8.4.2-1.2.osg33.el7\ncondor-bosco-8.4.2-1.2.osg33.el7\ncondor-classads-8.4.2-1.2.osg33.el7\ncondor-classads-devel-8.4.2-1.2.osg33.el7\ncondor-debuginfo-8.4.2-1.2.osg33.el7\ncondor-kbdd-8.4.2-1.2.osg33.el7\ncondor-procd-8.4.2-1.2.osg33.el7\ncondor-python-8.4.2-1.2.osg33.el7\ncondor-test-8.4.2-1.2.osg33.el7\ncondor-vm-gahp-8.4.2-1.2.osg33.el7\ngip-1.3.11-8.osg33.el7\nigtf-ca-certs-1.70-1.osg33.el7\nosg-ca-certs-1.51-1.osg33.el7\nosg-configure-1.2.5-1.osg33.el7\nosg-configure-ce-1.2.5-1.osg33.el7\nosg-configure-cemon-1.2.5-1.osg33.el7\nosg-configure-condor-1.2.5-1.osg33.el7\nosg-configure-gateway-1.2.5-1.osg33.el7\nosg-configure-gip-1.2.5-1.osg33.el7\nosg-configure-gratia-1.2.5-1.osg33.el7\nosg-configure-infoservices-1.2.5-1.osg33.el7\nosg-configure-lsf-1.2.5-1.osg33.el7\nosg-configure-managedfork-1.2.5-1.osg33.el7\nosg-configure-misc-1.2.5-1.osg33.el7\nosg-configure-monalisa-1.2.5-1.osg33.el7\nosg-configure-network-1.2.5-1.osg33.el7\nosg-configure-pbs-1.2.5-1.osg33.el7\nosg-configure-rsv-1.2.5-1.osg33.el7\nosg-configure-sge-1.2.5-1.osg33.el7\nosg-configure-slurm-1.2.5-1.osg33.el7\nosg-configure-squid-1.2.5-1.osg33.el7\nosg-configure-tests-1.2.5-1.osg33.el7\nosg-info-services-1.1.0-1.osg33.el7\nosg-test-1.4.32-1.osg33.el7\nosg-version-3.3.6-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-6/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-6/#enterprise-linux-6_2", 
            "text": "", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-6/#enterprise-linux-7_2", 
            "text": "", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-6/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-6/#enterprise-linux-6_3", 
            "text": "", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-6/#enterprise-linux-7_3", 
            "text": "", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-5/", 
            "text": "OSG Software Release 3.3.5\n\n\nRelease Date\n: 2015-11-19\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nlcmaps-plugins-scas-client\n\n\nFixed a leak that caused HTCondor CE to use excessive memory\n\n\n\n\n\n\nosg-configure\n\n\nFixed a crash when attempting to connect to the recently retired ReSS servers while configuring a CE\n\n\nNow reconfigures the HTCondor CE after generating job environment files\n\n\n\n\n\n\nHTCondor CE 1.20\n\n\nUsers can now add onto accounting group defaults set by the job router\n\n\nUse GSI mapping cache to reduce calls to GSI\n\n\n\n\n\n\nHTCondor 8.4.2\n\n\nRSV\n\n\nCorrected log rotation configuration error introduced in 3.2.30/3.3.4\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nKnown Issues\n\n\n\n\nCILogon CA certificate files have been reorganized between our various CA certificate packages.\n\nTo avoid file conflicts, you should upgrade your CA certificate RPMs at the same time, such as via the following command:\n\n\nroot@host # yum update '*-ca-cert*'\n\n\n\n\n\n\n\n\n\n\n\nHTCondor 8.4.0 has changed it's behavior in ways that cause the GlideinWMS frontend configuration to break. In order to correct this, the following setting needs to be added to the configuration file:\n\n\nCOLLECTOR_USES_SHARED_PORT = False\n\n\n\n\n\n\n\n\n\nStashCache packages need to be manually configured\n\n\n\n\nManual configuration for origin server\n\n\nAssuming that the origin server connects only to a redirector (not directly to cache server), minimal xrootd configuration is required. The configuration file, /etc/xrootd/xrootd-stashcache-origin-server.cfg, in this release is overkill. Here are recommended settings to use:\nxrd.port 1094\nall.role server\nall.manager stash-redirector.example.com 1213\nall.export / nostage\nxrootd.trace emsg login stall redirect\nofs.trace none\nxrd.trace conn\ncms.trace all\nsec.protocol  host\nsec.protbind  * none\nall.adminpath /var/run/xrootd\nall.pidpath /var/run/xrootd\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nManual configuration for cache server\n\n\n\n\nIn contrast to the origin server configuration, one needs to declare \npss.origin \nstash-redirector.example.com\n instead of configuring the cmsd or manager (only the xrootd daemon is required on the cache server). More detailed configuration of cache server for StashCache is \nhere\n.\n\n\n\n\n\n\n\n\nIn both cases, administrator needs to set the path of custom configuration file for its xrootd/cmds instance in /etc/sysconfig/xrootd, For example, change the cmds default from:\n\n\nCMSD_DEFAULT_OPTIONS=\n-l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg -k fifo\n\n\n\n\n\n\n\n\nto\n\n\nCMSD_DEFAULT_OPTIONS=\n-l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-stashcache-origin-server.marian -k fifo\n\n\n\n\n\n\n\n\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.15.bosco-3.osg33.el6\n\n\ncondor-8.4.2-1.1.osg33.el6\n\n\nhtcondor-ce-1.20-1.osg33.el6\n\n\nlcmaps-plugins-scas-client-0.5.5-1.1.osg33.el6\n\n\nosg-configure-1.2.4-1.osg33.el6\n\n\nosg-version-3.3.5-1.osg33.el6\n\n\nrsv-3.12.5-1.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.15.bosco-3.osg33.el7\n\n\ncondor-8.4.2-1.1.osg33.el7\n\n\nhtcondor-ce-1.20-1.osg33.el7\n\n\nlcmaps-plugins-scas-client-0.5.5-1.1.osg33.el7\n\n\nosg-configure-1.2.4-1.osg33.el7\n\n\nosg-version-3.3.5-1.osg33.el7\n\n\nrsv-3.12.5-1_clipped.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp htcondor-ce htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-debuginfo htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge lcmaps-plugins-scas-client lcmaps-plugins-scas-client-debuginfo osg-configure osg-configure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-version rsv rsv-consumers rsv-core rsv-metrics\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp-1.18.15.bosco-3.osg33.el6\nblahp-debuginfo-1.18.15.bosco-3.osg33.el6\ncondor-8.4.2-1.1.osg33.el6\ncondor-all-8.4.2-1.1.osg33.el6\ncondor-bosco-8.4.2-1.1.osg33.el6\ncondor-classads-8.4.2-1.1.osg33.el6\ncondor-classads-devel-8.4.2-1.1.osg33.el6\ncondor-cream-gahp-8.4.2-1.1.osg33.el6\ncondor-debuginfo-8.4.2-1.1.osg33.el6\ncondor-kbdd-8.4.2-1.1.osg33.el6\ncondor-procd-8.4.2-1.1.osg33.el6\ncondor-python-8.4.2-1.1.osg33.el6\ncondor-std-universe-8.4.2-1.1.osg33.el6\ncondor-test-8.4.2-1.1.osg33.el6\ncondor-vm-gahp-8.4.2-1.1.osg33.el6\nhtcondor-ce-1.20-1.osg33.el6\nhtcondor-ce-client-1.20-1.osg33.el6\nhtcondor-ce-collector-1.20-1.osg33.el6\nhtcondor-ce-condor-1.20-1.osg33.el6\nhtcondor-ce-debuginfo-1.20-1.osg33.el6\nhtcondor-ce-lsf-1.20-1.osg33.el6\nhtcondor-ce-pbs-1.20-1.osg33.el6\nhtcondor-ce-sge-1.20-1.osg33.el6\nlcmaps-plugins-scas-client-0.5.5-1.1.osg33.el6\nlcmaps-plugins-scas-client-debuginfo-0.5.5-1.1.osg33.el6\nosg-configure-1.2.4-1.osg33.el6\nosg-configure-ce-1.2.4-1.osg33.el6\nosg-configure-cemon-1.2.4-1.osg33.el6\nosg-configure-condor-1.2.4-1.osg33.el6\nosg-configure-gateway-1.2.4-1.osg33.el6\nosg-configure-gip-1.2.4-1.osg33.el6\nosg-configure-gratia-1.2.4-1.osg33.el6\nosg-configure-infoservices-1.2.4-1.osg33.el6\nosg-configure-lsf-1.2.4-1.osg33.el6\nosg-configure-managedfork-1.2.4-1.osg33.el6\nosg-configure-misc-1.2.4-1.osg33.el6\nosg-configure-monalisa-1.2.4-1.osg33.el6\nosg-configure-network-1.2.4-1.osg33.el6\nosg-configure-pbs-1.2.4-1.osg33.el6\nosg-configure-rsv-1.2.4-1.osg33.el6\nosg-configure-sge-1.2.4-1.osg33.el6\nosg-configure-slurm-1.2.4-1.osg33.el6\nosg-configure-squid-1.2.4-1.osg33.el6\nosg-configure-tests-1.2.4-1.osg33.el6\nosg-version-3.3.5-1.osg33.el6\nrsv-3.12.5-1.osg33.el6\nrsv-consumers-3.12.5-1.osg33.el6\nrsv-core-3.12.5-1.osg33.el6\nrsv-metrics-3.12.5-1.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp-1.18.15.bosco-3.osg33.el7\nblahp-debuginfo-1.18.15.bosco-3.osg33.el7\ncondor-8.4.2-1.1.osg33.el7\ncondor-all-8.4.2-1.1.osg33.el7\ncondor-bosco-8.4.2-1.1.osg33.el7\ncondor-classads-8.4.2-1.1.osg33.el7\ncondor-classads-devel-8.4.2-1.1.osg33.el7\ncondor-debuginfo-8.4.2-1.1.osg33.el7\ncondor-kbdd-8.4.2-1.1.osg33.el7\ncondor-procd-8.4.2-1.1.osg33.el7\ncondor-python-8.4.2-1.1.osg33.el7\ncondor-test-8.4.2-1.1.osg33.el7\ncondor-vm-gahp-8.4.2-1.1.osg33.el7\nhtcondor-ce-1.20-1.osg33.el7\nhtcondor-ce-client-1.20-1.osg33.el7\nhtcondor-ce-collector-1.20-1.osg33.el7\nhtcondor-ce-condor-1.20-1.osg33.el7\nhtcondor-ce-debuginfo-1.20-1.osg33.el7\nhtcondor-ce-lsf-1.20-1.osg33.el7\nhtcondor-ce-pbs-1.20-1.osg33.el7\nhtcondor-ce-sge-1.20-1.osg33.el7\nlcmaps-plugins-scas-client-0.5.5-1.1.osg33.el7\nlcmaps-plugins-scas-client-debuginfo-0.5.5-1.1.osg33.el7\nosg-configure-1.2.4-1.osg33.el7\nosg-configure-ce-1.2.4-1.osg33.el7\nosg-configure-cemon-1.2.4-1.osg33.el7\nosg-configure-condor-1.2.4-1.osg33.el7\nosg-configure-gateway-1.2.4-1.osg33.el7\nosg-configure-gip-1.2.4-1.osg33.el7\nosg-configure-gratia-1.2.4-1.osg33.el7\nosg-configure-infoservices-1.2.4-1.osg33.el7\nosg-configure-lsf-1.2.4-1.osg33.el7\nosg-configure-managedfork-1.2.4-1.osg33.el7\nosg-configure-misc-1.2.4-1.osg33.el7\nosg-configure-monalisa-1.2.4-1.osg33.el7\nosg-configure-network-1.2.4-1.osg33.el7\nosg-configure-pbs-1.2.4-1.osg33.el7\nosg-configure-rsv-1.2.4-1.osg33.el7\nosg-configure-sge-1.2.4-1.osg33.el7\nosg-configure-slurm-1.2.4-1.osg33.el7\nosg-configure-squid-1.2.4-1.osg33.el7\nosg-configure-tests-1.2.4-1.osg33.el7\nosg-version-3.3.5-1.osg33.el7\nrsv-3.12.5-1_clipped.osg33.el7\nrsv-consumers-3.12.5-1_clipped.osg33.el7\nrsv-core-3.12.5-1_clipped.osg33.el7\nrsv-metrics-3.12.5-1_clipped.osg33.el7\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\nEnterprise Linux 7\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\n\n\n\n\n\n\nEnterprise Linux 7", 
            "title": "OSG Release 3.3.5"
        }, 
        {
            "location": "/release/3.3/release-3-3-5/#osg-software-release-335", 
            "text": "Release Date : 2015-11-19", 
            "title": "OSG Software Release 3.3.5"
        }, 
        {
            "location": "/release/3.3/release-3-3-5/#summary-of-changes", 
            "text": "This release contains:   lcmaps-plugins-scas-client  Fixed a leak that caused HTCondor CE to use excessive memory    osg-configure  Fixed a crash when attempting to connect to the recently retired ReSS servers while configuring a CE  Now reconfigures the HTCondor CE after generating job environment files    HTCondor CE 1.20  Users can now add onto accounting group defaults set by the job router  Use GSI mapping cache to reduce calls to GSI    HTCondor 8.4.2  RSV  Corrected log rotation configuration error introduced in 3.2.30/3.3.4     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-5/#known-issues", 
            "text": "CILogon CA certificate files have been reorganized between our various CA certificate packages. \nTo avoid file conflicts, you should upgrade your CA certificate RPMs at the same time, such as via the following command:  root@host # yum update '*-ca-cert*'      HTCondor 8.4.0 has changed it's behavior in ways that cause the GlideinWMS frontend configuration to break. In order to correct this, the following setting needs to be added to the configuration file:  COLLECTOR_USES_SHARED_PORT = False    StashCache packages need to be manually configured   Manual configuration for origin server  Assuming that the origin server connects only to a redirector (not directly to cache server), minimal xrootd configuration is required. The configuration file, /etc/xrootd/xrootd-stashcache-origin-server.cfg, in this release is overkill. Here are recommended settings to use: xrd.port 1094\nall.role server\nall.manager stash-redirector.example.com 1213\nall.export / nostage\nxrootd.trace emsg login stall redirect\nofs.trace none\nxrd.trace conn\ncms.trace all\nsec.protocol  host\nsec.protbind  * none\nall.adminpath /var/run/xrootd\nall.pidpath /var/run/xrootd        Manual configuration for cache server   In contrast to the origin server configuration, one needs to declare  pss.origin  stash-redirector.example.com  instead of configuring the cmsd or manager (only the xrootd daemon is required on the cache server). More detailed configuration of cache server for StashCache is  here .     In both cases, administrator needs to set the path of custom configuration file for its xrootd/cmds instance in /etc/sysconfig/xrootd, For example, change the cmds default from:  CMSD_DEFAULT_OPTIONS= -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg -k fifo    to  CMSD_DEFAULT_OPTIONS= -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-stashcache-origin-server.marian -k fifo", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.3/release-3-3-5/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-5/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-5/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-5/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-5/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-5/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-5/#enterprise-linux-6", 
            "text": "blahp-1.18.15.bosco-3.osg33.el6  condor-8.4.2-1.1.osg33.el6  htcondor-ce-1.20-1.osg33.el6  lcmaps-plugins-scas-client-0.5.5-1.1.osg33.el6  osg-configure-1.2.4-1.osg33.el6  osg-version-3.3.5-1.osg33.el6  rsv-3.12.5-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-5/#enterprise-linux-7", 
            "text": "blahp-1.18.15.bosco-3.osg33.el7  condor-8.4.2-1.1.osg33.el7  htcondor-ce-1.20-1.osg33.el7  lcmaps-plugins-scas-client-0.5.5-1.1.osg33.el7  osg-configure-1.2.4-1.osg33.el7  osg-version-3.3.5-1.osg33.el7  rsv-3.12.5-1_clipped.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-5/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp htcondor-ce htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-debuginfo htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge lcmaps-plugins-scas-client lcmaps-plugins-scas-client-debuginfo osg-configure osg-configure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-version rsv rsv-consumers rsv-core rsv-metrics  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-5/#enterprise-linux-6_1", 
            "text": "blahp-1.18.15.bosco-3.osg33.el6\nblahp-debuginfo-1.18.15.bosco-3.osg33.el6\ncondor-8.4.2-1.1.osg33.el6\ncondor-all-8.4.2-1.1.osg33.el6\ncondor-bosco-8.4.2-1.1.osg33.el6\ncondor-classads-8.4.2-1.1.osg33.el6\ncondor-classads-devel-8.4.2-1.1.osg33.el6\ncondor-cream-gahp-8.4.2-1.1.osg33.el6\ncondor-debuginfo-8.4.2-1.1.osg33.el6\ncondor-kbdd-8.4.2-1.1.osg33.el6\ncondor-procd-8.4.2-1.1.osg33.el6\ncondor-python-8.4.2-1.1.osg33.el6\ncondor-std-universe-8.4.2-1.1.osg33.el6\ncondor-test-8.4.2-1.1.osg33.el6\ncondor-vm-gahp-8.4.2-1.1.osg33.el6\nhtcondor-ce-1.20-1.osg33.el6\nhtcondor-ce-client-1.20-1.osg33.el6\nhtcondor-ce-collector-1.20-1.osg33.el6\nhtcondor-ce-condor-1.20-1.osg33.el6\nhtcondor-ce-debuginfo-1.20-1.osg33.el6\nhtcondor-ce-lsf-1.20-1.osg33.el6\nhtcondor-ce-pbs-1.20-1.osg33.el6\nhtcondor-ce-sge-1.20-1.osg33.el6\nlcmaps-plugins-scas-client-0.5.5-1.1.osg33.el6\nlcmaps-plugins-scas-client-debuginfo-0.5.5-1.1.osg33.el6\nosg-configure-1.2.4-1.osg33.el6\nosg-configure-ce-1.2.4-1.osg33.el6\nosg-configure-cemon-1.2.4-1.osg33.el6\nosg-configure-condor-1.2.4-1.osg33.el6\nosg-configure-gateway-1.2.4-1.osg33.el6\nosg-configure-gip-1.2.4-1.osg33.el6\nosg-configure-gratia-1.2.4-1.osg33.el6\nosg-configure-infoservices-1.2.4-1.osg33.el6\nosg-configure-lsf-1.2.4-1.osg33.el6\nosg-configure-managedfork-1.2.4-1.osg33.el6\nosg-configure-misc-1.2.4-1.osg33.el6\nosg-configure-monalisa-1.2.4-1.osg33.el6\nosg-configure-network-1.2.4-1.osg33.el6\nosg-configure-pbs-1.2.4-1.osg33.el6\nosg-configure-rsv-1.2.4-1.osg33.el6\nosg-configure-sge-1.2.4-1.osg33.el6\nosg-configure-slurm-1.2.4-1.osg33.el6\nosg-configure-squid-1.2.4-1.osg33.el6\nosg-configure-tests-1.2.4-1.osg33.el6\nosg-version-3.3.5-1.osg33.el6\nrsv-3.12.5-1.osg33.el6\nrsv-consumers-3.12.5-1.osg33.el6\nrsv-core-3.12.5-1.osg33.el6\nrsv-metrics-3.12.5-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-5/#enterprise-linux-7_1", 
            "text": "blahp-1.18.15.bosco-3.osg33.el7\nblahp-debuginfo-1.18.15.bosco-3.osg33.el7\ncondor-8.4.2-1.1.osg33.el7\ncondor-all-8.4.2-1.1.osg33.el7\ncondor-bosco-8.4.2-1.1.osg33.el7\ncondor-classads-8.4.2-1.1.osg33.el7\ncondor-classads-devel-8.4.2-1.1.osg33.el7\ncondor-debuginfo-8.4.2-1.1.osg33.el7\ncondor-kbdd-8.4.2-1.1.osg33.el7\ncondor-procd-8.4.2-1.1.osg33.el7\ncondor-python-8.4.2-1.1.osg33.el7\ncondor-test-8.4.2-1.1.osg33.el7\ncondor-vm-gahp-8.4.2-1.1.osg33.el7\nhtcondor-ce-1.20-1.osg33.el7\nhtcondor-ce-client-1.20-1.osg33.el7\nhtcondor-ce-collector-1.20-1.osg33.el7\nhtcondor-ce-condor-1.20-1.osg33.el7\nhtcondor-ce-debuginfo-1.20-1.osg33.el7\nhtcondor-ce-lsf-1.20-1.osg33.el7\nhtcondor-ce-pbs-1.20-1.osg33.el7\nhtcondor-ce-sge-1.20-1.osg33.el7\nlcmaps-plugins-scas-client-0.5.5-1.1.osg33.el7\nlcmaps-plugins-scas-client-debuginfo-0.5.5-1.1.osg33.el7\nosg-configure-1.2.4-1.osg33.el7\nosg-configure-ce-1.2.4-1.osg33.el7\nosg-configure-cemon-1.2.4-1.osg33.el7\nosg-configure-condor-1.2.4-1.osg33.el7\nosg-configure-gateway-1.2.4-1.osg33.el7\nosg-configure-gip-1.2.4-1.osg33.el7\nosg-configure-gratia-1.2.4-1.osg33.el7\nosg-configure-infoservices-1.2.4-1.osg33.el7\nosg-configure-lsf-1.2.4-1.osg33.el7\nosg-configure-managedfork-1.2.4-1.osg33.el7\nosg-configure-misc-1.2.4-1.osg33.el7\nosg-configure-monalisa-1.2.4-1.osg33.el7\nosg-configure-network-1.2.4-1.osg33.el7\nosg-configure-pbs-1.2.4-1.osg33.el7\nosg-configure-rsv-1.2.4-1.osg33.el7\nosg-configure-sge-1.2.4-1.osg33.el7\nosg-configure-slurm-1.2.4-1.osg33.el7\nosg-configure-squid-1.2.4-1.osg33.el7\nosg-configure-tests-1.2.4-1.osg33.el7\nosg-version-3.3.5-1.osg33.el7\nrsv-3.12.5-1_clipped.osg33.el7\nrsv-consumers-3.12.5-1_clipped.osg33.el7\nrsv-core-3.12.5-1_clipped.osg33.el7\nrsv-metrics-3.12.5-1_clipped.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-5/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-5/#enterprise-linux-6_2", 
            "text": "", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-5/#enterprise-linux-7_2", 
            "text": "", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-5/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-5/#enterprise-linux-6_3", 
            "text": "", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-5/#enterprise-linux-7_3", 
            "text": "", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-4/", 
            "text": "OSG Software Release 3.3.4\n\n\nRelease Date\n: 2015-11-10\n\n\nSummary of changes\n\n\nThis release builds on the special OSG CA certificate release of November 3rd by making it easier to update from separate installations of the main CA certificate bundle and the new CILogon OSG CA. For hosts with only the main CA certificate bundle, there is no change.\n\n\nThis release contains:\n\n\n\n\nUpdated CVMFS configuration to support approved repositories from any domain\n\n\nFixed osg-ca-certs-updater to work when no \"compat\" packages are present\n\n\nAdded a new LCMAPS plug-in for process tracking\n\n\nAdded a new RSV PerfSONAR probe\n\n\nAdded a new RSV probe to check StashCache cache collector ads at the GOC\n\n\nEL 7: Fixed Frontier Squid to start up properly after a reboot\n\n\nEL 7: Fixed the MyProxy service to start up properly\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nKnown Issues\n\n\n\n\nCILogon CA certificate files have been reorganized between our various CA certificate packages.\n\nTo avoid file conflicts, you should upgrade your CA certificate RPMs at the same time, such as via the following command:\n\n\nroot@host # yum update '*-ca-cert*'\n\n\n\n\nWhen using osg-configure to configure a CE host, it will fail because it tries to contact a ReSS server that has been shut down permanently. The ReSS service has been deprecated since early 2014, and support for it will be removed from osg-configure in an upcoming version. To work around this osg-configure failure now, edit /etc/osg/config.d/30-infoservices.ini and set the option:\n\n\nress_servers = https://localhost[RAW]\n\n\n\n\n\n\n\n\n\nThere is a known memory leak in lcmaps-plugins-scas-client that HTCondor-CE triggers repeatedly. A special OSG release, coming soon, will fix the underlying memory leak, but in the meantime it is possible to make two small configuration changes to slow the rate at which memory leaks.\n\n\n\n\nIn /etc/condor-ce/config.d/01-ce-auth.conf, make sure the following attribute is defined as follows (note the $(USERS) at the end):\nCOLLECTOR.ALLOW_ADVERTISE_STARTD\n \n=\n  \n$(\nUNMAPPED_USERS\n)\n, \n$(\nUSERS\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlso in /etc/condor-ce/config.d/01-ce-auth.conf, add the following line to the end of the file:\n\n\n:::\n \nfile\n\n\nGSS_ASSIST_GRIDMAP_CACHE_EXPIRATION\n \n=\n \n30\n*\n$(\nMINUTE\n)\n\n\n\n\n\n\n\n\n\n\nVerify that the attributes are set properly by running the following command:\n\n\n::: file\n$ condor_ce_config_val COLLECTOR.ALLOW_ADVERTISE_STARTD GSS_ASSIST_GRIDMAP_CACHE_EXPIRATION\n*@unmapped.opensciencegrid.org, *@users.opensciencegrid.org\n30*60\n\n\n\n\n\n\n\n\n\nHTCondor 8.4.0 has changed it's behavior in ways that cause the GlideinWMS frontend configuration to break. In order to correct this, the following setting needs to be added to the configuration file:\n\n\nCOLLECTOR_USES_SHARED_PORT = False\n\n\n\n\n\n\n\n\n\nStashCache packages need to be manually configured\n\n\n\n\nManual configuration for origin server\n\n\nAssuming that the origin server connects only to a redirector (not directly to cache server), minimal xrootd configuration is required. The configuration file, /etc/xrootd/xrootd-stashcache-origin-server.cfg, in this release is overkill. Here are recommended settings to use:\nxrd.port 1094\nall.role server\nall.manager stash-redirector.example.com 1213\nall.export / nostage\nxrootd.trace emsg login stall redirect\nofs.trace none\nxrd.trace conn\ncms.trace all\nsec.protocol  host\nsec.protbind  * none\nall.adminpath /var/run/xrootd\nall.pidpath /var/run/xrootd\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nManual configuration for cache server\n\n\n\n\nIn contrast to the origin server configuration, one needs to declare \npss.origin \nstash-redirector.example.com\n instead of configuring the cmsd or manager (only the xrootd daemon is required on the cache server). More detailed configuration of cache server for StashCache is \nhere\n.\n\n\n\n\n\n\n\n\nIn both cases, administrator needs to set the path of custom configuration file for its xrootd/cmds instance in /etc/sysconfig/xrootd, For example, change the cmds default from:\n\n\nCMSD_DEFAULT_OPTIONS=\n-l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg -k fifo\n\n\n\n\n\n\n\n\nto\n\n\nCMSD_DEFAULT_OPTIONS=\n-l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-stashcache-origin-server.marian -k fifo\n\n\n\n\n\n\n\n\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\ncilogon-openid-ca-cert-1.1-3.osg33.el6\n\n\ncilogon-osg-ca-cert-1.0-2.osg33.el6\n\n\ncvmfs-config-osg-1.1-8.osg33.el6\n\n\nfrontier-squid-2.7.STABLE9-24.2.osg33.el6\n\n\njglobus-2.1.0-6.osg33.el6\n\n\nlcmaps-plugins-process-tracking-0.3-1.osg33.el6\n\n\nmyproxy-6.1.12-1.1.osg33.el6\n\n\nosg-ca-certs-updater-1.3-1.osg33.el6\n\n\nosg-oasis-5-3.osg33.el6\n\n\nosg-test-1.4.31-1.osg33.el6\n\n\nosg-version-3.3.4-1.osg33.el6\n\n\nrsv-3.12.0-1.osg33.el6\n\n\nrsv-perfsonar-1.1.1-1.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\ncilogon-openid-ca-cert-1.1-3.osg33.el7\n\n\ncilogon-osg-ca-cert-1.0-2.osg33.el7\n\n\ncvmfs-config-osg-1.1-8.osg33.el7\n\n\nfrontier-squid-2.7.STABLE9-24.2.osg33.el7\n\n\njglobus-2.1.0-6.osg33.el7\n\n\nlcmaps-plugins-process-tracking-0.3-1.osg33.el7\n\n\nmyproxy-6.1.12-1.1.osg33.el7\n\n\nosg-ca-certs-updater-1.3-1.osg33.el7\n\n\nosg-oasis-5-3.osg33.el7\n\n\nosg-test-1.4.31-1.osg33.el7\n\n\nosg-version-3.3.4-1.osg33.el7\n\n\nrsv-3.12.0-1_clipped.osg33.el7\n\n\nrsv-perfsonar-1.1.1-1.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\ncilogon-openid-ca-cert cilogon-osg-ca-cert cvmfs-config-osg frontier-squid frontier-squid-debuginfo jglobus lcmaps-plugins-process-tracking lcmaps-plugins-process-tracking-debuginfo myproxy myproxy-admin myproxy-debuginfo myproxy-devel myproxy-doc myproxy-libs myproxy-server myproxy-voms osg-ca-certs-updater osg-oasis osg-test osg-version rsv rsv-consumers rsv-core rsv-metrics rsv-perfsonar\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\ncilogon-openid-ca-cert-1.1-3.osg33.el6\ncilogon-osg-ca-cert-1.0-2.osg33.el6\ncvmfs-config-osg-1.1-8.osg33.el6\nfrontier-squid-2.7.STABLE9-24.2.osg33.el6\nfrontier-squid-debuginfo-2.7.STABLE9-24.2.osg33.el6\njglobus-2.1.0-6.osg33.el6\nlcmaps-plugins-process-tracking-0.3-1.osg33.el6\nlcmaps-plugins-process-tracking-debuginfo-0.3-1.osg33.el6\nmyproxy-6.1.12-1.1.osg33.el6\nmyproxy-admin-6.1.12-1.1.osg33.el6\nmyproxy-debuginfo-6.1.12-1.1.osg33.el6\nmyproxy-devel-6.1.12-1.1.osg33.el6\nmyproxy-doc-6.1.12-1.1.osg33.el6\nmyproxy-libs-6.1.12-1.1.osg33.el6\nmyproxy-server-6.1.12-1.1.osg33.el6\nmyproxy-voms-6.1.12-1.1.osg33.el6\nosg-ca-certs-updater-1.3-1.osg33.el6\nosg-oasis-5-3.osg33.el6\nosg-test-1.4.31-1.osg33.el6\nosg-version-3.3.4-1.osg33.el6\nrsv-3.12.0-1.osg33.el6\nrsv-consumers-3.12.0-1.osg33.el6\nrsv-core-3.12.0-1.osg33.el6\nrsv-metrics-3.12.0-1.osg33.el6\nrsv-perfsonar-1.1.1-1.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\ncilogon-openid-ca-cert-1.1-3.osg33.el7\ncilogon-osg-ca-cert-1.0-2.osg33.el7\ncvmfs-config-osg-1.1-8.osg33.el7\nfrontier-squid-2.7.STABLE9-24.2.osg33.el7\nfrontier-squid-debuginfo-2.7.STABLE9-24.2.osg33.el7\njglobus-2.1.0-6.osg33.el7\nlcmaps-plugins-process-tracking-0.3-1.osg33.el7\nlcmaps-plugins-process-tracking-debuginfo-0.3-1.osg33.el7\nmyproxy-6.1.12-1.1.osg33.el7\nmyproxy-admin-6.1.12-1.1.osg33.el7\nmyproxy-debuginfo-6.1.12-1.1.osg33.el7\nmyproxy-devel-6.1.12-1.1.osg33.el7\nmyproxy-doc-6.1.12-1.1.osg33.el7\nmyproxy-libs-6.1.12-1.1.osg33.el7\nmyproxy-server-6.1.12-1.1.osg33.el7\nmyproxy-voms-6.1.12-1.1.osg33.el7\nosg-ca-certs-updater-1.3-1.osg33.el7\nosg-oasis-5-3.osg33.el7\nosg-test-1.4.31-1.osg33.el7\nosg-version-3.3.4-1.osg33.el7\nrsv-3.12.0-1_clipped.osg33.el7\nrsv-consumers-3.12.0-1_clipped.osg33.el7\nrsv-core-3.12.0-1_clipped.osg33.el7\nrsv-metrics-3.12.0-1_clipped.osg33.el7\nrsv-perfsonar-1.1.1-1.osg33.el7\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\nEnterprise Linux 7\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\n\n\n\n\n\n\nEnterprise Linux 7", 
            "title": "OSG Release 3.3.4"
        }, 
        {
            "location": "/release/3.3/release-3-3-4/#osg-software-release-334", 
            "text": "Release Date : 2015-11-10", 
            "title": "OSG Software Release 3.3.4"
        }, 
        {
            "location": "/release/3.3/release-3-3-4/#summary-of-changes", 
            "text": "This release builds on the special OSG CA certificate release of November 3rd by making it easier to update from separate installations of the main CA certificate bundle and the new CILogon OSG CA. For hosts with only the main CA certificate bundle, there is no change.  This release contains:   Updated CVMFS configuration to support approved repositories from any domain  Fixed osg-ca-certs-updater to work when no \"compat\" packages are present  Added a new LCMAPS plug-in for process tracking  Added a new RSV PerfSONAR probe  Added a new RSV probe to check StashCache cache collector ads at the GOC  EL 7: Fixed Frontier Squid to start up properly after a reboot  EL 7: Fixed the MyProxy service to start up properly   These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-4/#known-issues", 
            "text": "CILogon CA certificate files have been reorganized between our various CA certificate packages. \nTo avoid file conflicts, you should upgrade your CA certificate RPMs at the same time, such as via the following command:  root@host # yum update '*-ca-cert*'   When using osg-configure to configure a CE host, it will fail because it tries to contact a ReSS server that has been shut down permanently. The ReSS service has been deprecated since early 2014, and support for it will be removed from osg-configure in an upcoming version. To work around this osg-configure failure now, edit /etc/osg/config.d/30-infoservices.ini and set the option:  ress_servers = https://localhost[RAW]    There is a known memory leak in lcmaps-plugins-scas-client that HTCondor-CE triggers repeatedly. A special OSG release, coming soon, will fix the underlying memory leak, but in the meantime it is possible to make two small configuration changes to slow the rate at which memory leaks.   In /etc/condor-ce/config.d/01-ce-auth.conf, make sure the following attribute is defined as follows (note the $(USERS) at the end): COLLECTOR.ALLOW_ADVERTISE_STARTD   =    $( UNMAPPED_USERS ) ,  $( USERS )       Also in /etc/condor-ce/config.d/01-ce-auth.conf, add the following line to the end of the file:  :::   file  GSS_ASSIST_GRIDMAP_CACHE_EXPIRATION   =   30 * $( MINUTE )     Verify that the attributes are set properly by running the following command:  ::: file\n$ condor_ce_config_val COLLECTOR.ALLOW_ADVERTISE_STARTD GSS_ASSIST_GRIDMAP_CACHE_EXPIRATION\n*@unmapped.opensciencegrid.org, *@users.opensciencegrid.org\n30*60    HTCondor 8.4.0 has changed it's behavior in ways that cause the GlideinWMS frontend configuration to break. In order to correct this, the following setting needs to be added to the configuration file:  COLLECTOR_USES_SHARED_PORT = False    StashCache packages need to be manually configured   Manual configuration for origin server  Assuming that the origin server connects only to a redirector (not directly to cache server), minimal xrootd configuration is required. The configuration file, /etc/xrootd/xrootd-stashcache-origin-server.cfg, in this release is overkill. Here are recommended settings to use: xrd.port 1094\nall.role server\nall.manager stash-redirector.example.com 1213\nall.export / nostage\nxrootd.trace emsg login stall redirect\nofs.trace none\nxrd.trace conn\ncms.trace all\nsec.protocol  host\nsec.protbind  * none\nall.adminpath /var/run/xrootd\nall.pidpath /var/run/xrootd        Manual configuration for cache server   In contrast to the origin server configuration, one needs to declare  pss.origin  stash-redirector.example.com  instead of configuring the cmsd or manager (only the xrootd daemon is required on the cache server). More detailed configuration of cache server for StashCache is  here .     In both cases, administrator needs to set the path of custom configuration file for its xrootd/cmds instance in /etc/sysconfig/xrootd, For example, change the cmds default from:  CMSD_DEFAULT_OPTIONS= -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg -k fifo    to  CMSD_DEFAULT_OPTIONS= -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-stashcache-origin-server.marian -k fifo", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.3/release-3-3-4/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-4/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-4/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-4/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-4/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-4/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-4/#enterprise-linux-6", 
            "text": "cilogon-openid-ca-cert-1.1-3.osg33.el6  cilogon-osg-ca-cert-1.0-2.osg33.el6  cvmfs-config-osg-1.1-8.osg33.el6  frontier-squid-2.7.STABLE9-24.2.osg33.el6  jglobus-2.1.0-6.osg33.el6  lcmaps-plugins-process-tracking-0.3-1.osg33.el6  myproxy-6.1.12-1.1.osg33.el6  osg-ca-certs-updater-1.3-1.osg33.el6  osg-oasis-5-3.osg33.el6  osg-test-1.4.31-1.osg33.el6  osg-version-3.3.4-1.osg33.el6  rsv-3.12.0-1.osg33.el6  rsv-perfsonar-1.1.1-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-4/#enterprise-linux-7", 
            "text": "cilogon-openid-ca-cert-1.1-3.osg33.el7  cilogon-osg-ca-cert-1.0-2.osg33.el7  cvmfs-config-osg-1.1-8.osg33.el7  frontier-squid-2.7.STABLE9-24.2.osg33.el7  jglobus-2.1.0-6.osg33.el7  lcmaps-plugins-process-tracking-0.3-1.osg33.el7  myproxy-6.1.12-1.1.osg33.el7  osg-ca-certs-updater-1.3-1.osg33.el7  osg-oasis-5-3.osg33.el7  osg-test-1.4.31-1.osg33.el7  osg-version-3.3.4-1.osg33.el7  rsv-3.12.0-1_clipped.osg33.el7  rsv-perfsonar-1.1.1-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-4/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  cilogon-openid-ca-cert cilogon-osg-ca-cert cvmfs-config-osg frontier-squid frontier-squid-debuginfo jglobus lcmaps-plugins-process-tracking lcmaps-plugins-process-tracking-debuginfo myproxy myproxy-admin myproxy-debuginfo myproxy-devel myproxy-doc myproxy-libs myproxy-server myproxy-voms osg-ca-certs-updater osg-oasis osg-test osg-version rsv rsv-consumers rsv-core rsv-metrics rsv-perfsonar  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-4/#enterprise-linux-6_1", 
            "text": "cilogon-openid-ca-cert-1.1-3.osg33.el6\ncilogon-osg-ca-cert-1.0-2.osg33.el6\ncvmfs-config-osg-1.1-8.osg33.el6\nfrontier-squid-2.7.STABLE9-24.2.osg33.el6\nfrontier-squid-debuginfo-2.7.STABLE9-24.2.osg33.el6\njglobus-2.1.0-6.osg33.el6\nlcmaps-plugins-process-tracking-0.3-1.osg33.el6\nlcmaps-plugins-process-tracking-debuginfo-0.3-1.osg33.el6\nmyproxy-6.1.12-1.1.osg33.el6\nmyproxy-admin-6.1.12-1.1.osg33.el6\nmyproxy-debuginfo-6.1.12-1.1.osg33.el6\nmyproxy-devel-6.1.12-1.1.osg33.el6\nmyproxy-doc-6.1.12-1.1.osg33.el6\nmyproxy-libs-6.1.12-1.1.osg33.el6\nmyproxy-server-6.1.12-1.1.osg33.el6\nmyproxy-voms-6.1.12-1.1.osg33.el6\nosg-ca-certs-updater-1.3-1.osg33.el6\nosg-oasis-5-3.osg33.el6\nosg-test-1.4.31-1.osg33.el6\nosg-version-3.3.4-1.osg33.el6\nrsv-3.12.0-1.osg33.el6\nrsv-consumers-3.12.0-1.osg33.el6\nrsv-core-3.12.0-1.osg33.el6\nrsv-metrics-3.12.0-1.osg33.el6\nrsv-perfsonar-1.1.1-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-4/#enterprise-linux-7_1", 
            "text": "cilogon-openid-ca-cert-1.1-3.osg33.el7\ncilogon-osg-ca-cert-1.0-2.osg33.el7\ncvmfs-config-osg-1.1-8.osg33.el7\nfrontier-squid-2.7.STABLE9-24.2.osg33.el7\nfrontier-squid-debuginfo-2.7.STABLE9-24.2.osg33.el7\njglobus-2.1.0-6.osg33.el7\nlcmaps-plugins-process-tracking-0.3-1.osg33.el7\nlcmaps-plugins-process-tracking-debuginfo-0.3-1.osg33.el7\nmyproxy-6.1.12-1.1.osg33.el7\nmyproxy-admin-6.1.12-1.1.osg33.el7\nmyproxy-debuginfo-6.1.12-1.1.osg33.el7\nmyproxy-devel-6.1.12-1.1.osg33.el7\nmyproxy-doc-6.1.12-1.1.osg33.el7\nmyproxy-libs-6.1.12-1.1.osg33.el7\nmyproxy-server-6.1.12-1.1.osg33.el7\nmyproxy-voms-6.1.12-1.1.osg33.el7\nosg-ca-certs-updater-1.3-1.osg33.el7\nosg-oasis-5-3.osg33.el7\nosg-test-1.4.31-1.osg33.el7\nosg-version-3.3.4-1.osg33.el7\nrsv-3.12.0-1_clipped.osg33.el7\nrsv-consumers-3.12.0-1_clipped.osg33.el7\nrsv-core-3.12.0-1_clipped.osg33.el7\nrsv-metrics-3.12.0-1_clipped.osg33.el7\nrsv-perfsonar-1.1.1-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-4/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-4/#enterprise-linux-6_2", 
            "text": "", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-4/#enterprise-linux-7_2", 
            "text": "", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-4/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-4/#enterprise-linux-6_3", 
            "text": "", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-4/#enterprise-linux-7_3", 
            "text": "", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-3/", 
            "text": "OSG Software Release 3.3.3\n\n\nRelease Date\n: 2015-11-03\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nCA certificates based on \nIGTF 1.69\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nCA Certificate Update Information\n\n\nWhat Is Changing\n\n\nA new IGTF Certificate Bundle (v1.69) has just been released. This release has a very important change and we would like our sites to install it as soon as they can.\n\n\nWho Is Impacted By This Change:\n\n\nAll OSG sites and users need to install the new CA bundle. Especially US-Atlas and US-CMS sites should install the bundle as soon as possible since their VOs will go under the transition in November/December timeframe.\n\n\nWhy This Change Is Happening:\n\n\nThe OSG CA is changing its backend service support from Digicert to CILogon HSM. As a result, a new OSG CA is created and just recently been accredited by IGTF. The official name of the new OSG CA in the IGTF bundle is CILogon OSG CA. Starting in November we will transition our VOs to start using the new OSG CA (CMS and Atlas being first ones). If a site has not installed the CA bundle by then, they will have authentication failures.\n\n\nWhat You Should Do:\n\n\nInstall the new CA bundle as soon as possible. The latest CA bundle will NOT be distributed in OSG Software v 3.1 because OSG no longer supports it.\n\n\nFor Linux servers (including worker nodes), ensure that the certificate bundle RPM is at version osg-ca-certs-1.50-1 or igtf-ca-certs-1.69-1 or greater.\n\n\nInstructions for installing server CA certificate bundles are at \nhttps://twiki.grid.iu.edu/bin/view/Documentation/Release3/InstallCertAuth\n\n\nWe also highly recommend that you use the CA Cert automatic updater \nhttps://twiki.grid.iu.edu/bin/view/Documentation/Release3/OsgCaCertsUpdater\n but note that you need to be using a current OSG software distribution for that to work, that is, OSG 3.2 or 3.3.\n\n\nOther Information:\n\n\nIf you have the CA certificate bundle installed on a server with OSG 3.1, you need to upgrade to OSG 3.2 or greater. Follow these instructions: \nhttps://twiki.grid.iu.edu/bin/view/Documentation/Release3/OSGReleaseSeries#Updating_from_OSG_3_1_or_3_2_to\n\n\nPlease email OSG Security Team with questions or comments\n\n\nKnown Issues\n\n\n\n\n\n\nHTCondor 8.4.0 has changed it's behavior in ways that cause the GlideinWMS frontend configuration to break. In order to correct this, the following setting needs to be added to the configuration file:\n\n\nCOLLECTOR_USES_SHARED_PORT = False\n\n\n\n\n\n\n\n\n\nStashCache packages need to be manually configured\n\n\n\n\nManual configuration for origin server\n\n\nAssuming that the origin server connects only to a redirector (not directly to cache server), minimal xrootd configuration is required. The configuration file, /etc/xrootd/xrootd-stashcache-origin-server.cfg, in this release is overkill. Here are recommended settings to use:\nxrd.port 1094\nall.role server\nall.manager stash-redirector.example.com 1213\nall.export / nostage\nxrootd.trace emsg login stall redirect\nofs.trace none\nxrd.trace conn\ncms.trace all\nsec.protocol  host\nsec.protbind  * none\nall.adminpath /var/run/xrootd\nall.pidpath /var/run/xrootd\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nManual configuration for cache server\n\n\n\n\nIn contrast to the origin server configuration, one needs to declare \npss.origin \nstash-redirector.example.com\n instead of configuring the cmsd or manager (only the xrootd daemon is required on the cache server). More detailed configuration of cache server for StashCache is \nhere\n.\n\n\n\n\n\n\n\n\nIn both cases, administrator needs to set the path of custom configuration file for its xrootd/cmds instance in /etc/sysconfig/xrootd, For example, change the cmds default from:\n\n\nCMSD_DEFAULT_OPTIONS=\n-l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg -k fifo\n\n\n\n\n\n\n\n\nto\n\n\nCMSD_DEFAULT_OPTIONS=\n-l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-stashcache-origin-server.marian -k fifo\n\n\n\n\n\n\n\n\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nigtf-ca-certs-1.69-1.osg33.el6\n\n\nosg-ca-certs-1.50-1.osg33.el6\n\n\nosg-version-3.3.3-1.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nigtf-ca-certs-1.69-1.osg33.el7\n\n\nosg-ca-certs-1.50-1.osg33.el7\n\n\nosg-version-3.3.3-1.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nigtf-ca-certs osg-ca-certs osg-version\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nigtf-ca-certs-1.69-1.osg33.el6\nosg-ca-certs-1.50-1.osg33.el6\nosg-version-3.3.3-1.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nigtf-ca-certs-1.69-1.osg33.el7\nosg-ca-certs-1.50-1.osg33.el7\nosg-version-3.3.3-1.osg33.el7\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\nEnterprise Linux 7\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\n\n\n\n\n\n\nEnterprise Linux 7", 
            "title": "OSG Release 3.3.3"
        }, 
        {
            "location": "/release/3.3/release-3-3-3/#osg-software-release-333", 
            "text": "Release Date : 2015-11-03", 
            "title": "OSG Software Release 3.3.3"
        }, 
        {
            "location": "/release/3.3/release-3-3-3/#summary-of-changes", 
            "text": "This release contains:   CA certificates based on  IGTF 1.69   These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-3/#ca-certificate-update-information", 
            "text": "", 
            "title": "CA Certificate Update Information"
        }, 
        {
            "location": "/release/3.3/release-3-3-3/#what-is-changing", 
            "text": "A new IGTF Certificate Bundle (v1.69) has just been released. This release has a very important change and we would like our sites to install it as soon as they can.", 
            "title": "What Is Changing"
        }, 
        {
            "location": "/release/3.3/release-3-3-3/#who-is-impacted-by-this-change", 
            "text": "All OSG sites and users need to install the new CA bundle. Especially US-Atlas and US-CMS sites should install the bundle as soon as possible since their VOs will go under the transition in November/December timeframe.", 
            "title": "Who Is Impacted By This Change:"
        }, 
        {
            "location": "/release/3.3/release-3-3-3/#why-this-change-is-happening", 
            "text": "The OSG CA is changing its backend service support from Digicert to CILogon HSM. As a result, a new OSG CA is created and just recently been accredited by IGTF. The official name of the new OSG CA in the IGTF bundle is CILogon OSG CA. Starting in November we will transition our VOs to start using the new OSG CA (CMS and Atlas being first ones). If a site has not installed the CA bundle by then, they will have authentication failures.", 
            "title": "Why This Change Is Happening:"
        }, 
        {
            "location": "/release/3.3/release-3-3-3/#what-you-should-do", 
            "text": "Install the new CA bundle as soon as possible. The latest CA bundle will NOT be distributed in OSG Software v 3.1 because OSG no longer supports it.  For Linux servers (including worker nodes), ensure that the certificate bundle RPM is at version osg-ca-certs-1.50-1 or igtf-ca-certs-1.69-1 or greater.  Instructions for installing server CA certificate bundles are at  https://twiki.grid.iu.edu/bin/view/Documentation/Release3/InstallCertAuth  We also highly recommend that you use the CA Cert automatic updater  https://twiki.grid.iu.edu/bin/view/Documentation/Release3/OsgCaCertsUpdater  but note that you need to be using a current OSG software distribution for that to work, that is, OSG 3.2 or 3.3.", 
            "title": "What You Should Do:"
        }, 
        {
            "location": "/release/3.3/release-3-3-3/#other-information", 
            "text": "If you have the CA certificate bundle installed on a server with OSG 3.1, you need to upgrade to OSG 3.2 or greater. Follow these instructions:  https://twiki.grid.iu.edu/bin/view/Documentation/Release3/OSGReleaseSeries#Updating_from_OSG_3_1_or_3_2_to  Please email OSG Security Team with questions or comments", 
            "title": "Other Information:"
        }, 
        {
            "location": "/release/3.3/release-3-3-3/#known-issues", 
            "text": "HTCondor 8.4.0 has changed it's behavior in ways that cause the GlideinWMS frontend configuration to break. In order to correct this, the following setting needs to be added to the configuration file:  COLLECTOR_USES_SHARED_PORT = False    StashCache packages need to be manually configured   Manual configuration for origin server  Assuming that the origin server connects only to a redirector (not directly to cache server), minimal xrootd configuration is required. The configuration file, /etc/xrootd/xrootd-stashcache-origin-server.cfg, in this release is overkill. Here are recommended settings to use: xrd.port 1094\nall.role server\nall.manager stash-redirector.example.com 1213\nall.export / nostage\nxrootd.trace emsg login stall redirect\nofs.trace none\nxrd.trace conn\ncms.trace all\nsec.protocol  host\nsec.protbind  * none\nall.adminpath /var/run/xrootd\nall.pidpath /var/run/xrootd        Manual configuration for cache server   In contrast to the origin server configuration, one needs to declare  pss.origin  stash-redirector.example.com  instead of configuring the cmsd or manager (only the xrootd daemon is required on the cache server). More detailed configuration of cache server for StashCache is  here .     In both cases, administrator needs to set the path of custom configuration file for its xrootd/cmds instance in /etc/sysconfig/xrootd, For example, change the cmds default from:  CMSD_DEFAULT_OPTIONS= -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg -k fifo    to  CMSD_DEFAULT_OPTIONS= -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-stashcache-origin-server.marian -k fifo", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.3/release-3-3-3/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-3/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-3/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-3/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-3/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-3/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-3/#enterprise-linux-6", 
            "text": "igtf-ca-certs-1.69-1.osg33.el6  osg-ca-certs-1.50-1.osg33.el6  osg-version-3.3.3-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-3/#enterprise-linux-7", 
            "text": "igtf-ca-certs-1.69-1.osg33.el7  osg-ca-certs-1.50-1.osg33.el7  osg-version-3.3.3-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-3/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  igtf-ca-certs osg-ca-certs osg-version  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-3/#enterprise-linux-6_1", 
            "text": "igtf-ca-certs-1.69-1.osg33.el6\nosg-ca-certs-1.50-1.osg33.el6\nosg-version-3.3.3-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-3/#enterprise-linux-7_1", 
            "text": "igtf-ca-certs-1.69-1.osg33.el7\nosg-ca-certs-1.50-1.osg33.el7\nosg-version-3.3.3-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-3/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-3/#enterprise-linux-6_2", 
            "text": "", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-3/#enterprise-linux-7_2", 
            "text": "", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-3/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-3/#enterprise-linux-6_3", 
            "text": "", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-3/#enterprise-linux-7_3", 
            "text": "", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-2/", 
            "text": "OSG Software Release 3.3.2\n\n\nRelease Date\n: 2015-10-13\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nGlideinWMS 3.2.11.2\n\n\nXRootD 4.2.3\n\n\nHTCondor 8.4.0\n\n\nStashCache 0.6\n\n\ndaemon refuses to start if host certificate is not present\n\n\nuse FQDN in stashcache-daemon\n\n\n\n\n\n\nGUMS 1.5.1\n\n\nReturn groupName for pool account mappers\n\n\nBug fixes\n\n\n\n\n\n\nHTCondor-CE 1.16\n\n\nUpdates for PBS variants\n\n\nAdd CERN host DN format to HTCondor-CE configuration defaults\n\n\n\n\n\n\nGIP support multiple SLURM queues\n\n\nosg-configure 1.2.2\n\n\nSupport IPv6 IP addresses in configuration files\n\n\nAdd sensible default values for Allowed VOs\n\n\n\n\n\n\nRSV - srmcp-srm-probe (delays to account for NFS caching behavior)\n\n\nAdd lcmaps-plugins-mount-under-scratch package\n\n\nUpdate to edg-mkgridmap 4.0.3, so it works on EL\u00a07\n\n\nCA certificates based on \nIGTF 1.68\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nKnown Issues\n\n\n\n\n\n\nHTCondor 8.4.0 has changed it's behavior in ways that cause the GlideinWMS frontend configuration to break. In order to correct this, the following setting needs to be added to the configuration file:\n\n\nCOLLECTOR_USES_SHARED_PORT = False\n\n\n\n\n\n\n\n\n\nStashCache packages need to be manually configured\n\n\n\n\nManual configuration for origin server\n\n\nAssuming that the origin server connects only to a redirector (not directly to cache server), minimal xrootd configuration is required. The configuration file, /etc/xrootd/xrootd-stashcache-origin-server.cfg, in this release is overkill. Here are recommended settings to use:\nxrd.port 1094\nall.role server\nall.manager stash-redirector.example.com 1213\nall.export / nostage\nxrootd.trace emsg login stall redirect\nofs.trace none\nxrd.trace conn\ncms.trace all\nsec.protocol  host\nsec.protbind  * none\nall.adminpath /var/run/xrootd\nall.pidpath /var/run/xrootd\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nManual configuration for cache server\n\n\n\n\nIn contrast to the origin server configuration, one needs to declare \npss.origin \nstash-redirector.example.com\n instead of configuring the cmsd or manager (only the xrootd daemon is required on the cache server). More detailed configuration of cache server for StashCache is \nhere\n.\n\n\n\n\n\n\n\n\nIn both cases, administrator needs to set the path of custom configuration file for its xrootd/cmds instance in /etc/sysconfig/xrootd, For example, change the cmds default from:\n\n\nCMSD_DEFAULT_OPTIONS=\n-l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg -k fifo\n\n\n\n\n\n\n\n\nto\n\n\nCMSD_DEFAULT_OPTIONS=\n-l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-stashcache-origin-server.marian -k fifo\n\n\n\n\n\n\n\n\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nbestman2-2.3.0-26.osg33.el6\n\n\nblahp-1.18.14.bosco-1.osg33.el6\n\n\ncondor-8.4.0-1.2.osg33.el6\n\n\nedg-mkgridmap-4.0.3-2.osg33.el6\n\n\ngip-1.3.11-7.osg33.el6\n\n\nglideinwms-3.2.11.2-4.osg33.el6\n\n\ngridftp-hdfs-0.5.4-22.osg33.el6\n\n\ngums-1.5.1-1.osg33.el6\n\n\nhtcondor-ce-1.16-1.osg33.el6\n\n\nigtf-ca-certs-1.68-1.osg33.el6\n\n\njglobus-2.1.0-5.osg33.el6\n\n\nlcmaps-plugins-mount-under-scratch-0.0.4-1.osg33.el6\n\n\nosg-ca-certs-1.49-1.osg33.el6\n\n\nosg-configure-1.2.2-1.osg33.el6\n\n\nosg-test-1.4.30-1.osg33.el6\n\n\nosg-tested-internal-3.3-5.osg33.el6\n\n\nosg-version-3.3.2-1.osg33.el6\n\n\nprivilege-xacml-2.6.5-1.osg33.el6\n\n\nrsv-3.10.4-1.osg33.el6\n\n\nstashcache-0.6-1.osg33.el6\n\n\nxrootd-4.2.3-1.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\naxis-1.4-23.1.osg33.el7\n\n\nblahp-1.18.14.bosco-1.osg33.el7\n\n\ncondor-8.4.0-1.2.osg33.el7\n\n\nedg-mkgridmap-4.0.3-2.osg33.el7\n\n\ngip-1.3.11-7.osg33.el7\n\n\nglideinwms-3.2.11.2-4.osg33.el7\n\n\nhtcondor-ce-1.16-1.osg33.el7\n\n\nigtf-ca-certs-1.68-1.osg33.el7\n\n\njavamail-1.5.0-6.osg33.el7\n\n\nlcmaps-plugins-mount-under-scratch-0.0.4-1.osg33.el7\n\n\nosg-ca-certs-1.49-1.osg33.el7\n\n\nosg-configure-1.2.2-1.osg33.el7\n\n\nosg-test-1.4.30-1.osg33.el7\n\n\nosg-tested-internal-3.3-5.osg33.el7\n\n\nosg-version-3.3.2-1.osg33.el7\n\n\nrsv-3.10.4-1_clipped.osg33.el7\n\n\nstashcache-0.6-1.osg33.el7\n\n\nwsdl4j-1.6.3-3.1.osg33.el7\n\n\nxrootd-4.2.3-1.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nbestman2-client bestman2-client-libs bestman2-common-libs bestman2-server bestman2-server-dep-libs bestman2-server-libs bestman2-tester bestman2-tester-libs blahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp edg-mkgridmap gip glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone gridftp-hdfs gridftp-hdfs-debuginfo gums gums-client gums-service htcondor-ce htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-debuginfo htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge igtf-ca-certs jglobus lcmaps-plugins-mount-under-scratch lcmaps-plugins-mount-under-scratch-debuginfo osg-ca-certs osg-configure osg-configure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-test osg-tested-internal osg-version privilege-xacml python-argparse python-backports-ssl_match_hostname python-requests python-urllib3 rsv rsv-consumers rsv-core rsv-metrics stashcache-cache-server stashcache-daemon stashcache-origin-server xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-libs xrootd-private-devel xrootd-python xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nbestman2-2.3.0-26.osg33.el6\nbestman2-client-2.3.0-26.osg33.el6\nbestman2-client-libs-2.3.0-26.osg33.el6\nbestman2-common-libs-2.3.0-26.osg33.el6\nbestman2-server-2.3.0-26.osg33.el6\nbestman2-server-dep-libs-2.3.0-26.osg33.el6\nbestman2-server-libs-2.3.0-26.osg33.el6\nbestman2-tester-2.3.0-26.osg33.el6\nbestman2-tester-libs-2.3.0-26.osg33.el6\nblahp-1.18.14.bosco-1.osg33.el6\nblahp-debuginfo-1.18.14.bosco-1.osg33.el6\ncondor-8.4.0-1.2.osg33.el6\ncondor-all-8.4.0-1.2.osg33.el6\ncondor-bosco-8.4.0-1.2.osg33.el6\ncondor-classads-8.4.0-1.2.osg33.el6\ncondor-classads-devel-8.4.0-1.2.osg33.el6\ncondor-cream-gahp-8.4.0-1.2.osg33.el6\ncondor-debuginfo-8.4.0-1.2.osg33.el6\ncondor-kbdd-8.4.0-1.2.osg33.el6\ncondor-procd-8.4.0-1.2.osg33.el6\ncondor-python-8.4.0-1.2.osg33.el6\ncondor-std-universe-8.4.0-1.2.osg33.el6\ncondor-test-8.4.0-1.2.osg33.el6\ncondor-vm-gahp-8.4.0-1.2.osg33.el6\nedg-mkgridmap-4.0.3-2.osg33.el6\ngip-1.3.11-7.osg33.el6\nglideinwms-3.2.11.2-4.osg33.el6\nglideinwms-common-tools-3.2.11.2-4.osg33.el6\nglideinwms-condor-common-config-3.2.11.2-4.osg33.el6\nglideinwms-factory-3.2.11.2-4.osg33.el6\nglideinwms-factory-condor-3.2.11.2-4.osg33.el6\nglideinwms-glidecondor-tools-3.2.11.2-4.osg33.el6\nglideinwms-libs-3.2.11.2-4.osg33.el6\nglideinwms-minimal-condor-3.2.11.2-4.osg33.el6\nglideinwms-usercollector-3.2.11.2-4.osg33.el6\nglideinwms-userschedd-3.2.11.2-4.osg33.el6\nglideinwms-vofrontend-3.2.11.2-4.osg33.el6\nglideinwms-vofrontend-standalone-3.2.11.2-4.osg33.el6\ngridftp-hdfs-0.5.4-22.osg33.el6\ngridftp-hdfs-debuginfo-0.5.4-22.osg33.el6\ngums-1.5.1-1.osg33.el6\ngums-client-1.5.1-1.osg33.el6\ngums-service-1.5.1-1.osg33.el6\nhtcondor-ce-1.16-1.osg33.el6\nhtcondor-ce-client-1.16-1.osg33.el6\nhtcondor-ce-collector-1.16-1.osg33.el6\nhtcondor-ce-condor-1.16-1.osg33.el6\nhtcondor-ce-debuginfo-1.16-1.osg33.el6\nhtcondor-ce-lsf-1.16-1.osg33.el6\nhtcondor-ce-pbs-1.16-1.osg33.el6\nhtcondor-ce-sge-1.16-1.osg33.el6\nigtf-ca-certs-1.68-1.osg33.el6\njglobus-2.1.0-5.osg33.el6\nlcmaps-plugins-mount-under-scratch-0.0.4-1.osg33.el6\nlcmaps-plugins-mount-under-scratch-debuginfo-0.0.4-1.osg33.el6\nosg-ca-certs-1.49-1.osg33.el6\nosg-configure-1.2.2-1.osg33.el6\nosg-configure-ce-1.2.2-1.osg33.el6\nosg-configure-cemon-1.2.2-1.osg33.el6\nosg-configure-condor-1.2.2-1.osg33.el6\nosg-configure-gateway-1.2.2-1.osg33.el6\nosg-configure-gip-1.2.2-1.osg33.el6\nosg-configure-gratia-1.2.2-1.osg33.el6\nosg-configure-infoservices-1.2.2-1.osg33.el6\nosg-configure-lsf-1.2.2-1.osg33.el6\nosg-configure-managedfork-1.2.2-1.osg33.el6\nosg-configure-misc-1.2.2-1.osg33.el6\nosg-configure-monalisa-1.2.2-1.osg33.el6\nosg-configure-network-1.2.2-1.osg33.el6\nosg-configure-pbs-1.2.2-1.osg33.el6\nosg-configure-rsv-1.2.2-1.osg33.el6\nosg-configure-sge-1.2.2-1.osg33.el6\nosg-configure-slurm-1.2.2-1.osg33.el6\nosg-configure-squid-1.2.2-1.osg33.el6\nosg-configure-tests-1.2.2-1.osg33.el6\nosg-test-1.4.30-1.osg33.el6\nosg-tested-internal-3.3-5.osg33.el6\nosg-version-3.3.2-1.osg33.el6\nprivilege-xacml-2.6.5-1.osg33.el6\nrsv-3.10.4-1.osg33.el6\nrsv-consumers-3.10.4-1.osg33.el6\nrsv-core-3.10.4-1.osg33.el6\nrsv-metrics-3.10.4-1.osg33.el6\nstashcache-0.6-1.osg33.el6\nstashcache-cache-server-0.6-1.osg33.el6\nstashcache-daemon-0.6-1.osg33.el6\nstashcache-origin-server-0.6-1.osg33.el6\nxrootd-4.2.3-1.osg33.el6\nxrootd-client-4.2.3-1.osg33.el6\nxrootd-client-devel-4.2.3-1.osg33.el6\nxrootd-client-libs-4.2.3-1.osg33.el6\nxrootd-debuginfo-4.2.3-1.osg33.el6\nxrootd-devel-4.2.3-1.osg33.el6\nxrootd-doc-4.2.3-1.osg33.el6\nxrootd-fuse-4.2.3-1.osg33.el6\nxrootd-libs-4.2.3-1.osg33.el6\nxrootd-private-devel-4.2.3-1.osg33.el6\nxrootd-python-4.2.3-1.osg33.el6\nxrootd-selinux-4.2.3-1.osg33.el6\nxrootd-server-4.2.3-1.osg33.el6\nxrootd-server-devel-4.2.3-1.osg33.el6\nxrootd-server-libs-4.2.3-1.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\naxis-1.4-23.1.osg33.el7\naxis-javadoc-1.4-23.1.osg33.el7\naxis-manual-1.4-23.1.osg33.el7\nblahp-1.18.14.bosco-1.osg33.el7\nblahp-debuginfo-1.18.14.bosco-1.osg33.el7\ncondor-8.4.0-1.2.osg33.el7\ncondor-all-8.4.0-1.2.osg33.el7\ncondor-bosco-8.4.0-1.2.osg33.el7\ncondor-classads-8.4.0-1.2.osg33.el7\ncondor-classads-devel-8.4.0-1.2.osg33.el7\ncondor-debuginfo-8.4.0-1.2.osg33.el7\ncondor-kbdd-8.4.0-1.2.osg33.el7\ncondor-procd-8.4.0-1.2.osg33.el7\ncondor-python-8.4.0-1.2.osg33.el7\ncondor-test-8.4.0-1.2.osg33.el7\ncondor-vm-gahp-8.4.0-1.2.osg33.el7\nedg-mkgridmap-4.0.3-2.osg33.el7\ngip-1.3.11-7.osg33.el7\nglideinwms-3.2.11.2-4.osg33.el7\nglideinwms-common-tools-3.2.11.2-4.osg33.el7\nglideinwms-condor-common-config-3.2.11.2-4.osg33.el7\nglideinwms-factory-3.2.11.2-4.osg33.el7\nglideinwms-factory-condor-3.2.11.2-4.osg33.el7\nglideinwms-glidecondor-tools-3.2.11.2-4.osg33.el7\nglideinwms-libs-3.2.11.2-4.osg33.el7\nglideinwms-minimal-condor-3.2.11.2-4.osg33.el7\nglideinwms-usercollector-3.2.11.2-4.osg33.el7\nglideinwms-userschedd-3.2.11.2-4.osg33.el7\nglideinwms-vofrontend-3.2.11.2-4.osg33.el7\nglideinwms-vofrontend-standalone-3.2.11.2-4.osg33.el7\nhtcondor-ce-1.16-1.osg33.el7\nhtcondor-ce-client-1.16-1.osg33.el7\nhtcondor-ce-collector-1.16-1.osg33.el7\nhtcondor-ce-condor-1.16-1.osg33.el7\nhtcondor-ce-debuginfo-1.16-1.osg33.el7\nhtcondor-ce-lsf-1.16-1.osg33.el7\nhtcondor-ce-pbs-1.16-1.osg33.el7\nhtcondor-ce-sge-1.16-1.osg33.el7\nigtf-ca-certs-1.68-1.osg33.el7\njavamail-1.5.0-6.osg33.el7\njavamail-javadoc-1.5.0-6.osg33.el7\nlcmaps-plugins-mount-under-scratch-0.0.4-1.osg33.el7\nlcmaps-plugins-mount-under-scratch-debuginfo-0.0.4-1.osg33.el7\nosg-ca-certs-1.49-1.osg33.el7\nosg-configure-1.2.2-1.osg33.el7\nosg-configure-ce-1.2.2-1.osg33.el7\nosg-configure-cemon-1.2.2-1.osg33.el7\nosg-configure-condor-1.2.2-1.osg33.el7\nosg-configure-gateway-1.2.2-1.osg33.el7\nosg-configure-gip-1.2.2-1.osg33.el7\nosg-configure-gratia-1.2.2-1.osg33.el7\nosg-configure-infoservices-1.2.2-1.osg33.el7\nosg-configure-lsf-1.2.2-1.osg33.el7\nosg-configure-managedfork-1.2.2-1.osg33.el7\nosg-configure-misc-1.2.2-1.osg33.el7\nosg-configure-monalisa-1.2.2-1.osg33.el7\nosg-configure-network-1.2.2-1.osg33.el7\nosg-configure-pbs-1.2.2-1.osg33.el7\nosg-configure-rsv-1.2.2-1.osg33.el7\nosg-configure-sge-1.2.2-1.osg33.el7\nosg-configure-slurm-1.2.2-1.osg33.el7\nosg-configure-squid-1.2.2-1.osg33.el7\nosg-configure-tests-1.2.2-1.osg33.el7\nosg-test-1.4.30-1.osg33.el7\nosg-tested-internal-3.3-5.osg33.el7\nosg-version-3.3.2-1.osg33.el7\nrsv-3.10.4-1_clipped.osg33.el7\nrsv-consumers-3.10.4-1_clipped.osg33.el7\nrsv-core-3.10.4-1_clipped.osg33.el7\nrsv-metrics-3.10.4-1_clipped.osg33.el7\nstashcache-0.6-1.osg33.el7\nstashcache-cache-server-0.6-1.osg33.el7\nstashcache-daemon-0.6-1.osg33.el7\nstashcache-origin-server-0.6-1.osg33.el7\nwsdl4j-1.6.3-3.1.osg33.el7\nwsdl4j-javadoc-1.6.3-3.1.osg33.el7\nxrootd-4.2.3-1.osg33.el7\nxrootd-client-4.2.3-1.osg33.el7\nxrootd-client-devel-4.2.3-1.osg33.el7\nxrootd-client-libs-4.2.3-1.osg33.el7\nxrootd-debuginfo-4.2.3-1.osg33.el7\nxrootd-devel-4.2.3-1.osg33.el7\nxrootd-doc-4.2.3-1.osg33.el7\nxrootd-fuse-4.2.3-1.osg33.el7\nxrootd-libs-4.2.3-1.osg33.el7\nxrootd-private-devel-4.2.3-1.osg33.el7\nxrootd-python-4.2.3-1.osg33.el7\nxrootd-selinux-4.2.3-1.osg33.el7\nxrootd-server-4.2.3-1.osg33.el7\nxrootd-server-devel-4.2.3-1.osg33.el7\nxrootd-server-libs-4.2.3-1.osg33.el7\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\nEnterprise Linux 7\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\n\n\n\n\n\n\nEnterprise Linux 7", 
            "title": "OSG Release 3.3.2"
        }, 
        {
            "location": "/release/3.3/release-3-3-2/#osg-software-release-332", 
            "text": "Release Date : 2015-10-13", 
            "title": "OSG Software Release 3.3.2"
        }, 
        {
            "location": "/release/3.3/release-3-3-2/#summary-of-changes", 
            "text": "This release contains:   GlideinWMS 3.2.11.2  XRootD 4.2.3  HTCondor 8.4.0  StashCache 0.6  daemon refuses to start if host certificate is not present  use FQDN in stashcache-daemon    GUMS 1.5.1  Return groupName for pool account mappers  Bug fixes    HTCondor-CE 1.16  Updates for PBS variants  Add CERN host DN format to HTCondor-CE configuration defaults    GIP support multiple SLURM queues  osg-configure 1.2.2  Support IPv6 IP addresses in configuration files  Add sensible default values for Allowed VOs    RSV - srmcp-srm-probe (delays to account for NFS caching behavior)  Add lcmaps-plugins-mount-under-scratch package  Update to edg-mkgridmap 4.0.3, so it works on EL\u00a07  CA certificates based on  IGTF 1.68   These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-2/#known-issues", 
            "text": "HTCondor 8.4.0 has changed it's behavior in ways that cause the GlideinWMS frontend configuration to break. In order to correct this, the following setting needs to be added to the configuration file:  COLLECTOR_USES_SHARED_PORT = False    StashCache packages need to be manually configured   Manual configuration for origin server  Assuming that the origin server connects only to a redirector (not directly to cache server), minimal xrootd configuration is required. The configuration file, /etc/xrootd/xrootd-stashcache-origin-server.cfg, in this release is overkill. Here are recommended settings to use: xrd.port 1094\nall.role server\nall.manager stash-redirector.example.com 1213\nall.export / nostage\nxrootd.trace emsg login stall redirect\nofs.trace none\nxrd.trace conn\ncms.trace all\nsec.protocol  host\nsec.protbind  * none\nall.adminpath /var/run/xrootd\nall.pidpath /var/run/xrootd        Manual configuration for cache server   In contrast to the origin server configuration, one needs to declare  pss.origin  stash-redirector.example.com  instead of configuring the cmsd or manager (only the xrootd daemon is required on the cache server). More detailed configuration of cache server for StashCache is  here .     In both cases, administrator needs to set the path of custom configuration file for its xrootd/cmds instance in /etc/sysconfig/xrootd, For example, change the cmds default from:  CMSD_DEFAULT_OPTIONS= -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg -k fifo    to  CMSD_DEFAULT_OPTIONS= -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-stashcache-origin-server.marian -k fifo", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.3/release-3-3-2/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-2/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-2/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-2/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-2/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-2/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-2/#enterprise-linux-6", 
            "text": "bestman2-2.3.0-26.osg33.el6  blahp-1.18.14.bosco-1.osg33.el6  condor-8.4.0-1.2.osg33.el6  edg-mkgridmap-4.0.3-2.osg33.el6  gip-1.3.11-7.osg33.el6  glideinwms-3.2.11.2-4.osg33.el6  gridftp-hdfs-0.5.4-22.osg33.el6  gums-1.5.1-1.osg33.el6  htcondor-ce-1.16-1.osg33.el6  igtf-ca-certs-1.68-1.osg33.el6  jglobus-2.1.0-5.osg33.el6  lcmaps-plugins-mount-under-scratch-0.0.4-1.osg33.el6  osg-ca-certs-1.49-1.osg33.el6  osg-configure-1.2.2-1.osg33.el6  osg-test-1.4.30-1.osg33.el6  osg-tested-internal-3.3-5.osg33.el6  osg-version-3.3.2-1.osg33.el6  privilege-xacml-2.6.5-1.osg33.el6  rsv-3.10.4-1.osg33.el6  stashcache-0.6-1.osg33.el6  xrootd-4.2.3-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-2/#enterprise-linux-7", 
            "text": "axis-1.4-23.1.osg33.el7  blahp-1.18.14.bosco-1.osg33.el7  condor-8.4.0-1.2.osg33.el7  edg-mkgridmap-4.0.3-2.osg33.el7  gip-1.3.11-7.osg33.el7  glideinwms-3.2.11.2-4.osg33.el7  htcondor-ce-1.16-1.osg33.el7  igtf-ca-certs-1.68-1.osg33.el7  javamail-1.5.0-6.osg33.el7  lcmaps-plugins-mount-under-scratch-0.0.4-1.osg33.el7  osg-ca-certs-1.49-1.osg33.el7  osg-configure-1.2.2-1.osg33.el7  osg-test-1.4.30-1.osg33.el7  osg-tested-internal-3.3-5.osg33.el7  osg-version-3.3.2-1.osg33.el7  rsv-3.10.4-1_clipped.osg33.el7  stashcache-0.6-1.osg33.el7  wsdl4j-1.6.3-3.1.osg33.el7  xrootd-4.2.3-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-2/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  bestman2-client bestman2-client-libs bestman2-common-libs bestman2-server bestman2-server-dep-libs bestman2-server-libs bestman2-tester bestman2-tester-libs blahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp edg-mkgridmap gip glideinwms-common-tools glideinwms-condor-common-config glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone gridftp-hdfs gridftp-hdfs-debuginfo gums gums-client gums-service htcondor-ce htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-debuginfo htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge igtf-ca-certs jglobus lcmaps-plugins-mount-under-scratch lcmaps-plugins-mount-under-scratch-debuginfo osg-ca-certs osg-configure osg-configure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-test osg-tested-internal osg-version privilege-xacml python-argparse python-backports-ssl_match_hostname python-requests python-urllib3 rsv rsv-consumers rsv-core rsv-metrics stashcache-cache-server stashcache-daemon stashcache-origin-server xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-fuse xrootd-libs xrootd-private-devel xrootd-python xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-2/#enterprise-linux-6_1", 
            "text": "bestman2-2.3.0-26.osg33.el6\nbestman2-client-2.3.0-26.osg33.el6\nbestman2-client-libs-2.3.0-26.osg33.el6\nbestman2-common-libs-2.3.0-26.osg33.el6\nbestman2-server-2.3.0-26.osg33.el6\nbestman2-server-dep-libs-2.3.0-26.osg33.el6\nbestman2-server-libs-2.3.0-26.osg33.el6\nbestman2-tester-2.3.0-26.osg33.el6\nbestman2-tester-libs-2.3.0-26.osg33.el6\nblahp-1.18.14.bosco-1.osg33.el6\nblahp-debuginfo-1.18.14.bosco-1.osg33.el6\ncondor-8.4.0-1.2.osg33.el6\ncondor-all-8.4.0-1.2.osg33.el6\ncondor-bosco-8.4.0-1.2.osg33.el6\ncondor-classads-8.4.0-1.2.osg33.el6\ncondor-classads-devel-8.4.0-1.2.osg33.el6\ncondor-cream-gahp-8.4.0-1.2.osg33.el6\ncondor-debuginfo-8.4.0-1.2.osg33.el6\ncondor-kbdd-8.4.0-1.2.osg33.el6\ncondor-procd-8.4.0-1.2.osg33.el6\ncondor-python-8.4.0-1.2.osg33.el6\ncondor-std-universe-8.4.0-1.2.osg33.el6\ncondor-test-8.4.0-1.2.osg33.el6\ncondor-vm-gahp-8.4.0-1.2.osg33.el6\nedg-mkgridmap-4.0.3-2.osg33.el6\ngip-1.3.11-7.osg33.el6\nglideinwms-3.2.11.2-4.osg33.el6\nglideinwms-common-tools-3.2.11.2-4.osg33.el6\nglideinwms-condor-common-config-3.2.11.2-4.osg33.el6\nglideinwms-factory-3.2.11.2-4.osg33.el6\nglideinwms-factory-condor-3.2.11.2-4.osg33.el6\nglideinwms-glidecondor-tools-3.2.11.2-4.osg33.el6\nglideinwms-libs-3.2.11.2-4.osg33.el6\nglideinwms-minimal-condor-3.2.11.2-4.osg33.el6\nglideinwms-usercollector-3.2.11.2-4.osg33.el6\nglideinwms-userschedd-3.2.11.2-4.osg33.el6\nglideinwms-vofrontend-3.2.11.2-4.osg33.el6\nglideinwms-vofrontend-standalone-3.2.11.2-4.osg33.el6\ngridftp-hdfs-0.5.4-22.osg33.el6\ngridftp-hdfs-debuginfo-0.5.4-22.osg33.el6\ngums-1.5.1-1.osg33.el6\ngums-client-1.5.1-1.osg33.el6\ngums-service-1.5.1-1.osg33.el6\nhtcondor-ce-1.16-1.osg33.el6\nhtcondor-ce-client-1.16-1.osg33.el6\nhtcondor-ce-collector-1.16-1.osg33.el6\nhtcondor-ce-condor-1.16-1.osg33.el6\nhtcondor-ce-debuginfo-1.16-1.osg33.el6\nhtcondor-ce-lsf-1.16-1.osg33.el6\nhtcondor-ce-pbs-1.16-1.osg33.el6\nhtcondor-ce-sge-1.16-1.osg33.el6\nigtf-ca-certs-1.68-1.osg33.el6\njglobus-2.1.0-5.osg33.el6\nlcmaps-plugins-mount-under-scratch-0.0.4-1.osg33.el6\nlcmaps-plugins-mount-under-scratch-debuginfo-0.0.4-1.osg33.el6\nosg-ca-certs-1.49-1.osg33.el6\nosg-configure-1.2.2-1.osg33.el6\nosg-configure-ce-1.2.2-1.osg33.el6\nosg-configure-cemon-1.2.2-1.osg33.el6\nosg-configure-condor-1.2.2-1.osg33.el6\nosg-configure-gateway-1.2.2-1.osg33.el6\nosg-configure-gip-1.2.2-1.osg33.el6\nosg-configure-gratia-1.2.2-1.osg33.el6\nosg-configure-infoservices-1.2.2-1.osg33.el6\nosg-configure-lsf-1.2.2-1.osg33.el6\nosg-configure-managedfork-1.2.2-1.osg33.el6\nosg-configure-misc-1.2.2-1.osg33.el6\nosg-configure-monalisa-1.2.2-1.osg33.el6\nosg-configure-network-1.2.2-1.osg33.el6\nosg-configure-pbs-1.2.2-1.osg33.el6\nosg-configure-rsv-1.2.2-1.osg33.el6\nosg-configure-sge-1.2.2-1.osg33.el6\nosg-configure-slurm-1.2.2-1.osg33.el6\nosg-configure-squid-1.2.2-1.osg33.el6\nosg-configure-tests-1.2.2-1.osg33.el6\nosg-test-1.4.30-1.osg33.el6\nosg-tested-internal-3.3-5.osg33.el6\nosg-version-3.3.2-1.osg33.el6\nprivilege-xacml-2.6.5-1.osg33.el6\nrsv-3.10.4-1.osg33.el6\nrsv-consumers-3.10.4-1.osg33.el6\nrsv-core-3.10.4-1.osg33.el6\nrsv-metrics-3.10.4-1.osg33.el6\nstashcache-0.6-1.osg33.el6\nstashcache-cache-server-0.6-1.osg33.el6\nstashcache-daemon-0.6-1.osg33.el6\nstashcache-origin-server-0.6-1.osg33.el6\nxrootd-4.2.3-1.osg33.el6\nxrootd-client-4.2.3-1.osg33.el6\nxrootd-client-devel-4.2.3-1.osg33.el6\nxrootd-client-libs-4.2.3-1.osg33.el6\nxrootd-debuginfo-4.2.3-1.osg33.el6\nxrootd-devel-4.2.3-1.osg33.el6\nxrootd-doc-4.2.3-1.osg33.el6\nxrootd-fuse-4.2.3-1.osg33.el6\nxrootd-libs-4.2.3-1.osg33.el6\nxrootd-private-devel-4.2.3-1.osg33.el6\nxrootd-python-4.2.3-1.osg33.el6\nxrootd-selinux-4.2.3-1.osg33.el6\nxrootd-server-4.2.3-1.osg33.el6\nxrootd-server-devel-4.2.3-1.osg33.el6\nxrootd-server-libs-4.2.3-1.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-2/#enterprise-linux-7_1", 
            "text": "axis-1.4-23.1.osg33.el7\naxis-javadoc-1.4-23.1.osg33.el7\naxis-manual-1.4-23.1.osg33.el7\nblahp-1.18.14.bosco-1.osg33.el7\nblahp-debuginfo-1.18.14.bosco-1.osg33.el7\ncondor-8.4.0-1.2.osg33.el7\ncondor-all-8.4.0-1.2.osg33.el7\ncondor-bosco-8.4.0-1.2.osg33.el7\ncondor-classads-8.4.0-1.2.osg33.el7\ncondor-classads-devel-8.4.0-1.2.osg33.el7\ncondor-debuginfo-8.4.0-1.2.osg33.el7\ncondor-kbdd-8.4.0-1.2.osg33.el7\ncondor-procd-8.4.0-1.2.osg33.el7\ncondor-python-8.4.0-1.2.osg33.el7\ncondor-test-8.4.0-1.2.osg33.el7\ncondor-vm-gahp-8.4.0-1.2.osg33.el7\nedg-mkgridmap-4.0.3-2.osg33.el7\ngip-1.3.11-7.osg33.el7\nglideinwms-3.2.11.2-4.osg33.el7\nglideinwms-common-tools-3.2.11.2-4.osg33.el7\nglideinwms-condor-common-config-3.2.11.2-4.osg33.el7\nglideinwms-factory-3.2.11.2-4.osg33.el7\nglideinwms-factory-condor-3.2.11.2-4.osg33.el7\nglideinwms-glidecondor-tools-3.2.11.2-4.osg33.el7\nglideinwms-libs-3.2.11.2-4.osg33.el7\nglideinwms-minimal-condor-3.2.11.2-4.osg33.el7\nglideinwms-usercollector-3.2.11.2-4.osg33.el7\nglideinwms-userschedd-3.2.11.2-4.osg33.el7\nglideinwms-vofrontend-3.2.11.2-4.osg33.el7\nglideinwms-vofrontend-standalone-3.2.11.2-4.osg33.el7\nhtcondor-ce-1.16-1.osg33.el7\nhtcondor-ce-client-1.16-1.osg33.el7\nhtcondor-ce-collector-1.16-1.osg33.el7\nhtcondor-ce-condor-1.16-1.osg33.el7\nhtcondor-ce-debuginfo-1.16-1.osg33.el7\nhtcondor-ce-lsf-1.16-1.osg33.el7\nhtcondor-ce-pbs-1.16-1.osg33.el7\nhtcondor-ce-sge-1.16-1.osg33.el7\nigtf-ca-certs-1.68-1.osg33.el7\njavamail-1.5.0-6.osg33.el7\njavamail-javadoc-1.5.0-6.osg33.el7\nlcmaps-plugins-mount-under-scratch-0.0.4-1.osg33.el7\nlcmaps-plugins-mount-under-scratch-debuginfo-0.0.4-1.osg33.el7\nosg-ca-certs-1.49-1.osg33.el7\nosg-configure-1.2.2-1.osg33.el7\nosg-configure-ce-1.2.2-1.osg33.el7\nosg-configure-cemon-1.2.2-1.osg33.el7\nosg-configure-condor-1.2.2-1.osg33.el7\nosg-configure-gateway-1.2.2-1.osg33.el7\nosg-configure-gip-1.2.2-1.osg33.el7\nosg-configure-gratia-1.2.2-1.osg33.el7\nosg-configure-infoservices-1.2.2-1.osg33.el7\nosg-configure-lsf-1.2.2-1.osg33.el7\nosg-configure-managedfork-1.2.2-1.osg33.el7\nosg-configure-misc-1.2.2-1.osg33.el7\nosg-configure-monalisa-1.2.2-1.osg33.el7\nosg-configure-network-1.2.2-1.osg33.el7\nosg-configure-pbs-1.2.2-1.osg33.el7\nosg-configure-rsv-1.2.2-1.osg33.el7\nosg-configure-sge-1.2.2-1.osg33.el7\nosg-configure-slurm-1.2.2-1.osg33.el7\nosg-configure-squid-1.2.2-1.osg33.el7\nosg-configure-tests-1.2.2-1.osg33.el7\nosg-test-1.4.30-1.osg33.el7\nosg-tested-internal-3.3-5.osg33.el7\nosg-version-3.3.2-1.osg33.el7\nrsv-3.10.4-1_clipped.osg33.el7\nrsv-consumers-3.10.4-1_clipped.osg33.el7\nrsv-core-3.10.4-1_clipped.osg33.el7\nrsv-metrics-3.10.4-1_clipped.osg33.el7\nstashcache-0.6-1.osg33.el7\nstashcache-cache-server-0.6-1.osg33.el7\nstashcache-daemon-0.6-1.osg33.el7\nstashcache-origin-server-0.6-1.osg33.el7\nwsdl4j-1.6.3-3.1.osg33.el7\nwsdl4j-javadoc-1.6.3-3.1.osg33.el7\nxrootd-4.2.3-1.osg33.el7\nxrootd-client-4.2.3-1.osg33.el7\nxrootd-client-devel-4.2.3-1.osg33.el7\nxrootd-client-libs-4.2.3-1.osg33.el7\nxrootd-debuginfo-4.2.3-1.osg33.el7\nxrootd-devel-4.2.3-1.osg33.el7\nxrootd-doc-4.2.3-1.osg33.el7\nxrootd-fuse-4.2.3-1.osg33.el7\nxrootd-libs-4.2.3-1.osg33.el7\nxrootd-private-devel-4.2.3-1.osg33.el7\nxrootd-python-4.2.3-1.osg33.el7\nxrootd-selinux-4.2.3-1.osg33.el7\nxrootd-server-4.2.3-1.osg33.el7\nxrootd-server-devel-4.2.3-1.osg33.el7\nxrootd-server-libs-4.2.3-1.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-2/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-2/#enterprise-linux-6_2", 
            "text": "", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-2/#enterprise-linux-7_2", 
            "text": "", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-2/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-2/#enterprise-linux-6_3", 
            "text": "", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-2/#enterprise-linux-7_3", 
            "text": "", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-1/", 
            "text": "OSG Software Release 3.3.1\n\n\nRelease Date\n: 2015-09-08\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nEnterprise Linux 7: Worker node support only\n\n\nGUMS 1.5.0\n\n\nA security-related bug was found in the GUMS administrative interface. The risk of this vulnerability has been assessed by OSG Security Team as \u201cCritical\u201d. A new GUMS version 1.5.0 has been released and it is included in OSG Software version 3.2.27/3.3.1. Site administrators should update ASAP.\n\n\n\n\n\n\nHTCondor 8.3.8\n\n\nHTCondor CE 1.15\n\n\nAdd 'default_remote_cerequirements' attribute to the JOB_ROUTER_DEFAULTS\n\n\nVerify the first route in JOB_ROUTER_ENTRIES in the init script\n\n\nhtcondor-ce-collecotr now uses /etc/sysconfig/condor-ce-collector for additional configuration\n\n\n\n\n\n\ngridFTP-HDFS checksum verification improvements\n\n\nChecksum algorithm names are checked with a case insensitive comparison\n\n\nError codes are properly returned to the user\n\n\n\n\n\n\nCA certificates based on \nIGTF 1.67\n\n\nStashCache improvements\n\n\nAdded logging to the StashCache daemon\n\n\nAdvertise the StashCache daemon version to the master ClassAd\n\n\nImproved example configuration files to include \"pss.trace all\"\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nKnown Issues\n\n\n\n\n\n\nThe HTCondor shared port daemon can run out of file descriptors.\n\n\n\n\nThis is a problem with HTCondor 8.3.7 and 8.3.8.\n\n\nThe problem will be fixed in the HTCondor 8.4.0 release.\n\n\nThe glideInWMS factories (and possibly front-ends) using the following configuration model will cause the HTCondor shared port daemon to leak file descriptors. The relevant configuration fragment is similar to:\nUSE_SHARED_PORT\n \n=\n FALSE\n\nDAEMON_LIST\n \n=\n \n$(\nDAEMON_LIST\n)\n SHARED_PORT\n\nSCHEDD.USE_SHARED_PORT\n \n=\n TRUE\n\nSHADOW.USE_SHADED_PORT\n \n=\n TRUE\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis non-standard shared-port configuration is probably used to avoid dealing with shared-port collector trees.\n\n\n\n\nThe work-around is to start the HTCondor master with CONDOR_PRIVATE_SHARED_PORT_COOKIE set in the environment, as it will propagate down to the target daemons. CONDOR_PRIVATE_SHARED_PORT_COOKIE should contain a 32-byte random number in hexadecimal format (64 characters). \nNote:\n care must be taken to ensure that this value is private. Upgrade to HTCondor 8.4.0 as soon as possible and stop using this workaround. * Running osg-configure with the new version of HTCondor installed causes a deprecation warning to be emitted. The warning does not affect the services. * StashCache packages need to be manually configured\n\n\n\n\nManual configuration for origin server\n\n\n\n\nAssuming that the origin server connects only to a redirector (not directly to cache server), minimal xrootd configuration is required. The configuration file, /etc/xrootd/xrootd-stashcache-origin-server.cfg, in this release is overkill. Here are recommended settings to use:\nxrd.port 1094\nall.role server\nall.manager stash-redirector.example.com 1213\nall.export / nostage\nxrootd.trace emsg login stall redirect\nofs.trace none\nxrd.trace conn\ncms.trace all\nsec.protocol  host\nsec.protbind  * none\nall.adminpath /var/run/xrootd\nall.pidpath /var/run/xrootd\n\n\n\n\n\n\n\n\n\n\n\n\n\nManual configuration for cache server\n\n\n\n\nIn contrast to the origin server configuration, one needs to declare \npss.origin \nstash-redirector.example.com\n instead of configuring the cmsd or manager (only the xrootd daemon is required on the cache server). More detailed configuration of cache server for StashCache is \nhere\n.\n\n\n\n\n\n\n\n\nIn both cases, administrator needs to set the path of custom configuration file for its xrootd/cmds instance in /etc/sysconfig/xrootd, For example, change the cmds default from:\n\n\nCMSD_DEFAULT_OPTIONS=\n-l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg -k fifo\n\n\n\n\n\n\n\n\nto\n\n\nCMSD_DEFAULT_OPTIONS=\n-l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-stashcache-origin-server.marian -k fifo\n\n\n\n\n\n\n\n\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.13.bosco-4.osg33.el6\n\n\ncondor-8.3.8-1.1.osg33.el6\n\n\nemi-trustmanager-tomcat-3.0.0-12.osg33.el6\n\n\ngridftp-hdfs-0.5.4-20.osg33.el6\n\n\ngums-1.5.0-1.osg33.el6\n\n\nhadoop-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\n\n\nhtcondor-ce-1.15-2.osg33.el6\n\n\nigtf-ca-certs-1.67-1.osg33.el6\n\n\nosg-build-1.6.1-1.osg33.el6\n\n\nosg-ca-certs-1.48-1.osg33.el6\n\n\nosg-test-1.4.29-1.osg33.el6\n\n\nosg-tested-internal-3.3-3.osg33.el6\n\n\nosg-version-3.3.1-1.osg33.el6\n\n\nrsv-3.10.3-1.osg33.el6\n\n\nstashcache-0.4-2.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.13.bosco-4.osg33.el7\n\n\ncondor-8.3.8-1.1.osg33.el7\n\n\nemi-trustmanager-tomcat-3.0.0-12.osg33.el7\n\n\nhtcondor-ce-1.15-2.osg33.el7\n\n\nigtf-ca-certs-1.67-1.osg33.el7\n\n\nosg-build-1.6.1-1.osg33.el7\n\n\nosg-ca-certs-1.48-1.osg33.el7\n\n\nosg-test-1.4.29-1.osg33.el7\n\n\nosg-tested-internal-3.3-3.osg33.el7\n\n\nosg-version-3.3.1-1.osg33.el7\n\n\nrsv-3.10.3-1_clipped.osg33.el7\n\n\nstashcache-0.4-2.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp emi-trustmanager-tomcat gridftp-hdfs gridftp-hdfs-debuginfo gums gums-client gums-service hadoop hadoop-client hadoop-conf-pseudo hadoop-debuginfo hadoop-doc hadoop-hdfs hadoop-hdfs-datanode hadoop-hdfs-fuse hadoop-hdfs-fuse-selinux hadoop-hdfs-journalnode hadoop-hdfs-namenode hadoop-hdfs-secondarynamenode hadoop-hdfs-zkfc hadoop-httpfs hadoop-libhdfs hadoop-mapreduce hadoop-yarn htcondor-ce htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-debuginfo htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge igtf-ca-certs osg-build osg-ca-certs osg-test osg-tested-internal osg-version python-six rsv rsv-consumers rsv-core rsv-metrics stashcache-cache-server stashcache-daemon stashcache-origin-server\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp-1.18.13.bosco-4.osg33.el6\nblahp-debuginfo-1.18.13.bosco-4.osg33.el6\ncondor-8.3.8-1.1.osg33.el6\ncondor-all-8.3.8-1.1.osg33.el6\ncondor-bosco-8.3.8-1.1.osg33.el6\ncondor-classads-8.3.8-1.1.osg33.el6\ncondor-classads-devel-8.3.8-1.1.osg33.el6\ncondor-cream-gahp-8.3.8-1.1.osg33.el6\ncondor-debuginfo-8.3.8-1.1.osg33.el6\ncondor-kbdd-8.3.8-1.1.osg33.el6\ncondor-procd-8.3.8-1.1.osg33.el6\ncondor-python-8.3.8-1.1.osg33.el6\ncondor-std-universe-8.3.8-1.1.osg33.el6\ncondor-test-8.3.8-1.1.osg33.el6\ncondor-vm-gahp-8.3.8-1.1.osg33.el6\nemi-trustmanager-tomcat-3.0.0-12.osg33.el6\ngridftp-hdfs-0.5.4-20.osg33.el6\ngridftp-hdfs-debuginfo-0.5.4-20.osg33.el6\ngums-1.5.0-1.osg33.el6\ngums-client-1.5.0-1.osg33.el6\ngums-service-1.5.0-1.osg33.el6\nhadoop-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\nhadoop-client-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\nhadoop-conf-pseudo-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\nhadoop-debuginfo-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\nhadoop-doc-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\nhadoop-hdfs-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\nhadoop-hdfs-datanode-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\nhadoop-hdfs-fuse-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\nhadoop-hdfs-fuse-selinux-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\nhadoop-hdfs-journalnode-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\nhadoop-hdfs-namenode-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\nhadoop-hdfs-secondarynamenode-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\nhadoop-hdfs-zkfc-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\nhadoop-httpfs-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\nhadoop-libhdfs-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\nhadoop-mapreduce-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\nhadoop-yarn-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\nhtcondor-ce-1.15-2.osg33.el6\nhtcondor-ce-client-1.15-2.osg33.el6\nhtcondor-ce-collector-1.15-2.osg33.el6\nhtcondor-ce-condor-1.15-2.osg33.el6\nhtcondor-ce-debuginfo-1.15-2.osg33.el6\nhtcondor-ce-lsf-1.15-2.osg33.el6\nhtcondor-ce-pbs-1.15-2.osg33.el6\nhtcondor-ce-sge-1.15-2.osg33.el6\nigtf-ca-certs-1.67-1.osg33.el6\nosg-build-1.6.1-1.osg33.el6\nosg-ca-certs-1.48-1.osg33.el6\nosg-test-1.4.29-1.osg33.el6\nosg-tested-internal-3.3-3.osg33.el6\nosg-version-3.3.1-1.osg33.el6\nrsv-3.10.3-1.osg33.el6\nrsv-consumers-3.10.3-1.osg33.el6\nrsv-core-3.10.3-1.osg33.el6\nrsv-metrics-3.10.3-1.osg33.el6\nstashcache-0.4-2.osg33.el6\nstashcache-cache-server-0.4-2.osg33.el6\nstashcache-daemon-0.4-2.osg33.el6\nstashcache-origin-server-0.4-2.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp-1.18.13.bosco-4.osg33.el7\nblahp-debuginfo-1.18.13.bosco-4.osg33.el7\ncondor-8.3.8-1.1.osg33.el7\ncondor-all-8.3.8-1.1.osg33.el7\ncondor-bosco-8.3.8-1.1.osg33.el7\ncondor-classads-8.3.8-1.1.osg33.el7\ncondor-classads-devel-8.3.8-1.1.osg33.el7\ncondor-debuginfo-8.3.8-1.1.osg33.el7\ncondor-kbdd-8.3.8-1.1.osg33.el7\ncondor-procd-8.3.8-1.1.osg33.el7\ncondor-python-8.3.8-1.1.osg33.el7\ncondor-test-8.3.8-1.1.osg33.el7\ncondor-vm-gahp-8.3.8-1.1.osg33.el7\nemi-trustmanager-tomcat-3.0.0-12.osg33.el7\nhtcondor-ce-1.15-2.osg33.el7\nhtcondor-ce-client-1.15-2.osg33.el7\nhtcondor-ce-collector-1.15-2.osg33.el7\nhtcondor-ce-condor-1.15-2.osg33.el7\nhtcondor-ce-debuginfo-1.15-2.osg33.el7\nhtcondor-ce-lsf-1.15-2.osg33.el7\nhtcondor-ce-pbs-1.15-2.osg33.el7\nhtcondor-ce-sge-1.15-2.osg33.el7\nigtf-ca-certs-1.67-1.osg33.el7\nosg-build-1.6.1-1.osg33.el7\nosg-ca-certs-1.48-1.osg33.el7\nosg-test-1.4.29-1.osg33.el7\nosg-tested-internal-3.3-3.osg33.el7\nosg-version-3.3.1-1.osg33.el7\nrsv-3.10.3-1_clipped.osg33.el7\nrsv-consumers-3.10.3-1_clipped.osg33.el7\nrsv-core-3.10.3-1_clipped.osg33.el7\nrsv-metrics-3.10.3-1_clipped.osg33.el7\nstashcache-0.4-2.osg33.el7\nstashcache-cache-server-0.4-2.osg33.el7\nstashcache-daemon-0.4-2.osg33.el7\nstashcache-origin-server-0.4-2.osg33.el7\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 5\n\n\nEnterprise Linux 6\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 5\n\n\n\n\n\n\n\n\nEnterprise Linux 6", 
            "title": "OSG Release 3.3.1"
        }, 
        {
            "location": "/release/3.3/release-3-3-1/#osg-software-release-331", 
            "text": "Release Date : 2015-09-08", 
            "title": "OSG Software Release 3.3.1"
        }, 
        {
            "location": "/release/3.3/release-3-3-1/#summary-of-changes", 
            "text": "This release contains:   Enterprise Linux 7: Worker node support only  GUMS 1.5.0  A security-related bug was found in the GUMS administrative interface. The risk of this vulnerability has been assessed by OSG Security Team as \u201cCritical\u201d. A new GUMS version 1.5.0 has been released and it is included in OSG Software version 3.2.27/3.3.1. Site administrators should update ASAP.    HTCondor 8.3.8  HTCondor CE 1.15  Add 'default_remote_cerequirements' attribute to the JOB_ROUTER_DEFAULTS  Verify the first route in JOB_ROUTER_ENTRIES in the init script  htcondor-ce-collecotr now uses /etc/sysconfig/condor-ce-collector for additional configuration    gridFTP-HDFS checksum verification improvements  Checksum algorithm names are checked with a case insensitive comparison  Error codes are properly returned to the user    CA certificates based on  IGTF 1.67  StashCache improvements  Added logging to the StashCache daemon  Advertise the StashCache daemon version to the master ClassAd  Improved example configuration files to include \"pss.trace all\"     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-1/#known-issues", 
            "text": "The HTCondor shared port daemon can run out of file descriptors.   This is a problem with HTCondor 8.3.7 and 8.3.8.  The problem will be fixed in the HTCondor 8.4.0 release.  The glideInWMS factories (and possibly front-ends) using the following configuration model will cause the HTCondor shared port daemon to leak file descriptors. The relevant configuration fragment is similar to: USE_SHARED_PORT   =  FALSE DAEMON_LIST   =   $( DAEMON_LIST )  SHARED_PORT SCHEDD.USE_SHARED_PORT   =  TRUE SHADOW.USE_SHADED_PORT   =  TRUE      This non-standard shared-port configuration is probably used to avoid dealing with shared-port collector trees.   The work-around is to start the HTCondor master with CONDOR_PRIVATE_SHARED_PORT_COOKIE set in the environment, as it will propagate down to the target daemons. CONDOR_PRIVATE_SHARED_PORT_COOKIE should contain a 32-byte random number in hexadecimal format (64 characters).  Note:  care must be taken to ensure that this value is private. Upgrade to HTCondor 8.4.0 as soon as possible and stop using this workaround. * Running osg-configure with the new version of HTCondor installed causes a deprecation warning to be emitted. The warning does not affect the services. * StashCache packages need to be manually configured   Manual configuration for origin server   Assuming that the origin server connects only to a redirector (not directly to cache server), minimal xrootd configuration is required. The configuration file, /etc/xrootd/xrootd-stashcache-origin-server.cfg, in this release is overkill. Here are recommended settings to use: xrd.port 1094\nall.role server\nall.manager stash-redirector.example.com 1213\nall.export / nostage\nxrootd.trace emsg login stall redirect\nofs.trace none\nxrd.trace conn\ncms.trace all\nsec.protocol  host\nsec.protbind  * none\nall.adminpath /var/run/xrootd\nall.pidpath /var/run/xrootd      Manual configuration for cache server   In contrast to the origin server configuration, one needs to declare  pss.origin  stash-redirector.example.com  instead of configuring the cmsd or manager (only the xrootd daemon is required on the cache server). More detailed configuration of cache server for StashCache is  here .     In both cases, administrator needs to set the path of custom configuration file for its xrootd/cmds instance in /etc/sysconfig/xrootd, For example, change the cmds default from:  CMSD_DEFAULT_OPTIONS= -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg -k fifo    to  CMSD_DEFAULT_OPTIONS= -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-stashcache-origin-server.marian -k fifo", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.3/release-3-3-1/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-1/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-1/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-1/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-1/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-1/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-1/#enterprise-linux-6", 
            "text": "blahp-1.18.13.bosco-4.osg33.el6  condor-8.3.8-1.1.osg33.el6  emi-trustmanager-tomcat-3.0.0-12.osg33.el6  gridftp-hdfs-0.5.4-20.osg33.el6  gums-1.5.0-1.osg33.el6  hadoop-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6  htcondor-ce-1.15-2.osg33.el6  igtf-ca-certs-1.67-1.osg33.el6  osg-build-1.6.1-1.osg33.el6  osg-ca-certs-1.48-1.osg33.el6  osg-test-1.4.29-1.osg33.el6  osg-tested-internal-3.3-3.osg33.el6  osg-version-3.3.1-1.osg33.el6  rsv-3.10.3-1.osg33.el6  stashcache-0.4-2.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-1/#enterprise-linux-7", 
            "text": "blahp-1.18.13.bosco-4.osg33.el7  condor-8.3.8-1.1.osg33.el7  emi-trustmanager-tomcat-3.0.0-12.osg33.el7  htcondor-ce-1.15-2.osg33.el7  igtf-ca-certs-1.67-1.osg33.el7  osg-build-1.6.1-1.osg33.el7  osg-ca-certs-1.48-1.osg33.el7  osg-test-1.4.29-1.osg33.el7  osg-tested-internal-3.3-3.osg33.el7  osg-version-3.3.1-1.osg33.el7  rsv-3.10.3-1_clipped.osg33.el7  stashcache-0.4-2.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-1/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp blahp-debuginfo condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp emi-trustmanager-tomcat gridftp-hdfs gridftp-hdfs-debuginfo gums gums-client gums-service hadoop hadoop-client hadoop-conf-pseudo hadoop-debuginfo hadoop-doc hadoop-hdfs hadoop-hdfs-datanode hadoop-hdfs-fuse hadoop-hdfs-fuse-selinux hadoop-hdfs-journalnode hadoop-hdfs-namenode hadoop-hdfs-secondarynamenode hadoop-hdfs-zkfc hadoop-httpfs hadoop-libhdfs hadoop-mapreduce hadoop-yarn htcondor-ce htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-debuginfo htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge igtf-ca-certs osg-build osg-ca-certs osg-test osg-tested-internal osg-version python-six rsv rsv-consumers rsv-core rsv-metrics stashcache-cache-server stashcache-daemon stashcache-origin-server  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-1/#enterprise-linux-6_1", 
            "text": "blahp-1.18.13.bosco-4.osg33.el6\nblahp-debuginfo-1.18.13.bosco-4.osg33.el6\ncondor-8.3.8-1.1.osg33.el6\ncondor-all-8.3.8-1.1.osg33.el6\ncondor-bosco-8.3.8-1.1.osg33.el6\ncondor-classads-8.3.8-1.1.osg33.el6\ncondor-classads-devel-8.3.8-1.1.osg33.el6\ncondor-cream-gahp-8.3.8-1.1.osg33.el6\ncondor-debuginfo-8.3.8-1.1.osg33.el6\ncondor-kbdd-8.3.8-1.1.osg33.el6\ncondor-procd-8.3.8-1.1.osg33.el6\ncondor-python-8.3.8-1.1.osg33.el6\ncondor-std-universe-8.3.8-1.1.osg33.el6\ncondor-test-8.3.8-1.1.osg33.el6\ncondor-vm-gahp-8.3.8-1.1.osg33.el6\nemi-trustmanager-tomcat-3.0.0-12.osg33.el6\ngridftp-hdfs-0.5.4-20.osg33.el6\ngridftp-hdfs-debuginfo-0.5.4-20.osg33.el6\ngums-1.5.0-1.osg33.el6\ngums-client-1.5.0-1.osg33.el6\ngums-service-1.5.0-1.osg33.el6\nhadoop-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\nhadoop-client-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\nhadoop-conf-pseudo-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\nhadoop-debuginfo-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\nhadoop-doc-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\nhadoop-hdfs-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\nhadoop-hdfs-datanode-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\nhadoop-hdfs-fuse-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\nhadoop-hdfs-fuse-selinux-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\nhadoop-hdfs-journalnode-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\nhadoop-hdfs-namenode-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\nhadoop-hdfs-secondarynamenode-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\nhadoop-hdfs-zkfc-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\nhadoop-httpfs-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\nhadoop-libhdfs-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\nhadoop-mapreduce-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\nhadoop-yarn-2.0.0+545-1.cdh4.1.1.p0.21.osg33.el6\nhtcondor-ce-1.15-2.osg33.el6\nhtcondor-ce-client-1.15-2.osg33.el6\nhtcondor-ce-collector-1.15-2.osg33.el6\nhtcondor-ce-condor-1.15-2.osg33.el6\nhtcondor-ce-debuginfo-1.15-2.osg33.el6\nhtcondor-ce-lsf-1.15-2.osg33.el6\nhtcondor-ce-pbs-1.15-2.osg33.el6\nhtcondor-ce-sge-1.15-2.osg33.el6\nigtf-ca-certs-1.67-1.osg33.el6\nosg-build-1.6.1-1.osg33.el6\nosg-ca-certs-1.48-1.osg33.el6\nosg-test-1.4.29-1.osg33.el6\nosg-tested-internal-3.3-3.osg33.el6\nosg-version-3.3.1-1.osg33.el6\nrsv-3.10.3-1.osg33.el6\nrsv-consumers-3.10.3-1.osg33.el6\nrsv-core-3.10.3-1.osg33.el6\nrsv-metrics-3.10.3-1.osg33.el6\nstashcache-0.4-2.osg33.el6\nstashcache-cache-server-0.4-2.osg33.el6\nstashcache-daemon-0.4-2.osg33.el6\nstashcache-origin-server-0.4-2.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-1/#enterprise-linux-7_1", 
            "text": "blahp-1.18.13.bosco-4.osg33.el7\nblahp-debuginfo-1.18.13.bosco-4.osg33.el7\ncondor-8.3.8-1.1.osg33.el7\ncondor-all-8.3.8-1.1.osg33.el7\ncondor-bosco-8.3.8-1.1.osg33.el7\ncondor-classads-8.3.8-1.1.osg33.el7\ncondor-classads-devel-8.3.8-1.1.osg33.el7\ncondor-debuginfo-8.3.8-1.1.osg33.el7\ncondor-kbdd-8.3.8-1.1.osg33.el7\ncondor-procd-8.3.8-1.1.osg33.el7\ncondor-python-8.3.8-1.1.osg33.el7\ncondor-test-8.3.8-1.1.osg33.el7\ncondor-vm-gahp-8.3.8-1.1.osg33.el7\nemi-trustmanager-tomcat-3.0.0-12.osg33.el7\nhtcondor-ce-1.15-2.osg33.el7\nhtcondor-ce-client-1.15-2.osg33.el7\nhtcondor-ce-collector-1.15-2.osg33.el7\nhtcondor-ce-condor-1.15-2.osg33.el7\nhtcondor-ce-debuginfo-1.15-2.osg33.el7\nhtcondor-ce-lsf-1.15-2.osg33.el7\nhtcondor-ce-pbs-1.15-2.osg33.el7\nhtcondor-ce-sge-1.15-2.osg33.el7\nigtf-ca-certs-1.67-1.osg33.el7\nosg-build-1.6.1-1.osg33.el7\nosg-ca-certs-1.48-1.osg33.el7\nosg-test-1.4.29-1.osg33.el7\nosg-tested-internal-3.3-3.osg33.el7\nosg-version-3.3.1-1.osg33.el7\nrsv-3.10.3-1_clipped.osg33.el7\nrsv-consumers-3.10.3-1_clipped.osg33.el7\nrsv-core-3.10.3-1_clipped.osg33.el7\nrsv-metrics-3.10.3-1_clipped.osg33.el7\nstashcache-0.4-2.osg33.el7\nstashcache-cache-server-0.4-2.osg33.el7\nstashcache-daemon-0.4-2.osg33.el7\nstashcache-origin-server-0.4-2.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-1/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-1/#enterprise-linux-5", 
            "text": "", 
            "title": "Enterprise Linux 5"
        }, 
        {
            "location": "/release/3.3/release-3-3-1/#enterprise-linux-6_2", 
            "text": "", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-1/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-1/#enterprise-linux-5_1", 
            "text": "", 
            "title": "Enterprise Linux 5"
        }, 
        {
            "location": "/release/3.3/release-3-3-1/#enterprise-linux-6_3", 
            "text": "", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-0/", 
            "text": "OSG Software Release 3.3.0\n\n\nRelease Date\n: 2015-08-11\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nEnterprise Linux 7: Worker node support only.\n\n\nXRootD 4.2.2\n\n\n2 patches to HTCondor to better support HTCondor CE\n\n\nImprove responsiveness of HTCondor CE by using light-weight polling\n\n\nPrevent causing jobs to error out upon restart of JobRouter\n\n\n\n\n\n\nimproved logging of failures in lcmaps-plugins-verify-proxy\n\n\nbetter credential handling and fix for crashes in lcamps-plugins-scas-client\n\n\nCA Certificates are no longer repackaged for compatibility with old software\n\n\nVO Package v61\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found in the \nRelease3\n area of the TWiki.\n\n\nKnown Issues\n\n\n\n\nHTCondor CE does not work with HTCondor 8.3.6 (in upcoming) on EL 5 platforms. So, we are holding HTCondor at version 8.3.4 (in upcoming) for EL 5 platforms.\n\n\n\n\nStashCache packages need to be manually configured\n\n\n\n\nManual configuration for origin server\n\n\nAssuming that the origin server connects only to a redirector (not directly to cache server), minimal xrootd configuration is required. The configuration file, /etc/xrootd/xrootd-stashcache-origin-server.cfg, in this release is overkill. Here are recommended settings to use:\nxrd.port 1094\nall.role server\nall.manager stash-redirector.example.com 1213\nall.export / nostage\nxrootd.trace emsg login stall redirect\nofs.trace none\nxrd.trace conn\ncms.trace all\nsec.protocol  host\nsec.protbind  * none\nall.adminpath /var/run/xrootd\nall.pidpath /var/run/xrootd\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nManual configuration for cache server\n\n\n\n\nIn contrast to the origin server configuration, one needs to declare \npss.origin \nstash-redirector.example.com\n instead of configuring the cmsd or manager (only the xrootd daemon is required on the cache server). More detailed configuration of cache server for StashCache is \nhere\n.\n\n\n\n\n\n\n\n\nIn both cases, administrator needs to set the path of custom configuration file for its xrootd/cmds instance in /etc/sysconfig/xrootd, For example, change the cmds default from:\n\n\nCMSD_DEFAULT_OPTIONS=\n-l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg -k fifo\n\n\n\n\n\n\n\n\nto\n\n\nCMSD_DEFAULT_OPTIONS=\n-l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-stashcache-origin-server.marian -k fifo\n\n\n\n\n\n\n\n\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n Please be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\n Watch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nbestman2-2.3.0-25.osg33.el6\n\n\nbigtop-jsvc-0.3.0-1.1.osg33.el6\n\n\nbigtop-utils-0.4+300-1.cdh4.0.1.p0.3.osg33.el6\n\n\nblahp-1.18.13.bosco-3.osg33.el6\n\n\nbwctl-1.4-7.osg33.el6\n\n\ncctools-4.4.0-1.osg33.el6\n\n\ncilogon-openid-ca-cert-1.1-2.osg33.el6\n\n\ncilogon-osg-ca-cert-1.0-1.osg33.el6\n\n\ncog-jglobus-axis-1.8.0-7.osg33.el6\n\n\ncondor-8.3.6-1.4.osg33.el6\n\n\ncondor-cron-1.0.9-4.osg33.el6\n\n\ncvmfs-2.1.20-1.osg33.el6\n\n\ncvmfs-config-osg-1.1-7.osg33.el6\n\n\nedg-mkgridmap-4.0.2-1.osg33.el6\n\n\nemi-trustmanager-3.0.3-8.osg33.el6\n\n\nemi-trustmanager-axis-1.0.1-1.4.osg33.el6\n\n\nemi-trustmanager-tomcat-3.0.0-10.osg33.el6\n\n\nfrontier-squid-2.7.STABLE9-24.1.osg33.el6\n\n\ngfal2-plugin-xrootd-0.3.pre1-2.1.osg33.el6\n\n\ngip-1.3.11-6.osg33.el6\n\n\nglexec-0.9.11-1.1.osg33.el6\n\n\nglexec-wrapper-scripts-0.0.7-1.1.osg33.el6\n\n\nglideinwms-3.2.10-1.1.osg33.el6\n\n\nglite-build-common-cpp-3.3.0.2-1.osg33.el6\n\n\nglite-ce-cream-client-api-c-1.14.0-4.10.osg33.el6\n\n\nglite-ce-cream-utils-1.2.0-4.3.osg33.el6\n\n\nglite-lbjp-common-gsoap-plugin-3.1.2-2.2.osg33.el6\n\n\nglite-lbjp-common-gss-3.1.3-2.2.osg33.el6\n\n\nglobus-authz-3.10-1.osg33.el6\n\n\nglobus-authz-callout-error-3.5-1.osg33.el6\n\n\nglobus-callout-3.13-1.osg33.el6\n\n\nglobus-common-15.27-1.osg33.el6\n\n\nglobus-ftp-client-8.19-1.2.osg33.el6\n\n\nglobus-ftp-control-6.6-1.1.osg33.el6\n\n\nglobus-gass-cache-9.5-1.osg33.el6\n\n\nglobus-gass-cache-program-6.5-1.osg33.el6\n\n\nglobus-gass-copy-9.13-1.osg33.el6\n\n\nglobus-gass-server-ez-5.7-1.osg33.el6\n\n\nglobus-gass-transfer-8.8-1.osg33.el6\n\n\nglobus-gatekeeper-10.9-1.2.osg33.el6\n\n\nglobus-gfork-4.7-1.osg33.el6\n\n\nglobus-gram-audit-4.4-1.osg33.el6\n\n\nglobus-gram-client-13.12-1.osg33.el6\n\n\nglobus-gram-client-tools-11.7-1.osg33.el6\n\n\nglobus-gram-job-manager-14.25-1.2.osg33.el6\n\n\nglobus-gram-job-manager-callout-error-3.5-1.osg33.el6\n\n\nglobus-gram-job-manager-condor-2.5-1.1.osg33.el6\n\n\nglobus-gram-job-manager-fork-2.4-1.1.osg33.el6\n\n\nglobus-gram-job-manager-lsf-2.6-1.2.osg33.el6\n\n\nglobus-gram-job-manager-managedfork-0.2-1.osg33.el6\n\n\nglobus-gram-job-manager-pbs-2.4-2.1.osg33.el6\n\n\nglobus-gram-job-manager-scripts-6.7-1.osg33.el6\n\n\nglobus-gram-job-manager-sge-2.5-1.1.osg33.el6\n\n\nglobus-gram-protocol-12.12-2.osg33.el6\n\n\nglobus-gridftp-server-7.20-1.1.osg33.el6\n\n\nglobus-gridftp-server-control-3.6-1.osg33.el6\n\n\nglobus-gridmap-callout-error-2.4-1.osg33.el6\n\n\nglobus-gsi-callback-5.7-1.osg33.el6\n\n\nglobus-gsi-cert-utils-9.10-1.osg33.el6\n\n\nglobus-gsi-credential-7.7-1.osg33.el6\n\n\nglobus-gsi-openssl-error-3.5-1.osg33.el6\n\n\nglobus-gsi-proxy-core-7.7-1.osg33.el6\n\n\nglobus-gsi-proxy-ssl-5.7-1.osg33.el6\n\n\nglobus-gsi-sysconfig-6.8-1.osg33.el6\n\n\nglobus-gssapi-error-5.4-1.osg33.el6\n\n\nglobus-gssapi-gsi-11.18-1.osg33.el6\n\n\nglobus-gss-assist-10.13-1.osg33.el6\n\n\nglobus-io-11.4-1.osg33.el6\n\n\nglobus-openssl-module-4.6-1.osg33.el6\n\n\nglobus-proxy-utils-6.9-1.osg33.el6\n\n\nglobus-rsl-10.9-1.osg33.el6\n\n\nglobus-scheduler-event-generator-5.10-2.1.osg33.el6\n\n\nglobus-simple-ca-4.18-1.osg33.el6\n\n\nglobus-usage-4.4-1.osg33.el6\n\n\nglobus-xio-5.7-1.1.osg33.el6\n\n\nglobus-xio-gsi-driver-3.7-1.osg33.el6\n\n\nglobus-xioperf-4.4-1.osg33.el6\n\n\nglobus-xio-pipe-driver-3.7-1.osg33.el6\n\n\nglobus-xio-popen-driver-3.5-1.osg33.el6\n\n\nglobus-xio-udt-driver-1.16-1.osg33.el6\n\n\ngratia-1.16.2-1.osg33.el6\n\n\ngratia-probe-1.14.2-6.osg33.el6\n\n\ngratia-reporting-email-1.15.1-1.osg33.el6\n\n\ngridftp-hdfs-0.5.4-19.osg33.el6\n\n\ngsi-openssh-5.7-1.1.osg33.el6\n\n\ngums-1.4.4-3.osg33.el6\n\n\nhadoop-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\n\n\nhtcondor-ce-1.14-4.osg33.el6\n\n\nI2util-1.1-2.osg33.el6\n\n\nigtf-ca-certs-1.65-1.osg33.el6\n\n\njavascriptrrd-1.1.1-1.osg33.el6\n\n\njetty-8.1.4.v20120524-2.osg33.el6\n\n\njglobus-2.0.6-4.osg33.el6\n\n\njoda-time-1.5.2-7.2.tzdata2008d.osg33.el6\n\n\nkoji-1.6.0-10.osg33.el6\n\n\nlcas-lcmaps-gt4-interface-0.2.6-1.1.osg33.el6\n\n\nlcmaps-1.6.6-1.1.osg33.el6\n\n\nlcmaps-plugins-basic-1.7.0-2.osg33.el6\n\n\nlcmaps-plugins-glexec-tracking-0.1.6-1.osg33.el6\n\n\nlcmaps-plugins-gums-client-0.0.2-4.osg33.el6\n\n\nlcmaps-plugins-scas-client-0.5.5-1.osg33.el6\n\n\nlcmaps-plugins-verify-proxy-1.5.7-1.osg33.el6\n\n\nllrun-0.1.3-1.3.osg33.el6\n\n\nmash-0.5.22-3.osg33.el6\n\n\nmkgltempdir-0.0.5-1.1.osg33.el6\n\n\nmyproxy-6.1.12-1.osg33.el6\n\n\nndt-3.6.5-3.osg33.el6\n\n\nnetlogger-4.2.0-9.osg33.el6\n\n\nnuttcp-6.1.2-1.osg33.el6\n\n\nosg-build-1.6.0-2.osg33.el6\n\n\nosg-ca-certs-1.47-1.osg33.el6\n\n\nosg-ca-certs-updater-1.0-1.osg33.el6\n\n\nosg-ca-scripts-1.1.5-2.osg33.el6\n\n\nosg-ce-3.3-3.osg33.el6\n\n\nosg-cert-scripts-2.7.2-2.osg33.el6\n\n\nosg-cleanup-1.7.2-1.osg33.el6\n\n\nosg-configure-1.1.1-1.osg33.el6\n\n\nosg-control-1.0.1-1.osg33.el6\n\n\nosg-gridftp-3.3-2.osg33.el6\n\n\nosg-gridftp-hdfs-3.3-2.osg33.el6\n\n\nosg-gridftp-xrootd-3.3-2.osg33.el6\n\n\nosg-gums-3.3-2.osg33.el6\n\n\nosg-info-services-1.0.2-1.osg33.el6\n\n\nosg-java7-compat-1.0-1.osg33.el6\n\n\nosg-oasis-5-2.osg33.el6\n\n\nosg-pki-tools-1.2.12-1.osg33.el6\n\n\nosg-release-3.3-2.osg33.el6\n\n\nosg-release-itb-3.3-2.osg33.el6\n\n\nosg-se-bestman-3.3-2.osg33.el6\n\n\nosg-se-bestman-xrootd-3.3-2.osg33.el6\n\n\nosg-se-hadoop-3.3-2.osg33.el6\n\n\nosg-system-profiler-1.2.0-1.osg33.el6\n\n\nosg-test-1.4.26-1.osg33.el6\n\n\nosg-tested-internal-3.3-1.osg33.el6\n\n\nosg-version-3.3.0-1.osg33.el6\n\n\nosg-vo-map-0.0.1-1.osg33.el6\n\n\nosg-voms-3.3-1.osg33.el6\n\n\nosg-webapp-common-1-2.osg33.el6\n\n\nosg-wn-client-3.3-5.osg33.el6\n\n\nowamp-3.2rc4-2.osg33.el6\n\n\npegasus-4.3.1-1.3.osg33.el6\n\n\nprivilege-xacml-2.6.4-1.osg33.el6\n\n\nrsv-3.10.2-1.osg33.el6\n\n\nrsv-perfsonar-1.0.19-1.osg33.el6\n\n\nrsv-vo-gwms-1.0.1-1.osg33.el6\n\n\nstashcache-0.3-4.osg33.el6\n\n\nstashcache-daemon-0.2-1.osg33.el6\n\n\nuberftp-2.8-2.1.osg33.el6\n\n\nvo-client-61-1.osg33.el6\n\n\nvoms-2.0.12-3.osg33.el6\n\n\nvoms-admin-client-2.0.17-1.1.osg33.el6\n\n\nvoms-admin-server-2.7.0-1.14.osg33.el6\n\n\nvoms-api-java-2.0.8-1.6.osg33.el6\n\n\nvoms-mysql-plugin-3.1.6-1.1.osg33.el6\n\n\nweb100_userland-1.7-6.osg33.el6\n\n\nxacml-1.5.0-1.osg33.el6\n\n\nxrootd-4.2.2-1.osg33.el6\n\n\nxrootd-dsi-3.0.4-16.osg33.el6\n\n\nxrootd-hdfs-1.8.4-4.osg33.el6\n\n\nxrootd-lcmaps-0.0.7-11.osg33.el6\n\n\nxrootd-status-probe-0.0.3-11.osg33.el6\n\n\nxrootd-voms-plugin-0.2.0-1.6.osg33.el6\n\n\nzookeeper-3.4.3+15-1.cdh4.0.1.p0.3.osg33.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nbigtop-jsvc-0.3.0-1.1.osg33.el7\n\n\nbigtop-utils-0.4+300-1.cdh4.0.1.p0.3.osg33.el7\n\n\nblahp-1.18.13.bosco-3.osg33.el7\n\n\nbwctl-1.4-7.osg33.el7\n\n\ncctools-4.4.0-1.osg33.el7\n\n\ncilogon-openid-ca-cert-1.1-2.osg33.el7\n\n\ncilogon-osg-ca-cert-1.0-1.osg33.el7\n\n\ncog-jglobus-axis-1.8.0-8.osg33.el7\n\n\ncondor-8.3.6-1.4.osg33.el7\n\n\ncondor-cron-1.0.9-4.osg33.el7\n\n\ncvmfs-2.1.20-1.osg33.el7\n\n\ncvmfs-config-osg-1.1-7.osg33.el7\n\n\nedg-mkgridmap-4.0.2-1.osg33.el7\n\n\nemi-trustmanager-3.0.3-8.osg33.el7\n\n\nemi-trustmanager-axis-1.0.1-1.4.osg33.el7\n\n\nfrontier-squid-2.7.STABLE9-24.1.osg33.el7\n\n\ngfal2-plugin-xrootd-0.3.pre1-2.1.osg33.el7\n\n\ngip-1.3.11-6.osg33.el7\n\n\nglexec-0.9.11-1.1.osg33.el7\n\n\nglexec-wrapper-scripts-0.0.7-1.1.osg33.el7\n\n\nglideinwms-3.2.10-1.1.osg33.el7\n\n\nglite-build-common-cpp-3.3.0.2-1.osg33.el7\n\n\nglite-lbjp-common-gsoap-plugin-3.1.2-2.2.osg33.el7\n\n\nglite-lbjp-common-gss-3.1.3-2.2.osg33.el7\n\n\nglobus-authz-3.10-1.osg33.el7\n\n\nglobus-authz-callout-error-3.5-1.osg33.el7\n\n\nglobus-callout-3.13-1.osg33.el7\n\n\nglobus-common-15.27-1.osg33.el7\n\n\nglobus-ftp-client-8.19-1.2.osg33.el7\n\n\nglobus-ftp-control-6.6-1.1.osg33.el7\n\n\nglobus-gass-cache-9.5-1.osg33.el7\n\n\nglobus-gass-cache-program-6.5-1.osg33.el7\n\n\nglobus-gass-copy-9.13-1.osg33.el7\n\n\nglobus-gass-server-ez-5.7-1.osg33.el7\n\n\nglobus-gass-transfer-8.8-1.osg33.el7\n\n\nglobus-gatekeeper-10.9-1.2.osg33.el7\n\n\nglobus-gfork-4.7-1.osg33.el7\n\n\nglobus-gram-audit-4.4-1.osg33.el7\n\n\nglobus-gram-client-13.12-1.osg33.el7\n\n\nglobus-gram-client-tools-11.7-1.osg33.el7\n\n\nglobus-gram-job-manager-14.25-1.2.osg33.el7\n\n\nglobus-gram-job-manager-callout-error-3.5-1.osg33.el7\n\n\nglobus-gram-job-manager-condor-2.5-1.1.osg33.el7\n\n\nglobus-gram-job-manager-fork-2.4-1.1.osg33.el7\n\n\nglobus-gram-job-manager-lsf-2.6-1.2.osg33.el7\n\n\nglobus-gram-job-manager-managedfork-0.2-1.osg33.el7\n\n\nglobus-gram-job-manager-pbs-2.4-2.1.osg33.el7\n\n\nglobus-gram-job-manager-scripts-6.7-1.osg33.el7\n\n\nglobus-gram-job-manager-sge-2.5-1.1.osg33.el7\n\n\nglobus-gram-protocol-12.12-2.osg33.el7\n\n\nglobus-gridftp-server-7.20-1.1.osg33.el7\n\n\nglobus-gridftp-server-control-3.6-1.osg33.el7\n\n\nglobus-gridmap-callout-error-2.4-1.osg33.el7\n\n\nglobus-gsi-callback-5.7-1.osg33.el7\n\n\nglobus-gsi-cert-utils-9.10-1.osg33.el7\n\n\nglobus-gsi-credential-7.7-1.osg33.el7\n\n\nglobus-gsi-openssl-error-3.5-1.osg33.el7\n\n\nglobus-gsi-proxy-core-7.7-1.osg33.el7\n\n\nglobus-gsi-proxy-ssl-5.7-1.osg33.el7\n\n\nglobus-gsi-sysconfig-6.8-1.osg33.el7\n\n\nglobus-gssapi-error-5.4-1.osg33.el7\n\n\nglobus-gssapi-gsi-11.18-1.osg33.el7\n\n\nglobus-gss-assist-10.13-1.osg33.el7\n\n\nglobus-io-11.4-1.osg33.el7\n\n\nglobus-openssl-module-4.6-1.osg33.el7\n\n\nglobus-proxy-utils-6.9-1.osg33.el7\n\n\nglobus-rsl-10.9-1.osg33.el7\n\n\nglobus-scheduler-event-generator-5.10-2.1.osg33.el7\n\n\nglobus-simple-ca-4.18-1.osg33.el7\n\n\nglobus-usage-4.4-1.osg33.el7\n\n\nglobus-xio-5.7-1.1.osg33.el7\n\n\nglobus-xio-gsi-driver-3.7-1.osg33.el7\n\n\nglobus-xioperf-4.4-1.osg33.el7\n\n\nglobus-xio-pipe-driver-3.7-1.osg33.el7\n\n\nglobus-xio-popen-driver-3.5-1.osg33.el7\n\n\nglobus-xio-udt-driver-1.16-1.osg33.el7\n\n\ngratia-1.16.2-1.osg33.el7\n\n\ngratia-probe-1.14.2-6.osg33.el7\n\n\ngratia-reporting-email-1.15.1-1.osg33.el7\n\n\ngsi-openssh-5.7-1.1.osg33.el7\n\n\nhtcondor-ce-1.14-4.osg33.el7\n\n\nI2util-1.1-2.osg33.el7\n\n\nigtf-ca-certs-1.65-1.osg33.el7\n\n\njavascriptrrd-1.1.1-1.osg33.el7\n\n\njetty-8.1.4.v20120524-2.osg33.el7\n\n\njoda-time-1.5.2-7.2.tzdata2008d.osg33.el7\n\n\nkoji-1.6.0-10.osg33.el7\n\n\nlcas-lcmaps-gt4-interface-0.2.6-1.1.osg33.el7\n\n\nlcmaps-1.6.6-1.1.osg33.el7\n\n\nlcmaps-plugins-basic-1.7.0-2.osg33.el7\n\n\nlcmaps-plugins-glexec-tracking-0.1.6-1.osg33.el7\n\n\nlcmaps-plugins-gums-client-0.0.2-4.osg33.el7\n\n\nlcmaps-plugins-scas-client-0.5.5-1.osg33.el7\n\n\nlcmaps-plugins-verify-proxy-1.5.7-1.osg33.el7\n\n\nllrun-0.1.3-1.3.osg33.el7\n\n\nmash-0.5.22-3.osg33.el7\n\n\nmkgltempdir-0.0.5-1.1.osg33.el7\n\n\nmyproxy-6.1.12-1.osg33.el7\n\n\nndt-3.6.5-3.osg33.el7\n\n\nnetlogger-4.2.0-9.osg33.el7\n\n\nnuttcp-6.1.2-1.osg33.el7\n\n\nosg-build-1.6.0-2.osg33.el7\n\n\nosg-ca-certs-1.47-1.osg33.el7\n\n\nosg-ca-certs-updater-1.0-1.osg33.el7\n\n\nosg-ca-scripts-1.1.5-2.osg33.el7\n\n\nosg-ce-3.3-3_clipped.osg33.el7\n\n\nosg-cert-scripts-2.7.2-2.osg33.el7\n\n\nosg-cleanup-1.7.2-1.osg33.el7\n\n\nosg-configure-1.1.1-1.osg33.el7\n\n\nosg-control-1.0.1-1.osg33.el7\n\n\nosg-gridftp-3.3-2_clipped.osg33.el7\n\n\nosg-gridftp-hdfs-3.3-2.osg33.el7\n\n\nosg-gridftp-xrootd-3.3-2.osg33.el7\n\n\nosg-gums-3.3-2.osg33.el7\n\n\nosg-info-services-1.0.2-1.osg33.el7\n\n\nosg-java7-compat-1.0-1.osg33.el7\n\n\nosg-oasis-5-2.osg33.el7\n\n\nosg-pki-tools-1.2.12-1.osg33.el7\n\n\nosg-release-3.3-2.osg33.el7\n\n\nosg-release-itb-3.3-2.osg33.el7\n\n\nosg-se-bestman-3.3-2.osg33.el7\n\n\nosg-se-bestman-xrootd-3.3-2.osg33.el7\n\n\nosg-se-hadoop-3.3-2.osg33.el7\n\n\nosg-system-profiler-1.2.0-1.osg33.el7\n\n\nosg-test-1.4.25-1.osg33.el7\n\n\nosg-tested-internal-3.3-1.osg33.el7\n\n\nosg-version-3.3.0-1.osg33.el7\n\n\nosg-vo-map-0.0.1-1.osg33.el7\n\n\nosg-voms-3.3-1.osg33.el7\n\n\nosg-webapp-common-1-2.osg33.el7\n\n\nosg-wn-client-3.3-5.osg33.el7\n\n\nowamp-3.2rc4-2.osg33.el7\n\n\npegasus-4.3.1-1.3.osg33.el7\n\n\nrsv-3.10.2-1_clipped.osg33.el7\n\n\nrsv-perfsonar-1.0.19-1.osg33.el7\n\n\nrsv-vo-gwms-1.0.1-1.osg33.el7\n\n\nstashcache-0.3-4.osg33.el7\n\n\nstashcache-daemon-0.2-1.osg33.el7\n\n\nuberftp-2.8-2.1.osg33.el7\n\n\nvo-client-61-1.osg33.el7\n\n\nvoms-2.0.12-3.osg33.el7\n\n\nvoms-admin-client-2.0.17-1.1.osg33.el7\n\n\nvoms-api-java-2.0.8-1.6.osg33.el7\n\n\nvoms-mysql-plugin-3.1.6-1.1.osg33.el7\n\n\nweb100_userland-1.7-6.osg33.el7\n\n\nxacml-1.5.0-1.osg33.el7\n\n\nxrootd-4.2.2-1.osg33.el7\n\n\nxrootd-dsi-3.0.4-16.osg33.el7\n\n\nxrootd-lcmaps-0.0.7-11.osg33.el7\n\n\nxrootd-status-probe-0.0.3-11.osg33.el7\n\n\nxrootd-voms-plugin-0.2.0-1.6.osg33.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nbestman2-client bestman2-client-libs bestman2-common-libs bestman2-server bestman2-server-dep-libs bestman2-server-libs bestman2-tester bestman2-tester-libs bigtop-jsvc bigtop-jsvc-debuginfo bigtop-utils blahp blahp-debuginfo bwctl bwctl-client bwctl-debuginfo bwctl-devel bwctl-server cctools-chirp cctools-debuginfo cctools-doc cctools-dttools cctools-makeflow cctools-parrot cctools-resource_monitor cctools-sand cctools-wavefront cctools-work_queue cilogon-openid-ca-cert cilogon-osg-ca-cert cog-jglobus-axis condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-cron condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp cvmfs cvmfs-config-osg cvmfs-devel cvmfs-server cvmfs-unittests edg-mkgridmap emi-trustmanager emi-trustmanager-axis emi-trustmanager-tomcat frontier-squid frontier-squid-debuginfo gfal2-plugin-xrootd gfal2-plugin-xrootd-debuginfo gip glexec glexec-debuginfo glexec-wrapper-scripts glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone glite-build-common-cpp glite-ce-cream-client-api-c glite-ce-cream-client-api-c-debuginfo glite-ce-cream-client-devel glite-ce-cream-utils glite-lbjp-common-gsoap-plugin glite-lbjp-common-gsoap-plugin-debuginfo glite-lbjp-common-gsoap-plugin-devel glite-lbjp-common-gss glite-lbjp-common-gss-debuginfo glite-lbjp-common-gss-devel globus-authz globus-authz-callout-error globus-authz-callout-error-debuginfo globus-authz-callout-error-devel globus-authz-callout-error-doc globus-authz-debuginfo globus-authz-devel globus-authz-doc globus-callout globus-callout-debuginfo globus-callout-devel globus-callout-doc globus-common globus-common-debuginfo globus-common-devel globus-common-doc globus-common-progs globus-ftp-client globus-ftp-client-debuginfo globus-ftp-client-devel globus-ftp-client-doc globus-ftp-control globus-ftp-control-debuginfo globus-ftp-control-devel globus-ftp-control-doc globus-gass-cache globus-gass-cache-debuginfo globus-gass-cache-devel globus-gass-cache-doc globus-gass-cache-program globus-gass-cache-program-debuginfo globus-gass-copy globus-gass-copy-debuginfo globus-gass-copy-devel globus-gass-copy-doc globus-gass-copy-progs globus-gass-server-ez globus-gass-server-ez-debuginfo globus-gass-server-ez-devel globus-gass-server-ez-progs globus-gass-transfer globus-gass-transfer-debuginfo globus-gass-transfer-devel globus-gass-transfer-doc globus-gatekeeper globus-gatekeeper-debuginfo globus-gfork globus-gfork-debuginfo globus-gfork-devel globus-gfork-progs globus-gram-audit globus-gram-client globus-gram-client-debuginfo globus-gram-client-devel globus-gram-client-doc globus-gram-client-tools globus-gram-client-tools-debuginfo globus-gram-job-manager globus-gram-job-manager-callout-error globus-gram-job-manager-callout-error-debuginfo globus-gram-job-manager-callout-error-devel globus-gram-job-manager-callout-error-doc globus-gram-job-manager-condor globus-gram-job-manager-debuginfo globus-gram-job-manager-fork globus-gram-job-manager-fork-debuginfo globus-gram-job-manager-fork-setup-poll globus-gram-job-manager-fork-setup-seg globus-gram-job-manager-lsf globus-gram-job-manager-lsf-debuginfo globus-gram-job-manager-lsf-setup-poll globus-gram-job-manager-lsf-setup-seg globus-gram-job-manager-managedfork globus-gram-job-manager-pbs globus-gram-job-manager-pbs-debuginfo globus-gram-job-manager-pbs-setup-poll globus-gram-job-manager-pbs-setup-seg globus-gram-job-manager-scripts globus-gram-job-manager-scripts-doc globus-gram-job-manager-sge globus-gram-job-manager-sge-debuginfo globus-gram-job-manager-sge-setup-poll globus-gram-job-manager-sge-setup-seg globus-gram-protocol globus-gram-protocol-debuginfo globus-gram-protocol-devel globus-gram-protocol-doc globus-gridftp-server globus-gridftp-server-control globus-gridftp-server-control-debuginfo globus-gridftp-server-control-devel globus-gridftp-server-debuginfo globus-gridftp-server-devel globus-gridftp-server-progs globus-gridmap-callout-error globus-gridmap-callout-error-debuginfo globus-gridmap-callout-error-devel globus-gridmap-callout-error-doc globus-gsi-callback globus-gsi-callback-debuginfo globus-gsi-callback-devel globus-gsi-callback-doc globus-gsi-cert-utils globus-gsi-cert-utils-debuginfo globus-gsi-cert-utils-devel globus-gsi-cert-utils-doc globus-gsi-cert-utils-progs globus-gsi-credential globus-gsi-credential-debuginfo globus-gsi-credential-devel globus-gsi-credential-doc globus-gsi-openssl-error globus-gsi-openssl-error-debuginfo globus-gsi-openssl-error-devel globus-gsi-openssl-error-doc globus-gsi-proxy-core globus-gsi-proxy-core-debuginfo globus-gsi-proxy-core-devel globus-gsi-proxy-core-doc globus-gsi-proxy-ssl globus-gsi-proxy-ssl-debuginfo globus-gsi-proxy-ssl-devel globus-gsi-proxy-ssl-doc globus-gsi-sysconfig globus-gsi-sysconfig-debuginfo globus-gsi-sysconfig-devel globus-gsi-sysconfig-doc globus-gssapi-error globus-gssapi-error-debuginfo globus-gssapi-error-devel globus-gssapi-error-doc globus-gssapi-gsi globus-gssapi-gsi-debuginfo globus-gssapi-gsi-devel globus-gssapi-gsi-doc globus-gss-assist globus-gss-assist-debuginfo globus-gss-assist-devel globus-gss-assist-doc globus-gss-assist-progs globus-io globus-io-debuginfo globus-io-devel globus-openssl-module globus-openssl-module-debuginfo globus-openssl-module-devel globus-openssl-module-doc globus-proxy-utils globus-proxy-utils-debuginfo globus-rsl globus-rsl-debuginfo globus-rsl-devel globus-rsl-doc globus-scheduler-event-generator globus-scheduler-event-generator-debuginfo globus-scheduler-event-generator-devel globus-scheduler-event-generator-doc globus-scheduler-event-generator-progs globus-simple-ca globus-usage globus-usage-debuginfo globus-usage-devel globus-xio globus-xio-debuginfo globus-xio-devel globus-xio-doc globus-xio-gsi-driver globus-xio-gsi-driver-debuginfo globus-xio-gsi-driver-devel globus-xio-gsi-driver-doc globus-xioperf globus-xioperf-debuginfo globus-xio-pipe-driver globus-xio-pipe-driver-debuginfo globus-xio-pipe-driver-devel globus-xio-popen-driver globus-xio-popen-driver-debuginfo globus-xio-popen-driver-devel globus-xio-udt-driver globus-xio-udt-driver-debuginfo globus-xio-udt-driver-devel gratia-debuginfo gratia-probe-bdii-status gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glexec gratia-probe-glideinwms gratia-probe-gram gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-psacct gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer gratia-reporting-email gratia-service gridftp-hdfs gridftp-hdfs-debuginfo gsi-openssh gsi-openssh-clients gsi-openssh-debuginfo gsi-openssh-server gums gums-client gums-service hadoop hadoop-client hadoop-conf-pseudo hadoop-debuginfo hadoop-doc hadoop-hdfs hadoop-hdfs-datanode hadoop-hdfs-fuse hadoop-hdfs-fuse-selinux hadoop-hdfs-journalnode hadoop-hdfs-namenode hadoop-hdfs-secondarynamenode hadoop-hdfs-zkfc hadoop-httpfs hadoop-libhdfs hadoop-mapreduce hadoop-yarn htcondor-ce htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-debuginfo htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge I2util I2util-debuginfo igtf-ca-certs javascriptrrd jetty jetty-ajp jetty-annotations jetty-client jetty-continuation jetty-deploy jetty-http jetty-io jetty-jmx jetty-jndi jetty-overlay-deployer jetty-plus jetty-policy jetty-rewrite jetty-security jetty-server jetty-servlet jetty-servlets jetty-util jetty-webapp jetty-websocket jetty-xml jglobus joda-time joda-time-javadoc koji koji-builder koji-hub koji-hub-plugins koji-utils koji-vm koji-web lcas-lcmaps-gt4-interface lcas-lcmaps-gt4-interface-debuginfo lcmaps lcmaps-common-devel lcmaps-debuginfo lcmaps-devel lcmaps-plugins-basic lcmaps-plugins-basic-debuginfo lcmaps-plugins-basic-ldap lcmaps-plugins-glexec-tracking lcmaps-plugins-glexec-tracking-debuginfo lcmaps-plugins-gums-client lcmaps-plugins-scas-client lcmaps-plugins-scas-client-debuginfo lcmaps-plugins-verify-proxy lcmaps-plugins-verify-proxy-debuginfo lcmaps-without-gsi lcmaps-without-gsi-devel llrun llrun-debuginfo mash mkgltempdir myproxy myproxy-admin myproxy-debuginfo myproxy-devel myproxy-doc myproxy-libs myproxy-server myproxy-voms ndt ndt-client ndt-debuginfo ndt-server netlogger nuttcp nuttcp-debuginfo osg-base-ce osg-base-ce-condor osg-base-ce-lsf osg-base-ce-pbs osg-base-ce-sge osg-base-ce-slurm osg-build osg-ca-certs osg-ca-certs-updater osg-ca-scripts osg-ce osg-ce-condor osg-ce-lsf osg-ce-pbs osg-cert-scripts osg-ce-sge osg-ce-slurm osg-cleanup osg-configure osg-configure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-control osg-gridftp osg-gridftp-hdfs osg-gridftp-xrootd osg-gums osg-gums-config osg-htcondor-ce osg-htcondor-ce-condor osg-htcondor-ce-lsf osg-htcondor-ce-pbs osg-htcondor-ce-sge osg-htcondor-ce-slurm osg-info-services osg-java7-compat osg-java7-compat-openjdk osg-java7-devel-compat osg-java7-devel-compat-openjdk osg-oasis osg-pki-tools osg-pki-tools-tests osg-release osg-release-itb osg-se-bestman osg-se-bestman-xrootd osg-se-hadoop osg-se-hadoop-client osg-se-hadoop-datanode osg-se-hadoop-gridftp osg-se-hadoop-namenode osg-se-hadoop-secondarynamenode osg-se-hadoop-srm osg-system-profiler osg-system-profiler-viewer osg-test osg-tested-internal osg-version osg-vo-map osg-voms osg-webapp-common osg-wn-client osg-wn-client-glexec owamp owamp-client owamp-debuginfo owamp-server pegasus pegasus-debuginfo privilege-xacml rsv rsv-consumers rsv-core rsv-metrics rsv-perfsonar rsv-vo-gwms stashcache-cache-server stashcache-daemon stashcache-origin-server uberftp uberftp-debuginfo vo-client vo-client-edgmkgridmap voms voms-admin-client voms-admin-server voms-api-java voms-api-java-javadoc voms-clients-cpp voms-debuginfo voms-devel voms-doc voms-mysql-plugin voms-mysql-plugin-debuginfo voms-server web100_userland web100_userland-debuginfo xacml xacml-debuginfo xacml-devel xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-dsi xrootd-dsi-debuginfo xrootd-fuse xrootd-hdfs xrootd-hdfs-debuginfo xrootd-hdfs-devel xrootd-lcmaps xrootd-lcmaps-debuginfo xrootd-libs xrootd-private-devel xrootd-python xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs xrootd-status-probe xrootd-status-probe-debuginfo xrootd-voms-plugin xrootd-voms-plugin-debuginfo xrootd-voms-plugin-devel zookeeper zookeeper-server\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nbestman2-2.3.0-25.osg33.el6\nbestman2-client-2.3.0-25.osg33.el6\nbestman2-client-libs-2.3.0-25.osg33.el6\nbestman2-common-libs-2.3.0-25.osg33.el6\nbestman2-server-2.3.0-25.osg33.el6\nbestman2-server-dep-libs-2.3.0-25.osg33.el6\nbestman2-server-libs-2.3.0-25.osg33.el6\nbestman2-tester-2.3.0-25.osg33.el6\nbestman2-tester-libs-2.3.0-25.osg33.el6\nbigtop-jsvc-0.3.0-1.1.osg33.el6\nbigtop-jsvc-debuginfo-0.3.0-1.1.osg33.el6\nbigtop-utils-0.4+300-1.cdh4.0.1.p0.3.osg33.el6\nblahp-1.18.13.bosco-3.osg33.el6\nblahp-debuginfo-1.18.13.bosco-3.osg33.el6\nbwctl-1.4-7.osg33.el6\nbwctl-client-1.4-7.osg33.el6\nbwctl-debuginfo-1.4-7.osg33.el6\nbwctl-devel-1.4-7.osg33.el6\nbwctl-server-1.4-7.osg33.el6\ncctools-4.4.0-1.osg33.el6\ncctools-chirp-4.4.0-1.osg33.el6\ncctools-debuginfo-4.4.0-1.osg33.el6\ncctools-doc-4.4.0-1.osg33.el6\ncctools-dttools-4.4.0-1.osg33.el6\ncctools-makeflow-4.4.0-1.osg33.el6\ncctools-parrot-4.4.0-1.osg33.el6\ncctools-resource_monitor-4.4.0-1.osg33.el6\ncctools-sand-4.4.0-1.osg33.el6\ncctools-wavefront-4.4.0-1.osg33.el6\ncctools-work_queue-4.4.0-1.osg33.el6\ncilogon-openid-ca-cert-1.1-2.osg33.el6\ncilogon-osg-ca-cert-1.0-1.osg33.el6\ncog-jglobus-axis-1.8.0-7.osg33.el6\ncondor-8.3.6-1.4.osg33.el6\ncondor-all-8.3.6-1.4.osg33.el6\ncondor-bosco-8.3.6-1.4.osg33.el6\ncondor-classads-8.3.6-1.4.osg33.el6\ncondor-classads-devel-8.3.6-1.4.osg33.el6\ncondor-cream-gahp-8.3.6-1.4.osg33.el6\ncondor-cron-1.0.9-4.osg33.el6\ncondor-debuginfo-8.3.6-1.4.osg33.el6\ncondor-kbdd-8.3.6-1.4.osg33.el6\ncondor-procd-8.3.6-1.4.osg33.el6\ncondor-python-8.3.6-1.4.osg33.el6\ncondor-std-universe-8.3.6-1.4.osg33.el6\ncondor-test-8.3.6-1.4.osg33.el6\ncondor-vm-gahp-8.3.6-1.4.osg33.el6\ncvmfs-2.1.20-1.osg33.el6\ncvmfs-config-osg-1.1-7.osg33.el6\ncvmfs-devel-2.1.20-1.osg33.el6\ncvmfs-server-2.1.20-1.osg33.el6\ncvmfs-unittests-2.1.20-1.osg33.el6\nedg-mkgridmap-4.0.2-1.osg33.el6\nemi-trustmanager-3.0.3-8.osg33.el6\nemi-trustmanager-axis-1.0.1-1.4.osg33.el6\nemi-trustmanager-tomcat-3.0.0-10.osg33.el6\nfrontier-squid-2.7.STABLE9-24.1.osg33.el6\nfrontier-squid-debuginfo-2.7.STABLE9-24.1.osg33.el6\ngfal2-plugin-xrootd-0.3.pre1-2.1.osg33.el6\ngfal2-plugin-xrootd-debuginfo-0.3.pre1-2.1.osg33.el6\ngip-1.3.11-6.osg33.el6\nglexec-0.9.11-1.1.osg33.el6\nglexec-debuginfo-0.9.11-1.1.osg33.el6\nglexec-wrapper-scripts-0.0.7-1.1.osg33.el6\nglideinwms-3.2.10-1.1.osg33.el6\nglideinwms-factory-3.2.10-1.1.osg33.el6\nglideinwms-factory-condor-3.2.10-1.1.osg33.el6\nglideinwms-glidecondor-tools-3.2.10-1.1.osg33.el6\nglideinwms-libs-3.2.10-1.1.osg33.el6\nglideinwms-minimal-condor-3.2.10-1.1.osg33.el6\nglideinwms-usercollector-3.2.10-1.1.osg33.el6\nglideinwms-userschedd-3.2.10-1.1.osg33.el6\nglideinwms-vofrontend-3.2.10-1.1.osg33.el6\nglideinwms-vofrontend-standalone-3.2.10-1.1.osg33.el6\nglite-build-common-cpp-3.3.0.2-1.osg33.el6\nglite-ce-cream-client-api-c-1.14.0-4.10.osg33.el6\nglite-ce-cream-client-api-c-debuginfo-1.14.0-4.10.osg33.el6\nglite-ce-cream-client-devel-1.14.0-4.10.osg33.el6\nglite-ce-cream-utils-1.2.0-4.3.osg33.el6\nglite-lbjp-common-gsoap-plugin-3.1.2-2.2.osg33.el6\nglite-lbjp-common-gsoap-plugin-debuginfo-3.1.2-2.2.osg33.el6\nglite-lbjp-common-gsoap-plugin-devel-3.1.2-2.2.osg33.el6\nglite-lbjp-common-gss-3.1.3-2.2.osg33.el6\nglite-lbjp-common-gss-debuginfo-3.1.3-2.2.osg33.el6\nglite-lbjp-common-gss-devel-3.1.3-2.2.osg33.el6\nglobus-authz-3.10-1.osg33.el6\nglobus-authz-callout-error-3.5-1.osg33.el6\nglobus-authz-callout-error-debuginfo-3.5-1.osg33.el6\nglobus-authz-callout-error-devel-3.5-1.osg33.el6\nglobus-authz-callout-error-doc-3.5-1.osg33.el6\nglobus-authz-debuginfo-3.10-1.osg33.el6\nglobus-authz-devel-3.10-1.osg33.el6\nglobus-authz-doc-3.10-1.osg33.el6\nglobus-callout-3.13-1.osg33.el6\nglobus-callout-debuginfo-3.13-1.osg33.el6\nglobus-callout-devel-3.13-1.osg33.el6\nglobus-callout-doc-3.13-1.osg33.el6\nglobus-common-15.27-1.osg33.el6\nglobus-common-debuginfo-15.27-1.osg33.el6\nglobus-common-devel-15.27-1.osg33.el6\nglobus-common-doc-15.27-1.osg33.el6\nglobus-common-progs-15.27-1.osg33.el6\nglobus-ftp-client-8.19-1.2.osg33.el6\nglobus-ftp-client-debuginfo-8.19-1.2.osg33.el6\nglobus-ftp-client-devel-8.19-1.2.osg33.el6\nglobus-ftp-client-doc-8.19-1.2.osg33.el6\nglobus-ftp-control-6.6-1.1.osg33.el6\nglobus-ftp-control-debuginfo-6.6-1.1.osg33.el6\nglobus-ftp-control-devel-6.6-1.1.osg33.el6\nglobus-ftp-control-doc-6.6-1.1.osg33.el6\nglobus-gass-cache-9.5-1.osg33.el6\nglobus-gass-cache-debuginfo-9.5-1.osg33.el6\nglobus-gass-cache-devel-9.5-1.osg33.el6\nglobus-gass-cache-doc-9.5-1.osg33.el6\nglobus-gass-cache-program-6.5-1.osg33.el6\nglobus-gass-cache-program-debuginfo-6.5-1.osg33.el6\nglobus-gass-copy-9.13-1.osg33.el6\nglobus-gass-copy-debuginfo-9.13-1.osg33.el6\nglobus-gass-copy-devel-9.13-1.osg33.el6\nglobus-gass-copy-doc-9.13-1.osg33.el6\nglobus-gass-copy-progs-9.13-1.osg33.el6\nglobus-gass-server-ez-5.7-1.osg33.el6\nglobus-gass-server-ez-debuginfo-5.7-1.osg33.el6\nglobus-gass-server-ez-devel-5.7-1.osg33.el6\nglobus-gass-server-ez-progs-5.7-1.osg33.el6\nglobus-gass-transfer-8.8-1.osg33.el6\nglobus-gass-transfer-debuginfo-8.8-1.osg33.el6\nglobus-gass-transfer-devel-8.8-1.osg33.el6\nglobus-gass-transfer-doc-8.8-1.osg33.el6\nglobus-gatekeeper-10.9-1.2.osg33.el6\nglobus-gatekeeper-debuginfo-10.9-1.2.osg33.el6\nglobus-gfork-4.7-1.osg33.el6\nglobus-gfork-debuginfo-4.7-1.osg33.el6\nglobus-gfork-devel-4.7-1.osg33.el6\nglobus-gfork-progs-4.7-1.osg33.el6\nglobus-gram-audit-4.4-1.osg33.el6\nglobus-gram-client-13.12-1.osg33.el6\nglobus-gram-client-debuginfo-13.12-1.osg33.el6\nglobus-gram-client-devel-13.12-1.osg33.el6\nglobus-gram-client-doc-13.12-1.osg33.el6\nglobus-gram-client-tools-11.7-1.osg33.el6\nglobus-gram-client-tools-debuginfo-11.7-1.osg33.el6\nglobus-gram-job-manager-14.25-1.2.osg33.el6\nglobus-gram-job-manager-callout-error-3.5-1.osg33.el6\nglobus-gram-job-manager-callout-error-debuginfo-3.5-1.osg33.el6\nglobus-gram-job-manager-callout-error-devel-3.5-1.osg33.el6\nglobus-gram-job-manager-callout-error-doc-3.5-1.osg33.el6\nglobus-gram-job-manager-condor-2.5-1.1.osg33.el6\nglobus-gram-job-manager-debuginfo-14.25-1.2.osg33.el6\nglobus-gram-job-manager-fork-2.4-1.1.osg33.el6\nglobus-gram-job-manager-fork-debuginfo-2.4-1.1.osg33.el6\nglobus-gram-job-manager-fork-setup-poll-2.4-1.1.osg33.el6\nglobus-gram-job-manager-fork-setup-seg-2.4-1.1.osg33.el6\nglobus-gram-job-manager-lsf-2.6-1.2.osg33.el6\nglobus-gram-job-manager-lsf-debuginfo-2.6-1.2.osg33.el6\nglobus-gram-job-manager-lsf-setup-poll-2.6-1.2.osg33.el6\nglobus-gram-job-manager-lsf-setup-seg-2.6-1.2.osg33.el6\nglobus-gram-job-manager-managedfork-0.2-1.osg33.el6\nglobus-gram-job-manager-pbs-2.4-2.1.osg33.el6\nglobus-gram-job-manager-pbs-debuginfo-2.4-2.1.osg33.el6\nglobus-gram-job-manager-pbs-setup-poll-2.4-2.1.osg33.el6\nglobus-gram-job-manager-pbs-setup-seg-2.4-2.1.osg33.el6\nglobus-gram-job-manager-scripts-6.7-1.osg33.el6\nglobus-gram-job-manager-scripts-doc-6.7-1.osg33.el6\nglobus-gram-job-manager-sge-2.5-1.1.osg33.el6\nglobus-gram-job-manager-sge-debuginfo-2.5-1.1.osg33.el6\nglobus-gram-job-manager-sge-setup-poll-2.5-1.1.osg33.el6\nglobus-gram-job-manager-sge-setup-seg-2.5-1.1.osg33.el6\nglobus-gram-protocol-12.12-2.osg33.el6\nglobus-gram-protocol-debuginfo-12.12-2.osg33.el6\nglobus-gram-protocol-devel-12.12-2.osg33.el6\nglobus-gram-protocol-doc-12.12-2.osg33.el6\nglobus-gridftp-server-7.20-1.1.osg33.el6\nglobus-gridftp-server-control-3.6-1.osg33.el6\nglobus-gridftp-server-control-debuginfo-3.6-1.osg33.el6\nglobus-gridftp-server-control-devel-3.6-1.osg33.el6\nglobus-gridftp-server-debuginfo-7.20-1.1.osg33.el6\nglobus-gridftp-server-devel-7.20-1.1.osg33.el6\nglobus-gridftp-server-progs-7.20-1.1.osg33.el6\nglobus-gridmap-callout-error-2.4-1.osg33.el6\nglobus-gridmap-callout-error-debuginfo-2.4-1.osg33.el6\nglobus-gridmap-callout-error-devel-2.4-1.osg33.el6\nglobus-gridmap-callout-error-doc-2.4-1.osg33.el6\nglobus-gsi-callback-5.7-1.osg33.el6\nglobus-gsi-callback-debuginfo-5.7-1.osg33.el6\nglobus-gsi-callback-devel-5.7-1.osg33.el6\nglobus-gsi-callback-doc-5.7-1.osg33.el6\nglobus-gsi-cert-utils-9.10-1.osg33.el6\nglobus-gsi-cert-utils-debuginfo-9.10-1.osg33.el6\nglobus-gsi-cert-utils-devel-9.10-1.osg33.el6\nglobus-gsi-cert-utils-doc-9.10-1.osg33.el6\nglobus-gsi-cert-utils-progs-9.10-1.osg33.el6\nglobus-gsi-credential-7.7-1.osg33.el6\nglobus-gsi-credential-debuginfo-7.7-1.osg33.el6\nglobus-gsi-credential-devel-7.7-1.osg33.el6\nglobus-gsi-credential-doc-7.7-1.osg33.el6\nglobus-gsi-openssl-error-3.5-1.osg33.el6\nglobus-gsi-openssl-error-debuginfo-3.5-1.osg33.el6\nglobus-gsi-openssl-error-devel-3.5-1.osg33.el6\nglobus-gsi-openssl-error-doc-3.5-1.osg33.el6\nglobus-gsi-proxy-core-7.7-1.osg33.el6\nglobus-gsi-proxy-core-debuginfo-7.7-1.osg33.el6\nglobus-gsi-proxy-core-devel-7.7-1.osg33.el6\nglobus-gsi-proxy-core-doc-7.7-1.osg33.el6\nglobus-gsi-proxy-ssl-5.7-1.osg33.el6\nglobus-gsi-proxy-ssl-debuginfo-5.7-1.osg33.el6\nglobus-gsi-proxy-ssl-devel-5.7-1.osg33.el6\nglobus-gsi-proxy-ssl-doc-5.7-1.osg33.el6\nglobus-gsi-sysconfig-6.8-1.osg33.el6\nglobus-gsi-sysconfig-debuginfo-6.8-1.osg33.el6\nglobus-gsi-sysconfig-devel-6.8-1.osg33.el6\nglobus-gsi-sysconfig-doc-6.8-1.osg33.el6\nglobus-gssapi-error-5.4-1.osg33.el6\nglobus-gssapi-error-debuginfo-5.4-1.osg33.el6\nglobus-gssapi-error-devel-5.4-1.osg33.el6\nglobus-gssapi-error-doc-5.4-1.osg33.el6\nglobus-gssapi-gsi-11.18-1.osg33.el6\nglobus-gssapi-gsi-debuginfo-11.18-1.osg33.el6\nglobus-gssapi-gsi-devel-11.18-1.osg33.el6\nglobus-gssapi-gsi-doc-11.18-1.osg33.el6\nglobus-gss-assist-10.13-1.osg33.el6\nglobus-gss-assist-debuginfo-10.13-1.osg33.el6\nglobus-gss-assist-devel-10.13-1.osg33.el6\nglobus-gss-assist-doc-10.13-1.osg33.el6\nglobus-gss-assist-progs-10.13-1.osg33.el6\nglobus-io-11.4-1.osg33.el6\nglobus-io-debuginfo-11.4-1.osg33.el6\nglobus-io-devel-11.4-1.osg33.el6\nglobus-openssl-module-4.6-1.osg33.el6\nglobus-openssl-module-debuginfo-4.6-1.osg33.el6\nglobus-openssl-module-devel-4.6-1.osg33.el6\nglobus-openssl-module-doc-4.6-1.osg33.el6\nglobus-proxy-utils-6.9-1.osg33.el6\nglobus-proxy-utils-debuginfo-6.9-1.osg33.el6\nglobus-rsl-10.9-1.osg33.el6\nglobus-rsl-debuginfo-10.9-1.osg33.el6\nglobus-rsl-devel-10.9-1.osg33.el6\nglobus-rsl-doc-10.9-1.osg33.el6\nglobus-scheduler-event-generator-5.10-2.1.osg33.el6\nglobus-scheduler-event-generator-debuginfo-5.10-2.1.osg33.el6\nglobus-scheduler-event-generator-devel-5.10-2.1.osg33.el6\nglobus-scheduler-event-generator-doc-5.10-2.1.osg33.el6\nglobus-scheduler-event-generator-progs-5.10-2.1.osg33.el6\nglobus-simple-ca-4.18-1.osg33.el6\nglobus-usage-4.4-1.osg33.el6\nglobus-usage-debuginfo-4.4-1.osg33.el6\nglobus-usage-devel-4.4-1.osg33.el6\nglobus-xio-5.7-1.1.osg33.el6\nglobus-xio-debuginfo-5.7-1.1.osg33.el6\nglobus-xio-devel-5.7-1.1.osg33.el6\nglobus-xio-doc-5.7-1.1.osg33.el6\nglobus-xio-gsi-driver-3.7-1.osg33.el6\nglobus-xio-gsi-driver-debuginfo-3.7-1.osg33.el6\nglobus-xio-gsi-driver-devel-3.7-1.osg33.el6\nglobus-xio-gsi-driver-doc-3.7-1.osg33.el6\nglobus-xioperf-4.4-1.osg33.el6\nglobus-xioperf-debuginfo-4.4-1.osg33.el6\nglobus-xio-pipe-driver-3.7-1.osg33.el6\nglobus-xio-pipe-driver-debuginfo-3.7-1.osg33.el6\nglobus-xio-pipe-driver-devel-3.7-1.osg33.el6\nglobus-xio-popen-driver-3.5-1.osg33.el6\nglobus-xio-popen-driver-debuginfo-3.5-1.osg33.el6\nglobus-xio-popen-driver-devel-3.5-1.osg33.el6\nglobus-xio-udt-driver-1.16-1.osg33.el6\nglobus-xio-udt-driver-debuginfo-1.16-1.osg33.el6\nglobus-xio-udt-driver-devel-1.16-1.osg33.el6\ngratia-1.16.2-1.osg33.el6\ngratia-debuginfo-1.16.2-1.osg33.el6\ngratia-probe-1.14.2-6.osg33.el6\ngratia-probe-bdii-status-1.14.2-6.osg33.el6\ngratia-probe-common-1.14.2-6.osg33.el6\ngratia-probe-condor-1.14.2-6.osg33.el6\ngratia-probe-condor-events-1.14.2-6.osg33.el6\ngratia-probe-dcache-storage-1.14.2-6.osg33.el6\ngratia-probe-dcache-storagegroup-1.14.2-6.osg33.el6\ngratia-probe-dcache-transfer-1.14.2-6.osg33.el6\ngratia-probe-debuginfo-1.14.2-6.osg33.el6\ngratia-probe-enstore-storage-1.14.2-6.osg33.el6\ngratia-probe-enstore-tapedrive-1.14.2-6.osg33.el6\ngratia-probe-enstore-transfer-1.14.2-6.osg33.el6\ngratia-probe-glexec-1.14.2-6.osg33.el6\ngratia-probe-glideinwms-1.14.2-6.osg33.el6\ngratia-probe-gram-1.14.2-6.osg33.el6\ngratia-probe-gridftp-transfer-1.14.2-6.osg33.el6\ngratia-probe-hadoop-storage-1.14.2-6.osg33.el6\ngratia-probe-lsf-1.14.2-6.osg33.el6\ngratia-probe-metric-1.14.2-6.osg33.el6\ngratia-probe-onevm-1.14.2-6.osg33.el6\ngratia-probe-pbs-lsf-1.14.2-6.osg33.el6\ngratia-probe-psacct-1.14.2-6.osg33.el6\ngratia-probe-services-1.14.2-6.osg33.el6\ngratia-probe-sge-1.14.2-6.osg33.el6\ngratia-probe-slurm-1.14.2-6.osg33.el6\ngratia-probe-xrootd-storage-1.14.2-6.osg33.el6\ngratia-probe-xrootd-transfer-1.14.2-6.osg33.el6\ngratia-reporting-email-1.15.1-1.osg33.el6\ngratia-service-1.16.2-1.osg33.el6\ngridftp-hdfs-0.5.4-19.osg33.el6\ngridftp-hdfs-debuginfo-0.5.4-19.osg33.el6\ngsi-openssh-5.7-1.1.osg33.el6\ngsi-openssh-clients-5.7-1.1.osg33.el6\ngsi-openssh-debuginfo-5.7-1.1.osg33.el6\ngsi-openssh-server-5.7-1.1.osg33.el6\ngums-1.4.4-3.osg33.el6\ngums-client-1.4.4-3.osg33.el6\ngums-service-1.4.4-3.osg33.el6\nhadoop-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\nhadoop-client-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\nhadoop-conf-pseudo-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\nhadoop-debuginfo-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\nhadoop-doc-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\nhadoop-hdfs-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\nhadoop-hdfs-datanode-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\nhadoop-hdfs-fuse-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\nhadoop-hdfs-fuse-selinux-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\nhadoop-hdfs-journalnode-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\nhadoop-hdfs-namenode-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\nhadoop-hdfs-secondarynamenode-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\nhadoop-hdfs-zkfc-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\nhadoop-httpfs-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\nhadoop-libhdfs-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\nhadoop-mapreduce-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\nhadoop-yarn-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\nhtcondor-ce-1.14-4.osg33.el6\nhtcondor-ce-client-1.14-4.osg33.el6\nhtcondor-ce-collector-1.14-4.osg33.el6\nhtcondor-ce-condor-1.14-4.osg33.el6\nhtcondor-ce-debuginfo-1.14-4.osg33.el6\nhtcondor-ce-lsf-1.14-4.osg33.el6\nhtcondor-ce-pbs-1.14-4.osg33.el6\nhtcondor-ce-sge-1.14-4.osg33.el6\nI2util-1.1-2.osg33.el6\nI2util-debuginfo-1.1-2.osg33.el6\nigtf-ca-certs-1.65-1.osg33.el6\njavascriptrrd-1.1.1-1.osg33.el6\njetty-8.1.4.v20120524-2.osg33.el6\njetty-ajp-8.1.4.v20120524-2.osg33.el6\njetty-annotations-8.1.4.v20120524-2.osg33.el6\njetty-client-8.1.4.v20120524-2.osg33.el6\njetty-continuation-8.1.4.v20120524-2.osg33.el6\njetty-deploy-8.1.4.v20120524-2.osg33.el6\njetty-http-8.1.4.v20120524-2.osg33.el6\njetty-io-8.1.4.v20120524-2.osg33.el6\njetty-jmx-8.1.4.v20120524-2.osg33.el6\njetty-jndi-8.1.4.v20120524-2.osg33.el6\njetty-overlay-deployer-8.1.4.v20120524-2.osg33.el6\njetty-plus-8.1.4.v20120524-2.osg33.el6\njetty-policy-8.1.4.v20120524-2.osg33.el6\njetty-rewrite-8.1.4.v20120524-2.osg33.el6\njetty-security-8.1.4.v20120524-2.osg33.el6\njetty-server-8.1.4.v20120524-2.osg33.el6\njetty-servlet-8.1.4.v20120524-2.osg33.el6\njetty-servlets-8.1.4.v20120524-2.osg33.el6\njetty-util-8.1.4.v20120524-2.osg33.el6\njetty-webapp-8.1.4.v20120524-2.osg33.el6\njetty-websocket-8.1.4.v20120524-2.osg33.el6\njetty-xml-8.1.4.v20120524-2.osg33.el6\njglobus-2.0.6-4.osg33.el6\njoda-time-1.5.2-7.2.tzdata2008d.osg33.el6\njoda-time-javadoc-1.5.2-7.2.tzdata2008d.osg33.el6\nkoji-1.6.0-10.osg33.el6\nkoji-builder-1.6.0-10.osg33.el6\nkoji-hub-1.6.0-10.osg33.el6\nkoji-hub-plugins-1.6.0-10.osg33.el6\nkoji-utils-1.6.0-10.osg33.el6\nkoji-vm-1.6.0-10.osg33.el6\nkoji-web-1.6.0-10.osg33.el6\nlcas-lcmaps-gt4-interface-0.2.6-1.1.osg33.el6\nlcas-lcmaps-gt4-interface-debuginfo-0.2.6-1.1.osg33.el6\nlcmaps-1.6.6-1.1.osg33.el6\nlcmaps-common-devel-1.6.6-1.1.osg33.el6\nlcmaps-debuginfo-1.6.6-1.1.osg33.el6\nlcmaps-devel-1.6.6-1.1.osg33.el6\nlcmaps-plugins-basic-1.7.0-2.osg33.el6\nlcmaps-plugins-basic-debuginfo-1.7.0-2.osg33.el6\nlcmaps-plugins-basic-ldap-1.7.0-2.osg33.el6\nlcmaps-plugins-glexec-tracking-0.1.6-1.osg33.el6\nlcmaps-plugins-glexec-tracking-debuginfo-0.1.6-1.osg33.el6\nlcmaps-plugins-gums-client-0.0.2-4.osg33.el6\nlcmaps-plugins-scas-client-0.5.5-1.osg33.el6\nlcmaps-plugins-scas-client-debuginfo-0.5.5-1.osg33.el6\nlcmaps-plugins-verify-proxy-1.5.7-1.osg33.el6\nlcmaps-plugins-verify-proxy-debuginfo-1.5.7-1.osg33.el6\nlcmaps-without-gsi-1.6.6-1.1.osg33.el6\nlcmaps-without-gsi-devel-1.6.6-1.1.osg33.el6\nllrun-0.1.3-1.3.osg33.el6\nllrun-debuginfo-0.1.3-1.3.osg33.el6\nmash-0.5.22-3.osg33.el6\nmkgltempdir-0.0.5-1.1.osg33.el6\nmyproxy-6.1.12-1.osg33.el6\nmyproxy-admin-6.1.12-1.osg33.el6\nmyproxy-debuginfo-6.1.12-1.osg33.el6\nmyproxy-devel-6.1.12-1.osg33.el6\nmyproxy-doc-6.1.12-1.osg33.el6\nmyproxy-libs-6.1.12-1.osg33.el6\nmyproxy-server-6.1.12-1.osg33.el6\nmyproxy-voms-6.1.12-1.osg33.el6\nndt-3.6.5-3.osg33.el6\nndt-client-3.6.5-3.osg33.el6\nndt-debuginfo-3.6.5-3.osg33.el6\nndt-server-3.6.5-3.osg33.el6\nnetlogger-4.2.0-9.osg33.el6\nnuttcp-6.1.2-1.osg33.el6\nnuttcp-debuginfo-6.1.2-1.osg33.el6\nosg-base-ce-3.3-3.osg33.el6\nosg-base-ce-condor-3.3-3.osg33.el6\nosg-base-ce-lsf-3.3-3.osg33.el6\nosg-base-ce-pbs-3.3-3.osg33.el6\nosg-base-ce-sge-3.3-3.osg33.el6\nosg-base-ce-slurm-3.3-3.osg33.el6\nosg-build-1.6.0-2.osg33.el6\nosg-ca-certs-1.47-1.osg33.el6\nosg-ca-certs-updater-1.0-1.osg33.el6\nosg-ca-scripts-1.1.5-2.osg33.el6\nosg-ce-3.3-3.osg33.el6\nosg-ce-condor-3.3-3.osg33.el6\nosg-ce-lsf-3.3-3.osg33.el6\nosg-ce-pbs-3.3-3.osg33.el6\nosg-cert-scripts-2.7.2-2.osg33.el6\nosg-ce-sge-3.3-3.osg33.el6\nosg-ce-slurm-3.3-3.osg33.el6\nosg-cleanup-1.7.2-1.osg33.el6\nosg-configure-1.1.1-1.osg33.el6\nosg-configure-ce-1.1.1-1.osg33.el6\nosg-configure-cemon-1.1.1-1.osg33.el6\nosg-configure-condor-1.1.1-1.osg33.el6\nosg-configure-gateway-1.1.1-1.osg33.el6\nosg-configure-gip-1.1.1-1.osg33.el6\nosg-configure-gratia-1.1.1-1.osg33.el6\nosg-configure-infoservices-1.1.1-1.osg33.el6\nosg-configure-lsf-1.1.1-1.osg33.el6\nosg-configure-managedfork-1.1.1-1.osg33.el6\nosg-configure-misc-1.1.1-1.osg33.el6\nosg-configure-monalisa-1.1.1-1.osg33.el6\nosg-configure-network-1.1.1-1.osg33.el6\nosg-configure-pbs-1.1.1-1.osg33.el6\nosg-configure-rsv-1.1.1-1.osg33.el6\nosg-configure-sge-1.1.1-1.osg33.el6\nosg-configure-slurm-1.1.1-1.osg33.el6\nosg-configure-squid-1.1.1-1.osg33.el6\nosg-configure-tests-1.1.1-1.osg33.el6\nosg-control-1.0.1-1.osg33.el6\nosg-gridftp-3.3-2.osg33.el6\nosg-gridftp-hdfs-3.3-2.osg33.el6\nosg-gridftp-xrootd-3.3-2.osg33.el6\nosg-gums-3.3-2.osg33.el6\nosg-gums-config-61-1.osg33.el6\nosg-htcondor-ce-3.3-3.osg33.el6\nosg-htcondor-ce-condor-3.3-3.osg33.el6\nosg-htcondor-ce-lsf-3.3-3.osg33.el6\nosg-htcondor-ce-pbs-3.3-3.osg33.el6\nosg-htcondor-ce-sge-3.3-3.osg33.el6\nosg-htcondor-ce-slurm-3.3-3.osg33.el6\nosg-info-services-1.0.2-1.osg33.el6\nosg-java7-compat-1.0-1.osg33.el6\nosg-java7-compat-openjdk-1.0-1.osg33.el6\nosg-java7-devel-compat-1.0-1.osg33.el6\nosg-java7-devel-compat-openjdk-1.0-1.osg33.el6\nosg-oasis-5-2.osg33.el6\nosg-pki-tools-1.2.12-1.osg33.el6\nosg-pki-tools-tests-1.2.12-1.osg33.el6\nosg-release-3.3-2.osg33.el6\nosg-release-itb-3.3-2.osg33.el6\nosg-se-bestman-3.3-2.osg33.el6\nosg-se-bestman-xrootd-3.3-2.osg33.el6\nosg-se-hadoop-3.3-2.osg33.el6\nosg-se-hadoop-client-3.3-2.osg33.el6\nosg-se-hadoop-datanode-3.3-2.osg33.el6\nosg-se-hadoop-gridftp-3.3-2.osg33.el6\nosg-se-hadoop-namenode-3.3-2.osg33.el6\nosg-se-hadoop-secondarynamenode-3.3-2.osg33.el6\nosg-se-hadoop-srm-3.3-2.osg33.el6\nosg-system-profiler-1.2.0-1.osg33.el6\nosg-system-profiler-viewer-1.2.0-1.osg33.el6\nosg-test-1.4.26-1.osg33.el6\nosg-tested-internal-3.3-1.osg33.el6\nosg-version-3.3.0-1.osg33.el6\nosg-vo-map-0.0.1-1.osg33.el6\nosg-voms-3.3-1.osg33.el6\nosg-webapp-common-1-2.osg33.el6\nosg-wn-client-3.3-5.osg33.el6\nosg-wn-client-glexec-3.3-5.osg33.el6\nowamp-3.2rc4-2.osg33.el6\nowamp-client-3.2rc4-2.osg33.el6\nowamp-debuginfo-3.2rc4-2.osg33.el6\nowamp-server-3.2rc4-2.osg33.el6\npegasus-4.3.1-1.3.osg33.el6\npegasus-debuginfo-4.3.1-1.3.osg33.el6\nprivilege-xacml-2.6.4-1.osg33.el6\nrsv-3.10.2-1.osg33.el6\nrsv-consumers-3.10.2-1.osg33.el6\nrsv-core-3.10.2-1.osg33.el6\nrsv-metrics-3.10.2-1.osg33.el6\nrsv-perfsonar-1.0.19-1.osg33.el6\nrsv-vo-gwms-1.0.1-1.osg33.el6\nstashcache-0.3-4.osg33.el6\nstashcache-cache-server-0.3-4.osg33.el6\nstashcache-daemon-0.2-1.osg33.el6\nstashcache-daemon-0.3-4.osg33.el6\nstashcache-origin-server-0.3-4.osg33.el6\nuberftp-2.8-2.1.osg33.el6\nuberftp-debuginfo-2.8-2.1.osg33.el6\nvo-client-61-1.osg33.el6\nvo-client-edgmkgridmap-61-1.osg33.el6\nvoms-2.0.12-3.osg33.el6\nvoms-admin-client-2.0.17-1.1.osg33.el6\nvoms-admin-server-2.7.0-1.14.osg33.el6\nvoms-api-java-2.0.8-1.6.osg33.el6\nvoms-api-java-javadoc-2.0.8-1.6.osg33.el6\nvoms-clients-cpp-2.0.12-3.osg33.el6\nvoms-debuginfo-2.0.12-3.osg33.el6\nvoms-devel-2.0.12-3.osg33.el6\nvoms-doc-2.0.12-3.osg33.el6\nvoms-mysql-plugin-3.1.6-1.1.osg33.el6\nvoms-mysql-plugin-debuginfo-3.1.6-1.1.osg33.el6\nvoms-server-2.0.12-3.osg33.el6\nweb100_userland-1.7-6.osg33.el6\nweb100_userland-debuginfo-1.7-6.osg33.el6\nxacml-1.5.0-1.osg33.el6\nxacml-debuginfo-1.5.0-1.osg33.el6\nxacml-devel-1.5.0-1.osg33.el6\nxrootd-4.2.2-1.osg33.el6\nxrootd-client-4.2.2-1.osg33.el6\nxrootd-client-devel-4.2.2-1.osg33.el6\nxrootd-client-libs-4.2.2-1.osg33.el6\nxrootd-debuginfo-4.2.2-1.osg33.el6\nxrootd-devel-4.2.2-1.osg33.el6\nxrootd-doc-4.2.2-1.osg33.el6\nxrootd-dsi-3.0.4-16.osg33.el6\nxrootd-dsi-debuginfo-3.0.4-16.osg33.el6\nxrootd-fuse-4.2.2-1.osg33.el6\nxrootd-hdfs-1.8.4-4.osg33.el6\nxrootd-hdfs-debuginfo-1.8.4-4.osg33.el6\nxrootd-hdfs-devel-1.8.4-4.osg33.el6\nxrootd-lcmaps-0.0.7-11.osg33.el6\nxrootd-lcmaps-debuginfo-0.0.7-11.osg33.el6\nxrootd-libs-4.2.2-1.osg33.el6\nxrootd-private-devel-4.2.2-1.osg33.el6\nxrootd-python-4.2.2-1.osg33.el6\nxrootd-selinux-4.2.2-1.osg33.el6\nxrootd-server-4.2.2-1.osg33.el6\nxrootd-server-devel-4.2.2-1.osg33.el6\nxrootd-server-libs-4.2.2-1.osg33.el6\nxrootd-status-probe-0.0.3-11.osg33.el6\nxrootd-status-probe-debuginfo-0.0.3-11.osg33.el6\nxrootd-voms-plugin-0.2.0-1.6.osg33.el6\nxrootd-voms-plugin-debuginfo-0.2.0-1.6.osg33.el6\nxrootd-voms-plugin-devel-0.2.0-1.6.osg33.el6\nzookeeper-3.4.3+15-1.cdh4.0.1.p0.3.osg33.el6\nzookeeper-server-3.4.3+15-1.cdh4.0.1.p0.3.osg33.el6\n\n\n\n\n\nEnterprise Linux 7\n\n\nbigtop-jsvc-0.3.0-1.1.osg33.el7\nbigtop-jsvc-debuginfo-0.3.0-1.1.osg33.el7\nbigtop-utils-0.4+300-1.cdh4.0.1.p0.3.osg33.el7\nblahp-1.18.13.bosco-3.osg33.el7\nblahp-debuginfo-1.18.13.bosco-3.osg33.el7\nbwctl-1.4-7.osg33.el7\nbwctl-client-1.4-7.osg33.el7\nbwctl-debuginfo-1.4-7.osg33.el7\nbwctl-devel-1.4-7.osg33.el7\nbwctl-server-1.4-7.osg33.el7\ncctools-4.4.0-1.osg33.el7\ncctools-chirp-4.4.0-1.osg33.el7\ncctools-debuginfo-4.4.0-1.osg33.el7\ncctools-doc-4.4.0-1.osg33.el7\ncctools-dttools-4.4.0-1.osg33.el7\ncctools-makeflow-4.4.0-1.osg33.el7\ncctools-parrot-4.4.0-1.osg33.el7\ncctools-resource_monitor-4.4.0-1.osg33.el7\ncctools-sand-4.4.0-1.osg33.el7\ncctools-wavefront-4.4.0-1.osg33.el7\ncctools-work_queue-4.4.0-1.osg33.el7\ncilogon-openid-ca-cert-1.1-2.osg33.el7\ncilogon-osg-ca-cert-1.0-1.osg33.el7\ncog-jglobus-axis-1.8.0-8.osg33.el7\ncondor-8.3.6-1.4.osg33.el7\ncondor-all-8.3.6-1.4.osg33.el7\ncondor-bosco-8.3.6-1.4.osg33.el7\ncondor-classads-8.3.6-1.4.osg33.el7\ncondor-classads-devel-8.3.6-1.4.osg33.el7\ncondor-cron-1.0.9-4.osg33.el7\ncondor-debuginfo-8.3.6-1.4.osg33.el7\ncondor-kbdd-8.3.6-1.4.osg33.el7\ncondor-procd-8.3.6-1.4.osg33.el7\ncondor-python-8.3.6-1.4.osg33.el7\ncondor-test-8.3.6-1.4.osg33.el7\ncondor-vm-gahp-8.3.6-1.4.osg33.el7\ncvmfs-2.1.20-1.osg33.el7\ncvmfs-config-osg-1.1-7.osg33.el7\ncvmfs-devel-2.1.20-1.osg33.el7\ncvmfs-server-2.1.20-1.osg33.el7\ncvmfs-unittests-2.1.20-1.osg33.el7\nedg-mkgridmap-4.0.2-1.osg33.el7\nemi-trustmanager-3.0.3-8.osg33.el7\nemi-trustmanager-axis-1.0.1-1.4.osg33.el7\nfrontier-squid-2.7.STABLE9-24.1.osg33.el7\nfrontier-squid-debuginfo-2.7.STABLE9-24.1.osg33.el7\ngfal2-plugin-xrootd-0.3.pre1-2.1.osg33.el7\ngfal2-plugin-xrootd-debuginfo-0.3.pre1-2.1.osg33.el7\ngip-1.3.11-6.osg33.el7\nglexec-0.9.11-1.1.osg33.el7\nglexec-debuginfo-0.9.11-1.1.osg33.el7\nglexec-wrapper-scripts-0.0.7-1.1.osg33.el7\nglideinwms-3.2.10-1.1.osg33.el7\nglideinwms-factory-3.2.10-1.1.osg33.el7\nglideinwms-factory-condor-3.2.10-1.1.osg33.el7\nglideinwms-glidecondor-tools-3.2.10-1.1.osg33.el7\nglideinwms-libs-3.2.10-1.1.osg33.el7\nglideinwms-minimal-condor-3.2.10-1.1.osg33.el7\nglideinwms-usercollector-3.2.10-1.1.osg33.el7\nglideinwms-userschedd-3.2.10-1.1.osg33.el7\nglideinwms-vofrontend-3.2.10-1.1.osg33.el7\nglideinwms-vofrontend-standalone-3.2.10-1.1.osg33.el7\nglite-build-common-cpp-3.3.0.2-1.osg33.el7\nglite-lbjp-common-gsoap-plugin-3.1.2-2.2.osg33.el7\nglite-lbjp-common-gsoap-plugin-debuginfo-3.1.2-2.2.osg33.el7\nglite-lbjp-common-gsoap-plugin-devel-3.1.2-2.2.osg33.el7\nglite-lbjp-common-gss-3.1.3-2.2.osg33.el7\nglite-lbjp-common-gss-debuginfo-3.1.3-2.2.osg33.el7\nglite-lbjp-common-gss-devel-3.1.3-2.2.osg33.el7\nglobus-authz-3.10-1.osg33.el7\nglobus-authz-callout-error-3.5-1.osg33.el7\nglobus-authz-callout-error-debuginfo-3.5-1.osg33.el7\nglobus-authz-callout-error-devel-3.5-1.osg33.el7\nglobus-authz-callout-error-doc-3.5-1.osg33.el7\nglobus-authz-debuginfo-3.10-1.osg33.el7\nglobus-authz-devel-3.10-1.osg33.el7\nglobus-authz-doc-3.10-1.osg33.el7\nglobus-callout-3.13-1.osg33.el7\nglobus-callout-debuginfo-3.13-1.osg33.el7\nglobus-callout-devel-3.13-1.osg33.el7\nglobus-callout-doc-3.13-1.osg33.el7\nglobus-common-15.27-1.osg33.el7\nglobus-common-debuginfo-15.27-1.osg33.el7\nglobus-common-devel-15.27-1.osg33.el7\nglobus-common-doc-15.27-1.osg33.el7\nglobus-common-progs-15.27-1.osg33.el7\nglobus-ftp-client-8.19-1.2.osg33.el7\nglobus-ftp-client-debuginfo-8.19-1.2.osg33.el7\nglobus-ftp-client-devel-8.19-1.2.osg33.el7\nglobus-ftp-client-doc-8.19-1.2.osg33.el7\nglobus-ftp-control-6.6-1.1.osg33.el7\nglobus-ftp-control-debuginfo-6.6-1.1.osg33.el7\nglobus-ftp-control-devel-6.6-1.1.osg33.el7\nglobus-ftp-control-doc-6.6-1.1.osg33.el7\nglobus-gass-cache-9.5-1.osg33.el7\nglobus-gass-cache-debuginfo-9.5-1.osg33.el7\nglobus-gass-cache-devel-9.5-1.osg33.el7\nglobus-gass-cache-doc-9.5-1.osg33.el7\nglobus-gass-cache-program-6.5-1.osg33.el7\nglobus-gass-cache-program-debuginfo-6.5-1.osg33.el7\nglobus-gass-copy-9.13-1.osg33.el7\nglobus-gass-copy-debuginfo-9.13-1.osg33.el7\nglobus-gass-copy-devel-9.13-1.osg33.el7\nglobus-gass-copy-doc-9.13-1.osg33.el7\nglobus-gass-copy-progs-9.13-1.osg33.el7\nglobus-gass-server-ez-5.7-1.osg33.el7\nglobus-gass-server-ez-debuginfo-5.7-1.osg33.el7\nglobus-gass-server-ez-devel-5.7-1.osg33.el7\nglobus-gass-server-ez-progs-5.7-1.osg33.el7\nglobus-gass-transfer-8.8-1.osg33.el7\nglobus-gass-transfer-debuginfo-8.8-1.osg33.el7\nglobus-gass-transfer-devel-8.8-1.osg33.el7\nglobus-gass-transfer-doc-8.8-1.osg33.el7\nglobus-gatekeeper-10.9-1.2.osg33.el7\nglobus-gatekeeper-debuginfo-10.9-1.2.osg33.el7\nglobus-gfork-4.7-1.osg33.el7\nglobus-gfork-debuginfo-4.7-1.osg33.el7\nglobus-gfork-devel-4.7-1.osg33.el7\nglobus-gfork-progs-4.7-1.osg33.el7\nglobus-gram-audit-4.4-1.osg33.el7\nglobus-gram-client-13.12-1.osg33.el7\nglobus-gram-client-debuginfo-13.12-1.osg33.el7\nglobus-gram-client-devel-13.12-1.osg33.el7\nglobus-gram-client-doc-13.12-1.osg33.el7\nglobus-gram-client-tools-11.7-1.osg33.el7\nglobus-gram-client-tools-debuginfo-11.7-1.osg33.el7\nglobus-gram-job-manager-14.25-1.2.osg33.el7\nglobus-gram-job-manager-callout-error-3.5-1.osg33.el7\nglobus-gram-job-manager-callout-error-debuginfo-3.5-1.osg33.el7\nglobus-gram-job-manager-callout-error-devel-3.5-1.osg33.el7\nglobus-gram-job-manager-callout-error-doc-3.5-1.osg33.el7\nglobus-gram-job-manager-condor-2.5-1.1.osg33.el7\nglobus-gram-job-manager-debuginfo-14.25-1.2.osg33.el7\nglobus-gram-job-manager-fork-2.4-1.1.osg33.el7\nglobus-gram-job-manager-fork-debuginfo-2.4-1.1.osg33.el7\nglobus-gram-job-manager-fork-setup-poll-2.4-1.1.osg33.el7\nglobus-gram-job-manager-fork-setup-seg-2.4-1.1.osg33.el7\nglobus-gram-job-manager-lsf-2.6-1.2.osg33.el7\nglobus-gram-job-manager-lsf-debuginfo-2.6-1.2.osg33.el7\nglobus-gram-job-manager-lsf-setup-poll-2.6-1.2.osg33.el7\nglobus-gram-job-manager-lsf-setup-seg-2.6-1.2.osg33.el7\nglobus-gram-job-manager-managedfork-0.2-1.osg33.el7\nglobus-gram-job-manager-pbs-2.4-2.1.osg33.el7\nglobus-gram-job-manager-pbs-debuginfo-2.4-2.1.osg33.el7\nglobus-gram-job-manager-pbs-setup-poll-2.4-2.1.osg33.el7\nglobus-gram-job-manager-pbs-setup-seg-2.4-2.1.osg33.el7\nglobus-gram-job-manager-scripts-6.7-1.osg33.el7\nglobus-gram-job-manager-scripts-doc-6.7-1.osg33.el7\nglobus-gram-job-manager-sge-2.5-1.1.osg33.el7\nglobus-gram-job-manager-sge-debuginfo-2.5-1.1.osg33.el7\nglobus-gram-job-manager-sge-setup-poll-2.5-1.1.osg33.el7\nglobus-gram-job-manager-sge-setup-seg-2.5-1.1.osg33.el7\nglobus-gram-protocol-12.12-2.osg33.el7\nglobus-gram-protocol-debuginfo-12.12-2.osg33.el7\nglobus-gram-protocol-devel-12.12-2.osg33.el7\nglobus-gram-protocol-doc-12.12-2.osg33.el7\nglobus-gridftp-server-7.20-1.1.osg33.el7\nglobus-gridftp-server-control-3.6-1.osg33.el7\nglobus-gridftp-server-control-debuginfo-3.6-1.osg33.el7\nglobus-gridftp-server-control-devel-3.6-1.osg33.el7\nglobus-gridftp-server-debuginfo-7.20-1.1.osg33.el7\nglobus-gridftp-server-devel-7.20-1.1.osg33.el7\nglobus-gridftp-server-progs-7.20-1.1.osg33.el7\nglobus-gridmap-callout-error-2.4-1.osg33.el7\nglobus-gridmap-callout-error-debuginfo-2.4-1.osg33.el7\nglobus-gridmap-callout-error-devel-2.4-1.osg33.el7\nglobus-gridmap-callout-error-doc-2.4-1.osg33.el7\nglobus-gsi-callback-5.7-1.osg33.el7\nglobus-gsi-callback-debuginfo-5.7-1.osg33.el7\nglobus-gsi-callback-devel-5.7-1.osg33.el7\nglobus-gsi-callback-doc-5.7-1.osg33.el7\nglobus-gsi-cert-utils-9.10-1.osg33.el7\nglobus-gsi-cert-utils-debuginfo-9.10-1.osg33.el7\nglobus-gsi-cert-utils-devel-9.10-1.osg33.el7\nglobus-gsi-cert-utils-doc-9.10-1.osg33.el7\nglobus-gsi-cert-utils-progs-9.10-1.osg33.el7\nglobus-gsi-credential-7.7-1.osg33.el7\nglobus-gsi-credential-debuginfo-7.7-1.osg33.el7\nglobus-gsi-credential-devel-7.7-1.osg33.el7\nglobus-gsi-credential-doc-7.7-1.osg33.el7\nglobus-gsi-openssl-error-3.5-1.osg33.el7\nglobus-gsi-openssl-error-debuginfo-3.5-1.osg33.el7\nglobus-gsi-openssl-error-devel-3.5-1.osg33.el7\nglobus-gsi-openssl-error-doc-3.5-1.osg33.el7\nglobus-gsi-proxy-core-7.7-1.osg33.el7\nglobus-gsi-proxy-core-debuginfo-7.7-1.osg33.el7\nglobus-gsi-proxy-core-devel-7.7-1.osg33.el7\nglobus-gsi-proxy-core-doc-7.7-1.osg33.el7\nglobus-gsi-proxy-ssl-5.7-1.osg33.el7\nglobus-gsi-proxy-ssl-debuginfo-5.7-1.osg33.el7\nglobus-gsi-proxy-ssl-devel-5.7-1.osg33.el7\nglobus-gsi-proxy-ssl-doc-5.7-1.osg33.el7\nglobus-gsi-sysconfig-6.8-1.osg33.el7\nglobus-gsi-sysconfig-debuginfo-6.8-1.osg33.el7\nglobus-gsi-sysconfig-devel-6.8-1.osg33.el7\nglobus-gsi-sysconfig-doc-6.8-1.osg33.el7\nglobus-gssapi-error-5.4-1.osg33.el7\nglobus-gssapi-error-debuginfo-5.4-1.osg33.el7\nglobus-gssapi-error-devel-5.4-1.osg33.el7\nglobus-gssapi-error-doc-5.4-1.osg33.el7\nglobus-gssapi-gsi-11.18-1.osg33.el7\nglobus-gssapi-gsi-debuginfo-11.18-1.osg33.el7\nglobus-gssapi-gsi-devel-11.18-1.osg33.el7\nglobus-gssapi-gsi-doc-11.18-1.osg33.el7\nglobus-gss-assist-10.13-1.osg33.el7\nglobus-gss-assist-debuginfo-10.13-1.osg33.el7\nglobus-gss-assist-devel-10.13-1.osg33.el7\nglobus-gss-assist-doc-10.13-1.osg33.el7\nglobus-gss-assist-progs-10.13-1.osg33.el7\nglobus-io-11.4-1.osg33.el7\nglobus-io-debuginfo-11.4-1.osg33.el7\nglobus-io-devel-11.4-1.osg33.el7\nglobus-openssl-module-4.6-1.osg33.el7\nglobus-openssl-module-debuginfo-4.6-1.osg33.el7\nglobus-openssl-module-devel-4.6-1.osg33.el7\nglobus-openssl-module-doc-4.6-1.osg33.el7\nglobus-proxy-utils-6.9-1.osg33.el7\nglobus-proxy-utils-debuginfo-6.9-1.osg33.el7\nglobus-rsl-10.9-1.osg33.el7\nglobus-rsl-debuginfo-10.9-1.osg33.el7\nglobus-rsl-devel-10.9-1.osg33.el7\nglobus-rsl-doc-10.9-1.osg33.el7\nglobus-scheduler-event-generator-5.10-2.1.osg33.el7\nglobus-scheduler-event-generator-debuginfo-5.10-2.1.osg33.el7\nglobus-scheduler-event-generator-devel-5.10-2.1.osg33.el7\nglobus-scheduler-event-generator-doc-5.10-2.1.osg33.el7\nglobus-scheduler-event-generator-progs-5.10-2.1.osg33.el7\nglobus-simple-ca-4.18-1.osg33.el7\nglobus-usage-4.4-1.osg33.el7\nglobus-usage-debuginfo-4.4-1.osg33.el7\nglobus-usage-devel-4.4-1.osg33.el7\nglobus-xio-5.7-1.1.osg33.el7\nglobus-xio-debuginfo-5.7-1.1.osg33.el7\nglobus-xio-devel-5.7-1.1.osg33.el7\nglobus-xio-doc-5.7-1.1.osg33.el7\nglobus-xio-gsi-driver-3.7-1.osg33.el7\nglobus-xio-gsi-driver-debuginfo-3.7-1.osg33.el7\nglobus-xio-gsi-driver-devel-3.7-1.osg33.el7\nglobus-xio-gsi-driver-doc-3.7-1.osg33.el7\nglobus-xioperf-4.4-1.osg33.el7\nglobus-xioperf-debuginfo-4.4-1.osg33.el7\nglobus-xio-pipe-driver-3.7-1.osg33.el7\nglobus-xio-pipe-driver-debuginfo-3.7-1.osg33.el7\nglobus-xio-pipe-driver-devel-3.7-1.osg33.el7\nglobus-xio-popen-driver-3.5-1.osg33.el7\nglobus-xio-popen-driver-debuginfo-3.5-1.osg33.el7\nglobus-xio-popen-driver-devel-3.5-1.osg33.el7\nglobus-xio-udt-driver-1.16-1.osg33.el7\nglobus-xio-udt-driver-debuginfo-1.16-1.osg33.el7\nglobus-xio-udt-driver-devel-1.16-1.osg33.el7\ngratia-1.16.2-1.osg33.el7\ngratia-debuginfo-1.16.2-1.osg33.el7\ngratia-probe-1.14.2-6.osg33.el7\ngratia-probe-bdii-status-1.14.2-6.osg33.el7\ngratia-probe-common-1.14.2-6.osg33.el7\ngratia-probe-condor-1.14.2-6.osg33.el7\ngratia-probe-condor-events-1.14.2-6.osg33.el7\ngratia-probe-dcache-storage-1.14.2-6.osg33.el7\ngratia-probe-dcache-storagegroup-1.14.2-6.osg33.el7\ngratia-probe-dcache-transfer-1.14.2-6.osg33.el7\ngratia-probe-debuginfo-1.14.2-6.osg33.el7\ngratia-probe-enstore-storage-1.14.2-6.osg33.el7\ngratia-probe-enstore-tapedrive-1.14.2-6.osg33.el7\ngratia-probe-enstore-transfer-1.14.2-6.osg33.el7\ngratia-probe-glexec-1.14.2-6.osg33.el7\ngratia-probe-glideinwms-1.14.2-6.osg33.el7\ngratia-probe-gram-1.14.2-6.osg33.el7\ngratia-probe-gridftp-transfer-1.14.2-6.osg33.el7\ngratia-probe-hadoop-storage-1.14.2-6.osg33.el7\ngratia-probe-lsf-1.14.2-6.osg33.el7\ngratia-probe-metric-1.14.2-6.osg33.el7\ngratia-probe-onevm-1.14.2-6.osg33.el7\ngratia-probe-pbs-lsf-1.14.2-6.osg33.el7\ngratia-probe-psacct-1.14.2-6.osg33.el7\ngratia-probe-services-1.14.2-6.osg33.el7\ngratia-probe-sge-1.14.2-6.osg33.el7\ngratia-probe-slurm-1.14.2-6.osg33.el7\ngratia-probe-xrootd-storage-1.14.2-6.osg33.el7\ngratia-probe-xrootd-transfer-1.14.2-6.osg33.el7\ngratia-reporting-email-1.15.1-1.osg33.el7\ngratia-service-1.16.2-1.osg33.el7\ngsi-openssh-5.7-1.1.osg33.el7\ngsi-openssh-clients-5.7-1.1.osg33.el7\ngsi-openssh-debuginfo-5.7-1.1.osg33.el7\ngsi-openssh-server-5.7-1.1.osg33.el7\nhtcondor-ce-1.14-4.osg33.el7\nhtcondor-ce-client-1.14-4.osg33.el7\nhtcondor-ce-collector-1.14-4.osg33.el7\nhtcondor-ce-condor-1.14-4.osg33.el7\nhtcondor-ce-debuginfo-1.14-4.osg33.el7\nhtcondor-ce-lsf-1.14-4.osg33.el7\nhtcondor-ce-pbs-1.14-4.osg33.el7\nhtcondor-ce-sge-1.14-4.osg33.el7\nI2util-1.1-2.osg33.el7\nI2util-debuginfo-1.1-2.osg33.el7\nigtf-ca-certs-1.65-1.osg33.el7\njavascriptrrd-1.1.1-1.osg33.el7\njetty-8.1.4.v20120524-2.osg33.el7\njetty-ajp-8.1.4.v20120524-2.osg33.el7\njetty-annotations-8.1.4.v20120524-2.osg33.el7\njetty-client-8.1.4.v20120524-2.osg33.el7\njetty-continuation-8.1.4.v20120524-2.osg33.el7\njetty-deploy-8.1.4.v20120524-2.osg33.el7\njetty-http-8.1.4.v20120524-2.osg33.el7\njetty-io-8.1.4.v20120524-2.osg33.el7\njetty-jmx-8.1.4.v20120524-2.osg33.el7\njetty-jndi-8.1.4.v20120524-2.osg33.el7\njetty-overlay-deployer-8.1.4.v20120524-2.osg33.el7\njetty-plus-8.1.4.v20120524-2.osg33.el7\njetty-policy-8.1.4.v20120524-2.osg33.el7\njetty-rewrite-8.1.4.v20120524-2.osg33.el7\njetty-security-8.1.4.v20120524-2.osg33.el7\njetty-server-8.1.4.v20120524-2.osg33.el7\njetty-servlet-8.1.4.v20120524-2.osg33.el7\njetty-servlets-8.1.4.v20120524-2.osg33.el7\njetty-util-8.1.4.v20120524-2.osg33.el7\njetty-webapp-8.1.4.v20120524-2.osg33.el7\njetty-websocket-8.1.4.v20120524-2.osg33.el7\njetty-xml-8.1.4.v20120524-2.osg33.el7\njoda-time-1.5.2-7.2.tzdata2008d.osg33.el7\njoda-time-javadoc-1.5.2-7.2.tzdata2008d.osg33.el7\nkoji-1.6.0-10.osg33.el7\nkoji-builder-1.6.0-10.osg33.el7\nkoji-hub-1.6.0-10.osg33.el7\nkoji-hub-plugins-1.6.0-10.osg33.el7\nkoji-utils-1.6.0-10.osg33.el7\nkoji-vm-1.6.0-10.osg33.el7\nkoji-web-1.6.0-10.osg33.el7\nlcas-lcmaps-gt4-interface-0.2.6-1.1.osg33.el7\nlcas-lcmaps-gt4-interface-debuginfo-0.2.6-1.1.osg33.el7\nlcmaps-1.6.6-1.1.osg33.el7\nlcmaps-common-devel-1.6.6-1.1.osg33.el7\nlcmaps-debuginfo-1.6.6-1.1.osg33.el7\nlcmaps-devel-1.6.6-1.1.osg33.el7\nlcmaps-plugins-basic-1.7.0-2.osg33.el7\nlcmaps-plugins-basic-debuginfo-1.7.0-2.osg33.el7\nlcmaps-plugins-basic-ldap-1.7.0-2.osg33.el7\nlcmaps-plugins-glexec-tracking-0.1.6-1.osg33.el7\nlcmaps-plugins-glexec-tracking-debuginfo-0.1.6-1.osg33.el7\nlcmaps-plugins-gums-client-0.0.2-4.osg33.el7\nlcmaps-plugins-scas-client-0.5.5-1.osg33.el7\nlcmaps-plugins-scas-client-debuginfo-0.5.5-1.osg33.el7\nlcmaps-plugins-verify-proxy-1.5.7-1.osg33.el7\nlcmaps-plugins-verify-proxy-debuginfo-1.5.7-1.osg33.el7\nlcmaps-without-gsi-1.6.6-1.1.osg33.el7\nlcmaps-without-gsi-devel-1.6.6-1.1.osg33.el7\nllrun-0.1.3-1.3.osg33.el7\nllrun-debuginfo-0.1.3-1.3.osg33.el7\nmash-0.5.22-3.osg33.el7\nmkgltempdir-0.0.5-1.1.osg33.el7\nmyproxy-6.1.12-1.osg33.el7\nmyproxy-admin-6.1.12-1.osg33.el7\nmyproxy-debuginfo-6.1.12-1.osg33.el7\nmyproxy-devel-6.1.12-1.osg33.el7\nmyproxy-doc-6.1.12-1.osg33.el7\nmyproxy-libs-6.1.12-1.osg33.el7\nmyproxy-server-6.1.12-1.osg33.el7\nmyproxy-voms-6.1.12-1.osg33.el7\nndt-3.6.5-3.osg33.el7\nndt-client-3.6.5-3.osg33.el7\nndt-debuginfo-3.6.5-3.osg33.el7\nndt-server-3.6.5-3.osg33.el7\nnetlogger-4.2.0-9.osg33.el7\nnuttcp-6.1.2-1.osg33.el7\nnuttcp-debuginfo-6.1.2-1.osg33.el7\nosg-base-ce-3.3-3_clipped.osg33.el7\nosg-base-ce-condor-3.3-3_clipped.osg33.el7\nosg-base-ce-lsf-3.3-3_clipped.osg33.el7\nosg-base-ce-pbs-3.3-3_clipped.osg33.el7\nosg-base-ce-sge-3.3-3_clipped.osg33.el7\nosg-base-ce-slurm-3.3-3_clipped.osg33.el7\nosg-build-1.6.0-2.osg33.el7\nosg-ca-certs-1.47-1.osg33.el7\nosg-ca-certs-updater-1.0-1.osg33.el7\nosg-ca-scripts-1.1.5-2.osg33.el7\nosg-ce-3.3-3_clipped.osg33.el7\nosg-ce-condor-3.3-3_clipped.osg33.el7\nosg-ce-lsf-3.3-3_clipped.osg33.el7\nosg-ce-pbs-3.3-3_clipped.osg33.el7\nosg-cert-scripts-2.7.2-2.osg33.el7\nosg-ce-sge-3.3-3_clipped.osg33.el7\nosg-ce-slurm-3.3-3_clipped.osg33.el7\nosg-cleanup-1.7.2-1.osg33.el7\nosg-configure-1.1.1-1.osg33.el7\nosg-configure-ce-1.1.1-1.osg33.el7\nosg-configure-cemon-1.1.1-1.osg33.el7\nosg-configure-condor-1.1.1-1.osg33.el7\nosg-configure-gateway-1.1.1-1.osg33.el7\nosg-configure-gip-1.1.1-1.osg33.el7\nosg-configure-gratia-1.1.1-1.osg33.el7\nosg-configure-infoservices-1.1.1-1.osg33.el7\nosg-configure-lsf-1.1.1-1.osg33.el7\nosg-configure-managedfork-1.1.1-1.osg33.el7\nosg-configure-misc-1.1.1-1.osg33.el7\nosg-configure-monalisa-1.1.1-1.osg33.el7\nosg-configure-network-1.1.1-1.osg33.el7\nosg-configure-pbs-1.1.1-1.osg33.el7\nosg-configure-rsv-1.1.1-1.osg33.el7\nosg-configure-sge-1.1.1-1.osg33.el7\nosg-configure-slurm-1.1.1-1.osg33.el7\nosg-configure-squid-1.1.1-1.osg33.el7\nosg-configure-tests-1.1.1-1.osg33.el7\nosg-control-1.0.1-1.osg33.el7\nosg-gridftp-3.3-2_clipped.osg33.el7\nosg-gridftp-hdfs-3.3-2.osg33.el7\nosg-gridftp-xrootd-3.3-2.osg33.el7\nosg-gums-3.3-2.osg33.el7\nosg-gums-config-61-1.osg33.el7\nosg-htcondor-ce-3.3-3_clipped.osg33.el7\nosg-htcondor-ce-condor-3.3-3_clipped.osg33.el7\nosg-htcondor-ce-lsf-3.3-3_clipped.osg33.el7\nosg-htcondor-ce-pbs-3.3-3_clipped.osg33.el7\nosg-htcondor-ce-sge-3.3-3_clipped.osg33.el7\nosg-htcondor-ce-slurm-3.3-3_clipped.osg33.el7\nosg-info-services-1.0.2-1.osg33.el7\nosg-java7-compat-1.0-1.osg33.el7\nosg-java7-compat-openjdk-1.0-1.osg33.el7\nosg-java7-devel-compat-1.0-1.osg33.el7\nosg-java7-devel-compat-openjdk-1.0-1.osg33.el7\nosg-oasis-5-2.osg33.el7\nosg-pki-tools-1.2.12-1.osg33.el7\nosg-pki-tools-tests-1.2.12-1.osg33.el7\nosg-release-3.3-2.osg33.el7\nosg-release-itb-3.3-2.osg33.el7\nosg-se-bestman-3.3-2.osg33.el7\nosg-se-bestman-xrootd-3.3-2.osg33.el7\nosg-se-hadoop-3.3-2.osg33.el7\nosg-se-hadoop-client-3.3-2.osg33.el7\nosg-se-hadoop-datanode-3.3-2.osg33.el7\nosg-se-hadoop-gridftp-3.3-2.osg33.el7\nosg-se-hadoop-namenode-3.3-2.osg33.el7\nosg-se-hadoop-secondarynamenode-3.3-2.osg33.el7\nosg-se-hadoop-srm-3.3-2.osg33.el7\nosg-system-profiler-1.2.0-1.osg33.el7\nosg-system-profiler-viewer-1.2.0-1.osg33.el7\nosg-test-1.4.25-1.osg33.el7\nosg-tested-internal-3.3-1.osg33.el7\nosg-version-3.3.0-1.osg33.el7\nosg-vo-map-0.0.1-1.osg33.el7\nosg-voms-3.3-1.osg33.el7\nosg-webapp-common-1-2.osg33.el7\nosg-wn-client-3.3-5.osg33.el7\nosg-wn-client-glexec-3.3-5.osg33.el7\nowamp-3.2rc4-2.osg33.el7\nowamp-client-3.2rc4-2.osg33.el7\nowamp-debuginfo-3.2rc4-2.osg33.el7\nowamp-server-3.2rc4-2.osg33.el7\npegasus-4.3.1-1.3.osg33.el7\npegasus-debuginfo-4.3.1-1.3.osg33.el7\nrsv-3.10.2-1_clipped.osg33.el7\nrsv-consumers-3.10.2-1_clipped.osg33.el7\nrsv-core-3.10.2-1_clipped.osg33.el7\nrsv-metrics-3.10.2-1_clipped.osg33.el7\nrsv-perfsonar-1.0.19-1.osg33.el7\nrsv-vo-gwms-1.0.1-1.osg33.el7\nstashcache-0.3-4.osg33.el7\nstashcache-cache-server-0.3-4.osg33.el7\nstashcache-daemon-0.2-1.osg33.el7\nstashcache-daemon-0.3-4.osg33.el7\nstashcache-origin-server-0.3-4.osg33.el7\nuberftp-2.8-2.1.osg33.el7\nuberftp-debuginfo-2.8-2.1.osg33.el7\nvo-client-61-1.osg33.el7\nvo-client-edgmkgridmap-61-1.osg33.el7\nvoms-2.0.12-3.osg33.el7\nvoms-admin-client-2.0.17-1.1.osg33.el7\nvoms-api-java-2.0.8-1.6.osg33.el7\nvoms-api-java-javadoc-2.0.8-1.6.osg33.el7\nvoms-clients-cpp-2.0.12-3.osg33.el7\nvoms-debuginfo-2.0.12-3.osg33.el7\nvoms-devel-2.0.12-3.osg33.el7\nvoms-doc-2.0.12-3.osg33.el7\nvoms-mysql-plugin-3.1.6-1.1.osg33.el7\nvoms-mysql-plugin-debuginfo-3.1.6-1.1.osg33.el7\nvoms-server-2.0.12-3.osg33.el7\nweb100_userland-1.7-6.osg33.el7\nweb100_userland-debuginfo-1.7-6.osg33.el7\nxacml-1.5.0-1.osg33.el7\nxacml-debuginfo-1.5.0-1.osg33.el7\nxacml-devel-1.5.0-1.osg33.el7\nxrootd-4.2.2-1.osg33.el7\nxrootd-client-4.2.2-1.osg33.el7\nxrootd-client-devel-4.2.2-1.osg33.el7\nxrootd-client-libs-4.2.2-1.osg33.el7\nxrootd-debuginfo-4.2.2-1.osg33.el7\nxrootd-devel-4.2.2-1.osg33.el7\nxrootd-doc-4.2.2-1.osg33.el7\nxrootd-dsi-3.0.4-16.osg33.el7\nxrootd-dsi-debuginfo-3.0.4-16.osg33.el7\nxrootd-fuse-4.2.2-1.osg33.el7\nxrootd-lcmaps-0.0.7-11.osg33.el7\nxrootd-lcmaps-debuginfo-0.0.7-11.osg33.el7\nxrootd-libs-4.2.2-1.osg33.el7\nxrootd-private-devel-4.2.2-1.osg33.el7\nxrootd-python-4.2.2-1.osg33.el7\nxrootd-selinux-4.2.2-1.osg33.el7\nxrootd-server-4.2.2-1.osg33.el7\nxrootd-server-devel-4.2.2-1.osg33.el7\nxrootd-server-libs-4.2.2-1.osg33.el7\nxrootd-status-probe-0.0.3-11.osg33.el7\nxrootd-status-probe-debuginfo-0.0.3-11.osg33.el7\nxrootd-voms-plugin-0.2.0-1.6.osg33.el7\nxrootd-voms-plugin-debuginfo-0.2.0-1.6.osg33.el7\nxrootd-voms-plugin-devel-0.2.0-1.6.osg33.el7\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 5\n\n\nEnterprise Linux 6\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 5\n\n\n\n\n\n\n\n\nEnterprise Linux 6", 
            "title": "OSG Release 3.3.0"
        }, 
        {
            "location": "/release/3.3/release-3-3-0/#osg-software-release-330", 
            "text": "Release Date : 2015-08-11", 
            "title": "OSG Software Release 3.3.0"
        }, 
        {
            "location": "/release/3.3/release-3-3-0/#summary-of-changes", 
            "text": "This release contains:   Enterprise Linux 7: Worker node support only.  XRootD 4.2.2  2 patches to HTCondor to better support HTCondor CE  Improve responsiveness of HTCondor CE by using light-weight polling  Prevent causing jobs to error out upon restart of JobRouter    improved logging of failures in lcmaps-plugins-verify-proxy  better credential handling and fix for crashes in lcamps-plugins-scas-client  CA Certificates are no longer repackaged for compatibility with old software  VO Package v61   These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found in the  Release3  area of the TWiki.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.3/release-3-3-0/#known-issues", 
            "text": "HTCondor CE does not work with HTCondor 8.3.6 (in upcoming) on EL 5 platforms. So, we are holding HTCondor at version 8.3.4 (in upcoming) for EL 5 platforms.   StashCache packages need to be manually configured   Manual configuration for origin server  Assuming that the origin server connects only to a redirector (not directly to cache server), minimal xrootd configuration is required. The configuration file, /etc/xrootd/xrootd-stashcache-origin-server.cfg, in this release is overkill. Here are recommended settings to use: xrd.port 1094\nall.role server\nall.manager stash-redirector.example.com 1213\nall.export / nostage\nxrootd.trace emsg login stall redirect\nofs.trace none\nxrd.trace conn\ncms.trace all\nsec.protocol  host\nsec.protbind  * none\nall.adminpath /var/run/xrootd\nall.pidpath /var/run/xrootd        Manual configuration for cache server   In contrast to the origin server configuration, one needs to declare  pss.origin  stash-redirector.example.com  instead of configuring the cmsd or manager (only the xrootd daemon is required on the cache server). More detailed configuration of cache server for StashCache is  here .     In both cases, administrator needs to set the path of custom configuration file for its xrootd/cmds instance in /etc/sysconfig/xrootd, For example, change the cmds default from:  CMSD_DEFAULT_OPTIONS= -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg -k fifo    to  CMSD_DEFAULT_OPTIONS= -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-stashcache-origin-server.marian -k fifo", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.3/release-3-3-0/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.3/release-3-3-0/#update-repositories", 
            "text": "To update to this series, you need  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.3/release-3-3-0/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.   Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been editted, and a new default version was to be installed. In that case, RPM does not overwrite the editted configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.3/release-3-3-0/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.3/release-3-3-0/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.3/release-3-3-0/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-0/#enterprise-linux-6", 
            "text": "bestman2-2.3.0-25.osg33.el6  bigtop-jsvc-0.3.0-1.1.osg33.el6  bigtop-utils-0.4+300-1.cdh4.0.1.p0.3.osg33.el6  blahp-1.18.13.bosco-3.osg33.el6  bwctl-1.4-7.osg33.el6  cctools-4.4.0-1.osg33.el6  cilogon-openid-ca-cert-1.1-2.osg33.el6  cilogon-osg-ca-cert-1.0-1.osg33.el6  cog-jglobus-axis-1.8.0-7.osg33.el6  condor-8.3.6-1.4.osg33.el6  condor-cron-1.0.9-4.osg33.el6  cvmfs-2.1.20-1.osg33.el6  cvmfs-config-osg-1.1-7.osg33.el6  edg-mkgridmap-4.0.2-1.osg33.el6  emi-trustmanager-3.0.3-8.osg33.el6  emi-trustmanager-axis-1.0.1-1.4.osg33.el6  emi-trustmanager-tomcat-3.0.0-10.osg33.el6  frontier-squid-2.7.STABLE9-24.1.osg33.el6  gfal2-plugin-xrootd-0.3.pre1-2.1.osg33.el6  gip-1.3.11-6.osg33.el6  glexec-0.9.11-1.1.osg33.el6  glexec-wrapper-scripts-0.0.7-1.1.osg33.el6  glideinwms-3.2.10-1.1.osg33.el6  glite-build-common-cpp-3.3.0.2-1.osg33.el6  glite-ce-cream-client-api-c-1.14.0-4.10.osg33.el6  glite-ce-cream-utils-1.2.0-4.3.osg33.el6  glite-lbjp-common-gsoap-plugin-3.1.2-2.2.osg33.el6  glite-lbjp-common-gss-3.1.3-2.2.osg33.el6  globus-authz-3.10-1.osg33.el6  globus-authz-callout-error-3.5-1.osg33.el6  globus-callout-3.13-1.osg33.el6  globus-common-15.27-1.osg33.el6  globus-ftp-client-8.19-1.2.osg33.el6  globus-ftp-control-6.6-1.1.osg33.el6  globus-gass-cache-9.5-1.osg33.el6  globus-gass-cache-program-6.5-1.osg33.el6  globus-gass-copy-9.13-1.osg33.el6  globus-gass-server-ez-5.7-1.osg33.el6  globus-gass-transfer-8.8-1.osg33.el6  globus-gatekeeper-10.9-1.2.osg33.el6  globus-gfork-4.7-1.osg33.el6  globus-gram-audit-4.4-1.osg33.el6  globus-gram-client-13.12-1.osg33.el6  globus-gram-client-tools-11.7-1.osg33.el6  globus-gram-job-manager-14.25-1.2.osg33.el6  globus-gram-job-manager-callout-error-3.5-1.osg33.el6  globus-gram-job-manager-condor-2.5-1.1.osg33.el6  globus-gram-job-manager-fork-2.4-1.1.osg33.el6  globus-gram-job-manager-lsf-2.6-1.2.osg33.el6  globus-gram-job-manager-managedfork-0.2-1.osg33.el6  globus-gram-job-manager-pbs-2.4-2.1.osg33.el6  globus-gram-job-manager-scripts-6.7-1.osg33.el6  globus-gram-job-manager-sge-2.5-1.1.osg33.el6  globus-gram-protocol-12.12-2.osg33.el6  globus-gridftp-server-7.20-1.1.osg33.el6  globus-gridftp-server-control-3.6-1.osg33.el6  globus-gridmap-callout-error-2.4-1.osg33.el6  globus-gsi-callback-5.7-1.osg33.el6  globus-gsi-cert-utils-9.10-1.osg33.el6  globus-gsi-credential-7.7-1.osg33.el6  globus-gsi-openssl-error-3.5-1.osg33.el6  globus-gsi-proxy-core-7.7-1.osg33.el6  globus-gsi-proxy-ssl-5.7-1.osg33.el6  globus-gsi-sysconfig-6.8-1.osg33.el6  globus-gssapi-error-5.4-1.osg33.el6  globus-gssapi-gsi-11.18-1.osg33.el6  globus-gss-assist-10.13-1.osg33.el6  globus-io-11.4-1.osg33.el6  globus-openssl-module-4.6-1.osg33.el6  globus-proxy-utils-6.9-1.osg33.el6  globus-rsl-10.9-1.osg33.el6  globus-scheduler-event-generator-5.10-2.1.osg33.el6  globus-simple-ca-4.18-1.osg33.el6  globus-usage-4.4-1.osg33.el6  globus-xio-5.7-1.1.osg33.el6  globus-xio-gsi-driver-3.7-1.osg33.el6  globus-xioperf-4.4-1.osg33.el6  globus-xio-pipe-driver-3.7-1.osg33.el6  globus-xio-popen-driver-3.5-1.osg33.el6  globus-xio-udt-driver-1.16-1.osg33.el6  gratia-1.16.2-1.osg33.el6  gratia-probe-1.14.2-6.osg33.el6  gratia-reporting-email-1.15.1-1.osg33.el6  gridftp-hdfs-0.5.4-19.osg33.el6  gsi-openssh-5.7-1.1.osg33.el6  gums-1.4.4-3.osg33.el6  hadoop-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6  htcondor-ce-1.14-4.osg33.el6  I2util-1.1-2.osg33.el6  igtf-ca-certs-1.65-1.osg33.el6  javascriptrrd-1.1.1-1.osg33.el6  jetty-8.1.4.v20120524-2.osg33.el6  jglobus-2.0.6-4.osg33.el6  joda-time-1.5.2-7.2.tzdata2008d.osg33.el6  koji-1.6.0-10.osg33.el6  lcas-lcmaps-gt4-interface-0.2.6-1.1.osg33.el6  lcmaps-1.6.6-1.1.osg33.el6  lcmaps-plugins-basic-1.7.0-2.osg33.el6  lcmaps-plugins-glexec-tracking-0.1.6-1.osg33.el6  lcmaps-plugins-gums-client-0.0.2-4.osg33.el6  lcmaps-plugins-scas-client-0.5.5-1.osg33.el6  lcmaps-plugins-verify-proxy-1.5.7-1.osg33.el6  llrun-0.1.3-1.3.osg33.el6  mash-0.5.22-3.osg33.el6  mkgltempdir-0.0.5-1.1.osg33.el6  myproxy-6.1.12-1.osg33.el6  ndt-3.6.5-3.osg33.el6  netlogger-4.2.0-9.osg33.el6  nuttcp-6.1.2-1.osg33.el6  osg-build-1.6.0-2.osg33.el6  osg-ca-certs-1.47-1.osg33.el6  osg-ca-certs-updater-1.0-1.osg33.el6  osg-ca-scripts-1.1.5-2.osg33.el6  osg-ce-3.3-3.osg33.el6  osg-cert-scripts-2.7.2-2.osg33.el6  osg-cleanup-1.7.2-1.osg33.el6  osg-configure-1.1.1-1.osg33.el6  osg-control-1.0.1-1.osg33.el6  osg-gridftp-3.3-2.osg33.el6  osg-gridftp-hdfs-3.3-2.osg33.el6  osg-gridftp-xrootd-3.3-2.osg33.el6  osg-gums-3.3-2.osg33.el6  osg-info-services-1.0.2-1.osg33.el6  osg-java7-compat-1.0-1.osg33.el6  osg-oasis-5-2.osg33.el6  osg-pki-tools-1.2.12-1.osg33.el6  osg-release-3.3-2.osg33.el6  osg-release-itb-3.3-2.osg33.el6  osg-se-bestman-3.3-2.osg33.el6  osg-se-bestman-xrootd-3.3-2.osg33.el6  osg-se-hadoop-3.3-2.osg33.el6  osg-system-profiler-1.2.0-1.osg33.el6  osg-test-1.4.26-1.osg33.el6  osg-tested-internal-3.3-1.osg33.el6  osg-version-3.3.0-1.osg33.el6  osg-vo-map-0.0.1-1.osg33.el6  osg-voms-3.3-1.osg33.el6  osg-webapp-common-1-2.osg33.el6  osg-wn-client-3.3-5.osg33.el6  owamp-3.2rc4-2.osg33.el6  pegasus-4.3.1-1.3.osg33.el6  privilege-xacml-2.6.4-1.osg33.el6  rsv-3.10.2-1.osg33.el6  rsv-perfsonar-1.0.19-1.osg33.el6  rsv-vo-gwms-1.0.1-1.osg33.el6  stashcache-0.3-4.osg33.el6  stashcache-daemon-0.2-1.osg33.el6  uberftp-2.8-2.1.osg33.el6  vo-client-61-1.osg33.el6  voms-2.0.12-3.osg33.el6  voms-admin-client-2.0.17-1.1.osg33.el6  voms-admin-server-2.7.0-1.14.osg33.el6  voms-api-java-2.0.8-1.6.osg33.el6  voms-mysql-plugin-3.1.6-1.1.osg33.el6  web100_userland-1.7-6.osg33.el6  xacml-1.5.0-1.osg33.el6  xrootd-4.2.2-1.osg33.el6  xrootd-dsi-3.0.4-16.osg33.el6  xrootd-hdfs-1.8.4-4.osg33.el6  xrootd-lcmaps-0.0.7-11.osg33.el6  xrootd-status-probe-0.0.3-11.osg33.el6  xrootd-voms-plugin-0.2.0-1.6.osg33.el6  zookeeper-3.4.3+15-1.cdh4.0.1.p0.3.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-0/#enterprise-linux-7", 
            "text": "bigtop-jsvc-0.3.0-1.1.osg33.el7  bigtop-utils-0.4+300-1.cdh4.0.1.p0.3.osg33.el7  blahp-1.18.13.bosco-3.osg33.el7  bwctl-1.4-7.osg33.el7  cctools-4.4.0-1.osg33.el7  cilogon-openid-ca-cert-1.1-2.osg33.el7  cilogon-osg-ca-cert-1.0-1.osg33.el7  cog-jglobus-axis-1.8.0-8.osg33.el7  condor-8.3.6-1.4.osg33.el7  condor-cron-1.0.9-4.osg33.el7  cvmfs-2.1.20-1.osg33.el7  cvmfs-config-osg-1.1-7.osg33.el7  edg-mkgridmap-4.0.2-1.osg33.el7  emi-trustmanager-3.0.3-8.osg33.el7  emi-trustmanager-axis-1.0.1-1.4.osg33.el7  frontier-squid-2.7.STABLE9-24.1.osg33.el7  gfal2-plugin-xrootd-0.3.pre1-2.1.osg33.el7  gip-1.3.11-6.osg33.el7  glexec-0.9.11-1.1.osg33.el7  glexec-wrapper-scripts-0.0.7-1.1.osg33.el7  glideinwms-3.2.10-1.1.osg33.el7  glite-build-common-cpp-3.3.0.2-1.osg33.el7  glite-lbjp-common-gsoap-plugin-3.1.2-2.2.osg33.el7  glite-lbjp-common-gss-3.1.3-2.2.osg33.el7  globus-authz-3.10-1.osg33.el7  globus-authz-callout-error-3.5-1.osg33.el7  globus-callout-3.13-1.osg33.el7  globus-common-15.27-1.osg33.el7  globus-ftp-client-8.19-1.2.osg33.el7  globus-ftp-control-6.6-1.1.osg33.el7  globus-gass-cache-9.5-1.osg33.el7  globus-gass-cache-program-6.5-1.osg33.el7  globus-gass-copy-9.13-1.osg33.el7  globus-gass-server-ez-5.7-1.osg33.el7  globus-gass-transfer-8.8-1.osg33.el7  globus-gatekeeper-10.9-1.2.osg33.el7  globus-gfork-4.7-1.osg33.el7  globus-gram-audit-4.4-1.osg33.el7  globus-gram-client-13.12-1.osg33.el7  globus-gram-client-tools-11.7-1.osg33.el7  globus-gram-job-manager-14.25-1.2.osg33.el7  globus-gram-job-manager-callout-error-3.5-1.osg33.el7  globus-gram-job-manager-condor-2.5-1.1.osg33.el7  globus-gram-job-manager-fork-2.4-1.1.osg33.el7  globus-gram-job-manager-lsf-2.6-1.2.osg33.el7  globus-gram-job-manager-managedfork-0.2-1.osg33.el7  globus-gram-job-manager-pbs-2.4-2.1.osg33.el7  globus-gram-job-manager-scripts-6.7-1.osg33.el7  globus-gram-job-manager-sge-2.5-1.1.osg33.el7  globus-gram-protocol-12.12-2.osg33.el7  globus-gridftp-server-7.20-1.1.osg33.el7  globus-gridftp-server-control-3.6-1.osg33.el7  globus-gridmap-callout-error-2.4-1.osg33.el7  globus-gsi-callback-5.7-1.osg33.el7  globus-gsi-cert-utils-9.10-1.osg33.el7  globus-gsi-credential-7.7-1.osg33.el7  globus-gsi-openssl-error-3.5-1.osg33.el7  globus-gsi-proxy-core-7.7-1.osg33.el7  globus-gsi-proxy-ssl-5.7-1.osg33.el7  globus-gsi-sysconfig-6.8-1.osg33.el7  globus-gssapi-error-5.4-1.osg33.el7  globus-gssapi-gsi-11.18-1.osg33.el7  globus-gss-assist-10.13-1.osg33.el7  globus-io-11.4-1.osg33.el7  globus-openssl-module-4.6-1.osg33.el7  globus-proxy-utils-6.9-1.osg33.el7  globus-rsl-10.9-1.osg33.el7  globus-scheduler-event-generator-5.10-2.1.osg33.el7  globus-simple-ca-4.18-1.osg33.el7  globus-usage-4.4-1.osg33.el7  globus-xio-5.7-1.1.osg33.el7  globus-xio-gsi-driver-3.7-1.osg33.el7  globus-xioperf-4.4-1.osg33.el7  globus-xio-pipe-driver-3.7-1.osg33.el7  globus-xio-popen-driver-3.5-1.osg33.el7  globus-xio-udt-driver-1.16-1.osg33.el7  gratia-1.16.2-1.osg33.el7  gratia-probe-1.14.2-6.osg33.el7  gratia-reporting-email-1.15.1-1.osg33.el7  gsi-openssh-5.7-1.1.osg33.el7  htcondor-ce-1.14-4.osg33.el7  I2util-1.1-2.osg33.el7  igtf-ca-certs-1.65-1.osg33.el7  javascriptrrd-1.1.1-1.osg33.el7  jetty-8.1.4.v20120524-2.osg33.el7  joda-time-1.5.2-7.2.tzdata2008d.osg33.el7  koji-1.6.0-10.osg33.el7  lcas-lcmaps-gt4-interface-0.2.6-1.1.osg33.el7  lcmaps-1.6.6-1.1.osg33.el7  lcmaps-plugins-basic-1.7.0-2.osg33.el7  lcmaps-plugins-glexec-tracking-0.1.6-1.osg33.el7  lcmaps-plugins-gums-client-0.0.2-4.osg33.el7  lcmaps-plugins-scas-client-0.5.5-1.osg33.el7  lcmaps-plugins-verify-proxy-1.5.7-1.osg33.el7  llrun-0.1.3-1.3.osg33.el7  mash-0.5.22-3.osg33.el7  mkgltempdir-0.0.5-1.1.osg33.el7  myproxy-6.1.12-1.osg33.el7  ndt-3.6.5-3.osg33.el7  netlogger-4.2.0-9.osg33.el7  nuttcp-6.1.2-1.osg33.el7  osg-build-1.6.0-2.osg33.el7  osg-ca-certs-1.47-1.osg33.el7  osg-ca-certs-updater-1.0-1.osg33.el7  osg-ca-scripts-1.1.5-2.osg33.el7  osg-ce-3.3-3_clipped.osg33.el7  osg-cert-scripts-2.7.2-2.osg33.el7  osg-cleanup-1.7.2-1.osg33.el7  osg-configure-1.1.1-1.osg33.el7  osg-control-1.0.1-1.osg33.el7  osg-gridftp-3.3-2_clipped.osg33.el7  osg-gridftp-hdfs-3.3-2.osg33.el7  osg-gridftp-xrootd-3.3-2.osg33.el7  osg-gums-3.3-2.osg33.el7  osg-info-services-1.0.2-1.osg33.el7  osg-java7-compat-1.0-1.osg33.el7  osg-oasis-5-2.osg33.el7  osg-pki-tools-1.2.12-1.osg33.el7  osg-release-3.3-2.osg33.el7  osg-release-itb-3.3-2.osg33.el7  osg-se-bestman-3.3-2.osg33.el7  osg-se-bestman-xrootd-3.3-2.osg33.el7  osg-se-hadoop-3.3-2.osg33.el7  osg-system-profiler-1.2.0-1.osg33.el7  osg-test-1.4.25-1.osg33.el7  osg-tested-internal-3.3-1.osg33.el7  osg-version-3.3.0-1.osg33.el7  osg-vo-map-0.0.1-1.osg33.el7  osg-voms-3.3-1.osg33.el7  osg-webapp-common-1-2.osg33.el7  osg-wn-client-3.3-5.osg33.el7  owamp-3.2rc4-2.osg33.el7  pegasus-4.3.1-1.3.osg33.el7  rsv-3.10.2-1_clipped.osg33.el7  rsv-perfsonar-1.0.19-1.osg33.el7  rsv-vo-gwms-1.0.1-1.osg33.el7  stashcache-0.3-4.osg33.el7  stashcache-daemon-0.2-1.osg33.el7  uberftp-2.8-2.1.osg33.el7  vo-client-61-1.osg33.el7  voms-2.0.12-3.osg33.el7  voms-admin-client-2.0.17-1.1.osg33.el7  voms-api-java-2.0.8-1.6.osg33.el7  voms-mysql-plugin-3.1.6-1.1.osg33.el7  web100_userland-1.7-6.osg33.el7  xacml-1.5.0-1.osg33.el7  xrootd-4.2.2-1.osg33.el7  xrootd-dsi-3.0.4-16.osg33.el7  xrootd-lcmaps-0.0.7-11.osg33.el7  xrootd-status-probe-0.0.3-11.osg33.el7  xrootd-voms-plugin-0.2.0-1.6.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-0/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  bestman2-client bestman2-client-libs bestman2-common-libs bestman2-server bestman2-server-dep-libs bestman2-server-libs bestman2-tester bestman2-tester-libs bigtop-jsvc bigtop-jsvc-debuginfo bigtop-utils blahp blahp-debuginfo bwctl bwctl-client bwctl-debuginfo bwctl-devel bwctl-server cctools-chirp cctools-debuginfo cctools-doc cctools-dttools cctools-makeflow cctools-parrot cctools-resource_monitor cctools-sand cctools-wavefront cctools-work_queue cilogon-openid-ca-cert cilogon-osg-ca-cert cog-jglobus-axis condor condor-all condor-bosco condor-classads condor-classads-devel condor-cream-gahp condor-cron condor-debuginfo condor-kbdd condor-procd condor-python condor-std-universe condor-test condor-vm-gahp cvmfs cvmfs-config-osg cvmfs-devel cvmfs-server cvmfs-unittests edg-mkgridmap emi-trustmanager emi-trustmanager-axis emi-trustmanager-tomcat frontier-squid frontier-squid-debuginfo gfal2-plugin-xrootd gfal2-plugin-xrootd-debuginfo gip glexec glexec-debuginfo glexec-wrapper-scripts glideinwms-factory glideinwms-factory-condor glideinwms-glidecondor-tools glideinwms-libs glideinwms-minimal-condor glideinwms-usercollector glideinwms-userschedd glideinwms-vofrontend glideinwms-vofrontend-standalone glite-build-common-cpp glite-ce-cream-client-api-c glite-ce-cream-client-api-c-debuginfo glite-ce-cream-client-devel glite-ce-cream-utils glite-lbjp-common-gsoap-plugin glite-lbjp-common-gsoap-plugin-debuginfo glite-lbjp-common-gsoap-plugin-devel glite-lbjp-common-gss glite-lbjp-common-gss-debuginfo glite-lbjp-common-gss-devel globus-authz globus-authz-callout-error globus-authz-callout-error-debuginfo globus-authz-callout-error-devel globus-authz-callout-error-doc globus-authz-debuginfo globus-authz-devel globus-authz-doc globus-callout globus-callout-debuginfo globus-callout-devel globus-callout-doc globus-common globus-common-debuginfo globus-common-devel globus-common-doc globus-common-progs globus-ftp-client globus-ftp-client-debuginfo globus-ftp-client-devel globus-ftp-client-doc globus-ftp-control globus-ftp-control-debuginfo globus-ftp-control-devel globus-ftp-control-doc globus-gass-cache globus-gass-cache-debuginfo globus-gass-cache-devel globus-gass-cache-doc globus-gass-cache-program globus-gass-cache-program-debuginfo globus-gass-copy globus-gass-copy-debuginfo globus-gass-copy-devel globus-gass-copy-doc globus-gass-copy-progs globus-gass-server-ez globus-gass-server-ez-debuginfo globus-gass-server-ez-devel globus-gass-server-ez-progs globus-gass-transfer globus-gass-transfer-debuginfo globus-gass-transfer-devel globus-gass-transfer-doc globus-gatekeeper globus-gatekeeper-debuginfo globus-gfork globus-gfork-debuginfo globus-gfork-devel globus-gfork-progs globus-gram-audit globus-gram-client globus-gram-client-debuginfo globus-gram-client-devel globus-gram-client-doc globus-gram-client-tools globus-gram-client-tools-debuginfo globus-gram-job-manager globus-gram-job-manager-callout-error globus-gram-job-manager-callout-error-debuginfo globus-gram-job-manager-callout-error-devel globus-gram-job-manager-callout-error-doc globus-gram-job-manager-condor globus-gram-job-manager-debuginfo globus-gram-job-manager-fork globus-gram-job-manager-fork-debuginfo globus-gram-job-manager-fork-setup-poll globus-gram-job-manager-fork-setup-seg globus-gram-job-manager-lsf globus-gram-job-manager-lsf-debuginfo globus-gram-job-manager-lsf-setup-poll globus-gram-job-manager-lsf-setup-seg globus-gram-job-manager-managedfork globus-gram-job-manager-pbs globus-gram-job-manager-pbs-debuginfo globus-gram-job-manager-pbs-setup-poll globus-gram-job-manager-pbs-setup-seg globus-gram-job-manager-scripts globus-gram-job-manager-scripts-doc globus-gram-job-manager-sge globus-gram-job-manager-sge-debuginfo globus-gram-job-manager-sge-setup-poll globus-gram-job-manager-sge-setup-seg globus-gram-protocol globus-gram-protocol-debuginfo globus-gram-protocol-devel globus-gram-protocol-doc globus-gridftp-server globus-gridftp-server-control globus-gridftp-server-control-debuginfo globus-gridftp-server-control-devel globus-gridftp-server-debuginfo globus-gridftp-server-devel globus-gridftp-server-progs globus-gridmap-callout-error globus-gridmap-callout-error-debuginfo globus-gridmap-callout-error-devel globus-gridmap-callout-error-doc globus-gsi-callback globus-gsi-callback-debuginfo globus-gsi-callback-devel globus-gsi-callback-doc globus-gsi-cert-utils globus-gsi-cert-utils-debuginfo globus-gsi-cert-utils-devel globus-gsi-cert-utils-doc globus-gsi-cert-utils-progs globus-gsi-credential globus-gsi-credential-debuginfo globus-gsi-credential-devel globus-gsi-credential-doc globus-gsi-openssl-error globus-gsi-openssl-error-debuginfo globus-gsi-openssl-error-devel globus-gsi-openssl-error-doc globus-gsi-proxy-core globus-gsi-proxy-core-debuginfo globus-gsi-proxy-core-devel globus-gsi-proxy-core-doc globus-gsi-proxy-ssl globus-gsi-proxy-ssl-debuginfo globus-gsi-proxy-ssl-devel globus-gsi-proxy-ssl-doc globus-gsi-sysconfig globus-gsi-sysconfig-debuginfo globus-gsi-sysconfig-devel globus-gsi-sysconfig-doc globus-gssapi-error globus-gssapi-error-debuginfo globus-gssapi-error-devel globus-gssapi-error-doc globus-gssapi-gsi globus-gssapi-gsi-debuginfo globus-gssapi-gsi-devel globus-gssapi-gsi-doc globus-gss-assist globus-gss-assist-debuginfo globus-gss-assist-devel globus-gss-assist-doc globus-gss-assist-progs globus-io globus-io-debuginfo globus-io-devel globus-openssl-module globus-openssl-module-debuginfo globus-openssl-module-devel globus-openssl-module-doc globus-proxy-utils globus-proxy-utils-debuginfo globus-rsl globus-rsl-debuginfo globus-rsl-devel globus-rsl-doc globus-scheduler-event-generator globus-scheduler-event-generator-debuginfo globus-scheduler-event-generator-devel globus-scheduler-event-generator-doc globus-scheduler-event-generator-progs globus-simple-ca globus-usage globus-usage-debuginfo globus-usage-devel globus-xio globus-xio-debuginfo globus-xio-devel globus-xio-doc globus-xio-gsi-driver globus-xio-gsi-driver-debuginfo globus-xio-gsi-driver-devel globus-xio-gsi-driver-doc globus-xioperf globus-xioperf-debuginfo globus-xio-pipe-driver globus-xio-pipe-driver-debuginfo globus-xio-pipe-driver-devel globus-xio-popen-driver globus-xio-popen-driver-debuginfo globus-xio-popen-driver-devel globus-xio-udt-driver globus-xio-udt-driver-debuginfo globus-xio-udt-driver-devel gratia-debuginfo gratia-probe-bdii-status gratia-probe-common gratia-probe-condor gratia-probe-condor-events gratia-probe-dcache-storage gratia-probe-dcache-storagegroup gratia-probe-dcache-transfer gratia-probe-debuginfo gratia-probe-enstore-storage gratia-probe-enstore-tapedrive gratia-probe-enstore-transfer gratia-probe-glexec gratia-probe-glideinwms gratia-probe-gram gratia-probe-gridftp-transfer gratia-probe-hadoop-storage gratia-probe-lsf gratia-probe-metric gratia-probe-onevm gratia-probe-pbs-lsf gratia-probe-psacct gratia-probe-services gratia-probe-sge gratia-probe-slurm gratia-probe-xrootd-storage gratia-probe-xrootd-transfer gratia-reporting-email gratia-service gridftp-hdfs gridftp-hdfs-debuginfo gsi-openssh gsi-openssh-clients gsi-openssh-debuginfo gsi-openssh-server gums gums-client gums-service hadoop hadoop-client hadoop-conf-pseudo hadoop-debuginfo hadoop-doc hadoop-hdfs hadoop-hdfs-datanode hadoop-hdfs-fuse hadoop-hdfs-fuse-selinux hadoop-hdfs-journalnode hadoop-hdfs-namenode hadoop-hdfs-secondarynamenode hadoop-hdfs-zkfc hadoop-httpfs hadoop-libhdfs hadoop-mapreduce hadoop-yarn htcondor-ce htcondor-ce-client htcondor-ce-collector htcondor-ce-condor htcondor-ce-debuginfo htcondor-ce-lsf htcondor-ce-pbs htcondor-ce-sge I2util I2util-debuginfo igtf-ca-certs javascriptrrd jetty jetty-ajp jetty-annotations jetty-client jetty-continuation jetty-deploy jetty-http jetty-io jetty-jmx jetty-jndi jetty-overlay-deployer jetty-plus jetty-policy jetty-rewrite jetty-security jetty-server jetty-servlet jetty-servlets jetty-util jetty-webapp jetty-websocket jetty-xml jglobus joda-time joda-time-javadoc koji koji-builder koji-hub koji-hub-plugins koji-utils koji-vm koji-web lcas-lcmaps-gt4-interface lcas-lcmaps-gt4-interface-debuginfo lcmaps lcmaps-common-devel lcmaps-debuginfo lcmaps-devel lcmaps-plugins-basic lcmaps-plugins-basic-debuginfo lcmaps-plugins-basic-ldap lcmaps-plugins-glexec-tracking lcmaps-plugins-glexec-tracking-debuginfo lcmaps-plugins-gums-client lcmaps-plugins-scas-client lcmaps-plugins-scas-client-debuginfo lcmaps-plugins-verify-proxy lcmaps-plugins-verify-proxy-debuginfo lcmaps-without-gsi lcmaps-without-gsi-devel llrun llrun-debuginfo mash mkgltempdir myproxy myproxy-admin myproxy-debuginfo myproxy-devel myproxy-doc myproxy-libs myproxy-server myproxy-voms ndt ndt-client ndt-debuginfo ndt-server netlogger nuttcp nuttcp-debuginfo osg-base-ce osg-base-ce-condor osg-base-ce-lsf osg-base-ce-pbs osg-base-ce-sge osg-base-ce-slurm osg-build osg-ca-certs osg-ca-certs-updater osg-ca-scripts osg-ce osg-ce-condor osg-ce-lsf osg-ce-pbs osg-cert-scripts osg-ce-sge osg-ce-slurm osg-cleanup osg-configure osg-configure-ce osg-configure-cemon osg-configure-condor osg-configure-gateway osg-configure-gip osg-configure-gratia osg-configure-infoservices osg-configure-lsf osg-configure-managedfork osg-configure-misc osg-configure-monalisa osg-configure-network osg-configure-pbs osg-configure-rsv osg-configure-sge osg-configure-slurm osg-configure-squid osg-configure-tests osg-control osg-gridftp osg-gridftp-hdfs osg-gridftp-xrootd osg-gums osg-gums-config osg-htcondor-ce osg-htcondor-ce-condor osg-htcondor-ce-lsf osg-htcondor-ce-pbs osg-htcondor-ce-sge osg-htcondor-ce-slurm osg-info-services osg-java7-compat osg-java7-compat-openjdk osg-java7-devel-compat osg-java7-devel-compat-openjdk osg-oasis osg-pki-tools osg-pki-tools-tests osg-release osg-release-itb osg-se-bestman osg-se-bestman-xrootd osg-se-hadoop osg-se-hadoop-client osg-se-hadoop-datanode osg-se-hadoop-gridftp osg-se-hadoop-namenode osg-se-hadoop-secondarynamenode osg-se-hadoop-srm osg-system-profiler osg-system-profiler-viewer osg-test osg-tested-internal osg-version osg-vo-map osg-voms osg-webapp-common osg-wn-client osg-wn-client-glexec owamp owamp-client owamp-debuginfo owamp-server pegasus pegasus-debuginfo privilege-xacml rsv rsv-consumers rsv-core rsv-metrics rsv-perfsonar rsv-vo-gwms stashcache-cache-server stashcache-daemon stashcache-origin-server uberftp uberftp-debuginfo vo-client vo-client-edgmkgridmap voms voms-admin-client voms-admin-server voms-api-java voms-api-java-javadoc voms-clients-cpp voms-debuginfo voms-devel voms-doc voms-mysql-plugin voms-mysql-plugin-debuginfo voms-server web100_userland web100_userland-debuginfo xacml xacml-debuginfo xacml-devel xrootd xrootd-client xrootd-client-devel xrootd-client-libs xrootd-debuginfo xrootd-devel xrootd-doc xrootd-dsi xrootd-dsi-debuginfo xrootd-fuse xrootd-hdfs xrootd-hdfs-debuginfo xrootd-hdfs-devel xrootd-lcmaps xrootd-lcmaps-debuginfo xrootd-libs xrootd-private-devel xrootd-python xrootd-selinux xrootd-server xrootd-server-devel xrootd-server-libs xrootd-status-probe xrootd-status-probe-debuginfo xrootd-voms-plugin xrootd-voms-plugin-debuginfo xrootd-voms-plugin-devel zookeeper zookeeper-server  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-0/#enterprise-linux-6_1", 
            "text": "bestman2-2.3.0-25.osg33.el6\nbestman2-client-2.3.0-25.osg33.el6\nbestman2-client-libs-2.3.0-25.osg33.el6\nbestman2-common-libs-2.3.0-25.osg33.el6\nbestman2-server-2.3.0-25.osg33.el6\nbestman2-server-dep-libs-2.3.0-25.osg33.el6\nbestman2-server-libs-2.3.0-25.osg33.el6\nbestman2-tester-2.3.0-25.osg33.el6\nbestman2-tester-libs-2.3.0-25.osg33.el6\nbigtop-jsvc-0.3.0-1.1.osg33.el6\nbigtop-jsvc-debuginfo-0.3.0-1.1.osg33.el6\nbigtop-utils-0.4+300-1.cdh4.0.1.p0.3.osg33.el6\nblahp-1.18.13.bosco-3.osg33.el6\nblahp-debuginfo-1.18.13.bosco-3.osg33.el6\nbwctl-1.4-7.osg33.el6\nbwctl-client-1.4-7.osg33.el6\nbwctl-debuginfo-1.4-7.osg33.el6\nbwctl-devel-1.4-7.osg33.el6\nbwctl-server-1.4-7.osg33.el6\ncctools-4.4.0-1.osg33.el6\ncctools-chirp-4.4.0-1.osg33.el6\ncctools-debuginfo-4.4.0-1.osg33.el6\ncctools-doc-4.4.0-1.osg33.el6\ncctools-dttools-4.4.0-1.osg33.el6\ncctools-makeflow-4.4.0-1.osg33.el6\ncctools-parrot-4.4.0-1.osg33.el6\ncctools-resource_monitor-4.4.0-1.osg33.el6\ncctools-sand-4.4.0-1.osg33.el6\ncctools-wavefront-4.4.0-1.osg33.el6\ncctools-work_queue-4.4.0-1.osg33.el6\ncilogon-openid-ca-cert-1.1-2.osg33.el6\ncilogon-osg-ca-cert-1.0-1.osg33.el6\ncog-jglobus-axis-1.8.0-7.osg33.el6\ncondor-8.3.6-1.4.osg33.el6\ncondor-all-8.3.6-1.4.osg33.el6\ncondor-bosco-8.3.6-1.4.osg33.el6\ncondor-classads-8.3.6-1.4.osg33.el6\ncondor-classads-devel-8.3.6-1.4.osg33.el6\ncondor-cream-gahp-8.3.6-1.4.osg33.el6\ncondor-cron-1.0.9-4.osg33.el6\ncondor-debuginfo-8.3.6-1.4.osg33.el6\ncondor-kbdd-8.3.6-1.4.osg33.el6\ncondor-procd-8.3.6-1.4.osg33.el6\ncondor-python-8.3.6-1.4.osg33.el6\ncondor-std-universe-8.3.6-1.4.osg33.el6\ncondor-test-8.3.6-1.4.osg33.el6\ncondor-vm-gahp-8.3.6-1.4.osg33.el6\ncvmfs-2.1.20-1.osg33.el6\ncvmfs-config-osg-1.1-7.osg33.el6\ncvmfs-devel-2.1.20-1.osg33.el6\ncvmfs-server-2.1.20-1.osg33.el6\ncvmfs-unittests-2.1.20-1.osg33.el6\nedg-mkgridmap-4.0.2-1.osg33.el6\nemi-trustmanager-3.0.3-8.osg33.el6\nemi-trustmanager-axis-1.0.1-1.4.osg33.el6\nemi-trustmanager-tomcat-3.0.0-10.osg33.el6\nfrontier-squid-2.7.STABLE9-24.1.osg33.el6\nfrontier-squid-debuginfo-2.7.STABLE9-24.1.osg33.el6\ngfal2-plugin-xrootd-0.3.pre1-2.1.osg33.el6\ngfal2-plugin-xrootd-debuginfo-0.3.pre1-2.1.osg33.el6\ngip-1.3.11-6.osg33.el6\nglexec-0.9.11-1.1.osg33.el6\nglexec-debuginfo-0.9.11-1.1.osg33.el6\nglexec-wrapper-scripts-0.0.7-1.1.osg33.el6\nglideinwms-3.2.10-1.1.osg33.el6\nglideinwms-factory-3.2.10-1.1.osg33.el6\nglideinwms-factory-condor-3.2.10-1.1.osg33.el6\nglideinwms-glidecondor-tools-3.2.10-1.1.osg33.el6\nglideinwms-libs-3.2.10-1.1.osg33.el6\nglideinwms-minimal-condor-3.2.10-1.1.osg33.el6\nglideinwms-usercollector-3.2.10-1.1.osg33.el6\nglideinwms-userschedd-3.2.10-1.1.osg33.el6\nglideinwms-vofrontend-3.2.10-1.1.osg33.el6\nglideinwms-vofrontend-standalone-3.2.10-1.1.osg33.el6\nglite-build-common-cpp-3.3.0.2-1.osg33.el6\nglite-ce-cream-client-api-c-1.14.0-4.10.osg33.el6\nglite-ce-cream-client-api-c-debuginfo-1.14.0-4.10.osg33.el6\nglite-ce-cream-client-devel-1.14.0-4.10.osg33.el6\nglite-ce-cream-utils-1.2.0-4.3.osg33.el6\nglite-lbjp-common-gsoap-plugin-3.1.2-2.2.osg33.el6\nglite-lbjp-common-gsoap-plugin-debuginfo-3.1.2-2.2.osg33.el6\nglite-lbjp-common-gsoap-plugin-devel-3.1.2-2.2.osg33.el6\nglite-lbjp-common-gss-3.1.3-2.2.osg33.el6\nglite-lbjp-common-gss-debuginfo-3.1.3-2.2.osg33.el6\nglite-lbjp-common-gss-devel-3.1.3-2.2.osg33.el6\nglobus-authz-3.10-1.osg33.el6\nglobus-authz-callout-error-3.5-1.osg33.el6\nglobus-authz-callout-error-debuginfo-3.5-1.osg33.el6\nglobus-authz-callout-error-devel-3.5-1.osg33.el6\nglobus-authz-callout-error-doc-3.5-1.osg33.el6\nglobus-authz-debuginfo-3.10-1.osg33.el6\nglobus-authz-devel-3.10-1.osg33.el6\nglobus-authz-doc-3.10-1.osg33.el6\nglobus-callout-3.13-1.osg33.el6\nglobus-callout-debuginfo-3.13-1.osg33.el6\nglobus-callout-devel-3.13-1.osg33.el6\nglobus-callout-doc-3.13-1.osg33.el6\nglobus-common-15.27-1.osg33.el6\nglobus-common-debuginfo-15.27-1.osg33.el6\nglobus-common-devel-15.27-1.osg33.el6\nglobus-common-doc-15.27-1.osg33.el6\nglobus-common-progs-15.27-1.osg33.el6\nglobus-ftp-client-8.19-1.2.osg33.el6\nglobus-ftp-client-debuginfo-8.19-1.2.osg33.el6\nglobus-ftp-client-devel-8.19-1.2.osg33.el6\nglobus-ftp-client-doc-8.19-1.2.osg33.el6\nglobus-ftp-control-6.6-1.1.osg33.el6\nglobus-ftp-control-debuginfo-6.6-1.1.osg33.el6\nglobus-ftp-control-devel-6.6-1.1.osg33.el6\nglobus-ftp-control-doc-6.6-1.1.osg33.el6\nglobus-gass-cache-9.5-1.osg33.el6\nglobus-gass-cache-debuginfo-9.5-1.osg33.el6\nglobus-gass-cache-devel-9.5-1.osg33.el6\nglobus-gass-cache-doc-9.5-1.osg33.el6\nglobus-gass-cache-program-6.5-1.osg33.el6\nglobus-gass-cache-program-debuginfo-6.5-1.osg33.el6\nglobus-gass-copy-9.13-1.osg33.el6\nglobus-gass-copy-debuginfo-9.13-1.osg33.el6\nglobus-gass-copy-devel-9.13-1.osg33.el6\nglobus-gass-copy-doc-9.13-1.osg33.el6\nglobus-gass-copy-progs-9.13-1.osg33.el6\nglobus-gass-server-ez-5.7-1.osg33.el6\nglobus-gass-server-ez-debuginfo-5.7-1.osg33.el6\nglobus-gass-server-ez-devel-5.7-1.osg33.el6\nglobus-gass-server-ez-progs-5.7-1.osg33.el6\nglobus-gass-transfer-8.8-1.osg33.el6\nglobus-gass-transfer-debuginfo-8.8-1.osg33.el6\nglobus-gass-transfer-devel-8.8-1.osg33.el6\nglobus-gass-transfer-doc-8.8-1.osg33.el6\nglobus-gatekeeper-10.9-1.2.osg33.el6\nglobus-gatekeeper-debuginfo-10.9-1.2.osg33.el6\nglobus-gfork-4.7-1.osg33.el6\nglobus-gfork-debuginfo-4.7-1.osg33.el6\nglobus-gfork-devel-4.7-1.osg33.el6\nglobus-gfork-progs-4.7-1.osg33.el6\nglobus-gram-audit-4.4-1.osg33.el6\nglobus-gram-client-13.12-1.osg33.el6\nglobus-gram-client-debuginfo-13.12-1.osg33.el6\nglobus-gram-client-devel-13.12-1.osg33.el6\nglobus-gram-client-doc-13.12-1.osg33.el6\nglobus-gram-client-tools-11.7-1.osg33.el6\nglobus-gram-client-tools-debuginfo-11.7-1.osg33.el6\nglobus-gram-job-manager-14.25-1.2.osg33.el6\nglobus-gram-job-manager-callout-error-3.5-1.osg33.el6\nglobus-gram-job-manager-callout-error-debuginfo-3.5-1.osg33.el6\nglobus-gram-job-manager-callout-error-devel-3.5-1.osg33.el6\nglobus-gram-job-manager-callout-error-doc-3.5-1.osg33.el6\nglobus-gram-job-manager-condor-2.5-1.1.osg33.el6\nglobus-gram-job-manager-debuginfo-14.25-1.2.osg33.el6\nglobus-gram-job-manager-fork-2.4-1.1.osg33.el6\nglobus-gram-job-manager-fork-debuginfo-2.4-1.1.osg33.el6\nglobus-gram-job-manager-fork-setup-poll-2.4-1.1.osg33.el6\nglobus-gram-job-manager-fork-setup-seg-2.4-1.1.osg33.el6\nglobus-gram-job-manager-lsf-2.6-1.2.osg33.el6\nglobus-gram-job-manager-lsf-debuginfo-2.6-1.2.osg33.el6\nglobus-gram-job-manager-lsf-setup-poll-2.6-1.2.osg33.el6\nglobus-gram-job-manager-lsf-setup-seg-2.6-1.2.osg33.el6\nglobus-gram-job-manager-managedfork-0.2-1.osg33.el6\nglobus-gram-job-manager-pbs-2.4-2.1.osg33.el6\nglobus-gram-job-manager-pbs-debuginfo-2.4-2.1.osg33.el6\nglobus-gram-job-manager-pbs-setup-poll-2.4-2.1.osg33.el6\nglobus-gram-job-manager-pbs-setup-seg-2.4-2.1.osg33.el6\nglobus-gram-job-manager-scripts-6.7-1.osg33.el6\nglobus-gram-job-manager-scripts-doc-6.7-1.osg33.el6\nglobus-gram-job-manager-sge-2.5-1.1.osg33.el6\nglobus-gram-job-manager-sge-debuginfo-2.5-1.1.osg33.el6\nglobus-gram-job-manager-sge-setup-poll-2.5-1.1.osg33.el6\nglobus-gram-job-manager-sge-setup-seg-2.5-1.1.osg33.el6\nglobus-gram-protocol-12.12-2.osg33.el6\nglobus-gram-protocol-debuginfo-12.12-2.osg33.el6\nglobus-gram-protocol-devel-12.12-2.osg33.el6\nglobus-gram-protocol-doc-12.12-2.osg33.el6\nglobus-gridftp-server-7.20-1.1.osg33.el6\nglobus-gridftp-server-control-3.6-1.osg33.el6\nglobus-gridftp-server-control-debuginfo-3.6-1.osg33.el6\nglobus-gridftp-server-control-devel-3.6-1.osg33.el6\nglobus-gridftp-server-debuginfo-7.20-1.1.osg33.el6\nglobus-gridftp-server-devel-7.20-1.1.osg33.el6\nglobus-gridftp-server-progs-7.20-1.1.osg33.el6\nglobus-gridmap-callout-error-2.4-1.osg33.el6\nglobus-gridmap-callout-error-debuginfo-2.4-1.osg33.el6\nglobus-gridmap-callout-error-devel-2.4-1.osg33.el6\nglobus-gridmap-callout-error-doc-2.4-1.osg33.el6\nglobus-gsi-callback-5.7-1.osg33.el6\nglobus-gsi-callback-debuginfo-5.7-1.osg33.el6\nglobus-gsi-callback-devel-5.7-1.osg33.el6\nglobus-gsi-callback-doc-5.7-1.osg33.el6\nglobus-gsi-cert-utils-9.10-1.osg33.el6\nglobus-gsi-cert-utils-debuginfo-9.10-1.osg33.el6\nglobus-gsi-cert-utils-devel-9.10-1.osg33.el6\nglobus-gsi-cert-utils-doc-9.10-1.osg33.el6\nglobus-gsi-cert-utils-progs-9.10-1.osg33.el6\nglobus-gsi-credential-7.7-1.osg33.el6\nglobus-gsi-credential-debuginfo-7.7-1.osg33.el6\nglobus-gsi-credential-devel-7.7-1.osg33.el6\nglobus-gsi-credential-doc-7.7-1.osg33.el6\nglobus-gsi-openssl-error-3.5-1.osg33.el6\nglobus-gsi-openssl-error-debuginfo-3.5-1.osg33.el6\nglobus-gsi-openssl-error-devel-3.5-1.osg33.el6\nglobus-gsi-openssl-error-doc-3.5-1.osg33.el6\nglobus-gsi-proxy-core-7.7-1.osg33.el6\nglobus-gsi-proxy-core-debuginfo-7.7-1.osg33.el6\nglobus-gsi-proxy-core-devel-7.7-1.osg33.el6\nglobus-gsi-proxy-core-doc-7.7-1.osg33.el6\nglobus-gsi-proxy-ssl-5.7-1.osg33.el6\nglobus-gsi-proxy-ssl-debuginfo-5.7-1.osg33.el6\nglobus-gsi-proxy-ssl-devel-5.7-1.osg33.el6\nglobus-gsi-proxy-ssl-doc-5.7-1.osg33.el6\nglobus-gsi-sysconfig-6.8-1.osg33.el6\nglobus-gsi-sysconfig-debuginfo-6.8-1.osg33.el6\nglobus-gsi-sysconfig-devel-6.8-1.osg33.el6\nglobus-gsi-sysconfig-doc-6.8-1.osg33.el6\nglobus-gssapi-error-5.4-1.osg33.el6\nglobus-gssapi-error-debuginfo-5.4-1.osg33.el6\nglobus-gssapi-error-devel-5.4-1.osg33.el6\nglobus-gssapi-error-doc-5.4-1.osg33.el6\nglobus-gssapi-gsi-11.18-1.osg33.el6\nglobus-gssapi-gsi-debuginfo-11.18-1.osg33.el6\nglobus-gssapi-gsi-devel-11.18-1.osg33.el6\nglobus-gssapi-gsi-doc-11.18-1.osg33.el6\nglobus-gss-assist-10.13-1.osg33.el6\nglobus-gss-assist-debuginfo-10.13-1.osg33.el6\nglobus-gss-assist-devel-10.13-1.osg33.el6\nglobus-gss-assist-doc-10.13-1.osg33.el6\nglobus-gss-assist-progs-10.13-1.osg33.el6\nglobus-io-11.4-1.osg33.el6\nglobus-io-debuginfo-11.4-1.osg33.el6\nglobus-io-devel-11.4-1.osg33.el6\nglobus-openssl-module-4.6-1.osg33.el6\nglobus-openssl-module-debuginfo-4.6-1.osg33.el6\nglobus-openssl-module-devel-4.6-1.osg33.el6\nglobus-openssl-module-doc-4.6-1.osg33.el6\nglobus-proxy-utils-6.9-1.osg33.el6\nglobus-proxy-utils-debuginfo-6.9-1.osg33.el6\nglobus-rsl-10.9-1.osg33.el6\nglobus-rsl-debuginfo-10.9-1.osg33.el6\nglobus-rsl-devel-10.9-1.osg33.el6\nglobus-rsl-doc-10.9-1.osg33.el6\nglobus-scheduler-event-generator-5.10-2.1.osg33.el6\nglobus-scheduler-event-generator-debuginfo-5.10-2.1.osg33.el6\nglobus-scheduler-event-generator-devel-5.10-2.1.osg33.el6\nglobus-scheduler-event-generator-doc-5.10-2.1.osg33.el6\nglobus-scheduler-event-generator-progs-5.10-2.1.osg33.el6\nglobus-simple-ca-4.18-1.osg33.el6\nglobus-usage-4.4-1.osg33.el6\nglobus-usage-debuginfo-4.4-1.osg33.el6\nglobus-usage-devel-4.4-1.osg33.el6\nglobus-xio-5.7-1.1.osg33.el6\nglobus-xio-debuginfo-5.7-1.1.osg33.el6\nglobus-xio-devel-5.7-1.1.osg33.el6\nglobus-xio-doc-5.7-1.1.osg33.el6\nglobus-xio-gsi-driver-3.7-1.osg33.el6\nglobus-xio-gsi-driver-debuginfo-3.7-1.osg33.el6\nglobus-xio-gsi-driver-devel-3.7-1.osg33.el6\nglobus-xio-gsi-driver-doc-3.7-1.osg33.el6\nglobus-xioperf-4.4-1.osg33.el6\nglobus-xioperf-debuginfo-4.4-1.osg33.el6\nglobus-xio-pipe-driver-3.7-1.osg33.el6\nglobus-xio-pipe-driver-debuginfo-3.7-1.osg33.el6\nglobus-xio-pipe-driver-devel-3.7-1.osg33.el6\nglobus-xio-popen-driver-3.5-1.osg33.el6\nglobus-xio-popen-driver-debuginfo-3.5-1.osg33.el6\nglobus-xio-popen-driver-devel-3.5-1.osg33.el6\nglobus-xio-udt-driver-1.16-1.osg33.el6\nglobus-xio-udt-driver-debuginfo-1.16-1.osg33.el6\nglobus-xio-udt-driver-devel-1.16-1.osg33.el6\ngratia-1.16.2-1.osg33.el6\ngratia-debuginfo-1.16.2-1.osg33.el6\ngratia-probe-1.14.2-6.osg33.el6\ngratia-probe-bdii-status-1.14.2-6.osg33.el6\ngratia-probe-common-1.14.2-6.osg33.el6\ngratia-probe-condor-1.14.2-6.osg33.el6\ngratia-probe-condor-events-1.14.2-6.osg33.el6\ngratia-probe-dcache-storage-1.14.2-6.osg33.el6\ngratia-probe-dcache-storagegroup-1.14.2-6.osg33.el6\ngratia-probe-dcache-transfer-1.14.2-6.osg33.el6\ngratia-probe-debuginfo-1.14.2-6.osg33.el6\ngratia-probe-enstore-storage-1.14.2-6.osg33.el6\ngratia-probe-enstore-tapedrive-1.14.2-6.osg33.el6\ngratia-probe-enstore-transfer-1.14.2-6.osg33.el6\ngratia-probe-glexec-1.14.2-6.osg33.el6\ngratia-probe-glideinwms-1.14.2-6.osg33.el6\ngratia-probe-gram-1.14.2-6.osg33.el6\ngratia-probe-gridftp-transfer-1.14.2-6.osg33.el6\ngratia-probe-hadoop-storage-1.14.2-6.osg33.el6\ngratia-probe-lsf-1.14.2-6.osg33.el6\ngratia-probe-metric-1.14.2-6.osg33.el6\ngratia-probe-onevm-1.14.2-6.osg33.el6\ngratia-probe-pbs-lsf-1.14.2-6.osg33.el6\ngratia-probe-psacct-1.14.2-6.osg33.el6\ngratia-probe-services-1.14.2-6.osg33.el6\ngratia-probe-sge-1.14.2-6.osg33.el6\ngratia-probe-slurm-1.14.2-6.osg33.el6\ngratia-probe-xrootd-storage-1.14.2-6.osg33.el6\ngratia-probe-xrootd-transfer-1.14.2-6.osg33.el6\ngratia-reporting-email-1.15.1-1.osg33.el6\ngratia-service-1.16.2-1.osg33.el6\ngridftp-hdfs-0.5.4-19.osg33.el6\ngridftp-hdfs-debuginfo-0.5.4-19.osg33.el6\ngsi-openssh-5.7-1.1.osg33.el6\ngsi-openssh-clients-5.7-1.1.osg33.el6\ngsi-openssh-debuginfo-5.7-1.1.osg33.el6\ngsi-openssh-server-5.7-1.1.osg33.el6\ngums-1.4.4-3.osg33.el6\ngums-client-1.4.4-3.osg33.el6\ngums-service-1.4.4-3.osg33.el6\nhadoop-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\nhadoop-client-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\nhadoop-conf-pseudo-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\nhadoop-debuginfo-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\nhadoop-doc-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\nhadoop-hdfs-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\nhadoop-hdfs-datanode-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\nhadoop-hdfs-fuse-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\nhadoop-hdfs-fuse-selinux-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\nhadoop-hdfs-journalnode-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\nhadoop-hdfs-namenode-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\nhadoop-hdfs-secondarynamenode-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\nhadoop-hdfs-zkfc-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\nhadoop-httpfs-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\nhadoop-libhdfs-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\nhadoop-mapreduce-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\nhadoop-yarn-2.0.0+545-1.cdh4.1.1.p0.20.osg33.el6\nhtcondor-ce-1.14-4.osg33.el6\nhtcondor-ce-client-1.14-4.osg33.el6\nhtcondor-ce-collector-1.14-4.osg33.el6\nhtcondor-ce-condor-1.14-4.osg33.el6\nhtcondor-ce-debuginfo-1.14-4.osg33.el6\nhtcondor-ce-lsf-1.14-4.osg33.el6\nhtcondor-ce-pbs-1.14-4.osg33.el6\nhtcondor-ce-sge-1.14-4.osg33.el6\nI2util-1.1-2.osg33.el6\nI2util-debuginfo-1.1-2.osg33.el6\nigtf-ca-certs-1.65-1.osg33.el6\njavascriptrrd-1.1.1-1.osg33.el6\njetty-8.1.4.v20120524-2.osg33.el6\njetty-ajp-8.1.4.v20120524-2.osg33.el6\njetty-annotations-8.1.4.v20120524-2.osg33.el6\njetty-client-8.1.4.v20120524-2.osg33.el6\njetty-continuation-8.1.4.v20120524-2.osg33.el6\njetty-deploy-8.1.4.v20120524-2.osg33.el6\njetty-http-8.1.4.v20120524-2.osg33.el6\njetty-io-8.1.4.v20120524-2.osg33.el6\njetty-jmx-8.1.4.v20120524-2.osg33.el6\njetty-jndi-8.1.4.v20120524-2.osg33.el6\njetty-overlay-deployer-8.1.4.v20120524-2.osg33.el6\njetty-plus-8.1.4.v20120524-2.osg33.el6\njetty-policy-8.1.4.v20120524-2.osg33.el6\njetty-rewrite-8.1.4.v20120524-2.osg33.el6\njetty-security-8.1.4.v20120524-2.osg33.el6\njetty-server-8.1.4.v20120524-2.osg33.el6\njetty-servlet-8.1.4.v20120524-2.osg33.el6\njetty-servlets-8.1.4.v20120524-2.osg33.el6\njetty-util-8.1.4.v20120524-2.osg33.el6\njetty-webapp-8.1.4.v20120524-2.osg33.el6\njetty-websocket-8.1.4.v20120524-2.osg33.el6\njetty-xml-8.1.4.v20120524-2.osg33.el6\njglobus-2.0.6-4.osg33.el6\njoda-time-1.5.2-7.2.tzdata2008d.osg33.el6\njoda-time-javadoc-1.5.2-7.2.tzdata2008d.osg33.el6\nkoji-1.6.0-10.osg33.el6\nkoji-builder-1.6.0-10.osg33.el6\nkoji-hub-1.6.0-10.osg33.el6\nkoji-hub-plugins-1.6.0-10.osg33.el6\nkoji-utils-1.6.0-10.osg33.el6\nkoji-vm-1.6.0-10.osg33.el6\nkoji-web-1.6.0-10.osg33.el6\nlcas-lcmaps-gt4-interface-0.2.6-1.1.osg33.el6\nlcas-lcmaps-gt4-interface-debuginfo-0.2.6-1.1.osg33.el6\nlcmaps-1.6.6-1.1.osg33.el6\nlcmaps-common-devel-1.6.6-1.1.osg33.el6\nlcmaps-debuginfo-1.6.6-1.1.osg33.el6\nlcmaps-devel-1.6.6-1.1.osg33.el6\nlcmaps-plugins-basic-1.7.0-2.osg33.el6\nlcmaps-plugins-basic-debuginfo-1.7.0-2.osg33.el6\nlcmaps-plugins-basic-ldap-1.7.0-2.osg33.el6\nlcmaps-plugins-glexec-tracking-0.1.6-1.osg33.el6\nlcmaps-plugins-glexec-tracking-debuginfo-0.1.6-1.osg33.el6\nlcmaps-plugins-gums-client-0.0.2-4.osg33.el6\nlcmaps-plugins-scas-client-0.5.5-1.osg33.el6\nlcmaps-plugins-scas-client-debuginfo-0.5.5-1.osg33.el6\nlcmaps-plugins-verify-proxy-1.5.7-1.osg33.el6\nlcmaps-plugins-verify-proxy-debuginfo-1.5.7-1.osg33.el6\nlcmaps-without-gsi-1.6.6-1.1.osg33.el6\nlcmaps-without-gsi-devel-1.6.6-1.1.osg33.el6\nllrun-0.1.3-1.3.osg33.el6\nllrun-debuginfo-0.1.3-1.3.osg33.el6\nmash-0.5.22-3.osg33.el6\nmkgltempdir-0.0.5-1.1.osg33.el6\nmyproxy-6.1.12-1.osg33.el6\nmyproxy-admin-6.1.12-1.osg33.el6\nmyproxy-debuginfo-6.1.12-1.osg33.el6\nmyproxy-devel-6.1.12-1.osg33.el6\nmyproxy-doc-6.1.12-1.osg33.el6\nmyproxy-libs-6.1.12-1.osg33.el6\nmyproxy-server-6.1.12-1.osg33.el6\nmyproxy-voms-6.1.12-1.osg33.el6\nndt-3.6.5-3.osg33.el6\nndt-client-3.6.5-3.osg33.el6\nndt-debuginfo-3.6.5-3.osg33.el6\nndt-server-3.6.5-3.osg33.el6\nnetlogger-4.2.0-9.osg33.el6\nnuttcp-6.1.2-1.osg33.el6\nnuttcp-debuginfo-6.1.2-1.osg33.el6\nosg-base-ce-3.3-3.osg33.el6\nosg-base-ce-condor-3.3-3.osg33.el6\nosg-base-ce-lsf-3.3-3.osg33.el6\nosg-base-ce-pbs-3.3-3.osg33.el6\nosg-base-ce-sge-3.3-3.osg33.el6\nosg-base-ce-slurm-3.3-3.osg33.el6\nosg-build-1.6.0-2.osg33.el6\nosg-ca-certs-1.47-1.osg33.el6\nosg-ca-certs-updater-1.0-1.osg33.el6\nosg-ca-scripts-1.1.5-2.osg33.el6\nosg-ce-3.3-3.osg33.el6\nosg-ce-condor-3.3-3.osg33.el6\nosg-ce-lsf-3.3-3.osg33.el6\nosg-ce-pbs-3.3-3.osg33.el6\nosg-cert-scripts-2.7.2-2.osg33.el6\nosg-ce-sge-3.3-3.osg33.el6\nosg-ce-slurm-3.3-3.osg33.el6\nosg-cleanup-1.7.2-1.osg33.el6\nosg-configure-1.1.1-1.osg33.el6\nosg-configure-ce-1.1.1-1.osg33.el6\nosg-configure-cemon-1.1.1-1.osg33.el6\nosg-configure-condor-1.1.1-1.osg33.el6\nosg-configure-gateway-1.1.1-1.osg33.el6\nosg-configure-gip-1.1.1-1.osg33.el6\nosg-configure-gratia-1.1.1-1.osg33.el6\nosg-configure-infoservices-1.1.1-1.osg33.el6\nosg-configure-lsf-1.1.1-1.osg33.el6\nosg-configure-managedfork-1.1.1-1.osg33.el6\nosg-configure-misc-1.1.1-1.osg33.el6\nosg-configure-monalisa-1.1.1-1.osg33.el6\nosg-configure-network-1.1.1-1.osg33.el6\nosg-configure-pbs-1.1.1-1.osg33.el6\nosg-configure-rsv-1.1.1-1.osg33.el6\nosg-configure-sge-1.1.1-1.osg33.el6\nosg-configure-slurm-1.1.1-1.osg33.el6\nosg-configure-squid-1.1.1-1.osg33.el6\nosg-configure-tests-1.1.1-1.osg33.el6\nosg-control-1.0.1-1.osg33.el6\nosg-gridftp-3.3-2.osg33.el6\nosg-gridftp-hdfs-3.3-2.osg33.el6\nosg-gridftp-xrootd-3.3-2.osg33.el6\nosg-gums-3.3-2.osg33.el6\nosg-gums-config-61-1.osg33.el6\nosg-htcondor-ce-3.3-3.osg33.el6\nosg-htcondor-ce-condor-3.3-3.osg33.el6\nosg-htcondor-ce-lsf-3.3-3.osg33.el6\nosg-htcondor-ce-pbs-3.3-3.osg33.el6\nosg-htcondor-ce-sge-3.3-3.osg33.el6\nosg-htcondor-ce-slurm-3.3-3.osg33.el6\nosg-info-services-1.0.2-1.osg33.el6\nosg-java7-compat-1.0-1.osg33.el6\nosg-java7-compat-openjdk-1.0-1.osg33.el6\nosg-java7-devel-compat-1.0-1.osg33.el6\nosg-java7-devel-compat-openjdk-1.0-1.osg33.el6\nosg-oasis-5-2.osg33.el6\nosg-pki-tools-1.2.12-1.osg33.el6\nosg-pki-tools-tests-1.2.12-1.osg33.el6\nosg-release-3.3-2.osg33.el6\nosg-release-itb-3.3-2.osg33.el6\nosg-se-bestman-3.3-2.osg33.el6\nosg-se-bestman-xrootd-3.3-2.osg33.el6\nosg-se-hadoop-3.3-2.osg33.el6\nosg-se-hadoop-client-3.3-2.osg33.el6\nosg-se-hadoop-datanode-3.3-2.osg33.el6\nosg-se-hadoop-gridftp-3.3-2.osg33.el6\nosg-se-hadoop-namenode-3.3-2.osg33.el6\nosg-se-hadoop-secondarynamenode-3.3-2.osg33.el6\nosg-se-hadoop-srm-3.3-2.osg33.el6\nosg-system-profiler-1.2.0-1.osg33.el6\nosg-system-profiler-viewer-1.2.0-1.osg33.el6\nosg-test-1.4.26-1.osg33.el6\nosg-tested-internal-3.3-1.osg33.el6\nosg-version-3.3.0-1.osg33.el6\nosg-vo-map-0.0.1-1.osg33.el6\nosg-voms-3.3-1.osg33.el6\nosg-webapp-common-1-2.osg33.el6\nosg-wn-client-3.3-5.osg33.el6\nosg-wn-client-glexec-3.3-5.osg33.el6\nowamp-3.2rc4-2.osg33.el6\nowamp-client-3.2rc4-2.osg33.el6\nowamp-debuginfo-3.2rc4-2.osg33.el6\nowamp-server-3.2rc4-2.osg33.el6\npegasus-4.3.1-1.3.osg33.el6\npegasus-debuginfo-4.3.1-1.3.osg33.el6\nprivilege-xacml-2.6.4-1.osg33.el6\nrsv-3.10.2-1.osg33.el6\nrsv-consumers-3.10.2-1.osg33.el6\nrsv-core-3.10.2-1.osg33.el6\nrsv-metrics-3.10.2-1.osg33.el6\nrsv-perfsonar-1.0.19-1.osg33.el6\nrsv-vo-gwms-1.0.1-1.osg33.el6\nstashcache-0.3-4.osg33.el6\nstashcache-cache-server-0.3-4.osg33.el6\nstashcache-daemon-0.2-1.osg33.el6\nstashcache-daemon-0.3-4.osg33.el6\nstashcache-origin-server-0.3-4.osg33.el6\nuberftp-2.8-2.1.osg33.el6\nuberftp-debuginfo-2.8-2.1.osg33.el6\nvo-client-61-1.osg33.el6\nvo-client-edgmkgridmap-61-1.osg33.el6\nvoms-2.0.12-3.osg33.el6\nvoms-admin-client-2.0.17-1.1.osg33.el6\nvoms-admin-server-2.7.0-1.14.osg33.el6\nvoms-api-java-2.0.8-1.6.osg33.el6\nvoms-api-java-javadoc-2.0.8-1.6.osg33.el6\nvoms-clients-cpp-2.0.12-3.osg33.el6\nvoms-debuginfo-2.0.12-3.osg33.el6\nvoms-devel-2.0.12-3.osg33.el6\nvoms-doc-2.0.12-3.osg33.el6\nvoms-mysql-plugin-3.1.6-1.1.osg33.el6\nvoms-mysql-plugin-debuginfo-3.1.6-1.1.osg33.el6\nvoms-server-2.0.12-3.osg33.el6\nweb100_userland-1.7-6.osg33.el6\nweb100_userland-debuginfo-1.7-6.osg33.el6\nxacml-1.5.0-1.osg33.el6\nxacml-debuginfo-1.5.0-1.osg33.el6\nxacml-devel-1.5.0-1.osg33.el6\nxrootd-4.2.2-1.osg33.el6\nxrootd-client-4.2.2-1.osg33.el6\nxrootd-client-devel-4.2.2-1.osg33.el6\nxrootd-client-libs-4.2.2-1.osg33.el6\nxrootd-debuginfo-4.2.2-1.osg33.el6\nxrootd-devel-4.2.2-1.osg33.el6\nxrootd-doc-4.2.2-1.osg33.el6\nxrootd-dsi-3.0.4-16.osg33.el6\nxrootd-dsi-debuginfo-3.0.4-16.osg33.el6\nxrootd-fuse-4.2.2-1.osg33.el6\nxrootd-hdfs-1.8.4-4.osg33.el6\nxrootd-hdfs-debuginfo-1.8.4-4.osg33.el6\nxrootd-hdfs-devel-1.8.4-4.osg33.el6\nxrootd-lcmaps-0.0.7-11.osg33.el6\nxrootd-lcmaps-debuginfo-0.0.7-11.osg33.el6\nxrootd-libs-4.2.2-1.osg33.el6\nxrootd-private-devel-4.2.2-1.osg33.el6\nxrootd-python-4.2.2-1.osg33.el6\nxrootd-selinux-4.2.2-1.osg33.el6\nxrootd-server-4.2.2-1.osg33.el6\nxrootd-server-devel-4.2.2-1.osg33.el6\nxrootd-server-libs-4.2.2-1.osg33.el6\nxrootd-status-probe-0.0.3-11.osg33.el6\nxrootd-status-probe-debuginfo-0.0.3-11.osg33.el6\nxrootd-voms-plugin-0.2.0-1.6.osg33.el6\nxrootd-voms-plugin-debuginfo-0.2.0-1.6.osg33.el6\nxrootd-voms-plugin-devel-0.2.0-1.6.osg33.el6\nzookeeper-3.4.3+15-1.cdh4.0.1.p0.3.osg33.el6\nzookeeper-server-3.4.3+15-1.cdh4.0.1.p0.3.osg33.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-0/#enterprise-linux-7_1", 
            "text": "bigtop-jsvc-0.3.0-1.1.osg33.el7\nbigtop-jsvc-debuginfo-0.3.0-1.1.osg33.el7\nbigtop-utils-0.4+300-1.cdh4.0.1.p0.3.osg33.el7\nblahp-1.18.13.bosco-3.osg33.el7\nblahp-debuginfo-1.18.13.bosco-3.osg33.el7\nbwctl-1.4-7.osg33.el7\nbwctl-client-1.4-7.osg33.el7\nbwctl-debuginfo-1.4-7.osg33.el7\nbwctl-devel-1.4-7.osg33.el7\nbwctl-server-1.4-7.osg33.el7\ncctools-4.4.0-1.osg33.el7\ncctools-chirp-4.4.0-1.osg33.el7\ncctools-debuginfo-4.4.0-1.osg33.el7\ncctools-doc-4.4.0-1.osg33.el7\ncctools-dttools-4.4.0-1.osg33.el7\ncctools-makeflow-4.4.0-1.osg33.el7\ncctools-parrot-4.4.0-1.osg33.el7\ncctools-resource_monitor-4.4.0-1.osg33.el7\ncctools-sand-4.4.0-1.osg33.el7\ncctools-wavefront-4.4.0-1.osg33.el7\ncctools-work_queue-4.4.0-1.osg33.el7\ncilogon-openid-ca-cert-1.1-2.osg33.el7\ncilogon-osg-ca-cert-1.0-1.osg33.el7\ncog-jglobus-axis-1.8.0-8.osg33.el7\ncondor-8.3.6-1.4.osg33.el7\ncondor-all-8.3.6-1.4.osg33.el7\ncondor-bosco-8.3.6-1.4.osg33.el7\ncondor-classads-8.3.6-1.4.osg33.el7\ncondor-classads-devel-8.3.6-1.4.osg33.el7\ncondor-cron-1.0.9-4.osg33.el7\ncondor-debuginfo-8.3.6-1.4.osg33.el7\ncondor-kbdd-8.3.6-1.4.osg33.el7\ncondor-procd-8.3.6-1.4.osg33.el7\ncondor-python-8.3.6-1.4.osg33.el7\ncondor-test-8.3.6-1.4.osg33.el7\ncondor-vm-gahp-8.3.6-1.4.osg33.el7\ncvmfs-2.1.20-1.osg33.el7\ncvmfs-config-osg-1.1-7.osg33.el7\ncvmfs-devel-2.1.20-1.osg33.el7\ncvmfs-server-2.1.20-1.osg33.el7\ncvmfs-unittests-2.1.20-1.osg33.el7\nedg-mkgridmap-4.0.2-1.osg33.el7\nemi-trustmanager-3.0.3-8.osg33.el7\nemi-trustmanager-axis-1.0.1-1.4.osg33.el7\nfrontier-squid-2.7.STABLE9-24.1.osg33.el7\nfrontier-squid-debuginfo-2.7.STABLE9-24.1.osg33.el7\ngfal2-plugin-xrootd-0.3.pre1-2.1.osg33.el7\ngfal2-plugin-xrootd-debuginfo-0.3.pre1-2.1.osg33.el7\ngip-1.3.11-6.osg33.el7\nglexec-0.9.11-1.1.osg33.el7\nglexec-debuginfo-0.9.11-1.1.osg33.el7\nglexec-wrapper-scripts-0.0.7-1.1.osg33.el7\nglideinwms-3.2.10-1.1.osg33.el7\nglideinwms-factory-3.2.10-1.1.osg33.el7\nglideinwms-factory-condor-3.2.10-1.1.osg33.el7\nglideinwms-glidecondor-tools-3.2.10-1.1.osg33.el7\nglideinwms-libs-3.2.10-1.1.osg33.el7\nglideinwms-minimal-condor-3.2.10-1.1.osg33.el7\nglideinwms-usercollector-3.2.10-1.1.osg33.el7\nglideinwms-userschedd-3.2.10-1.1.osg33.el7\nglideinwms-vofrontend-3.2.10-1.1.osg33.el7\nglideinwms-vofrontend-standalone-3.2.10-1.1.osg33.el7\nglite-build-common-cpp-3.3.0.2-1.osg33.el7\nglite-lbjp-common-gsoap-plugin-3.1.2-2.2.osg33.el7\nglite-lbjp-common-gsoap-plugin-debuginfo-3.1.2-2.2.osg33.el7\nglite-lbjp-common-gsoap-plugin-devel-3.1.2-2.2.osg33.el7\nglite-lbjp-common-gss-3.1.3-2.2.osg33.el7\nglite-lbjp-common-gss-debuginfo-3.1.3-2.2.osg33.el7\nglite-lbjp-common-gss-devel-3.1.3-2.2.osg33.el7\nglobus-authz-3.10-1.osg33.el7\nglobus-authz-callout-error-3.5-1.osg33.el7\nglobus-authz-callout-error-debuginfo-3.5-1.osg33.el7\nglobus-authz-callout-error-devel-3.5-1.osg33.el7\nglobus-authz-callout-error-doc-3.5-1.osg33.el7\nglobus-authz-debuginfo-3.10-1.osg33.el7\nglobus-authz-devel-3.10-1.osg33.el7\nglobus-authz-doc-3.10-1.osg33.el7\nglobus-callout-3.13-1.osg33.el7\nglobus-callout-debuginfo-3.13-1.osg33.el7\nglobus-callout-devel-3.13-1.osg33.el7\nglobus-callout-doc-3.13-1.osg33.el7\nglobus-common-15.27-1.osg33.el7\nglobus-common-debuginfo-15.27-1.osg33.el7\nglobus-common-devel-15.27-1.osg33.el7\nglobus-common-doc-15.27-1.osg33.el7\nglobus-common-progs-15.27-1.osg33.el7\nglobus-ftp-client-8.19-1.2.osg33.el7\nglobus-ftp-client-debuginfo-8.19-1.2.osg33.el7\nglobus-ftp-client-devel-8.19-1.2.osg33.el7\nglobus-ftp-client-doc-8.19-1.2.osg33.el7\nglobus-ftp-control-6.6-1.1.osg33.el7\nglobus-ftp-control-debuginfo-6.6-1.1.osg33.el7\nglobus-ftp-control-devel-6.6-1.1.osg33.el7\nglobus-ftp-control-doc-6.6-1.1.osg33.el7\nglobus-gass-cache-9.5-1.osg33.el7\nglobus-gass-cache-debuginfo-9.5-1.osg33.el7\nglobus-gass-cache-devel-9.5-1.osg33.el7\nglobus-gass-cache-doc-9.5-1.osg33.el7\nglobus-gass-cache-program-6.5-1.osg33.el7\nglobus-gass-cache-program-debuginfo-6.5-1.osg33.el7\nglobus-gass-copy-9.13-1.osg33.el7\nglobus-gass-copy-debuginfo-9.13-1.osg33.el7\nglobus-gass-copy-devel-9.13-1.osg33.el7\nglobus-gass-copy-doc-9.13-1.osg33.el7\nglobus-gass-copy-progs-9.13-1.osg33.el7\nglobus-gass-server-ez-5.7-1.osg33.el7\nglobus-gass-server-ez-debuginfo-5.7-1.osg33.el7\nglobus-gass-server-ez-devel-5.7-1.osg33.el7\nglobus-gass-server-ez-progs-5.7-1.osg33.el7\nglobus-gass-transfer-8.8-1.osg33.el7\nglobus-gass-transfer-debuginfo-8.8-1.osg33.el7\nglobus-gass-transfer-devel-8.8-1.osg33.el7\nglobus-gass-transfer-doc-8.8-1.osg33.el7\nglobus-gatekeeper-10.9-1.2.osg33.el7\nglobus-gatekeeper-debuginfo-10.9-1.2.osg33.el7\nglobus-gfork-4.7-1.osg33.el7\nglobus-gfork-debuginfo-4.7-1.osg33.el7\nglobus-gfork-devel-4.7-1.osg33.el7\nglobus-gfork-progs-4.7-1.osg33.el7\nglobus-gram-audit-4.4-1.osg33.el7\nglobus-gram-client-13.12-1.osg33.el7\nglobus-gram-client-debuginfo-13.12-1.osg33.el7\nglobus-gram-client-devel-13.12-1.osg33.el7\nglobus-gram-client-doc-13.12-1.osg33.el7\nglobus-gram-client-tools-11.7-1.osg33.el7\nglobus-gram-client-tools-debuginfo-11.7-1.osg33.el7\nglobus-gram-job-manager-14.25-1.2.osg33.el7\nglobus-gram-job-manager-callout-error-3.5-1.osg33.el7\nglobus-gram-job-manager-callout-error-debuginfo-3.5-1.osg33.el7\nglobus-gram-job-manager-callout-error-devel-3.5-1.osg33.el7\nglobus-gram-job-manager-callout-error-doc-3.5-1.osg33.el7\nglobus-gram-job-manager-condor-2.5-1.1.osg33.el7\nglobus-gram-job-manager-debuginfo-14.25-1.2.osg33.el7\nglobus-gram-job-manager-fork-2.4-1.1.osg33.el7\nglobus-gram-job-manager-fork-debuginfo-2.4-1.1.osg33.el7\nglobus-gram-job-manager-fork-setup-poll-2.4-1.1.osg33.el7\nglobus-gram-job-manager-fork-setup-seg-2.4-1.1.osg33.el7\nglobus-gram-job-manager-lsf-2.6-1.2.osg33.el7\nglobus-gram-job-manager-lsf-debuginfo-2.6-1.2.osg33.el7\nglobus-gram-job-manager-lsf-setup-poll-2.6-1.2.osg33.el7\nglobus-gram-job-manager-lsf-setup-seg-2.6-1.2.osg33.el7\nglobus-gram-job-manager-managedfork-0.2-1.osg33.el7\nglobus-gram-job-manager-pbs-2.4-2.1.osg33.el7\nglobus-gram-job-manager-pbs-debuginfo-2.4-2.1.osg33.el7\nglobus-gram-job-manager-pbs-setup-poll-2.4-2.1.osg33.el7\nglobus-gram-job-manager-pbs-setup-seg-2.4-2.1.osg33.el7\nglobus-gram-job-manager-scripts-6.7-1.osg33.el7\nglobus-gram-job-manager-scripts-doc-6.7-1.osg33.el7\nglobus-gram-job-manager-sge-2.5-1.1.osg33.el7\nglobus-gram-job-manager-sge-debuginfo-2.5-1.1.osg33.el7\nglobus-gram-job-manager-sge-setup-poll-2.5-1.1.osg33.el7\nglobus-gram-job-manager-sge-setup-seg-2.5-1.1.osg33.el7\nglobus-gram-protocol-12.12-2.osg33.el7\nglobus-gram-protocol-debuginfo-12.12-2.osg33.el7\nglobus-gram-protocol-devel-12.12-2.osg33.el7\nglobus-gram-protocol-doc-12.12-2.osg33.el7\nglobus-gridftp-server-7.20-1.1.osg33.el7\nglobus-gridftp-server-control-3.6-1.osg33.el7\nglobus-gridftp-server-control-debuginfo-3.6-1.osg33.el7\nglobus-gridftp-server-control-devel-3.6-1.osg33.el7\nglobus-gridftp-server-debuginfo-7.20-1.1.osg33.el7\nglobus-gridftp-server-devel-7.20-1.1.osg33.el7\nglobus-gridftp-server-progs-7.20-1.1.osg33.el7\nglobus-gridmap-callout-error-2.4-1.osg33.el7\nglobus-gridmap-callout-error-debuginfo-2.4-1.osg33.el7\nglobus-gridmap-callout-error-devel-2.4-1.osg33.el7\nglobus-gridmap-callout-error-doc-2.4-1.osg33.el7\nglobus-gsi-callback-5.7-1.osg33.el7\nglobus-gsi-callback-debuginfo-5.7-1.osg33.el7\nglobus-gsi-callback-devel-5.7-1.osg33.el7\nglobus-gsi-callback-doc-5.7-1.osg33.el7\nglobus-gsi-cert-utils-9.10-1.osg33.el7\nglobus-gsi-cert-utils-debuginfo-9.10-1.osg33.el7\nglobus-gsi-cert-utils-devel-9.10-1.osg33.el7\nglobus-gsi-cert-utils-doc-9.10-1.osg33.el7\nglobus-gsi-cert-utils-progs-9.10-1.osg33.el7\nglobus-gsi-credential-7.7-1.osg33.el7\nglobus-gsi-credential-debuginfo-7.7-1.osg33.el7\nglobus-gsi-credential-devel-7.7-1.osg33.el7\nglobus-gsi-credential-doc-7.7-1.osg33.el7\nglobus-gsi-openssl-error-3.5-1.osg33.el7\nglobus-gsi-openssl-error-debuginfo-3.5-1.osg33.el7\nglobus-gsi-openssl-error-devel-3.5-1.osg33.el7\nglobus-gsi-openssl-error-doc-3.5-1.osg33.el7\nglobus-gsi-proxy-core-7.7-1.osg33.el7\nglobus-gsi-proxy-core-debuginfo-7.7-1.osg33.el7\nglobus-gsi-proxy-core-devel-7.7-1.osg33.el7\nglobus-gsi-proxy-core-doc-7.7-1.osg33.el7\nglobus-gsi-proxy-ssl-5.7-1.osg33.el7\nglobus-gsi-proxy-ssl-debuginfo-5.7-1.osg33.el7\nglobus-gsi-proxy-ssl-devel-5.7-1.osg33.el7\nglobus-gsi-proxy-ssl-doc-5.7-1.osg33.el7\nglobus-gsi-sysconfig-6.8-1.osg33.el7\nglobus-gsi-sysconfig-debuginfo-6.8-1.osg33.el7\nglobus-gsi-sysconfig-devel-6.8-1.osg33.el7\nglobus-gsi-sysconfig-doc-6.8-1.osg33.el7\nglobus-gssapi-error-5.4-1.osg33.el7\nglobus-gssapi-error-debuginfo-5.4-1.osg33.el7\nglobus-gssapi-error-devel-5.4-1.osg33.el7\nglobus-gssapi-error-doc-5.4-1.osg33.el7\nglobus-gssapi-gsi-11.18-1.osg33.el7\nglobus-gssapi-gsi-debuginfo-11.18-1.osg33.el7\nglobus-gssapi-gsi-devel-11.18-1.osg33.el7\nglobus-gssapi-gsi-doc-11.18-1.osg33.el7\nglobus-gss-assist-10.13-1.osg33.el7\nglobus-gss-assist-debuginfo-10.13-1.osg33.el7\nglobus-gss-assist-devel-10.13-1.osg33.el7\nglobus-gss-assist-doc-10.13-1.osg33.el7\nglobus-gss-assist-progs-10.13-1.osg33.el7\nglobus-io-11.4-1.osg33.el7\nglobus-io-debuginfo-11.4-1.osg33.el7\nglobus-io-devel-11.4-1.osg33.el7\nglobus-openssl-module-4.6-1.osg33.el7\nglobus-openssl-module-debuginfo-4.6-1.osg33.el7\nglobus-openssl-module-devel-4.6-1.osg33.el7\nglobus-openssl-module-doc-4.6-1.osg33.el7\nglobus-proxy-utils-6.9-1.osg33.el7\nglobus-proxy-utils-debuginfo-6.9-1.osg33.el7\nglobus-rsl-10.9-1.osg33.el7\nglobus-rsl-debuginfo-10.9-1.osg33.el7\nglobus-rsl-devel-10.9-1.osg33.el7\nglobus-rsl-doc-10.9-1.osg33.el7\nglobus-scheduler-event-generator-5.10-2.1.osg33.el7\nglobus-scheduler-event-generator-debuginfo-5.10-2.1.osg33.el7\nglobus-scheduler-event-generator-devel-5.10-2.1.osg33.el7\nglobus-scheduler-event-generator-doc-5.10-2.1.osg33.el7\nglobus-scheduler-event-generator-progs-5.10-2.1.osg33.el7\nglobus-simple-ca-4.18-1.osg33.el7\nglobus-usage-4.4-1.osg33.el7\nglobus-usage-debuginfo-4.4-1.osg33.el7\nglobus-usage-devel-4.4-1.osg33.el7\nglobus-xio-5.7-1.1.osg33.el7\nglobus-xio-debuginfo-5.7-1.1.osg33.el7\nglobus-xio-devel-5.7-1.1.osg33.el7\nglobus-xio-doc-5.7-1.1.osg33.el7\nglobus-xio-gsi-driver-3.7-1.osg33.el7\nglobus-xio-gsi-driver-debuginfo-3.7-1.osg33.el7\nglobus-xio-gsi-driver-devel-3.7-1.osg33.el7\nglobus-xio-gsi-driver-doc-3.7-1.osg33.el7\nglobus-xioperf-4.4-1.osg33.el7\nglobus-xioperf-debuginfo-4.4-1.osg33.el7\nglobus-xio-pipe-driver-3.7-1.osg33.el7\nglobus-xio-pipe-driver-debuginfo-3.7-1.osg33.el7\nglobus-xio-pipe-driver-devel-3.7-1.osg33.el7\nglobus-xio-popen-driver-3.5-1.osg33.el7\nglobus-xio-popen-driver-debuginfo-3.5-1.osg33.el7\nglobus-xio-popen-driver-devel-3.5-1.osg33.el7\nglobus-xio-udt-driver-1.16-1.osg33.el7\nglobus-xio-udt-driver-debuginfo-1.16-1.osg33.el7\nglobus-xio-udt-driver-devel-1.16-1.osg33.el7\ngratia-1.16.2-1.osg33.el7\ngratia-debuginfo-1.16.2-1.osg33.el7\ngratia-probe-1.14.2-6.osg33.el7\ngratia-probe-bdii-status-1.14.2-6.osg33.el7\ngratia-probe-common-1.14.2-6.osg33.el7\ngratia-probe-condor-1.14.2-6.osg33.el7\ngratia-probe-condor-events-1.14.2-6.osg33.el7\ngratia-probe-dcache-storage-1.14.2-6.osg33.el7\ngratia-probe-dcache-storagegroup-1.14.2-6.osg33.el7\ngratia-probe-dcache-transfer-1.14.2-6.osg33.el7\ngratia-probe-debuginfo-1.14.2-6.osg33.el7\ngratia-probe-enstore-storage-1.14.2-6.osg33.el7\ngratia-probe-enstore-tapedrive-1.14.2-6.osg33.el7\ngratia-probe-enstore-transfer-1.14.2-6.osg33.el7\ngratia-probe-glexec-1.14.2-6.osg33.el7\ngratia-probe-glideinwms-1.14.2-6.osg33.el7\ngratia-probe-gram-1.14.2-6.osg33.el7\ngratia-probe-gridftp-transfer-1.14.2-6.osg33.el7\ngratia-probe-hadoop-storage-1.14.2-6.osg33.el7\ngratia-probe-lsf-1.14.2-6.osg33.el7\ngratia-probe-metric-1.14.2-6.osg33.el7\ngratia-probe-onevm-1.14.2-6.osg33.el7\ngratia-probe-pbs-lsf-1.14.2-6.osg33.el7\ngratia-probe-psacct-1.14.2-6.osg33.el7\ngratia-probe-services-1.14.2-6.osg33.el7\ngratia-probe-sge-1.14.2-6.osg33.el7\ngratia-probe-slurm-1.14.2-6.osg33.el7\ngratia-probe-xrootd-storage-1.14.2-6.osg33.el7\ngratia-probe-xrootd-transfer-1.14.2-6.osg33.el7\ngratia-reporting-email-1.15.1-1.osg33.el7\ngratia-service-1.16.2-1.osg33.el7\ngsi-openssh-5.7-1.1.osg33.el7\ngsi-openssh-clients-5.7-1.1.osg33.el7\ngsi-openssh-debuginfo-5.7-1.1.osg33.el7\ngsi-openssh-server-5.7-1.1.osg33.el7\nhtcondor-ce-1.14-4.osg33.el7\nhtcondor-ce-client-1.14-4.osg33.el7\nhtcondor-ce-collector-1.14-4.osg33.el7\nhtcondor-ce-condor-1.14-4.osg33.el7\nhtcondor-ce-debuginfo-1.14-4.osg33.el7\nhtcondor-ce-lsf-1.14-4.osg33.el7\nhtcondor-ce-pbs-1.14-4.osg33.el7\nhtcondor-ce-sge-1.14-4.osg33.el7\nI2util-1.1-2.osg33.el7\nI2util-debuginfo-1.1-2.osg33.el7\nigtf-ca-certs-1.65-1.osg33.el7\njavascriptrrd-1.1.1-1.osg33.el7\njetty-8.1.4.v20120524-2.osg33.el7\njetty-ajp-8.1.4.v20120524-2.osg33.el7\njetty-annotations-8.1.4.v20120524-2.osg33.el7\njetty-client-8.1.4.v20120524-2.osg33.el7\njetty-continuation-8.1.4.v20120524-2.osg33.el7\njetty-deploy-8.1.4.v20120524-2.osg33.el7\njetty-http-8.1.4.v20120524-2.osg33.el7\njetty-io-8.1.4.v20120524-2.osg33.el7\njetty-jmx-8.1.4.v20120524-2.osg33.el7\njetty-jndi-8.1.4.v20120524-2.osg33.el7\njetty-overlay-deployer-8.1.4.v20120524-2.osg33.el7\njetty-plus-8.1.4.v20120524-2.osg33.el7\njetty-policy-8.1.4.v20120524-2.osg33.el7\njetty-rewrite-8.1.4.v20120524-2.osg33.el7\njetty-security-8.1.4.v20120524-2.osg33.el7\njetty-server-8.1.4.v20120524-2.osg33.el7\njetty-servlet-8.1.4.v20120524-2.osg33.el7\njetty-servlets-8.1.4.v20120524-2.osg33.el7\njetty-util-8.1.4.v20120524-2.osg33.el7\njetty-webapp-8.1.4.v20120524-2.osg33.el7\njetty-websocket-8.1.4.v20120524-2.osg33.el7\njetty-xml-8.1.4.v20120524-2.osg33.el7\njoda-time-1.5.2-7.2.tzdata2008d.osg33.el7\njoda-time-javadoc-1.5.2-7.2.tzdata2008d.osg33.el7\nkoji-1.6.0-10.osg33.el7\nkoji-builder-1.6.0-10.osg33.el7\nkoji-hub-1.6.0-10.osg33.el7\nkoji-hub-plugins-1.6.0-10.osg33.el7\nkoji-utils-1.6.0-10.osg33.el7\nkoji-vm-1.6.0-10.osg33.el7\nkoji-web-1.6.0-10.osg33.el7\nlcas-lcmaps-gt4-interface-0.2.6-1.1.osg33.el7\nlcas-lcmaps-gt4-interface-debuginfo-0.2.6-1.1.osg33.el7\nlcmaps-1.6.6-1.1.osg33.el7\nlcmaps-common-devel-1.6.6-1.1.osg33.el7\nlcmaps-debuginfo-1.6.6-1.1.osg33.el7\nlcmaps-devel-1.6.6-1.1.osg33.el7\nlcmaps-plugins-basic-1.7.0-2.osg33.el7\nlcmaps-plugins-basic-debuginfo-1.7.0-2.osg33.el7\nlcmaps-plugins-basic-ldap-1.7.0-2.osg33.el7\nlcmaps-plugins-glexec-tracking-0.1.6-1.osg33.el7\nlcmaps-plugins-glexec-tracking-debuginfo-0.1.6-1.osg33.el7\nlcmaps-plugins-gums-client-0.0.2-4.osg33.el7\nlcmaps-plugins-scas-client-0.5.5-1.osg33.el7\nlcmaps-plugins-scas-client-debuginfo-0.5.5-1.osg33.el7\nlcmaps-plugins-verify-proxy-1.5.7-1.osg33.el7\nlcmaps-plugins-verify-proxy-debuginfo-1.5.7-1.osg33.el7\nlcmaps-without-gsi-1.6.6-1.1.osg33.el7\nlcmaps-without-gsi-devel-1.6.6-1.1.osg33.el7\nllrun-0.1.3-1.3.osg33.el7\nllrun-debuginfo-0.1.3-1.3.osg33.el7\nmash-0.5.22-3.osg33.el7\nmkgltempdir-0.0.5-1.1.osg33.el7\nmyproxy-6.1.12-1.osg33.el7\nmyproxy-admin-6.1.12-1.osg33.el7\nmyproxy-debuginfo-6.1.12-1.osg33.el7\nmyproxy-devel-6.1.12-1.osg33.el7\nmyproxy-doc-6.1.12-1.osg33.el7\nmyproxy-libs-6.1.12-1.osg33.el7\nmyproxy-server-6.1.12-1.osg33.el7\nmyproxy-voms-6.1.12-1.osg33.el7\nndt-3.6.5-3.osg33.el7\nndt-client-3.6.5-3.osg33.el7\nndt-debuginfo-3.6.5-3.osg33.el7\nndt-server-3.6.5-3.osg33.el7\nnetlogger-4.2.0-9.osg33.el7\nnuttcp-6.1.2-1.osg33.el7\nnuttcp-debuginfo-6.1.2-1.osg33.el7\nosg-base-ce-3.3-3_clipped.osg33.el7\nosg-base-ce-condor-3.3-3_clipped.osg33.el7\nosg-base-ce-lsf-3.3-3_clipped.osg33.el7\nosg-base-ce-pbs-3.3-3_clipped.osg33.el7\nosg-base-ce-sge-3.3-3_clipped.osg33.el7\nosg-base-ce-slurm-3.3-3_clipped.osg33.el7\nosg-build-1.6.0-2.osg33.el7\nosg-ca-certs-1.47-1.osg33.el7\nosg-ca-certs-updater-1.0-1.osg33.el7\nosg-ca-scripts-1.1.5-2.osg33.el7\nosg-ce-3.3-3_clipped.osg33.el7\nosg-ce-condor-3.3-3_clipped.osg33.el7\nosg-ce-lsf-3.3-3_clipped.osg33.el7\nosg-ce-pbs-3.3-3_clipped.osg33.el7\nosg-cert-scripts-2.7.2-2.osg33.el7\nosg-ce-sge-3.3-3_clipped.osg33.el7\nosg-ce-slurm-3.3-3_clipped.osg33.el7\nosg-cleanup-1.7.2-1.osg33.el7\nosg-configure-1.1.1-1.osg33.el7\nosg-configure-ce-1.1.1-1.osg33.el7\nosg-configure-cemon-1.1.1-1.osg33.el7\nosg-configure-condor-1.1.1-1.osg33.el7\nosg-configure-gateway-1.1.1-1.osg33.el7\nosg-configure-gip-1.1.1-1.osg33.el7\nosg-configure-gratia-1.1.1-1.osg33.el7\nosg-configure-infoservices-1.1.1-1.osg33.el7\nosg-configure-lsf-1.1.1-1.osg33.el7\nosg-configure-managedfork-1.1.1-1.osg33.el7\nosg-configure-misc-1.1.1-1.osg33.el7\nosg-configure-monalisa-1.1.1-1.osg33.el7\nosg-configure-network-1.1.1-1.osg33.el7\nosg-configure-pbs-1.1.1-1.osg33.el7\nosg-configure-rsv-1.1.1-1.osg33.el7\nosg-configure-sge-1.1.1-1.osg33.el7\nosg-configure-slurm-1.1.1-1.osg33.el7\nosg-configure-squid-1.1.1-1.osg33.el7\nosg-configure-tests-1.1.1-1.osg33.el7\nosg-control-1.0.1-1.osg33.el7\nosg-gridftp-3.3-2_clipped.osg33.el7\nosg-gridftp-hdfs-3.3-2.osg33.el7\nosg-gridftp-xrootd-3.3-2.osg33.el7\nosg-gums-3.3-2.osg33.el7\nosg-gums-config-61-1.osg33.el7\nosg-htcondor-ce-3.3-3_clipped.osg33.el7\nosg-htcondor-ce-condor-3.3-3_clipped.osg33.el7\nosg-htcondor-ce-lsf-3.3-3_clipped.osg33.el7\nosg-htcondor-ce-pbs-3.3-3_clipped.osg33.el7\nosg-htcondor-ce-sge-3.3-3_clipped.osg33.el7\nosg-htcondor-ce-slurm-3.3-3_clipped.osg33.el7\nosg-info-services-1.0.2-1.osg33.el7\nosg-java7-compat-1.0-1.osg33.el7\nosg-java7-compat-openjdk-1.0-1.osg33.el7\nosg-java7-devel-compat-1.0-1.osg33.el7\nosg-java7-devel-compat-openjdk-1.0-1.osg33.el7\nosg-oasis-5-2.osg33.el7\nosg-pki-tools-1.2.12-1.osg33.el7\nosg-pki-tools-tests-1.2.12-1.osg33.el7\nosg-release-3.3-2.osg33.el7\nosg-release-itb-3.3-2.osg33.el7\nosg-se-bestman-3.3-2.osg33.el7\nosg-se-bestman-xrootd-3.3-2.osg33.el7\nosg-se-hadoop-3.3-2.osg33.el7\nosg-se-hadoop-client-3.3-2.osg33.el7\nosg-se-hadoop-datanode-3.3-2.osg33.el7\nosg-se-hadoop-gridftp-3.3-2.osg33.el7\nosg-se-hadoop-namenode-3.3-2.osg33.el7\nosg-se-hadoop-secondarynamenode-3.3-2.osg33.el7\nosg-se-hadoop-srm-3.3-2.osg33.el7\nosg-system-profiler-1.2.0-1.osg33.el7\nosg-system-profiler-viewer-1.2.0-1.osg33.el7\nosg-test-1.4.25-1.osg33.el7\nosg-tested-internal-3.3-1.osg33.el7\nosg-version-3.3.0-1.osg33.el7\nosg-vo-map-0.0.1-1.osg33.el7\nosg-voms-3.3-1.osg33.el7\nosg-webapp-common-1-2.osg33.el7\nosg-wn-client-3.3-5.osg33.el7\nosg-wn-client-glexec-3.3-5.osg33.el7\nowamp-3.2rc4-2.osg33.el7\nowamp-client-3.2rc4-2.osg33.el7\nowamp-debuginfo-3.2rc4-2.osg33.el7\nowamp-server-3.2rc4-2.osg33.el7\npegasus-4.3.1-1.3.osg33.el7\npegasus-debuginfo-4.3.1-1.3.osg33.el7\nrsv-3.10.2-1_clipped.osg33.el7\nrsv-consumers-3.10.2-1_clipped.osg33.el7\nrsv-core-3.10.2-1_clipped.osg33.el7\nrsv-metrics-3.10.2-1_clipped.osg33.el7\nrsv-perfsonar-1.0.19-1.osg33.el7\nrsv-vo-gwms-1.0.1-1.osg33.el7\nstashcache-0.3-4.osg33.el7\nstashcache-cache-server-0.3-4.osg33.el7\nstashcache-daemon-0.2-1.osg33.el7\nstashcache-daemon-0.3-4.osg33.el7\nstashcache-origin-server-0.3-4.osg33.el7\nuberftp-2.8-2.1.osg33.el7\nuberftp-debuginfo-2.8-2.1.osg33.el7\nvo-client-61-1.osg33.el7\nvo-client-edgmkgridmap-61-1.osg33.el7\nvoms-2.0.12-3.osg33.el7\nvoms-admin-client-2.0.17-1.1.osg33.el7\nvoms-api-java-2.0.8-1.6.osg33.el7\nvoms-api-java-javadoc-2.0.8-1.6.osg33.el7\nvoms-clients-cpp-2.0.12-3.osg33.el7\nvoms-debuginfo-2.0.12-3.osg33.el7\nvoms-devel-2.0.12-3.osg33.el7\nvoms-doc-2.0.12-3.osg33.el7\nvoms-mysql-plugin-3.1.6-1.1.osg33.el7\nvoms-mysql-plugin-debuginfo-3.1.6-1.1.osg33.el7\nvoms-server-2.0.12-3.osg33.el7\nweb100_userland-1.7-6.osg33.el7\nweb100_userland-debuginfo-1.7-6.osg33.el7\nxacml-1.5.0-1.osg33.el7\nxacml-debuginfo-1.5.0-1.osg33.el7\nxacml-devel-1.5.0-1.osg33.el7\nxrootd-4.2.2-1.osg33.el7\nxrootd-client-4.2.2-1.osg33.el7\nxrootd-client-devel-4.2.2-1.osg33.el7\nxrootd-client-libs-4.2.2-1.osg33.el7\nxrootd-debuginfo-4.2.2-1.osg33.el7\nxrootd-devel-4.2.2-1.osg33.el7\nxrootd-doc-4.2.2-1.osg33.el7\nxrootd-dsi-3.0.4-16.osg33.el7\nxrootd-dsi-debuginfo-3.0.4-16.osg33.el7\nxrootd-fuse-4.2.2-1.osg33.el7\nxrootd-lcmaps-0.0.7-11.osg33.el7\nxrootd-lcmaps-debuginfo-0.0.7-11.osg33.el7\nxrootd-libs-4.2.2-1.osg33.el7\nxrootd-private-devel-4.2.2-1.osg33.el7\nxrootd-python-4.2.2-1.osg33.el7\nxrootd-selinux-4.2.2-1.osg33.el7\nxrootd-server-4.2.2-1.osg33.el7\nxrootd-server-devel-4.2.2-1.osg33.el7\nxrootd-server-libs-4.2.2-1.osg33.el7\nxrootd-status-probe-0.0.3-11.osg33.el7\nxrootd-status-probe-debuginfo-0.0.3-11.osg33.el7\nxrootd-voms-plugin-0.2.0-1.6.osg33.el7\nxrootd-voms-plugin-debuginfo-0.2.0-1.6.osg33.el7\nxrootd-voms-plugin-devel-0.2.0-1.6.osg33.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.3/release-3-3-0/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.3/release-3-3-0/#enterprise-linux-5", 
            "text": "", 
            "title": "Enterprise Linux 5"
        }, 
        {
            "location": "/release/3.3/release-3-3-0/#enterprise-linux-6_2", 
            "text": "", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.3/release-3-3-0/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.3/release-3-3-0/#enterprise-linux-5_1", 
            "text": "", 
            "title": "Enterprise Linux 5"
        }, 
        {
            "location": "/release/3.3/release-3-3-0/#enterprise-linux-6_3", 
            "text": "", 
            "title": "Enterprise Linux 6"
        }
    ]
}